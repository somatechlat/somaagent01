"""Process message use case - orchestrates message processing.

This use case coordinates:
- Session retrieval via SessionRepositoryPort
- Context building via MemoryAdapterPort
- Event persistence via SessionRepositoryPort

It contains NO infrastructure code - only business logic coordination.
"""

from typing import Any, Dict, List

from src.core.application.dto import ProcessMessageInput, ProcessMessageOutput
from src.core.domain.ports import MemoryAdapterPort, SessionRepositoryPort


class ProcessMessageUseCase:
    """Orchestrates message processing through domain ports.
    
    This use case:
    1. Retrieves session envelope
    2. Builds context using memory adapter
    3. Persists the user message event
    4. Returns processing metadata
    
    The actual LLM response generation happens in the conversation worker.
    """
    
    def __init__(
        self,
        session_repo: SessionRepositoryPort,
        memory_adapter: MemoryAdapterPort,
    ):
        self._session_repo = session_repo
        self._memory_adapter = memory_adapter
    
    async def execute(self, input_data: ProcessMessageInput) -> ProcessMessageOutput:
        """Process an incoming message.
        
        Args:
            input_data: Message processing input
            
        Returns:
            Processing output with metadata
        """
        # 1. Get session envelope
        envelope = await self._session_repo.get_envelope(input_data.session_id)
        
        # 2. Build context using memory adapter
        context_payload = {
            "session_id": input_data.session_id,
            "message": input_data.message,
            "persona_id": input_data.persona_id,
            "tenant": input_data.tenant,
        }
        context: List[Dict[str, Any]] = await self._memory_adapter.build_context(context_payload)
        
        # 3. Persist user message event
        event = {
            "role": "user",
            "content": input_data.message,
            "session_id": input_data.session_id,
            "persona_id": input_data.persona_id,
            "metadata": input_data.metadata,
        }
        await self._session_repo.append_event(input_data.session_id, event)
        
        # 4. Return processing metadata
        return ProcessMessageOutput(
            response="",  # Response generated by LLM in conversation worker
            session_id=input_data.session_id,
            tool_calls=[],
            metadata={
                "context_size": len(context),
                "has_envelope": envelope is not None,
            },
        )
