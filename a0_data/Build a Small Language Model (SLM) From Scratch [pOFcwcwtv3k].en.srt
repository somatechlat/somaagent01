1
00:00:00,160 --> 00:00:02,389

Hello everyone. My name is Dr. Raj

2
00:00:02,389 --> 00:00:02,399
Hello everyone. My name is Dr. Raj
 

3
00:00:02,399 --> 00:00:05,030
Hello everyone. My name is Dr. Raj
Gandekar. I graduated with a PhD in

4
00:00:05,030 --> 00:00:05,040
Gandekar. I graduated with a PhD in
 

5
00:00:05,040 --> 00:00:07,110
Gandekar. I graduated with a PhD in
machine learning from the Massachusetts

6
00:00:07,110 --> 00:00:07,120
machine learning from the Massachusetts
 

7
00:00:07,120 --> 00:00:09,509
machine learning from the Massachusetts
Institute of Technology and I'm one of

8
00:00:09,509 --> 00:00:09,519
Institute of Technology and I'm one of
 

9
00:00:09,519 --> 00:00:12,350
Institute of Technology and I'm one of
the three co-founders of Vijuara AI

10
00:00:12,350 --> 00:00:12,360
the three co-founders of Vijuara AI
 

11
00:00:12,360 --> 00:00:15,430
the three co-founders of Vijuara AI
Labs. Today I'm incredibly excited to

12
00:00:15,430 --> 00:00:15,440
Labs. Today I'm incredibly excited to
 

13
00:00:15,440 --> 00:00:17,830
Labs. Today I'm incredibly excited to
present this video to you in which we

14
00:00:17,830 --> 00:00:17,840
present this video to you in which we
 

15
00:00:17,840 --> 00:00:19,510
present this video to you in which we
are going to build a small language

16
00:00:19,510 --> 00:00:19,520
are going to build a small language
 

17
00:00:19,520 --> 00:00:21,990
are going to build a small language
model fully from scratch. I've been

18
00:00:21,990 --> 00:00:22,000
model fully from scratch. I've been
 

19
00:00:22,000 --> 00:00:24,070
model fully from scratch. I've been
working on this video for the past 5 to

20
00:00:24,070 --> 00:00:24,080
working on this video for the past 5 to
 

21
00:00:24,080 --> 00:00:26,790
working on this video for the past 5 to
6 months and I'm truly excited right now

22
00:00:26,790 --> 00:00:26,800
6 months and I'm truly excited right now
 

23
00:00:26,800 --> 00:00:28,950
6 months and I'm truly excited right now
to assemble all of it together and

24
00:00:28,950 --> 00:00:28,960
to assemble all of it together and
 

25
00:00:28,960 --> 00:00:31,750
to assemble all of it together and
present it to all of you. Like we do in

26
00:00:31,750 --> 00:00:31,760
present it to all of you. Like we do in
 

27
00:00:31,760 --> 00:00:33,830
present it to all of you. Like we do in
all of our videos, we'll be showing you

28
00:00:33,830 --> 00:00:33,840
all of our videos, we'll be showing you
 

29
00:00:33,840 --> 00:00:36,150
all of our videos, we'll be showing you
everything from the nuts and bolts. I'll

30
00:00:36,150 --> 00:00:36,160
everything from the nuts and bolts. I'll
 

31
00:00:36,160 --> 00:00:38,470
everything from the nuts and bolts. I'll
not assume anything. I'll show you each

32
00:00:38,470 --> 00:00:38,480
not assume anything. I'll show you each
 

33
00:00:38,480 --> 00:00:41,270
not assume anything. I'll show you each
and every calculation, each and every

34
00:00:41,270 --> 00:00:41,280
and every calculation, each and every
 

35
00:00:41,280 --> 00:00:43,430
and every calculation, each and every
mathematical detail on the whiteboard

36
00:00:43,430 --> 00:00:43,440
mathematical detail on the whiteboard
 

37
00:00:43,440 --> 00:00:46,069
mathematical detail on the whiteboard
and then we'll also do coding. Towards

38
00:00:46,069 --> 00:00:46,079
and then we'll also do coding. Towards
 

39
00:00:46,079 --> 00:00:47,910
and then we'll also do coding. Towards
the end, we'll build a fully functional

40
00:00:47,910 --> 00:00:47,920
the end, we'll build a fully functional
 

41
00:00:47,920 --> 00:00:49,750
the end, we'll build a fully functional
small language model which gives good

42
00:00:49,750 --> 00:00:49,760
small language model which gives good
 

43
00:00:49,760 --> 00:00:53,490
small language model which gives good
quality responses. So let's get

44
00:00:53,490 --> 00:00:53,500
quality responses. So let's get
 

45
00:00:53,500 --> 00:00:57,950
quality responses. So let's get
[Music]

46
00:00:57,950 --> 00:00:57,960

 

47
00:00:57,960 --> 00:01:01,349

started. All right. So let's get started

48
00:01:01,349 --> 00:01:01,359
started. All right. So let's get started
 

49
00:01:01,359 --> 00:01:05,030
started. All right. So let's get started
on this journey. Today our main goal is

50
00:01:05,030 --> 00:01:05,040
on this journey. Today our main goal is
 

51
00:01:05,040 --> 00:01:07,270
on this journey. Today our main goal is
that we are going to build a small

52
00:01:07,270 --> 00:01:07,280
that we are going to build a small
 

53
00:01:07,280 --> 00:01:10,030
that we are going to build a small
language model fully from

54
00:01:10,030 --> 00:01:10,040
language model fully from
 

55
00:01:10,040 --> 00:01:12,390
language model fully from
scratch. Let me first show you what we

56
00:01:12,390 --> 00:01:12,400
scratch. Let me first show you what we
 

57
00:01:12,400 --> 00:01:13,990
scratch. Let me first show you what we
are going to build today. Here is the

58
00:01:13,990 --> 00:01:14,000
are going to build today. Here is the
 

59
00:01:14,000 --> 00:01:15,830
are going to build today. Here is the
Google collab code file which we are

60
00:01:15,830 --> 00:01:15,840
Google collab code file which we are
 

61
00:01:15,840 --> 00:01:17,510
Google collab code file which we are
going to run fully. I'll walk you

62
00:01:17,510 --> 00:01:17,520
going to run fully. I'll walk you
 

63
00:01:17,520 --> 00:01:19,510
going to run fully. I'll walk you
through each and every line of code over

64
00:01:19,510 --> 00:01:19,520
through each and every line of code over
 

65
00:01:19,520 --> 00:01:21,910
through each and every line of code over
here and towards the end as you can see

66
00:01:21,910 --> 00:01:21,920
here and towards the end as you can see
 

67
00:01:21,920 --> 00:01:24,789
here and towards the end as you can see
over here we are going to give uh a

68
00:01:24,789 --> 00:01:24,799
over here we are going to give uh a
 

69
00:01:24,799 --> 00:01:27,030
over here we are going to give uh a
sample sentence and we are going to

70
00:01:27,030 --> 00:01:27,040
sample sentence and we are going to
 

71
00:01:27,040 --> 00:01:29,710
sample sentence and we are going to
generate certain stories out of

72
00:01:29,710 --> 00:01:29,720
generate certain stories out of
 

73
00:01:29,720 --> 00:01:32,469
generate certain stories out of
it and I'm I'll make sure that the

74
00:01:32,469 --> 00:01:32,479
it and I'm I'll make sure that the
 

75
00:01:32,479 --> 00:01:34,990
it and I'm I'll make sure that the
response of the

76
00:01:34,990 --> 00:01:35,000
response of the
 

77
00:01:35,000 --> 00:01:37,749
response of the
um LLM or the response of the small

78
00:01:37,749 --> 00:01:37,759
um LLM or the response of the small
 

79
00:01:37,759 --> 00:01:41,190
um LLM or the response of the small
language model which we build is good

80
00:01:41,190 --> 00:01:41,200
language model which we build is good
 

81
00:01:41,200 --> 00:01:43,990
language model which we build is good
quality responses. So you can think of

82
00:01:43,990 --> 00:01:44,000
quality responses. So you can think of
 

83
00:01:44,000 --> 00:01:46,469
quality responses. So you can think of
this tutorial as a bit of a production

84
00:01:46,469 --> 00:01:46,479
this tutorial as a bit of a production
 

85
00:01:46,479 --> 00:01:48,870
this tutorial as a bit of a production
level tutorial because I'll also teach

86
00:01:48,870 --> 00:01:48,880
level tutorial because I'll also teach
 

87
00:01:48,880 --> 00:01:51,190
level tutorial because I'll also teach
you some things which we usually need to

88
00:01:51,190 --> 00:01:51,200
you some things which we usually need to
 

89
00:01:51,200 --> 00:01:53,830
you some things which we usually need to
do to accelerate the training process.

90
00:01:53,830 --> 00:01:53,840
do to accelerate the training process.
 

91
00:01:53,840 --> 00:01:55,510
do to accelerate the training process.
Some things which we usually need to do

92
00:01:55,510 --> 00:01:55,520
Some things which we usually need to do
 

93
00:01:55,520 --> 00:01:59,190
Some things which we usually need to do
so that computations become memory uh

94
00:01:59,190 --> 00:01:59,200
so that computations become memory uh
 

95
00:01:59,200 --> 00:02:01,149
so that computations become memory uh
memory efficient

96
00:02:01,149 --> 00:02:01,159
memory efficient
 

97
00:02:01,159 --> 00:02:03,910
memory efficient
etc. So this is not just a toy project

98
00:02:03,910 --> 00:02:03,920
etc. So this is not just a toy project
 

99
00:02:03,920 --> 00:02:05,429
etc. So this is not just a toy project
which we'll be assembling and the

100
00:02:05,429 --> 00:02:05,439
which we'll be assembling and the
 

101
00:02:05,439 --> 00:02:06,870
which we'll be assembling and the
results which you get towards the end

102
00:02:06,870 --> 00:02:06,880
results which you get towards the end
 

103
00:02:06,880 --> 00:02:09,190
results which you get towards the end
are do not make sense. But as you will

104
00:02:09,190 --> 00:02:09,200
are do not make sense. But as you will
 

105
00:02:09,200 --> 00:02:11,270
are do not make sense. But as you will
see the sentences which our small

106
00:02:11,270 --> 00:02:11,280
see the sentences which our small
 

107
00:02:11,280 --> 00:02:13,190
see the sentences which our small
language model can generate actually

108
00:02:13,190 --> 00:02:13,200
language model can generate actually
 

109
00:02:13,200 --> 00:02:14,990
language model can generate actually
start to make a lot of

110
00:02:14,990 --> 00:02:15,000
start to make a lot of
 

111
00:02:15,000 --> 00:02:18,150
start to make a lot of
sense. Okay. So this is what our goal is

112
00:02:18,150 --> 00:02:18,160
sense. Okay. So this is what our goal is
 

113
00:02:18,160 --> 00:02:23,110
sense. Okay. So this is what our goal is
from this tutorial video. First let me

114
00:02:23,110 --> 00:02:23,120
from this tutorial video. First let me
 

115
00:02:23,120 --> 00:02:25,510
from this tutorial video. First let me
clarify to all of you what does a small

116
00:02:25,510 --> 00:02:25,520
clarify to all of you what does a small
 

117
00:02:25,520 --> 00:02:27,910
clarify to all of you what does a small
language model actually mean. So if you

118
00:02:27,910 --> 00:02:27,920
language model actually mean. So if you
 

119
00:02:27,920 --> 00:02:30,550
language model actually mean. So if you
look at language modeling itself, models

120
00:02:30,550 --> 00:02:30,560
look at language modeling itself, models
 

121
00:02:30,560 --> 00:02:32,470
look at language modeling itself, models
just keep on getting larger and larger

122
00:02:32,470 --> 00:02:32,480
just keep on getting larger and larger
 

123
00:02:32,480 --> 00:02:35,350
just keep on getting larger and larger
and larger. Right? GPT4 if you see over

124
00:02:35,350 --> 00:02:35,360
and larger. Right? GPT4 if you see over
 

125
00:02:35,360 --> 00:02:36,509
and larger. Right? GPT4 if you see over
here was

126
00:02:36,509 --> 00:02:36,519
here was
 

127
00:02:36,519 --> 00:02:39,190
here was
about 1 trillion parameters although

128
00:02:39,190 --> 00:02:39,200
about 1 trillion parameters although
 

129
00:02:39,200 --> 00:02:42,070
about 1 trillion parameters although
it's not public information yet. GPT3

130
00:02:42,070 --> 00:02:42,080
it's not public information yet. GPT3
 

131
00:02:42,080 --> 00:02:45,470
it's not public information yet. GPT3
the largest model was around 175 billion

132
00:02:45,470 --> 00:02:45,480
the largest model was around 175 billion
 

133
00:02:45,480 --> 00:02:47,830
the largest model was around 175 billion
parameters. And there's the scaling law

134
00:02:47,830 --> 00:02:47,840
parameters. And there's the scaling law
 

135
00:02:47,840 --> 00:02:49,270
parameters. And there's the scaling law
which says that as the number of

136
00:02:49,270 --> 00:02:49,280
which says that as the number of
 

137
00:02:49,280 --> 00:02:51,750
which says that as the number of
parameters increase the performance of

138
00:02:51,750 --> 00:02:51,760
parameters increase the performance of
 

139
00:02:51,760 --> 00:02:54,110
parameters increase the performance of
the language model increases

140
00:02:54,110 --> 00:02:54,120
the language model increases
 

141
00:02:54,120 --> 00:02:56,949
the language model increases
proportionately. So researchers went on

142
00:02:56,949 --> 00:02:56,959
proportionately. So researchers went on
 

143
00:02:56,959 --> 00:02:58,790
proportionately. So researchers went on
increasing the size of language models

144
00:02:58,790 --> 00:02:58,800
increasing the size of language models
 

145
00:02:58,800 --> 00:03:00,949
increasing the size of language models
to get better and better performance out

146
00:03:00,949 --> 00:03:00,959
to get better and better performance out
 

147
00:03:00,959 --> 00:03:04,070
to get better and better performance out
of them. However, people soon realize

148
00:03:04,070 --> 00:03:04,080
of them. However, people soon realize
 

149
00:03:04,080 --> 00:03:07,750
of them. However, people soon realize
that what if let's say if there's a huge

150
00:03:07,750 --> 00:03:07,760
that what if let's say if there's a huge
 

151
00:03:07,760 --> 00:03:09,910
that what if let's say if there's a huge
architecture for the language model

152
00:03:09,910 --> 00:03:09,920
architecture for the language model
 

153
00:03:09,920 --> 00:03:13,229
architecture for the language model
which let me show here by this circle

154
00:03:13,229 --> 00:03:13,239
which let me show here by this circle
 

155
00:03:13,239 --> 00:03:16,149
which let me show here by this circle
right and this huge architecture gets

156
00:03:16,149 --> 00:03:16,159
right and this huge architecture gets
 

157
00:03:16,159 --> 00:03:18,430
right and this huge architecture gets
the job done it produces coherent

158
00:03:18,430 --> 00:03:18,440
the job done it produces coherent
 

159
00:03:18,440 --> 00:03:20,790
the job done it produces coherent
responses what if I can get similar

160
00:03:20,790 --> 00:03:20,800
responses what if I can get similar
 

161
00:03:20,800 --> 00:03:22,869
responses what if I can get similar
responses but with a smaller size

162
00:03:22,869 --> 00:03:22,879
responses but with a smaller size
 

163
00:03:22,879 --> 00:03:25,270
responses but with a smaller size
language model what if I reduce the

164
00:03:25,270 --> 00:03:25,280
language model what if I reduce the
 

165
00:03:25,280 --> 00:03:27,990
language model what if I reduce the
number of parameters to not as large as

166
00:03:27,990 --> 00:03:28,000
number of parameters to not as large as
 

167
00:03:28,000 --> 00:03:31,190
number of parameters to not as large as
1 trillion or 175 billion but what if I

168
00:03:31,190 --> 00:03:31,200
1 trillion or 175 billion but what if I
 

169
00:03:31,200 --> 00:03:33,270
1 trillion or 175 billion but what if I
can take it down to a million parameters

170
00:03:33,270 --> 00:03:33,280
can take it down to a million parameters
 

171
00:03:33,280 --> 00:03:36,070
can take it down to a million parameters
or a few million parameters. Can my

172
00:03:36,070 --> 00:03:36,080
or a few million parameters. Can my
 

173
00:03:36,080 --> 00:03:39,670
or a few million parameters. Can my
model still do the task well or instead

174
00:03:39,670 --> 00:03:39,680
model still do the task well or instead
 

175
00:03:39,680 --> 00:03:41,670
model still do the task well or instead
of a million what if first we try a few

176
00:03:41,670 --> 00:03:41,680
of a million what if first we try a few
 

177
00:03:41,680 --> 00:03:44,869
of a million what if first we try a few
billion and see if the model does the

178
00:03:44,869 --> 00:03:44,879
billion and see if the model does the
 

179
00:03:44,879 --> 00:03:45,789
billion and see if the model does the
task

180
00:03:45,789 --> 00:03:45,799
task
 

181
00:03:45,799 --> 00:03:48,550
task
well. So if you see over here here I

182
00:03:48,550 --> 00:03:48,560
well. So if you see over here here I
 

183
00:03:48,560 --> 00:03:51,030
well. So if you see over here here I
have presented some models which work

184
00:03:51,030 --> 00:03:51,040
have presented some models which work
 

185
00:03:51,040 --> 00:03:53,190
have presented some models which work
relatively well and their sizes you can

186
00:03:53,190 --> 00:03:53,200
relatively well and their sizes you can
 

187
00:03:53,200 --> 00:03:55,270
relatively well and their sizes you can
see here start from around 94 million

188
00:03:55,270 --> 00:03:55,280
see here start from around 94 million
 

189
00:03:55,280 --> 00:03:58,550
see here start from around 94 million
340 million 1.5 million etc. In this

190
00:03:58,550 --> 00:03:58,560
340 million 1.5 million etc. In this
 

191
00:03:58,560 --> 00:04:01,270
340 million 1.5 million etc. In this
figure also you can see the Quen 2.5

192
00:04:01,270 --> 00:04:01,280
figure also you can see the Quen 2.5
 

193
00:04:01,280 --> 00:04:03,429
figure also you can see the Quen 2.5
billion model, the Quen 1.5 billion

194
00:04:03,429 --> 00:04:03,439
billion model, the Quen 1.5 billion
 

195
00:04:03,439 --> 00:04:06,390
billion model, the Quen 1.5 billion
model, they perform very well and their

196
00:04:06,390 --> 00:04:06,400
model, they perform very well and their
 

197
00:04:06,400 --> 00:04:08,949
model, they perform very well and their
size if you see 1.5 billion is about 100

198
00:04:08,949 --> 00:04:08,959
size if you see 1.5 billion is about 100
 

199
00:04:08,959 --> 00:04:11,270
size if you see 1.5 billion is about 100
times lesser than 175 billion right

200
00:04:11,270 --> 00:04:11,280
times lesser than 175 billion right
 

201
00:04:11,280 --> 00:04:14,390
times lesser than 175 billion right
which is GPT3 size and it's about a,000

202
00:04:14,390 --> 00:04:14,400
which is GPT3 size and it's about a,000
 

203
00:04:14,400 --> 00:04:16,870
which is GPT3 size and it's about a,000
times lesser than GPT4 size and still

204
00:04:16,870 --> 00:04:16,880
times lesser than GPT4 size and still
 

205
00:04:16,880 --> 00:04:19,349
times lesser than GPT4 size and still
they perform relatively well. The appeal

206
00:04:19,349 --> 00:04:19,359
they perform relatively well. The appeal
 

207
00:04:19,359 --> 00:04:21,270
they perform relatively well. The appeal
of small language models is that as the

208
00:04:21,270 --> 00:04:21,280
of small language models is that as the
 

209
00:04:21,280 --> 00:04:23,590
of small language models is that as the
model sites get smaller, of course, it's

210
00:04:23,590 --> 00:04:23,600
model sites get smaller, of course, it's
 

211
00:04:23,600 --> 00:04:25,629
model sites get smaller, of course, it's
more easier to run, it's more faster to

212
00:04:25,629 --> 00:04:25,639
more easier to run, it's more faster to
 

213
00:04:25,639 --> 00:04:28,710
more easier to run, it's more faster to
implement. And if you connect the

214
00:04:28,710 --> 00:04:28,720
implement. And if you connect the
 

215
00:04:28,720 --> 00:04:30,950
implement. And if you connect the
development of language models and apply

216
00:04:30,950 --> 00:04:30,960
development of language models and apply
 

217
00:04:30,960 --> 00:04:33,030
development of language models and apply
it into production, you of course want

218
00:04:33,030 --> 00:04:33,040
it into production, you of course want
 

219
00:04:33,040 --> 00:04:36,469
it into production, you of course want
things which work fast. So similar to

220
00:04:36,469 --> 00:04:36,479
things which work fast. So similar to
 

221
00:04:36,479 --> 00:04:38,390
things which work fast. So similar to
the MOS law for the chip size, right?

222
00:04:38,390 --> 00:04:38,400
the MOS law for the chip size, right?
 

223
00:04:38,400 --> 00:04:40,230
the MOS law for the chip size, right?
And the chip size reduced exponentially

224
00:04:40,230 --> 00:04:40,240
And the chip size reduced exponentially
 

225
00:04:40,240 --> 00:04:43,030
And the chip size reduced exponentially
over time. I believe that the size of

226
00:04:43,030 --> 00:04:43,040
over time. I believe that the size of
 

227
00:04:43,040 --> 00:04:44,950
over time. I believe that the size of
language models is also going to come

228
00:04:44,950 --> 00:04:44,960
language models is also going to come
 

229
00:04:44,960 --> 00:04:47,510
language models is also going to come
down and within four to five years we

230
00:04:47,510 --> 00:04:47,520
down and within four to five years we
 

231
00:04:47,520 --> 00:04:49,510
down and within four to five years we
may reach a stage where all of us will

232
00:04:49,510 --> 00:04:49,520
may reach a stage where all of us will
 

233
00:04:49,520 --> 00:04:51,749
may reach a stage where all of us will
have a language model downloaded locally

234
00:04:51,749 --> 00:04:51,759
have a language model downloaded locally
 

235
00:04:51,759 --> 00:04:54,590
have a language model downloaded locally
on our mobiles which we can interact

236
00:04:54,590 --> 00:04:54,600
on our mobiles which we can interact
 

237
00:04:54,600 --> 00:04:58,510
on our mobiles which we can interact
with. This whole field of trying to

238
00:04:58,510 --> 00:04:58,520
with. This whole field of trying to
 

239
00:04:58,520 --> 00:05:01,189
with. This whole field of trying to
reduce the size of language models

240
00:05:01,189 --> 00:05:01,199
reduce the size of language models
 

241
00:05:01,199 --> 00:05:02,710
reduce the size of language models
essentially and to retain their

242
00:05:02,710 --> 00:05:02,720
essentially and to retain their
 

243
00:05:02,720 --> 00:05:05,189
essentially and to retain their
performance is what's called small

244
00:05:05,189 --> 00:05:05,199
performance is what's called small
 

245
00:05:05,199 --> 00:05:08,390
performance is what's called small
language modeling. Now what is small

246
00:05:08,390 --> 00:05:08,400
language modeling. Now what is small
 

247
00:05:08,400 --> 00:05:10,469
language modeling. Now what is small
that's not clearly defined but anything

248
00:05:10,469 --> 00:05:10,479
that's not clearly defined but anything
 

249
00:05:10,479 --> 00:05:12,629
that's not clearly defined but anything
which is less than let's say 1 billion

250
00:05:12,629 --> 00:05:12,639
which is less than let's say 1 billion
 

251
00:05:12,639 --> 00:05:14,390
which is less than let's say 1 billion
for example would be considered a small

252
00:05:14,390 --> 00:05:14,400
for example would be considered a small
 

253
00:05:14,400 --> 00:05:16,629
for example would be considered a small
language model currently if you manage

254
00:05:16,629 --> 00:05:16,639
language model currently if you manage
 

255
00:05:16,639 --> 00:05:18,150
language model currently if you manage
to have something which is of the size

256
00:05:18,150 --> 00:05:18,160
to have something which is of the size
 

257
00:05:18,160 --> 00:05:20,550
to have something which is of the size
of 1 to 50 million that's awesome

258
00:05:20,550 --> 00:05:20,560
of 1 to 50 million that's awesome
 

259
00:05:20,560 --> 00:05:21,990
of 1 to 50 million that's awesome
because that's definitely a small

260
00:05:21,990 --> 00:05:22,000
because that's definitely a small
 

261
00:05:22,000 --> 00:05:22,909
because that's definitely a small
language

262
00:05:22,909 --> 00:05:22,919
language
 

263
00:05:22,919 --> 00:05:25,270
language
model so today we are going to build

264
00:05:25,270 --> 00:05:25,280
model so today we are going to build
 

265
00:05:25,280 --> 00:05:27,110
model so today we are going to build
such model

266
00:05:27,110 --> 00:05:27,120
such model
 

267
00:05:27,120 --> 00:05:29,590
such model
um and we are going to use a GPU but I'm

268
00:05:29,590 --> 00:05:29,600
um and we are going to use a GPU but I'm
 

269
00:05:29,600 --> 00:05:31,110
um and we are going to use a GPU but I'm
going to show you how you can get access

270
00:05:31,110 --> 00:05:31,120
going to show you how you can get access
 

271
00:05:31,120 --> 00:05:33,749
going to show you how you can get access
to that GPU etc but we are not going to

272
00:05:33,749 --> 00:05:33,759
to that GPU etc but we are not going to
 

273
00:05:33,759 --> 00:05:36,390
to that GPU etc but we are not going to
use a very very deeply computational

274
00:05:36,390 --> 00:05:36,400
use a very very deeply computational
 

275
00:05:36,400 --> 00:05:38,230
use a very very deeply computational
traditionally heavy architecture with a

276
00:05:38,230 --> 00:05:38,240
traditionally heavy architecture with a
 

277
00:05:38,240 --> 00:05:40,070
traditionally heavy architecture with a
reasonably light architecture we are

278
00:05:40,070 --> 00:05:40,080
reasonably light architecture we are
 

279
00:05:40,080 --> 00:05:41,749
reasonably light architecture we are
going to get good results when we

280
00:05:41,749 --> 00:05:41,759
going to get good results when we
 

281
00:05:41,759 --> 00:05:44,150
going to get good results when we
develop a small language model. So the

282
00:05:44,150 --> 00:05:44,160
develop a small language model. So the
 

283
00:05:44,160 --> 00:05:45,430
develop a small language model. So the
language model which we are going to

284
00:05:45,430 --> 00:05:45,440
language model which we are going to
 

285
00:05:45,440 --> 00:05:47,350
language model which we are going to
develop will have a size of around 15

286
00:05:47,350 --> 00:05:47,360
develop will have a size of around 15
 

287
00:05:47,360 --> 00:05:48,270
develop will have a size of around 15
million

288
00:05:48,270 --> 00:05:48,280
million
 

289
00:05:48,280 --> 00:05:50,469
million
parameters and that's why it's safe to

290
00:05:50,469 --> 00:05:50,479
parameters and that's why it's safe to
 

291
00:05:50,479 --> 00:05:52,270
parameters and that's why it's safe to
call it a small language

292
00:05:52,270 --> 00:05:52,280
call it a small language
 

293
00:05:52,280 --> 00:05:55,110
call it a small language
model. So if you take this into context

294
00:05:55,110 --> 00:05:55,120
model. So if you take this into context
 

295
00:05:55,120 --> 00:05:59,830
model. So if you take this into context
of GPT3 this is probably around uh 1,000

296
00:05:59,830 --> 00:05:59,840
of GPT3 this is probably around uh 1,000
 

297
00:05:59,840 --> 00:06:04,070
of GPT3 this is probably around uh 1,000
or 10 to 5 times lower than

298
00:06:04,070 --> 00:06:04,080
or 10 to 5 times lower than
 

299
00:06:04,080 --> 00:06:07,110
or 10 to 5 times lower than
uh GPT3 because that's 175 billion and

300
00:06:07,110 --> 00:06:07,120
uh GPT3 because that's 175 billion and
 

301
00:06:07,120 --> 00:06:09,990
uh GPT3 because that's 175 billion and
that's even around 10 to 6 or 10 to 7

302
00:06:09,990 --> 00:06:10,000
that's even around 10 to 6 or 10 to 7
 

303
00:06:10,000 --> 00:06:12,390
that's even around 10 to 6 or 10 to 7
times smaller than let's say GPT4 the

304
00:06:12,390 --> 00:06:12,400
times smaller than let's say GPT4 the
 

305
00:06:12,400 --> 00:06:13,909
times smaller than let's say GPT4 the
size of the model which we are going to

306
00:06:13,909 --> 00:06:13,919
size of the model which we are going to
 

307
00:06:13,919 --> 00:06:17,029
size of the model which we are going to
generate today. So that's the scale of

308
00:06:17,029 --> 00:06:17,039
generate today. So that's the scale of
 

309
00:06:17,039 --> 00:06:19,590
generate today. So that's the scale of
uh the model size we are going to

310
00:06:19,590 --> 00:06:19,600
uh the model size we are going to
 

311
00:06:19,600 --> 00:06:21,749
uh the model size we are going to
operate on in this tutorial. But you'll

312
00:06:21,749 --> 00:06:21,759
operate on in this tutorial. But you'll
 

313
00:06:21,759 --> 00:06:23,189
operate on in this tutorial. But you'll
be surprised to know that at the

314
00:06:23,189 --> 00:06:23,199
be surprised to know that at the
 

315
00:06:23,199 --> 00:06:25,510
be surprised to know that at the
specific task which we look at the model

316
00:06:25,510 --> 00:06:25,520
specific task which we look at the model
 

317
00:06:25,520 --> 00:06:28,309
specific task which we look at the model
does relatively well with this context

318
00:06:28,309 --> 00:06:28,319
does relatively well with this context
 

319
00:06:28,319 --> 00:06:30,710
does relatively well with this context
set. Let's get started. We are going to

320
00:06:30,710 --> 00:06:30,720
set. Let's get started. We are going to
 

321
00:06:30,720 --> 00:06:33,350
set. Let's get started. We are going to
divide this project into six parts.

322
00:06:33,350 --> 00:06:33,360
divide this project into six parts.
 

323
00:06:33,360 --> 00:06:34,950
divide this project into six parts.
First we are going to go through our

324
00:06:34,950 --> 00:06:34,960
First we are going to go through our
 

325
00:06:34,960 --> 00:06:38,110
First we are going to go through our
data set. Then we will do a bit of data

326
00:06:38,110 --> 00:06:38,120
data set. Then we will do a bit of data
 

327
00:06:38,120 --> 00:06:40,070
data set. Then we will do a bit of data
prep-processing. Then we will assemble

328
00:06:40,070 --> 00:06:40,080
prep-processing. Then we will assemble
 

329
00:06:40,080 --> 00:06:42,629
prep-processing. Then we will assemble
the entire model architecture.

330
00:06:42,629 --> 00:06:42,639
the entire model architecture.
 

331
00:06:42,639 --> 00:06:44,550
the entire model architecture.
Then we'll set up the training pipeline

332
00:06:44,550 --> 00:06:44,560
Then we'll set up the training pipeline
 

333
00:06:44,560 --> 00:06:46,870
Then we'll set up the training pipeline
of the small language model. Then we'll

334
00:06:46,870 --> 00:06:46,880
of the small language model. Then we'll
 

335
00:06:46,880 --> 00:06:48,710
of the small language model. Then we'll
pre-train the entire small language

336
00:06:48,710 --> 00:06:48,720
pre-train the entire small language
 

337
00:06:48,720 --> 00:06:50,590
pre-train the entire small language
model. And finally we'll do the

338
00:06:50,590 --> 00:06:50,600
model. And finally we'll do the
 

339
00:06:50,600 --> 00:06:53,749
model. And finally we'll do the
inference. Don't worry if these terms or

340
00:06:53,749 --> 00:06:53,759
inference. Don't worry if these terms or
 

341
00:06:53,759 --> 00:06:55,510
inference. Don't worry if these terms or
some of the terms I'm saying are not

342
00:06:55,510 --> 00:06:55,520
some of the terms I'm saying are not
 

343
00:06:55,520 --> 00:06:58,469
some of the terms I'm saying are not
making sense right now. I'll try to go

344
00:06:58,469 --> 00:06:58,479
making sense right now. I'll try to go
 

345
00:06:58,479 --> 00:07:00,309
making sense right now. I'll try to go
through everything in a lot of detail so

346
00:07:00,309 --> 00:07:00,319
through everything in a lot of detail so
 

347
00:07:00,319 --> 00:07:02,150
through everything in a lot of detail so
that you understand every single aspect

348
00:07:02,150 --> 00:07:02,160
that you understand every single aspect
 

349
00:07:02,160 --> 00:07:04,230
that you understand every single aspect
of this pipeline. These are the

350
00:07:04,230 --> 00:07:04,240
of this pipeline. These are the
 

351
00:07:04,240 --> 00:07:06,070
of this pipeline. These are the
whiteboard notes which I'll be referring

352
00:07:06,070 --> 00:07:06,080
whiteboard notes which I'll be referring
 

353
00:07:06,080 --> 00:07:08,710
whiteboard notes which I'll be referring
to throughout this project and the

354
00:07:08,710 --> 00:07:08,720
to throughout this project and the
 

355
00:07:08,720 --> 00:07:11,430
to throughout this project and the
Google collab note or the Google collab

356
00:07:11,430 --> 00:07:11,440
Google collab note or the Google collab
 

357
00:07:11,440 --> 00:07:13,670
Google collab note or the Google collab
code file which I've already shown you.

358
00:07:13,670 --> 00:07:13,680
code file which I've already shown you.
 

359
00:07:13,680 --> 00:07:15,589
code file which I've already shown you.
We'll be going through this code file

360
00:07:15,589 --> 00:07:15,599
We'll be going through this code file
 

361
00:07:15,599 --> 00:07:17,830
We'll be going through this code file
after we finish each and every part on

362
00:07:17,830 --> 00:07:17,840
after we finish each and every part on
 

363
00:07:17,840 --> 00:07:19,390
after we finish each and every part on
the

364
00:07:19,390 --> 00:07:19,400
the
 

365
00:07:19,400 --> 00:07:22,070
the
whiteboard. Okay. So with that let's get

366
00:07:22,070 --> 00:07:22,080
whiteboard. Okay. So with that let's get
 

367
00:07:22,080 --> 00:07:23,749
whiteboard. Okay. So with that let's get
started with the first part of this

368
00:07:23,749 --> 00:07:23,759
started with the first part of this
 

369
00:07:23,759 --> 00:07:25,749
started with the first part of this
project which is the data set which we

370
00:07:25,749 --> 00:07:25,759
project which is the data set which we
 

371
00:07:25,759 --> 00:07:28,950
project which is the data set which we
have. Every language modeling project

372
00:07:28,950 --> 00:07:28,960
have. Every language modeling project
 

373
00:07:28,960 --> 00:07:32,070
have. Every language modeling project
begins with your data set right and as

374
00:07:32,070 --> 00:07:32,080
begins with your data set right and as
 

375
00:07:32,080 --> 00:07:35,110
begins with your data set right and as
you might be knowing GPT3, GPT4 etc was

376
00:07:35,110 --> 00:07:35,120
you might be knowing GPT3, GPT4 etc was
 

377
00:07:35,120 --> 00:07:37,430
you might be knowing GPT3, GPT4 etc was
trained on billions and billions of data

378
00:07:37,430 --> 00:07:37,440
trained on billions and billions of data
 

379
00:07:37,440 --> 00:07:40,950
trained on billions and billions of data
set which we do not have right now or I

380
00:07:40,950 --> 00:07:40,960
set which we do not have right now or I
 

381
00:07:40,960 --> 00:07:42,629
set which we do not have right now or I
don't have access to this right now and

382
00:07:42,629 --> 00:07:42,639
don't have access to this right now and
 

383
00:07:42,639 --> 00:07:44,550
don't have access to this right now and
would have been very difficult to

384
00:07:44,550 --> 00:07:44,560
would have been very difficult to
 

385
00:07:44,560 --> 00:07:46,909
would have been very difficult to
condense all of this in this small

386
00:07:46,909 --> 00:07:46,919
condense all of this in this small
 

387
00:07:46,919 --> 00:07:50,390
condense all of this in this small
tutorial. So so I turned to a paper

388
00:07:50,390 --> 00:07:50,400
tutorial. So so I turned to a paper
 

389
00:07:50,400 --> 00:07:53,510
tutorial. So so I turned to a paper
which became very popular in 2023 and

390
00:07:53,510 --> 00:07:53,520
which became very popular in 2023 and
 

391
00:07:53,520 --> 00:07:55,510
which became very popular in 2023 and
this paper is called as tiny stories.

392
00:07:55,510 --> 00:07:55,520
this paper is called as tiny stories.
 

393
00:07:55,520 --> 00:07:57,110
this paper is called as tiny stories.
You can check the number of citations

394
00:07:57,110 --> 00:07:57,120
You can check the number of citations
 

395
00:07:57,120 --> 00:07:58,589
You can check the number of citations
which this paper

396
00:07:58,589 --> 00:07:58,599
which this paper
 

397
00:07:58,599 --> 00:08:01,029
which this paper
have. So you'll see that it has around

398
00:08:01,029 --> 00:08:01,039
have. So you'll see that it has around
 

399
00:08:01,039 --> 00:08:04,469
have. So you'll see that it has around
219 citations, right? And what this

400
00:08:04,469 --> 00:08:04,479
219 citations, right? And what this
 

401
00:08:04,479 --> 00:08:06,710
219 citations, right? And what this
paper tried to do, so they tried to

402
00:08:06,710 --> 00:08:06,720
paper tried to do, so they tried to
 

403
00:08:06,720 --> 00:08:08,629
paper tried to do, so they tried to
answer the question that how small can

404
00:08:08,629 --> 00:08:08,639
answer the question that how small can
 

405
00:08:08,639 --> 00:08:10,629
answer the question that how small can
language models be and still speak

406
00:08:10,629 --> 00:08:10,639
language models be and still speak
 

407
00:08:10,639 --> 00:08:14,070
language models be and still speak
coherent English. And their main claim

408
00:08:14,070 --> 00:08:14,080
coherent English. And their main claim
 

409
00:08:14,080 --> 00:08:15,550
coherent English. And their main claim
was

410
00:08:15,550 --> 00:08:15,560
was
 

411
00:08:15,560 --> 00:08:18,150
was
that small language models were not

412
00:08:18,150 --> 00:08:18,160
that small language models were not
 

413
00:08:18,160 --> 00:08:20,150
that small language models were not
working that well until the time this

414
00:08:20,150 --> 00:08:20,160
working that well until the time this
 

415
00:08:20,160 --> 00:08:21,830
working that well until the time this
paper was published. And their main

416
00:08:21,830 --> 00:08:21,840
paper was published. And their main
 

417
00:08:21,840 --> 00:08:23,909
paper was published. And their main
claim was that small language models are

418
00:08:23,909 --> 00:08:23,919
claim was that small language models are
 

419
00:08:23,919 --> 00:08:25,990
claim was that small language models are
trained on this entire data set which is

420
00:08:25,990 --> 00:08:26,000
trained on this entire data set which is
 

421
00:08:26,000 --> 00:08:28,469
trained on this entire data set which is
available all over the internet. Right?

422
00:08:28,469 --> 00:08:28,479
available all over the internet. Right?
 

423
00:08:28,479 --> 00:08:30,469
available all over the internet. Right?
But they said that what if we reduce the

424
00:08:30,469 --> 00:08:30,479
But they said that what if we reduce the
 

425
00:08:30,479 --> 00:08:32,949
But they said that what if we reduce the
size of this data set itself. What if we

426
00:08:32,949 --> 00:08:32,959
size of this data set itself. What if we
 

427
00:08:32,959 --> 00:08:34,790
size of this data set itself. What if we
reduce the size of the data set so that

428
00:08:34,790 --> 00:08:34,800
reduce the size of the data set so that
 

429
00:08:34,800 --> 00:08:37,509
reduce the size of the data set so that
the data set is for a specific task. So

430
00:08:37,509 --> 00:08:37,519
the data set is for a specific task. So
 

431
00:08:37,519 --> 00:08:39,670
the data set is for a specific task. So
this is called as a specific task data

432
00:08:39,670 --> 00:08:39,680
this is called as a specific task data
 

433
00:08:39,680 --> 00:08:42,310
this is called as a specific task data
set.

434
00:08:42,310 --> 00:08:42,320
set.
 

435
00:08:42,320 --> 00:08:44,630
set.
And suppose let's suppose that we very

436
00:08:44,630 --> 00:08:44,640
And suppose let's suppose that we very
 

437
00:08:44,640 --> 00:08:46,630
And suppose let's suppose that we very
intelligently curate this data set so

438
00:08:46,630 --> 00:08:46,640
intelligently curate this data set so
 

439
00:08:46,640 --> 00:08:48,790
intelligently curate this data set so
that it captures all the nuances of the

440
00:08:48,790 --> 00:08:48,800
that it captures all the nuances of the
 

441
00:08:48,800 --> 00:08:49,949
that it captures all the nuances of the
English

442
00:08:49,949 --> 00:08:49,959
English
 

443
00:08:49,959 --> 00:08:52,630
English
language. Can we now train a small

444
00:08:52,630 --> 00:08:52,640
language. Can we now train a small
 

445
00:08:52,640 --> 00:08:55,230
language. Can we now train a small
language model on this small data

446
00:08:55,230 --> 00:08:55,240
language model on this small data
 

447
00:08:55,240 --> 00:08:58,310
language model on this small data
set? Let me explain this further to you.

448
00:08:58,310 --> 00:08:58,320
set? Let me explain this further to you.
 

449
00:08:58,320 --> 00:08:59,910
set? Let me explain this further to you.
In fact, the way these researchers

450
00:08:59,910 --> 00:08:59,920
In fact, the way these researchers
 

451
00:08:59,920 --> 00:09:02,590
In fact, the way these researchers
constructed this data set is that they

452
00:09:02,590 --> 00:09:02,600
constructed this data set is that they
 

453
00:09:02,600 --> 00:09:05,990
constructed this data set is that they
basically looked at stories which would

454
00:09:05,990 --> 00:09:06,000
basically looked at stories which would
 

455
00:09:06,000 --> 00:09:08,070
basically looked at stories which would
be appropriate for 3 to four year old

456
00:09:08,070 --> 00:09:08,080
be appropriate for 3 to four year old
 

457
00:09:08,080 --> 00:09:11,910
be appropriate for 3 to four year old
kids.

458
00:09:11,910 --> 00:09:11,920

 

459
00:09:11,920 --> 00:09:13,910

They only looked at stories for 3 to

460
00:09:13,910 --> 00:09:13,920
They only looked at stories for 3 to
 

461
00:09:13,920 --> 00:09:15,350
They only looked at stories for 3 to
four year old kids and that's why they

462
00:09:15,350 --> 00:09:15,360
four year old kids and that's why they
 

463
00:09:15,360 --> 00:09:17,910
four year old kids and that's why they
called this data set as tiny stories.

464
00:09:17,910 --> 00:09:17,920
called this data set as tiny stories.
 

465
00:09:17,920 --> 00:09:19,829
called this data set as tiny stories.
They constructed this data set. The way

466
00:09:19,829 --> 00:09:19,839
They constructed this data set. The way
 

467
00:09:19,839 --> 00:09:21,949
They constructed this data set. The way
they constructed this data set is from

468
00:09:21,949 --> 00:09:21,959
they constructed this data set is from
 

469
00:09:21,959 --> 00:09:25,190
they constructed this data set is from
GPT4. So they asked GPT4 to construct a

470
00:09:25,190 --> 00:09:25,200
GPT4. So they asked GPT4 to construct a
 

471
00:09:25,200 --> 00:09:27,190
GPT4. So they asked GPT4 to construct a
huge amount of stories for 3 to four

472
00:09:27,190 --> 00:09:27,200
huge amount of stories for 3 to four
 

473
00:09:27,200 --> 00:09:30,389
huge amount of stories for 3 to four
year old kids. And the smartness or the

474
00:09:30,389 --> 00:09:30,399
year old kids. And the smartness or the
 

475
00:09:30,399 --> 00:09:32,710
year old kids. And the smartness or the
beauty of this approach is that this

476
00:09:32,710 --> 00:09:32,720
beauty of this approach is that this
 

477
00:09:32,720 --> 00:09:34,630
beauty of this approach is that this
will be a very small data set, right? It

478
00:09:34,630 --> 00:09:34,640
will be a very small data set, right? It
 

479
00:09:34,640 --> 00:09:36,550
will be a very small data set, right? It
won't why would we need all the

480
00:09:36,550 --> 00:09:36,560
won't why would we need all the
 

481
00:09:36,560 --> 00:09:39,030
won't why would we need all the
information of the world for this? We

482
00:09:39,030 --> 00:09:39,040
information of the world for this? We
 

483
00:09:39,040 --> 00:09:41,750
information of the world for this? We
only need stories for that age group. So

484
00:09:41,750 --> 00:09:41,760
only need stories for that age group. So
 

485
00:09:41,760 --> 00:09:44,230
only need stories for that age group. So
those will only contain a specific words

486
00:09:44,230 --> 00:09:44,240
those will only contain a specific words
 

487
00:09:44,240 --> 00:09:47,190
those will only contain a specific words
but they capture all the nuances of the

488
00:09:47,190 --> 00:09:47,200
but they capture all the nuances of the
 

489
00:09:47,200 --> 00:09:48,710
but they capture all the nuances of the
English language because the stories

490
00:09:48,710 --> 00:09:48,720
English language because the stories
 

491
00:09:48,720 --> 00:09:50,790
English language because the stories
have to be written in fluent English.

492
00:09:50,790 --> 00:09:50,800
have to be written in fluent English.
 

493
00:09:50,800 --> 00:09:53,269
have to be written in fluent English.
The grammar needs to be correct. So if

494
00:09:53,269 --> 00:09:53,279
The grammar needs to be correct. So if
 

495
00:09:53,279 --> 00:09:55,509
The grammar needs to be correct. So if
we are testing that can a language model

496
00:09:55,509 --> 00:09:55,519
we are testing that can a language model
 

497
00:09:55,519 --> 00:09:57,269
we are testing that can a language model
essentially learn from all these data

498
00:09:57,269 --> 00:09:57,279
essentially learn from all these data
 

499
00:09:57,279 --> 00:09:59,949
essentially learn from all these data
set and produce coherent

500
00:09:59,949 --> 00:09:59,959
set and produce coherent
 

501
00:09:59,959 --> 00:10:02,870
set and produce coherent
English then ideally this is a great

502
00:10:02,870 --> 00:10:02,880
English then ideally this is a great
 

503
00:10:02,880 --> 00:10:05,430
English then ideally this is a great
data set to have. So what is our goal

504
00:10:05,430 --> 00:10:05,440
data set to have. So what is our goal
 

505
00:10:05,440 --> 00:10:08,910
data set to have. So what is our goal
here? Our goal here is that can my

506
00:10:08,910 --> 00:10:08,920
here? Our goal here is that can my
 

507
00:10:08,920 --> 00:10:13,590
here? Our goal here is that can my
model or can my 15 million can my 15

508
00:10:13,590 --> 00:10:13,600
model or can my 15 million can my 15
 

509
00:10:13,600 --> 00:10:15,389
model or can my 15 million can my 15
million

510
00:10:15,389 --> 00:10:15,399
million
 

511
00:10:15,399 --> 00:10:18,990
million
model learn from this data

512
00:10:18,990 --> 00:10:19,000
model learn from this data
 

513
00:10:19,000 --> 00:10:22,790
model learn from this data
set? learn from this data set. And what

514
00:10:22,790 --> 00:10:22,800
set? learn from this data set. And what
 

515
00:10:22,800 --> 00:10:24,710
set? learn from this data set. And what
does it mean learning from the data set?

516
00:10:24,710 --> 00:10:24,720
does it mean learning from the data set?
 

517
00:10:24,720 --> 00:10:26,470
does it mean learning from the data set?
It actually means that my model should

518
00:10:26,470 --> 00:10:26,480
It actually means that my model should
 

519
00:10:26,480 --> 00:10:28,550
It actually means that my model should
be able to do two things. My model

520
00:10:28,550 --> 00:10:28,560
be able to do two things. My model
 

521
00:10:28,560 --> 00:10:30,069
be able to do two things. My model
should understand the form of the

522
00:10:30,069 --> 00:10:30,079
should understand the form of the
 

523
00:10:30,079 --> 00:10:32,550
should understand the form of the
English language. Sorry, my model should

524
00:10:32,550 --> 00:10:32,560
English language. Sorry, my model should
 

525
00:10:32,560 --> 00:10:34,069
English language. Sorry, my model should
understand let's say the structure or

526
00:10:34,069 --> 00:10:34,079
understand let's say the structure or
 

527
00:10:34,079 --> 00:10:35,670
understand let's say the structure or
the form of the English language which

528
00:10:35,670 --> 00:10:35,680
the form of the English language which
 

529
00:10:35,680 --> 00:10:37,829
the form of the English language which
means it should generate grammatically

530
00:10:37,829 --> 00:10:37,839
means it should generate grammatically
 

531
00:10:37,839 --> 00:10:40,550
means it should generate grammatically
correct sentences and second my model

532
00:10:40,550 --> 00:10:40,560
correct sentences and second my model
 

533
00:10:40,560 --> 00:10:42,069
correct sentences and second my model
should also understand something about

534
00:10:42,069 --> 00:10:42,079
should also understand something about
 

535
00:10:42,079 --> 00:10:45,030
should also understand something about
the meaning which means that my model

536
00:10:45,030 --> 00:10:45,040
the meaning which means that my model
 

537
00:10:45,040 --> 00:10:46,710
the meaning which means that my model
should be able to come up with stories

538
00:10:46,710 --> 00:10:46,720
should be able to come up with stories
 

539
00:10:46,720 --> 00:10:49,190
should be able to come up with stories
on its own which are suitable for 3 to

540
00:10:49,190 --> 00:10:49,200
on its own which are suitable for 3 to
 

541
00:10:49,200 --> 00:10:51,590
on its own which are suitable for 3 to
four year old kids and if we could

542
00:10:51,590 --> 00:10:51,600
four year old kids and if we could
 

543
00:10:51,600 --> 00:10:53,069
four year old kids and if we could
achieve this that would be truly

544
00:10:53,069 --> 00:10:53,079
achieve this that would be truly
 

545
00:10:53,079 --> 00:10:55,829
achieve this that would be truly
incredible right because just from this

546
00:10:55,829 --> 00:10:55,839
incredible right because just from this
 

547
00:10:55,839 --> 00:10:58,710
incredible right because just from this
small data set now I'm able to teach a

548
00:10:58,710 --> 00:10:58,720
small data set now I'm able to teach a
 

549
00:10:58,720 --> 00:11:00,470
small data set now I'm able to teach a
language model about the English

550
00:11:00,470 --> 00:11:00,480
language model about the English
 

551
00:11:00,480 --> 00:11:01,590
language model about the English
language

552
00:11:01,590 --> 00:11:01,600
language
 

553
00:11:01,600 --> 00:11:04,069
language
The model starts from no knowledge. It

554
00:11:04,069 --> 00:11:04,079
The model starts from no knowledge. It
 

555
00:11:04,079 --> 00:11:06,069
The model starts from no knowledge. It
has no information about language. It

556
00:11:06,069 --> 00:11:06,079
has no information about language. It
 

557
00:11:06,079 --> 00:11:08,310
has no information about language. It
has no information about the fact that

558
00:11:08,310 --> 00:11:08,320
has no information about the fact that
 

559
00:11:08,320 --> 00:11:10,030
has no information about the fact that
let's say

560
00:11:10,030 --> 00:11:10,040
let's say
 

561
00:11:10,040 --> 00:11:12,790
let's say
uh subject verb object that is the

562
00:11:12,790 --> 00:11:12,800
uh subject verb object that is the
 

563
00:11:12,800 --> 00:11:14,310
uh subject verb object that is the
structure followed in English language.

564
00:11:14,310 --> 00:11:14,320
structure followed in English language.
 

565
00:11:14,320 --> 00:11:17,190
structure followed in English language.
It does not know this. But can we teach

566
00:11:17,190 --> 00:11:17,200
It does not know this. But can we teach
 

567
00:11:17,200 --> 00:11:19,030
It does not know this. But can we teach
this? Can the model learn this from this

568
00:11:19,030 --> 00:11:19,040
this? Can the model learn this from this
 

569
00:11:19,040 --> 00:11:20,550
this? Can the model learn this from this
data set? And if we are able to

570
00:11:20,550 --> 00:11:20,560
data set? And if we are able to
 

571
00:11:20,560 --> 00:11:23,030
data set? And if we are able to
successfully create this model, which

572
00:11:23,030 --> 00:11:23,040
successfully create this model, which
 

573
00:11:23,040 --> 00:11:24,310
successfully create this model, which
means that if we are able to

574
00:11:24,310 --> 00:11:24,320
means that if we are able to
 

575
00:11:24,320 --> 00:11:26,389
means that if we are able to
successfully create a model which

576
00:11:26,389 --> 00:11:26,399
successfully create a model which
 

577
00:11:26,399 --> 00:11:29,110
successfully create a model which
outputs coherent English stories, we are

578
00:11:29,110 --> 00:11:29,120
outputs coherent English stories, we are
 

579
00:11:29,120 --> 00:11:30,949
outputs coherent English stories, we are
successful at our task of creating a

580
00:11:30,949 --> 00:11:30,959
successful at our task of creating a
 

581
00:11:30,959 --> 00:11:33,269
successful at our task of creating a
small language model. And what is small?

582
00:11:33,269 --> 00:11:33,279
small language model. And what is small?
 

583
00:11:33,279 --> 00:11:35,269
small language model. And what is small?
We are going to create only a 15 million

584
00:11:35,269 --> 00:11:35,279
We are going to create only a 15 million
 

585
00:11:35,279 --> 00:11:37,829
We are going to create only a 15 million
parameter model. Okay. So we are going

586
00:11:37,829 --> 00:11:37,839
parameter model. Okay. So we are going
 

587
00:11:37,839 --> 00:11:40,230
parameter model. Okay. So we are going
to work with the tiny stories data set.

588
00:11:40,230 --> 00:11:40,240
to work with the tiny stories data set.
 

589
00:11:40,240 --> 00:11:41,910
to work with the tiny stories data set.
This these researchers have made their

590
00:11:41,910 --> 00:11:41,920
This these researchers have made their
 

591
00:11:41,920 --> 00:11:44,230
This these researchers have made their
data set public. So we are going to use

592
00:11:44,230 --> 00:11:44,240
data set public. So we are going to use
 

593
00:11:44,240 --> 00:11:46,470
data set public. So we are going to use
this data set. If you scroll over to

594
00:11:46,470 --> 00:11:46,480
this data set. If you scroll over to
 

595
00:11:46,480 --> 00:11:49,190
this data set. If you scroll over to
this, this data set is also available on

596
00:11:49,190 --> 00:11:49,200
this, this data set is also available on
 

597
00:11:49,200 --> 00:11:51,350
this, this data set is also available on
hugging face. So you'll see that they

598
00:11:51,350 --> 00:11:51,360
hugging face. So you'll see that they
 

599
00:11:51,360 --> 00:11:53,350
hugging face. So you'll see that they
split the data set into two parts. They

600
00:11:53,350 --> 00:11:53,360
split the data set into two parts. They
 

601
00:11:53,360 --> 00:11:55,829
split the data set into two parts. They
split it into training and validation.

602
00:11:55,829 --> 00:11:55,839
split it into training and validation.
 

603
00:11:55,839 --> 00:11:58,150
split it into training and validation.
So even although I say this is a tiny

604
00:11:58,150 --> 00:11:58,160
So even although I say this is a tiny
 

605
00:11:58,160 --> 00:12:00,509
So even although I say this is a tiny
data set, it still has 2 million

606
00:12:00,509 --> 00:12:00,519
data set, it still has 2 million
 

607
00:12:00,519 --> 00:12:03,750
data set, it still has 2 million
rows. And uh every row of this data set

608
00:12:03,750 --> 00:12:03,760
rows. And uh every row of this data set
 

609
00:12:03,760 --> 00:12:05,910
rows. And uh every row of this data set
is a story meant for 3 to four year old

610
00:12:05,910 --> 00:12:05,920
is a story meant for 3 to four year old
 

611
00:12:05,920 --> 00:12:08,470
is a story meant for 3 to four year old
kids. So for example, if you see this

612
00:12:08,470 --> 00:12:08,480
kids. So for example, if you see this
 

613
00:12:08,480 --> 00:12:10,870
kids. So for example, if you see this
row, we see the first story, right? One

614
00:12:10,870 --> 00:12:10,880
row, we see the first story, right? One
 

615
00:12:10,880 --> 00:12:12,790
row, we see the first story, right? One
day a little girl named Lily found a

616
00:12:12,790 --> 00:12:12,800
day a little girl named Lily found a
 

617
00:12:12,800 --> 00:12:14,790
day a little girl named Lily found a
needle in her room. She knew it was

618
00:12:14,790 --> 00:12:14,800
needle in her room. She knew it was
 

619
00:12:14,800 --> 00:12:16,710
needle in her room. She knew it was
difficult to play with it because it was

620
00:12:16,710 --> 00:12:16,720
difficult to play with it because it was
 

621
00:12:16,720 --> 00:12:18,949
difficult to play with it because it was
sharp. Lily wanted to share the needle

622
00:12:18,949 --> 00:12:18,959
sharp. Lily wanted to share the needle
 

623
00:12:18,959 --> 00:12:21,110
sharp. Lily wanted to share the needle
with her mom etc etc. And this whole

624
00:12:21,110 --> 00:12:21,120
with her mom etc etc. And this whole
 

625
00:12:21,120 --> 00:12:23,430
with her mom etc etc. And this whole
story proceeds like this. Remember these

626
00:12:23,430 --> 00:12:23,440
story proceeds like this. Remember these
 

627
00:12:23,440 --> 00:12:25,829
story proceeds like this. Remember these
stories have been generated from GPT4

628
00:12:25,829 --> 00:12:25,839
stories have been generated from GPT4
 

629
00:12:25,839 --> 00:12:27,670
stories have been generated from GPT4
but they constitute our data set like

630
00:12:27,670 --> 00:12:27,680
but they constitute our data set like
 

631
00:12:27,680 --> 00:12:30,150
but they constitute our data set like
that. This is one story. Similarly, we

632
00:12:30,150 --> 00:12:30,160
that. This is one story. Similarly, we
 

633
00:12:30,160 --> 00:12:32,470
that. This is one story. Similarly, we
have 2 million stories like this which

634
00:12:32,470 --> 00:12:32,480
have 2 million stories like this which
 

635
00:12:32,480 --> 00:12:34,710
have 2 million stories like this which
are in our training data set and about

636
00:12:34,710 --> 00:12:34,720
are in our training data set and about
 

637
00:12:34,720 --> 00:12:37,350
are in our training data set and about
20,000 stories in our testing data set.

638
00:12:37,350 --> 00:12:37,360
20,000 stories in our testing data set.
 

639
00:12:37,360 --> 00:12:39,509
20,000 stories in our testing data set.
So, we are going to work with this data.

640
00:12:39,509 --> 00:12:39,519
So, we are going to work with this data.
 

641
00:12:39,519 --> 00:12:41,990
So, we are going to work with this data.
As I mentioned at the start of this

642
00:12:41,990 --> 00:12:42,000
As I mentioned at the start of this
 

643
00:12:42,000 --> 00:12:43,750
As I mentioned at the start of this
video, this is not going to be a toy

644
00:12:43,750 --> 00:12:43,760
video, this is not going to be a toy
 

645
00:12:43,760 --> 00:12:45,509
video, this is not going to be a toy
project video, but we are actually going

646
00:12:45,509 --> 00:12:45,519
project video, but we are actually going
 

647
00:12:45,519 --> 00:12:48,190
project video, but we are actually going
to work with real data sets over here.

648
00:12:48,190 --> 00:12:48,200
to work with real data sets over here.
 

649
00:12:48,200 --> 00:12:50,870
to work with real data sets over here.
Okay. So, this is the data set and we'll

650
00:12:50,870 --> 00:12:50,880
Okay. So, this is the data set and we'll
 

651
00:12:50,880 --> 00:12:52,790
Okay. So, this is the data set and we'll
use this data set which is specifically

652
00:12:52,790 --> 00:12:52,800
use this data set which is specifically
 

653
00:12:52,800 --> 00:12:55,509
use this data set which is specifically
meant for 3 to four year old kids. This

654
00:12:55,509 --> 00:12:55,519
meant for 3 to four year old kids. This
 

655
00:12:55,519 --> 00:12:57,190
meant for 3 to four year old kids. This
will help us reduce the size of our

656
00:12:57,190 --> 00:12:57,200
will help us reduce the size of our
 

657
00:12:57,200 --> 00:12:58,870
will help us reduce the size of our
language model because now I'm not

658
00:12:58,870 --> 00:12:58,880
language model because now I'm not
 

659
00:12:58,880 --> 00:13:00,710
language model because now I'm not
assembling all the data from all over

660
00:13:00,710 --> 00:13:00,720
assembling all the data from all over
 

661
00:13:00,720 --> 00:13:02,870
assembling all the data from all over
the internet. I'm looking at a specific

662
00:13:02,870 --> 00:13:02,880
the internet. I'm looking at a specific
 

663
00:13:02,880 --> 00:13:04,790
the internet. I'm looking at a specific
corpus of data meant for 3 to four year

664
00:13:04,790 --> 00:13:04,800
corpus of data meant for 3 to four year
 

665
00:13:04,800 --> 00:13:07,910
corpus of data meant for 3 to four year
old kids. And the question is that can

666
00:13:07,910 --> 00:13:07,920
old kids. And the question is that can
 

667
00:13:07,920 --> 00:13:10,870
old kids. And the question is that can
we construct can we construct a language

668
00:13:10,870 --> 00:13:10,880
we construct can we construct a language
 

669
00:13:10,880 --> 00:13:12,790
we construct can we construct a language
model with just 10 to 15 million

670
00:13:12,790 --> 00:13:12,800
model with just 10 to 15 million
 

671
00:13:12,800 --> 00:13:14,949
model with just 10 to 15 million
parameters and which produces coherent

672
00:13:14,949 --> 00:13:14,959
parameters and which produces coherent
 

673
00:13:14,959 --> 00:13:17,590
parameters and which produces coherent
text. And if you think about this sounds

674
00:13:17,590 --> 00:13:17,600
text. And if you think about this sounds
 

675
00:13:17,600 --> 00:13:19,350
text. And if you think about this sounds
like an incredibly difficult problem,

676
00:13:19,350 --> 00:13:19,360
like an incredibly difficult problem,
 

677
00:13:19,360 --> 00:13:21,829
like an incredibly difficult problem,
right? Because you have to teach a model

678
00:13:21,829 --> 00:13:21,839
right? Because you have to teach a model
 

679
00:13:21,839 --> 00:13:23,590
right? Because you have to teach a model
something about language otherwise how

680
00:13:23,590 --> 00:13:23,600
something about language otherwise how
 

681
00:13:23,600 --> 00:13:25,990
something about language otherwise how
can the model come up with stories. We

682
00:13:25,990 --> 00:13:26,000
can the model come up with stories. We
 

683
00:13:26,000 --> 00:13:27,829
can the model come up with stories. We
want the model to produce coherent

684
00:13:27,829 --> 00:13:27,839
want the model to produce coherent
 

685
00:13:27,839 --> 00:13:30,069
want the model to produce coherent
stories towards the end of this. We want

686
00:13:30,069 --> 00:13:30,079
stories towards the end of this. We want
 

687
00:13:30,079 --> 00:13:31,829
stories towards the end of this. We want
the model to produce stories which make

688
00:13:31,829 --> 00:13:31,839
the model to produce stories which make
 

689
00:13:31,839 --> 00:13:34,230
the model to produce stories which make
sense on its own. So it's like imagine

690
00:13:34,230 --> 00:13:34,240
sense on its own. So it's like imagine
 

691
00:13:34,240 --> 00:13:37,110
sense on its own. So it's like imagine
an alien coming to the earth and you

692
00:13:37,110 --> 00:13:37,120
an alien coming to the earth and you
 

693
00:13:37,120 --> 00:13:39,590
an alien coming to the earth and you
want the alien to speak English. So

694
00:13:39,590 --> 00:13:39,600
want the alien to speak English. So
 

695
00:13:39,600 --> 00:13:41,350
want the alien to speak English. So
instead of loading the alien with all

696
00:13:41,350 --> 00:13:41,360
instead of loading the alien with all
 

697
00:13:41,360 --> 00:13:44,230
instead of loading the alien with all
the information available on Wikipedia,

698
00:13:44,230 --> 00:13:44,240
the information available on Wikipedia,
 

699
00:13:44,240 --> 00:13:46,670
the information available on Wikipedia,
on Google, everywhere, you just give the

700
00:13:46,670 --> 00:13:46,680
on Google, everywhere, you just give the
 

701
00:13:46,680 --> 00:13:49,750
on Google, everywhere, you just give the
alien stories for 3 to four year old

702
00:13:49,750 --> 00:13:49,760
alien stories for 3 to four year old
 

703
00:13:49,760 --> 00:13:51,350
alien stories for 3 to four year old
because through those stories also the

704
00:13:51,350 --> 00:13:51,360
because through those stories also the
 

705
00:13:51,360 --> 00:13:53,590
because through those stories also the
alien can learn the English language and

706
00:13:53,590 --> 00:13:53,600
alien can learn the English language and
 

707
00:13:53,600 --> 00:13:55,670
alien can learn the English language and
then you feed all this data to the alien

708
00:13:55,670 --> 00:13:55,680
then you feed all this data to the alien
 

709
00:13:55,680 --> 00:13:57,829
then you feed all this data to the alien
and then you hope that the alien will

710
00:13:57,829 --> 00:13:57,839
and then you hope that the alien will
 

711
00:13:57,839 --> 00:13:59,509
and then you hope that the alien will
learn everything and come up with

712
00:13:59,509 --> 00:13:59,519
learn everything and come up with
 

713
00:13:59,519 --> 00:14:02,870
learn everything and come up with
coherent stories on his or her own.

714
00:14:02,870 --> 00:14:02,880
coherent stories on his or her own.
 

715
00:14:02,880 --> 00:14:04,470
coherent stories on his or her own.
So this sounds like a challenging

716
00:14:04,470 --> 00:14:04,480
So this sounds like a challenging
 

717
00:14:04,480 --> 00:14:06,389
So this sounds like a challenging
problem but that's what's so exciting

718
00:14:06,389 --> 00:14:06,399
problem but that's what's so exciting
 

719
00:14:06,399 --> 00:14:08,790
problem but that's what's so exciting
about this project. Okay. So now I'm

720
00:14:08,790 --> 00:14:08,800
about this project. Okay. So now I'm
 

721
00:14:08,800 --> 00:14:10,870
about this project. Okay. So now I'm
going to go to Google Collab and we are

722
00:14:10,870 --> 00:14:10,880
going to go to Google Collab and we are
 

723
00:14:10,880 --> 00:14:13,269
going to go to Google Collab and we are
going to just start executing some code

724
00:14:13,269 --> 00:14:13,279
going to just start executing some code
 

725
00:14:13,279 --> 00:14:17,189
going to just start executing some code
blocks as we uh finish understanding the

726
00:14:17,189 --> 00:14:17,199
blocks as we uh finish understanding the
 

727
00:14:17,199 --> 00:14:20,150
blocks as we uh finish understanding the
steps on the whiteboard. So the part one

728
00:14:20,150 --> 00:14:20,160
steps on the whiteboard. So the part one
 

729
00:14:20,160 --> 00:14:22,389
steps on the whiteboard. So the part one
is loading our data set. So I go over

730
00:14:22,389 --> 00:14:22,399
is loading our data set. So I go over
 

731
00:14:22,399 --> 00:14:25,269
is loading our data set. So I go over
here and here you can see that you can

732
00:14:25,269 --> 00:14:25,279
here and here you can see that you can
 

733
00:14:25,279 --> 00:14:27,910
here and here you can see that you can
um this data set is present on hugging

734
00:14:27,910 --> 00:14:27,920
um this data set is present on hugging
 

735
00:14:27,920 --> 00:14:30,069
um this data set is present on hugging
face right. So you have to first install

736
00:14:30,069 --> 00:14:30,079
face right. So you have to first install
 

737
00:14:30,079 --> 00:14:32,870
face right. So you have to first install
data sets uh which is a package through

738
00:14:32,870 --> 00:14:32,880
data sets uh which is a package through
 

739
00:14:32,880 --> 00:14:34,629
data sets uh which is a package through
which you can load any data set on

740
00:14:34,629 --> 00:14:34,639
which you can load any data set on
 

741
00:14:34,639 --> 00:14:35,990
which you can load any data set on
hugging phase. So you can run this

742
00:14:35,990 --> 00:14:36,000
hugging phase. So you can run this
 

743
00:14:36,000 --> 00:14:37,670
hugging phase. So you can run this
command and it will take some time to

744
00:14:37,670 --> 00:14:37,680
command and it will take some time to
 

745
00:14:37,680 --> 00:14:39,910
command and it will take some time to
install this. It took about 30 seconds

746
00:14:39,910 --> 00:14:39,920
install this. It took about 30 seconds
 

747
00:14:39,920 --> 00:14:43,430
install this. It took about 30 seconds
on mine. Okay. After this point you have

748
00:14:43,430 --> 00:14:43,440
on mine. Okay. After this point you have
 

749
00:14:43,440 --> 00:14:46,790
on mine. Okay. After this point you have
to import the load data set function and

750
00:14:46,790 --> 00:14:46,800
to import the load data set function and
 

751
00:14:46,800 --> 00:14:50,150
to import the load data set function and
here you have to pass the URL of the uh

752
00:14:50,150 --> 00:14:50,160
here you have to pass the URL of the uh
 

753
00:14:50,160 --> 00:14:52,310
here you have to pass the URL of the uh
here you have to pass the URL of your

754
00:14:52,310 --> 00:14:52,320
here you have to pass the URL of your
 

755
00:14:52,320 --> 00:14:54,470
here you have to pass the URL of your
data set on hugging face. So if I go to

756
00:14:54,470 --> 00:14:54,480
data set on hugging face. So if I go to
 

757
00:14:54,480 --> 00:14:56,069
data set on hugging face. So if I go to
hugging face right now, you'll see that

758
00:14:56,069 --> 00:14:56,079
hugging face right now, you'll see that
 

759
00:14:56,079 --> 00:14:58,949
hugging face right now, you'll see that
this is the URL, right? Uh this is the

760
00:14:58,949 --> 00:14:58,959
this is the URL, right? Uh this is the
 

761
00:14:58,959 --> 00:15:00,710
this is the URL, right? Uh this is the
URL which I have marked over here and

762
00:15:00,710 --> 00:15:00,720
URL which I have marked over here and
 

763
00:15:00,720 --> 00:15:02,470
URL which I have marked over here and
that's the URL which I'm passing over

764
00:15:02,470 --> 00:15:02,480
that's the URL which I'm passing over
 

765
00:15:02,480 --> 00:15:06,230
that's the URL which I'm passing over
here. So load data set and this URL. And

766
00:15:06,230 --> 00:15:06,240
here. So load data set and this URL. And
 

767
00:15:06,240 --> 00:15:07,829
here. So load data set and this URL. And
when you run this, you'll see that the

768
00:15:07,829 --> 00:15:07,839
when you run this, you'll see that the
 

769
00:15:07,839 --> 00:15:09,949
when you run this, you'll see that the
data set has been loaded now

770
00:15:09,949 --> 00:15:09,959
data set has been loaded now
 

771
00:15:09,959 --> 00:15:13,269
data set has been loaded now
fully. And we see that the since the

772
00:15:13,269 --> 00:15:13,279
fully. And we see that the since the
 

773
00:15:13,279 --> 00:15:15,030
fully. And we see that the since the
data had two parts already, right? We

774
00:15:15,030 --> 00:15:15,040
data had two parts already, right? We
 

775
00:15:15,040 --> 00:15:17,670
data had two parts already, right? We
had a training data and validation data.

776
00:15:17,670 --> 00:15:17,680
had a training data and validation data.
 

777
00:15:17,680 --> 00:15:19,350
had a training data and validation data.
We have two types of data which have

778
00:15:19,350 --> 00:15:19,360
We have two types of data which have
 

779
00:15:19,360 --> 00:15:21,350
We have two types of data which have
already been loaded. The train split and

780
00:15:21,350 --> 00:15:21,360
already been loaded. The train split and
 

781
00:15:21,360 --> 00:15:23,910
already been loaded. The train split and
the validation split. validation split.

782
00:15:23,910 --> 00:15:23,920
the validation split. validation split.
 

783
00:15:23,920 --> 00:15:26,150
the validation split. validation split.
Okay. So the data set has fully been

784
00:15:26,150 --> 00:15:26,160
Okay. So the data set has fully been
 

785
00:15:26,160 --> 00:15:28,629
Okay. So the data set has fully been
loaded. Now this loading of the data set

786
00:15:28,629 --> 00:15:28,639
loaded. Now this loading of the data set
 

787
00:15:28,639 --> 00:15:30,389
loaded. Now this loading of the data set
might take some time for you because

788
00:15:30,389 --> 00:15:30,399
might take some time for you because
 

789
00:15:30,399 --> 00:15:32,470
might take some time for you because
remember we are loading 2 million rows,

790
00:15:32,470 --> 00:15:32,480
remember we are loading 2 million rows,
 

791
00:15:32,480 --> 00:15:34,629
remember we are loading 2 million rows,
right? We are loading 2 million stories

792
00:15:34,629 --> 00:15:34,639
right? We are loading 2 million stories
 

793
00:15:34,639 --> 00:15:37,829
right? We are loading 2 million stories
in our training data. Um so as you can

794
00:15:37,829 --> 00:15:37,839
in our training data. Um so as you can
 

795
00:15:37,839 --> 00:15:39,910
in our training data. Um so as you can
see here we are 2 million stories in our

796
00:15:39,910 --> 00:15:39,920
see here we are 2 million stories in our
 

797
00:15:39,920 --> 00:15:41,910
see here we are 2 million stories in our
training data and we have about 20,000

798
00:15:41,910 --> 00:15:41,920
training data and we have about 20,000
 

799
00:15:41,920 --> 00:15:44,870
training data and we have about 20,000
stories in our validation data. So this

800
00:15:44,870 --> 00:15:44,880
stories in our validation data. So this
 

801
00:15:44,880 --> 00:15:47,590
stories in our validation data. So this
does take some time but once this is

802
00:15:47,590 --> 00:15:47,600
does take some time but once this is
 

803
00:15:47,600 --> 00:15:49,189
does take some time but once this is
done your data set is successfully

804
00:15:49,189 --> 00:15:49,199
done your data set is successfully
 

805
00:15:49,199 --> 00:15:51,350
done your data set is successfully
loaded. One more thing which I want to

806
00:15:51,350 --> 00:15:51,360
loaded. One more thing which I want to
 

807
00:15:51,360 --> 00:15:53,030
loaded. One more thing which I want to
mention right at the start is that I'm

808
00:15:53,030 --> 00:15:53,040
mention right at the start is that I'm
 

809
00:15:53,040 --> 00:15:56,590
mention right at the start is that I'm
changing the runtime here to T uh A100

810
00:15:56,590 --> 00:15:56,600
changing the runtime here to T uh A100
 

811
00:15:56,600 --> 00:15:59,509
changing the runtime here to T uh A100
GPU. So now if you are on the Google

812
00:15:59,509 --> 00:15:59,519
GPU. So now if you are on the Google
 

813
00:15:59,519 --> 00:16:03,269
GPU. So now if you are on the Google
Collab free tire you can do T4 GPU but

814
00:16:03,269 --> 00:16:03,279
Collab free tire you can do T4 GPU but
 

815
00:16:03,279 --> 00:16:05,030
Collab free tire you can do T4 GPU but
the training will take a huge amount of

816
00:16:05,030 --> 00:16:05,040
the training will take a huge amount of
 

817
00:16:05,040 --> 00:16:07,749
the training will take a huge amount of
time on T4 GPU. You can run it overnight

818
00:16:07,749 --> 00:16:07,759
time on T4 GPU. You can run it overnight
 

819
00:16:07,759 --> 00:16:09,670
time on T4 GPU. You can run it overnight
for 8 to 10 hours and then it will be

820
00:16:09,670 --> 00:16:09,680
for 8 to 10 hours and then it will be
 

821
00:16:09,680 --> 00:16:11,670
for 8 to 10 hours and then it will be
complete. But if you switch to Google

822
00:16:11,670 --> 00:16:11,680
complete. But if you switch to Google
 

823
00:16:11,680 --> 00:16:14,550
complete. But if you switch to Google
Collab Pro which is just $10 per month,

824
00:16:14,550 --> 00:16:14,560
Collab Pro which is just $10 per month,
 

825
00:16:14,560 --> 00:16:16,629
Collab Pro which is just $10 per month,
you can run the training with an A100

826
00:16:16,629 --> 00:16:16,639
you can run the training with an A100
 

827
00:16:16,639 --> 00:16:19,509
you can run the training with an A100
GPU or the L4 GPU. Both are very strong

828
00:16:19,509 --> 00:16:19,519
GPU or the L4 GPU. Both are very strong
 

829
00:16:19,519 --> 00:16:21,749
GPU or the L4 GPU. Both are very strong
and the training roughly takes just 25

830
00:16:21,749 --> 00:16:21,759
and the training roughly takes just 25
 

831
00:16:21,759 --> 00:16:24,949
and the training roughly takes just 25
to 30 minutes on an A100 GPU. But if you

832
00:16:24,949 --> 00:16:24,959
to 30 minutes on an A100 GPU. But if you
 

833
00:16:24,959 --> 00:16:27,030
to 30 minutes on an A100 GPU. But if you
have a T4 GPU, it might easily take

834
00:16:27,030 --> 00:16:27,040
have a T4 GPU, it might easily take
 

835
00:16:27,040 --> 00:16:29,990
have a T4 GPU, it might easily take
around 6 to 7 hours. It can be run on a

836
00:16:29,990 --> 00:16:30,000
around 6 to 7 hours. It can be run on a
 

837
00:16:30,000 --> 00:16:33,430
around 6 to 7 hours. It can be run on a
free tire also. So no problem if you

838
00:16:33,430 --> 00:16:33,440
free tire also. So no problem if you
 

839
00:16:33,440 --> 00:16:36,150
free tire also. So no problem if you
just have a T4 GPU. But keep in mind

840
00:16:36,150 --> 00:16:36,160
just have a T4 GPU. But keep in mind
 

841
00:16:36,160 --> 00:16:37,990
just have a T4 GPU. But keep in mind
that it might take some time. And never

842
00:16:37,990 --> 00:16:38,000
that it might take some time. And never
 

843
00:16:38,000 --> 00:16:39,910
that it might take some time. And never
run this code on CPU because it will

844
00:16:39,910 --> 00:16:39,920
run this code on CPU because it will
 

845
00:16:39,920 --> 00:16:41,829
run this code on CPU because it will
fail. You will not be able to run this

846
00:16:41,829 --> 00:16:41,839
fail. You will not be able to run this
 

847
00:16:41,839 --> 00:16:43,590
fail. You will not be able to run this
code on CPU. So it should be either of

848
00:16:43,590 --> 00:16:43,600
code on CPU. So it should be either of
 

849
00:16:43,600 --> 00:16:46,110
code on CPU. So it should be either of
these GPUs which is provided by Google

850
00:16:46,110 --> 00:16:46,120
these GPUs which is provided by Google
 

851
00:16:46,120 --> 00:16:48,710
these GPUs which is provided by Google
Collab. Okay. So let's go to the

852
00:16:48,710 --> 00:16:48,720
Collab. Okay. So let's go to the
 

853
00:16:48,720 --> 00:16:50,790
Collab. Okay. So let's go to the
whiteboard. Right now the step one is

854
00:16:50,790 --> 00:16:50,800
whiteboard. Right now the step one is
 

855
00:16:50,800 --> 00:16:52,949
whiteboard. Right now the step one is
done where we have loaded our data set.

856
00:16:52,949 --> 00:16:52,959
done where we have loaded our data set.
 

857
00:16:52,959 --> 00:16:54,870
done where we have loaded our data set.
I hope until this point all of you have

858
00:16:54,870 --> 00:16:54,880
I hope until this point all of you have
 

859
00:16:54,880 --> 00:16:58,470
I hope until this point all of you have
understood the aim here. Our data set is

860
00:16:58,470 --> 00:16:58,480
understood the aim here. Our data set is
 

861
00:16:58,480 --> 00:17:00,470
understood the aim here. Our data set is
only stories understood by 3 to four

862
00:17:00,470 --> 00:17:00,480
only stories understood by 3 to four
 

863
00:17:00,480 --> 00:17:02,710
only stories understood by 3 to four
year old kids. We are going to use this

864
00:17:02,710 --> 00:17:02,720
year old kids. We are going to use this
 

865
00:17:02,720 --> 00:17:04,789
year old kids. We are going to use this
data set to teach our language model

866
00:17:04,789 --> 00:17:04,799
data set to teach our language model
 

867
00:17:04,799 --> 00:17:06,710
data set to teach our language model
about the English language. So that

868
00:17:06,710 --> 00:17:06,720
about the English language. So that
 

869
00:17:06,720 --> 00:17:08,470
about the English language. So that
ultimately it produces stories which

870
00:17:08,470 --> 00:17:08,480
ultimately it produces stories which
 

871
00:17:08,480 --> 00:17:10,630
ultimately it produces stories which
look like the ones in the data set.

872
00:17:10,630 --> 00:17:10,640
look like the ones in the data set.
 

873
00:17:10,640 --> 00:17:12,470
look like the ones in the data set.
Okay, that's going to be the purpose of

874
00:17:12,470 --> 00:17:12,480
Okay, that's going to be the purpose of
 

875
00:17:12,480 --> 00:17:14,949
Okay, that's going to be the purpose of
our small language model. Now let's move

876
00:17:14,949 --> 00:17:14,959
our small language model. Now let's move
 

877
00:17:14,959 --> 00:17:17,789
our small language model. Now let's move
to part two which is data

878
00:17:17,789 --> 00:17:17,799
to part two which is data
 

879
00:17:17,799 --> 00:17:20,150
to part two which is data
prep-processing. All right. So data

880
00:17:20,150 --> 00:17:20,160
prep-processing. All right. So data
 

881
00:17:20,160 --> 00:17:22,069
prep-processing. All right. So data
prep-processing is one of the most

882
00:17:22,069 --> 00:17:22,079
prep-processing is one of the most
 

883
00:17:22,079 --> 00:17:25,189
prep-processing is one of the most
important parts of essentially training

884
00:17:25,189 --> 00:17:25,199
important parts of essentially training
 

885
00:17:25,199 --> 00:17:28,309
important parts of essentially training
a large language model. And let me show

886
00:17:28,309 --> 00:17:28,319
a large language model. And let me show
 

887
00:17:28,319 --> 00:17:30,310
a large language model. And let me show
you the whole training pipeline

888
00:17:30,310 --> 00:17:30,320
you the whole training pipeline
 

889
00:17:30,320 --> 00:17:32,710
you the whole training pipeline
actually. So if you look at the pipeline

890
00:17:32,710 --> 00:17:32,720
actually. So if you look at the pipeline
 

891
00:17:32,720 --> 00:17:35,110
actually. So if you look at the pipeline
of training a large language model,

892
00:17:35,110 --> 00:17:35,120
of training a large language model,
 

893
00:17:35,120 --> 00:17:37,750
of training a large language model,
usually we have three blocks, right? We

894
00:17:37,750 --> 00:17:37,760
usually we have three blocks, right? We
 

895
00:17:37,760 --> 00:17:39,830
usually we have three blocks, right? We
have the input block which I've shown

896
00:17:39,830 --> 00:17:39,840
have the input block which I've shown
 

897
00:17:39,840 --> 00:17:42,470
have the input block which I've shown
over here. We have the processor block

898
00:17:42,470 --> 00:17:42,480
over here. We have the processor block
 

899
00:17:42,480 --> 00:17:44,230
over here. We have the processor block
which I've marked over here and then we

900
00:17:44,230 --> 00:17:44,240
which I've marked over here and then we
 

901
00:17:44,240 --> 00:17:46,630
which I've marked over here and then we
have the output block. We are going to

902
00:17:46,630 --> 00:17:46,640
have the output block. We are going to
 

903
00:17:46,640 --> 00:17:48,789
have the output block. We are going to
assemble all three blocks today. But

904
00:17:48,789 --> 00:17:48,799
assemble all three blocks today. But
 

905
00:17:48,799 --> 00:17:51,110
assemble all three blocks today. But
just keep in mind that in literature and

906
00:17:51,110 --> 00:17:51,120
just keep in mind that in literature and
 

907
00:17:51,120 --> 00:17:53,990
just keep in mind that in literature and
even in uh everywhere everyone talks

908
00:17:53,990 --> 00:17:54,000
even in uh everywhere everyone talks
 

909
00:17:54,000 --> 00:17:55,669
even in uh everywhere everyone talks
about the processor block because that's

910
00:17:55,669 --> 00:17:55,679
about the processor block because that's
 

911
00:17:55,679 --> 00:17:58,549
about the processor block because that's
where cool stuff is happening. But input

912
00:17:58,549 --> 00:17:58,559
where cool stuff is happening. But input
 

913
00:17:58,559 --> 00:18:00,230
where cool stuff is happening. But input
block does not receive that much

914
00:18:00,230 --> 00:18:00,240
block does not receive that much
 

915
00:18:00,240 --> 00:18:01,830
block does not receive that much
attention although this is one of the

916
00:18:01,830 --> 00:18:01,840
attention although this is one of the
 

917
00:18:01,840 --> 00:18:04,909
attention although this is one of the
most important steps. Here is where the

918
00:18:04,909 --> 00:18:04,919
most important steps. Here is where the
 

919
00:18:04,919 --> 00:18:07,750
most important steps. Here is where the
data data processing actually happens

920
00:18:07,750 --> 00:18:07,760
data data processing actually happens
 

921
00:18:07,760 --> 00:18:11,190
data data processing actually happens
for the input text which we have and uh

922
00:18:11,190 --> 00:18:11,200
for the input text which we have and uh
 

923
00:18:11,200 --> 00:18:12,950
for the input text which we have and uh
that's why I'm going to go into a fair

924
00:18:12,950 --> 00:18:12,960
that's why I'm going to go into a fair
 

925
00:18:12,960 --> 00:18:15,510
that's why I'm going to go into a fair
amount of detail for this part because

926
00:18:15,510 --> 00:18:15,520
amount of detail for this part because
 

927
00:18:15,520 --> 00:18:17,350
amount of detail for this part because
without understanding how the data is

928
00:18:17,350 --> 00:18:17,360
without understanding how the data is
 

929
00:18:17,360 --> 00:18:19,190
without understanding how the data is
processed in the input block you won't

930
00:18:19,190 --> 00:18:19,200
processed in the input block you won't
 

931
00:18:19,200 --> 00:18:21,029
processed in the input block you won't
understand what happens to the data when

932
00:18:21,029 --> 00:18:21,039
understand what happens to the data when
 

933
00:18:21,039 --> 00:18:23,350
understand what happens to the data when
it enters the processor or what happens

934
00:18:23,350 --> 00:18:23,360
it enters the processor or what happens
 

935
00:18:23,360 --> 00:18:24,789
it enters the processor or what happens
to the data when it enters the

936
00:18:24,789 --> 00:18:24,799
to the data when it enters the
 

937
00:18:24,799 --> 00:18:27,430
to the data when it enters the
transformer block. Okay. So let's

938
00:18:27,430 --> 00:18:27,440
transformer block. Okay. So let's
 

939
00:18:27,440 --> 00:18:29,750
transformer block. Okay. So let's
understand data prep-processing. So

940
00:18:29,750 --> 00:18:29,760
understand data prep-processing. So
 

941
00:18:29,760 --> 00:18:33,750
understand data prep-processing. So
remember first that LLMs or essentially

942
00:18:33,750 --> 00:18:33,760
remember first that LLMs or essentially
 

943
00:18:33,760 --> 00:18:36,590
remember first that LLMs or essentially
computer for that matter it cannot

944
00:18:36,590 --> 00:18:36,600
computer for that matter it cannot
 

945
00:18:36,600 --> 00:18:39,750
computer for that matter it cannot
understand sentences right. It cannot

946
00:18:39,750 --> 00:18:39,760
understand sentences right. It cannot
 

947
00:18:39,760 --> 00:18:41,430
understand sentences right. It cannot
understand sentences. It cannot

948
00:18:41,430 --> 00:18:41,440
understand sentences. It cannot
 

949
00:18:41,440 --> 00:18:44,750
understand sentences. It cannot
understand text or words like humans

950
00:18:44,750 --> 00:18:44,760
understand text or words like humans
 

951
00:18:44,760 --> 00:18:49,029
understand text or words like humans
do. It only understands numerical. It

952
00:18:49,029 --> 00:18:49,039
do. It only understands numerical. It
 

953
00:18:49,039 --> 00:18:54,750
do. It only understands numerical. It
only understands numerical data.

954
00:18:54,750 --> 00:18:54,760

 

955
00:18:54,760 --> 00:18:57,990

So what we have to do is that somehow we

956
00:18:57,990 --> 00:18:58,000
So what we have to do is that somehow we
 

957
00:18:58,000 --> 00:18:59,830
So what we have to do is that somehow we
have to take our input data which has

958
00:18:59,830 --> 00:18:59,840
have to take our input data which has
 

959
00:18:59,840 --> 00:19:02,230
have to take our input data which has
been present over here and we have to

960
00:19:02,230 --> 00:19:02,240
been present over here and we have to
 

961
00:19:02,240 --> 00:19:04,830
been present over here and we have to
convert it into a numerical

962
00:19:04,830 --> 00:19:04,840
convert it into a numerical
 

963
00:19:04,840 --> 00:19:07,430
convert it into a numerical
format. Now think about how would you

964
00:19:07,430 --> 00:19:07,440
format. Now think about how would you
 

965
00:19:07,440 --> 00:19:09,669
format. Now think about how would you
actually do this right? Let's say let me

966
00:19:09,669 --> 00:19:09,679
actually do this right? Let's say let me
 

967
00:19:09,679 --> 00:19:11,669
actually do this right? Let's say let me
take this story over here. Let me take a

968
00:19:11,669 --> 00:19:11,679
take this story over here. Let me take a
 

969
00:19:11,679 --> 00:19:13,510
take this story over here. Let me take a
screenshot. And if you understand how

970
00:19:13,510 --> 00:19:13,520
screenshot. And if you understand how
 

971
00:19:13,520 --> 00:19:15,830
screenshot. And if you understand how
this is done for one story, we'll of

972
00:19:15,830 --> 00:19:15,840
this is done for one story, we'll of
 

973
00:19:15,840 --> 00:19:18,310
this is done for one story, we'll of
course understand how it can be done for

974
00:19:18,310 --> 00:19:18,320
course understand how it can be done for
 

975
00:19:18,320 --> 00:19:20,390
course understand how it can be done for
all the stories. Right? So I've taken a

976
00:19:20,390 --> 00:19:20,400
all the stories. Right? So I've taken a
 

977
00:19:20,400 --> 00:19:22,630
all the stories. Right? So I've taken a
screenshot over here and I want us to

978
00:19:22,630 --> 00:19:22,640
screenshot over here and I want us to
 

979
00:19:22,640 --> 00:19:25,270
screenshot over here and I want us to
think how will you convert this entire

980
00:19:25,270 --> 00:19:25,280
think how will you convert this entire
 

981
00:19:25,280 --> 00:19:27,549
think how will you convert this entire
paragraph into

982
00:19:27,549 --> 00:19:27,559
paragraph into
 

983
00:19:27,559 --> 00:19:29,830
paragraph into
numbers. So the first thing which might

984
00:19:29,830 --> 00:19:29,840
numbers. So the first thing which might
 

985
00:19:29,840 --> 00:19:32,310
numbers. So the first thing which might
come to your mind is that I'll do uh

986
00:19:32,310 --> 00:19:32,320
come to your mind is that I'll do uh
 

987
00:19:32,320 --> 00:19:34,230
come to your mind is that I'll do uh
every word will be assigned some number.

988
00:19:34,230 --> 00:19:34,240
every word will be assigned some number.
 

989
00:19:34,240 --> 00:19:35,990
every word will be assigned some number.
So I'll divide this paragraph into bunch

990
00:19:35,990 --> 00:19:36,000
So I'll divide this paragraph into bunch
 

991
00:19:36,000 --> 00:19:37,750
So I'll divide this paragraph into bunch
of words and then I'll assign every word

992
00:19:37,750 --> 00:19:37,760
of words and then I'll assign every word
 

993
00:19:37,760 --> 00:19:40,070
of words and then I'll assign every word
a number. Or the second thing which

994
00:19:40,070 --> 00:19:40,080
a number. Or the second thing which
 

995
00:19:40,080 --> 00:19:41,590
a number. Or the second thing which
might come to your mind is that I'll

996
00:19:41,590 --> 00:19:41,600
might come to your mind is that I'll
 

997
00:19:41,600 --> 00:19:44,070
might come to your mind is that I'll
divide this paragraph into characters

998
00:19:44,070 --> 00:19:44,080
divide this paragraph into characters
 

999
00:19:44,080 --> 00:19:45,830
divide this paragraph into characters
and I'll assign every character a

1000
00:19:45,830 --> 00:19:45,840
and I'll assign every character a
 

1001
00:19:45,840 --> 00:19:48,310
and I'll assign every character a
number. The first type of method in

1002
00:19:48,310 --> 00:19:48,320
number. The first type of method in
 

1003
00:19:48,320 --> 00:19:50,549
number. The first type of method in
which every word is assigned a number is

1004
00:19:50,549 --> 00:19:50,559
which every word is assigned a number is
 

1005
00:19:50,559 --> 00:19:56,110
which every word is assigned a number is
called as word based

1006
00:19:56,110 --> 00:19:56,120

 

1007
00:19:56,120 --> 00:19:58,310

tokenization. And the second type of

1008
00:19:58,310 --> 00:19:58,320
tokenization. And the second type of
 

1009
00:19:58,320 --> 00:19:59,830
tokenization. And the second type of
method where every character is

1010
00:19:59,830 --> 00:19:59,840
method where every character is
 

1011
00:19:59,840 --> 00:20:02,070
method where every character is
essentially assigned a number or a token

1012
00:20:02,070 --> 00:20:02,080
essentially assigned a number or a token
 

1013
00:20:02,080 --> 00:20:04,990
essentially assigned a number or a token
that's called as character based

1014
00:20:04,990 --> 00:20:05,000
that's called as character based
 

1015
00:20:05,000 --> 00:20:08,230
that's called as character based
tokenization. Now it turns out that to

1016
00:20:08,230 --> 00:20:08,240
tokenization. Now it turns out that to
 

1017
00:20:08,240 --> 00:20:10,710
tokenization. Now it turns out that to
develop language models both of these

1018
00:20:10,710 --> 00:20:10,720
develop language models both of these
 

1019
00:20:10,720 --> 00:20:12,750
develop language models both of these
tokenization forms are not that

1020
00:20:12,750 --> 00:20:12,760
tokenization forms are not that
 

1021
00:20:12,760 --> 00:20:15,029
tokenization forms are not that
efficient. Why? Because when you have

1022
00:20:15,029 --> 00:20:15,039
efficient. Why? Because when you have
 

1023
00:20:15,039 --> 00:20:16,430
efficient. Why? Because when you have
character level

1024
00:20:16,430 --> 00:20:16,440
character level
 

1025
00:20:16,440 --> 00:20:18,470
character level
tokenization, every character is a

1026
00:20:18,470 --> 00:20:18,480
tokenization, every character is a
 

1027
00:20:18,480 --> 00:20:20,549
tokenization, every character is a
token. So the input text will be get

1028
00:20:20,549 --> 00:20:20,559
token. So the input text will be get
 

1029
00:20:20,559 --> 00:20:22,950
token. So the input text will be get
converted into a huge number of tokens

1030
00:20:22,950 --> 00:20:22,960
converted into a huge number of tokens
 

1031
00:20:22,960 --> 00:20:25,029
converted into a huge number of tokens
and our transformer block which is the

1032
00:20:25,029 --> 00:20:25,039
and our transformer block which is the
 

1033
00:20:25,039 --> 00:20:27,110
and our transformer block which is the
processor can only pay attention to a

1034
00:20:27,110 --> 00:20:27,120
processor can only pay attention to a
 

1035
00:20:27,120 --> 00:20:29,430
processor can only pay attention to a
certain number of tokens at a time. So

1036
00:20:29,430 --> 00:20:29,440
certain number of tokens at a time. So
 

1037
00:20:29,440 --> 00:20:31,270
certain number of tokens at a time. So
character based tokenization does not

1038
00:20:31,270 --> 00:20:31,280
character based tokenization does not
 

1039
00:20:31,280 --> 00:20:34,470
character based tokenization does not
work at all. Um second word based

1040
00:20:34,470 --> 00:20:34,480
work at all. Um second word based
 

1041
00:20:34,480 --> 00:20:37,270
work at all. Um second word based
tokenization right. So if every word is

1042
00:20:37,270 --> 00:20:37,280
tokenization right. So if every word is
 

1043
00:20:37,280 --> 00:20:39,430
tokenization right. So if every word is
essentially one token, the issue happens

1044
00:20:39,430 --> 00:20:39,440
essentially one token, the issue happens
 

1045
00:20:39,440 --> 00:20:41,070
essentially one token, the issue happens
when we have

1046
00:20:41,070 --> 00:20:41,080
when we have
 

1047
00:20:41,080 --> 00:20:43,669
when we have
uh let's say words where spelling

1048
00:20:43,669 --> 00:20:43,679
uh let's say words where spelling
 

1049
00:20:43,679 --> 00:20:46,230
uh let's say words where spelling
mistakes are there. If words have

1050
00:20:46,230 --> 00:20:46,240
mistakes are there. If words have
 

1051
00:20:46,240 --> 00:20:47,830
mistakes are there. If words have
spelling mistakes, they will not be

1052
00:20:47,830 --> 00:20:47,840
spelling mistakes, they will not be
 

1053
00:20:47,840 --> 00:20:51,029
spelling mistakes, they will not be
mapped to any token. That's the first

1054
00:20:51,029 --> 00:20:51,039
mapped to any token. That's the first
 

1055
00:20:51,039 --> 00:20:53,430
mapped to any token. That's the first
problem. And the second problem is that

1056
00:20:53,430 --> 00:20:53,440
problem. And the second problem is that
 

1057
00:20:53,440 --> 00:20:55,270
problem. And the second problem is that
whenever you do tokenization, you

1058
00:20:55,270 --> 00:20:55,280
whenever you do tokenization, you
 

1059
00:20:55,280 --> 00:20:56,830
whenever you do tokenization, you
maintain something called as the

1060
00:20:56,830 --> 00:20:56,840
maintain something called as the
 

1061
00:20:56,840 --> 00:20:58,789
maintain something called as the
vocabulary. You maintain something

1062
00:20:58,789 --> 00:20:58,799
vocabulary. You maintain something
 

1063
00:20:58,799 --> 00:21:00,310
vocabulary. You maintain something
called as the vocabulary. This

1064
00:21:00,310 --> 00:21:00,320
called as the vocabulary. This
 

1065
00:21:00,320 --> 00:21:02,350
called as the vocabulary. This
vocabulary consists of the list of

1066
00:21:02,350 --> 00:21:02,360
vocabulary consists of the list of
 

1067
00:21:02,360 --> 00:21:05,310
vocabulary consists of the list of
tokens and their corresponding token

1068
00:21:05,310 --> 00:21:05,320
tokens and their corresponding token
 

1069
00:21:05,320 --> 00:21:07,590
tokens and their corresponding token
ids. So if you do word based

1070
00:21:07,590 --> 00:21:07,600
ids. So if you do word based
 

1071
00:21:07,600 --> 00:21:09,750
ids. So if you do word based
tokenization, these tokens will be all

1072
00:21:09,750 --> 00:21:09,760
tokenization, these tokens will be all
 

1073
00:21:09,760 --> 00:21:12,549
tokenization, these tokens will be all
the words. And if you just look at the

1074
00:21:12,549 --> 00:21:12,559
the words. And if you just look at the
 

1075
00:21:12,559 --> 00:21:14,390
the words. And if you just look at the
English language itself, we have about

1076
00:21:14,390 --> 00:21:14,400
English language itself, we have about
 

1077
00:21:14,400 --> 00:21:16,870
English language itself, we have about
half a million or 500,000 words in the

1078
00:21:16,870 --> 00:21:16,880
half a million or 500,000 words in the
 

1079
00:21:16,880 --> 00:21:19,430
half a million or 500,000 words in the
English language itself. So in the case

1080
00:21:19,430 --> 00:21:19,440
English language itself. So in the case
 

1081
00:21:19,440 --> 00:21:21,110
English language itself. So in the case
of word based tokenization the

1082
00:21:21,110 --> 00:21:21,120
of word based tokenization the
 

1083
00:21:21,120 --> 00:21:24,070
of word based tokenization the
vocabulary size becomes very large and

1084
00:21:24,070 --> 00:21:24,080
vocabulary size becomes very large and
 

1085
00:21:24,080 --> 00:21:25,590
vocabulary size becomes very large and
that's a problem for us because

1086
00:21:25,590 --> 00:21:25,600
that's a problem for us because
 

1087
00:21:25,600 --> 00:21:27,990
that's a problem for us because
vocabulary size affects our computations

1088
00:21:27,990 --> 00:21:28,000
vocabulary size affects our computations
 

1089
00:21:28,000 --> 00:21:29,510
vocabulary size affects our computations
when we are going to predict the next

1090
00:21:29,510 --> 00:21:29,520
when we are going to predict the next
 

1091
00:21:29,520 --> 00:21:31,630
when we are going to predict the next
token in a large language

1092
00:21:31,630 --> 00:21:31,640
token in a large language
 

1093
00:21:31,640 --> 00:21:34,310
token in a large language
model. Usually the higher the vocabulary

1094
00:21:34,310 --> 00:21:34,320
model. Usually the higher the vocabulary
 

1095
00:21:34,320 --> 00:21:37,390
model. Usually the higher the vocabulary
size the longer the

1096
00:21:37,390 --> 00:21:37,400
size the longer the
 

1097
00:21:37,400 --> 00:21:39,190
size the longer the
computational the longer the

1098
00:21:39,190 --> 00:21:39,200
computational the longer the
 

1099
00:21:39,200 --> 00:21:40,950
computational the longer the
computational time and the higher is the

1100
00:21:40,950 --> 00:21:40,960
computational time and the higher is the
 

1101
00:21:40,960 --> 00:21:43,190
computational time and the higher is the
computational memory. So both of these

1102
00:21:43,190 --> 00:21:43,200
computational memory. So both of these
 

1103
00:21:43,200 --> 00:21:45,270
computational memory. So both of these
are tokenization methods which we don't

1104
00:21:45,270 --> 00:21:45,280
are tokenization methods which we don't
 

1105
00:21:45,280 --> 00:21:48,149
are tokenization methods which we don't
want. So then what's the best solution?

1106
00:21:48,149 --> 00:21:48,159
want. So then what's the best solution?
 

1107
00:21:48,159 --> 00:21:49,669
want. So then what's the best solution?
The best solution is something

1108
00:21:49,669 --> 00:21:49,679
The best solution is something
 

1109
00:21:49,679 --> 00:21:51,350
The best solution is something
intermediate and that's that

1110
00:21:51,350 --> 00:21:51,360
intermediate and that's that
 

1111
00:21:51,360 --> 00:21:53,870
intermediate and that's that
intermediate stage is called as subword

1112
00:21:53,870 --> 00:21:53,880
intermediate stage is called as subword
 

1113
00:21:53,880 --> 00:21:56,390
intermediate stage is called as subword
tokenization. That intermediate stage is

1114
00:21:56,390 --> 00:21:56,400
tokenization. That intermediate stage is
 

1115
00:21:56,400 --> 00:21:58,950
tokenization. That intermediate stage is
called subword tokenization. In subword

1116
00:21:58,950 --> 00:21:58,960
called subword tokenization. In subword
 

1117
00:21:58,960 --> 00:22:00,710
called subword tokenization. In subword
tokenization what we do is that we

1118
00:22:00,710 --> 00:22:00,720
tokenization what we do is that we
 

1119
00:22:00,720 --> 00:22:02,669
tokenization what we do is that we
maintain a vocabulary of

1120
00:22:02,669 --> 00:22:02,679
maintain a vocabulary of
 

1121
00:22:02,679 --> 00:22:06,149
maintain a vocabulary of
tokens which can be characters which can

1122
00:22:06,149 --> 00:22:06,159
tokens which can be characters which can
 

1123
00:22:06,159 --> 00:22:10,630
tokens which can be characters which can
be words and which can also be subwords.

1124
00:22:10,630 --> 00:22:10,640
be words and which can also be subwords.
 

1125
00:22:10,640 --> 00:22:12,470
be words and which can also be subwords.
So words which are not very commonly

1126
00:22:12,470 --> 00:22:12,480
So words which are not very commonly
 

1127
00:22:12,480 --> 00:22:13,990
So words which are not very commonly
occurring right like for example

1128
00:22:13,990 --> 00:22:14,000
occurring right like for example
 

1129
00:22:14,000 --> 00:22:16,870
occurring right like for example
tokenization for example it is usually

1130
00:22:16,870 --> 00:22:16,880
tokenization for example it is usually
 

1131
00:22:16,880 --> 00:22:18,789
tokenization for example it is usually
split into two words isation so

1132
00:22:18,789 --> 00:22:18,799
split into two words isation so
 

1133
00:22:18,799 --> 00:22:20,549
split into two words isation so
isization becomes one subword that

1134
00:22:20,549 --> 00:22:20,559
isization becomes one subword that
 

1135
00:22:20,559 --> 00:22:22,870
isization becomes one subword that
becomes a separate token and this token

1136
00:22:22,870 --> 00:22:22,880
becomes a separate token and this token
 

1137
00:22:22,880 --> 00:22:25,750
becomes a separate token and this token
becomes another token so in token in

1138
00:22:25,750 --> 00:22:25,760
becomes another token so in token in
 

1139
00:22:25,760 --> 00:22:27,830
becomes another token so in token in
subword based tokenization what we do is

1140
00:22:27,830 --> 00:22:27,840
subword based tokenization what we do is
 

1141
00:22:27,840 --> 00:22:29,990
subword based tokenization what we do is
that characters are of course retained

1142
00:22:29,990 --> 00:22:30,000
that characters are of course retained
 

1143
00:22:30,000 --> 00:22:32,950
that characters are of course retained
as individual tokens that much is fine

1144
00:22:32,950 --> 00:22:32,960
as individual tokens that much is fine
 

1145
00:22:32,960 --> 00:22:34,630
as individual tokens that much is fine
all commonly occurring words are

1146
00:22:34,630 --> 00:22:34,640
all commonly occurring words are
 

1147
00:22:34,640 --> 00:22:36,710
all commonly occurring words are
retained as tokens but all the words

1148
00:22:36,710 --> 00:22:36,720
retained as tokens but all the words
 

1149
00:22:36,720 --> 00:22:38,310
retained as tokens but all the words
which are not that common they are

1150
00:22:38,310 --> 00:22:38,320
which are not that common they are
 

1151
00:22:38,320 --> 00:22:40,230
which are not that common they are
broken down into subwords

1152
00:22:40,230 --> 00:22:40,240
broken down into subwords
 

1153
00:22:40,240 --> 00:22:42,230
broken down into subwords
This solves our vocabulary issue because

1154
00:22:42,230 --> 00:22:42,240
This solves our vocabulary issue because
 

1155
00:22:42,240 --> 00:22:44,549
This solves our vocabulary issue because
every word is now not a separate token.

1156
00:22:44,549 --> 00:22:44,559
every word is now not a separate token.
 

1157
00:22:44,559 --> 00:22:46,070
every word is now not a separate token.
Some words can be broken down into

1158
00:22:46,070 --> 00:22:46,080
Some words can be broken down into
 

1159
00:22:46,080 --> 00:22:49,110
Some words can be broken down into
subwords and this also solves the

1160
00:22:49,110 --> 00:22:49,120
subwords and this also solves the
 

1161
00:22:49,120 --> 00:22:50,950
subwords and this also solves the
character based tokenization issue which

1162
00:22:50,950 --> 00:22:50,960
character based tokenization issue which
 

1163
00:22:50,960 --> 00:22:52,990
character based tokenization issue which
breaks the text into huge number of

1164
00:22:52,990 --> 00:22:53,000
breaks the text into huge number of
 

1165
00:22:53,000 --> 00:22:55,750
breaks the text into huge number of
tokens. So subword tokenization is that

1166
00:22:55,750 --> 00:22:55,760
tokens. So subword tokenization is that
 

1167
00:22:55,760 --> 00:22:57,909
tokens. So subword tokenization is that
optimal scheme. So usually what is done

1168
00:22:57,909 --> 00:22:57,919
optimal scheme. So usually what is done
 

1169
00:22:57,919 --> 00:22:59,669
optimal scheme. So usually what is done
in subboard tokenization is that there

1170
00:22:59,669 --> 00:22:59,679
in subboard tokenization is that there
 

1171
00:22:59,679 --> 00:23:01,190
in subboard tokenization is that there
is an algorithm which is called as the

1172
00:23:01,190 --> 00:23:01,200
is an algorithm which is called as the
 

1173
00:23:01,200 --> 00:23:03,870
is an algorithm which is called as the
bite pair encoding

1174
00:23:03,870 --> 00:23:03,880
bite pair encoding
 

1175
00:23:03,880 --> 00:23:06,470
bite pair encoding
algorithm. Bite pair encoding algorithm

1176
00:23:06,470 --> 00:23:06,480
algorithm. Bite pair encoding algorithm
 

1177
00:23:06,480 --> 00:23:09,029
algorithm. Bite pair encoding algorithm
and that's also called BPE. This is an

1178
00:23:09,029 --> 00:23:09,039
and that's also called BPE. This is an
 

1179
00:23:09,039 --> 00:23:13,029
and that's also called BPE. This is an
algorithm which takes in a data set.

1180
00:23:13,029 --> 00:23:13,039
algorithm which takes in a data set.
 

1181
00:23:13,039 --> 00:23:15,430
algorithm which takes in a data set.
This algorithm takes in a data set and

1182
00:23:15,430 --> 00:23:15,440
This algorithm takes in a data set and
 

1183
00:23:15,440 --> 00:23:17,909
This algorithm takes in a data set and
it gives us the subwords or it gives us

1184
00:23:17,909 --> 00:23:17,919
it gives us the subwords or it gives us
 

1185
00:23:17,919 --> 00:23:20,390
it gives us the subwords or it gives us
the list of tokens which the data set

1186
00:23:20,390 --> 00:23:20,400
the list of tokens which the data set
 

1187
00:23:20,400 --> 00:23:22,110
the list of tokens which the data set
needs to be converted

1188
00:23:22,110 --> 00:23:22,120
needs to be converted
 

1189
00:23:22,120 --> 00:23:24,710
needs to be converted
into. Right? And these tokens as I

1190
00:23:24,710 --> 00:23:24,720
into. Right? And these tokens as I
 

1191
00:23:24,720 --> 00:23:26,710
into. Right? And these tokens as I
mentioned can be characters, these can

1192
00:23:26,710 --> 00:23:26,720
mentioned can be characters, these can
 

1193
00:23:26,720 --> 00:23:29,430
mentioned can be characters, these can
be words or these can be subwords. All

1194
00:23:29,430 --> 00:23:29,440
be words or these can be subwords. All
 

1195
00:23:29,440 --> 00:23:31,590
be words or these can be subwords. All
three are possible. Essentially what

1196
00:23:31,590 --> 00:23:31,600
three are possible. Essentially what
 

1197
00:23:31,600 --> 00:23:33,430
three are possible. Essentially what
bite pair encoding does is that it just

1198
00:23:33,430 --> 00:23:33,440
bite pair encoding does is that it just
 

1199
00:23:33,440 --> 00:23:35,190
bite pair encoding does is that it just
merges the commonly occurring bytes

1200
00:23:35,190 --> 00:23:35,200
merges the commonly occurring bytes
 

1201
00:23:35,200 --> 00:23:37,110
merges the commonly occurring bytes
together until you reach a prescribed

1202
00:23:37,110 --> 00:23:37,120
together until you reach a prescribed
 

1203
00:23:37,120 --> 00:23:39,750
together until you reach a prescribed
vocabulary size. I'm not going into too

1204
00:23:39,750 --> 00:23:39,760
vocabulary size. I'm not going into too
 

1205
00:23:39,760 --> 00:23:41,669
vocabulary size. I'm not going into too
many details of bite pair encoding here

1206
00:23:41,669 --> 00:23:41,679
many details of bite pair encoding here
 

1207
00:23:41,679 --> 00:23:44,390
many details of bite pair encoding here
for the purposes of this tutorial. It's

1208
00:23:44,390 --> 00:23:44,400
for the purposes of this tutorial. It's
 

1209
00:23:44,400 --> 00:23:45,990
for the purposes of this tutorial. It's
okay if you have a higher level

1210
00:23:45,990 --> 00:23:46,000
okay if you have a higher level
 

1211
00:23:46,000 --> 00:23:47,830
okay if you have a higher level
understanding of what is exactly done in

1212
00:23:47,830 --> 00:23:47,840
understanding of what is exactly done in
 

1213
00:23:47,840 --> 00:23:50,390
understanding of what is exactly done in
bite pair encoding. So what now we are

1214
00:23:50,390 --> 00:23:50,400
bite pair encoding. So what now we are
 

1215
00:23:50,400 --> 00:23:51,830
bite pair encoding. So what now we are
going to do is that we are going to feed

1216
00:23:51,830 --> 00:23:51,840
going to do is that we are going to feed
 

1217
00:23:51,840 --> 00:23:53,990
going to do is that we are going to feed
our entire data set into this bite pair

1218
00:23:53,990 --> 00:23:54,000
our entire data set into this bite pair
 

1219
00:23:54,000 --> 00:23:56,789
our entire data set into this bite pair
encoding model and this is also called

1220
00:23:56,789 --> 00:23:56,799
encoding model and this is also called
 

1221
00:23:56,799 --> 00:23:59,590
encoding model and this is also called
as the tokenizer. So the tokenizer what

1222
00:23:59,590 --> 00:23:59,600
as the tokenizer. So the tokenizer what
 

1223
00:23:59,600 --> 00:24:01,830
as the tokenizer. So the tokenizer what
the tokenizer does is that the tokenizer

1224
00:24:01,830 --> 00:24:01,840
the tokenizer does is that the tokenizer
 

1225
00:24:01,840 --> 00:24:03,710
the tokenizer does is that the tokenizer
takes our data

1226
00:24:03,710 --> 00:24:03,720
takes our data
 

1227
00:24:03,720 --> 00:24:08,110
takes our data
set. The tokenizer takes our data

1228
00:24:08,110 --> 00:24:08,120
set. The tokenizer takes our data
 

1229
00:24:08,120 --> 00:24:11,190
set. The tokenizer takes our data
set and then it converts it into a bunch

1230
00:24:11,190 --> 00:24:11,200
set and then it converts it into a bunch
 

1231
00:24:11,200 --> 00:24:12,269
set and then it converts it into a bunch
of

1232
00:24:12,269 --> 00:24:12,279
of
 

1233
00:24:12,279 --> 00:24:15,590
of
tokens. These tokens are each individual

1234
00:24:15,590 --> 00:24:15,600
tokens. These tokens are each individual
 

1235
00:24:15,600 --> 00:24:17,830
tokens. These tokens are each individual
units and then every token will

1236
00:24:17,830 --> 00:24:17,840
units and then every token will
 

1237
00:24:17,840 --> 00:24:20,510
units and then every token will
essentially be assigned a token

1238
00:24:20,510 --> 00:24:20,520
essentially be assigned a token
 

1239
00:24:20,520 --> 00:24:22,950
essentially be assigned a token
ID. That's this whole process which

1240
00:24:22,950 --> 00:24:22,960
ID. That's this whole process which
 

1241
00:24:22,960 --> 00:24:24,630
ID. That's this whole process which
needs to happen at the start before a

1242
00:24:24,630 --> 00:24:24,640
needs to happen at the start before a
 

1243
00:24:24,640 --> 00:24:26,950
needs to happen at the start before a
data is fed to the transformer block.

1244
00:24:26,950 --> 00:24:26,960
data is fed to the transformer block.
 

1245
00:24:26,960 --> 00:24:29,029
data is fed to the transformer block.
There are many other steps also but the

1246
00:24:29,029 --> 00:24:29,039
There are many other steps also but the
 

1247
00:24:29,039 --> 00:24:31,510
There are many other steps also but the
first thing is you take the data set and

1248
00:24:31,510 --> 00:24:31,520
first thing is you take the data set and
 

1249
00:24:31,520 --> 00:24:33,510
first thing is you take the data set and
then what you do is that you convert the

1250
00:24:33,510 --> 00:24:33,520
then what you do is that you convert the
 

1251
00:24:33,520 --> 00:24:37,590
then what you do is that you convert the
data set into uh essentially a bunch of

1252
00:24:37,590 --> 00:24:37,600
data set into uh essentially a bunch of
 

1253
00:24:37,600 --> 00:24:40,149
data set into uh essentially a bunch of
tokens and then you assign the tok each

1254
00:24:40,149 --> 00:24:40,159
tokens and then you assign the tok each
 

1255
00:24:40,159 --> 00:24:42,789
tokens and then you assign the tok each
token a token ID and the way the data

1256
00:24:42,789 --> 00:24:42,799
token a token ID and the way the data
 

1257
00:24:42,799 --> 00:24:44,310
token a token ID and the way the data
sets are converted into a bunch of

1258
00:24:44,310 --> 00:24:44,320
sets are converted into a bunch of
 

1259
00:24:44,320 --> 00:24:46,950
sets are converted into a bunch of
tokens is through the tokenizer. Now we

1260
00:24:46,950 --> 00:24:46,960
tokens is through the tokenizer. Now we
 

1261
00:24:46,960 --> 00:24:49,470
tokens is through the tokenizer. Now we
are going to use a tokenizer used by

1262
00:24:49,470 --> 00:24:49,480
are going to use a tokenizer used by
 

1263
00:24:49,480 --> 00:24:52,149
are going to use a tokenizer used by
GPT2 which is the same as what I had

1264
00:24:52,149 --> 00:24:52,159
GPT2 which is the same as what I had
 

1265
00:24:52,159 --> 00:24:53,750
GPT2 which is the same as what I had
mentioned earlier that's the bite pair

1266
00:24:53,750 --> 00:24:53,760
mentioned earlier that's the bite pair
 

1267
00:24:53,760 --> 00:24:57,269
mentioned earlier that's the bite pair
encoding. Okay. And we are going to pass

1268
00:24:57,269 --> 00:24:57,279
encoding. Okay. And we are going to pass
 

1269
00:24:57,279 --> 00:24:59,269
encoding. Okay. And we are going to pass
all of our stories over here as the data

1270
00:24:59,269 --> 00:24:59,279
all of our stories over here as the data
 

1271
00:24:59,279 --> 00:25:00,950
all of our stories over here as the data
set. We are going to pass all of our

1272
00:25:00,950 --> 00:25:00,960
set. We are going to pass all of our
 

1273
00:25:00,960 --> 00:25:04,310
set. We are going to pass all of our
stories over here. But instead of uh

1274
00:25:04,310 --> 00:25:04,320
stories over here. But instead of uh
 

1275
00:25:04,320 --> 00:25:06,230
stories over here. But instead of uh
just passing every story and converting

1276
00:25:06,230 --> 00:25:06,240
just passing every story and converting
 

1277
00:25:06,240 --> 00:25:10,149
just passing every story and converting
it into a bunch of tokens, there is uh

1278
00:25:10,149 --> 00:25:10,159
it into a bunch of tokens, there is uh
 

1279
00:25:10,159 --> 00:25:11,230
it into a bunch of tokens, there is uh
uh there is

1280
00:25:11,230 --> 00:25:11,240
uh there is
 

1281
00:25:11,240 --> 00:25:14,070
uh there is
a common way or there is a better way to

1282
00:25:14,070 --> 00:25:14,080
a common way or there is a better way to
 

1283
00:25:14,080 --> 00:25:17,750
a common way or there is a better way to
do this which is good in terms of uh

1284
00:25:17,750 --> 00:25:17,760
do this which is good in terms of uh
 

1285
00:25:17,760 --> 00:25:20,470
do this which is good in terms of uh
computational efficiency. And for this I

1286
00:25:20,470 --> 00:25:20,480
computational efficiency. And for this I
 

1287
00:25:20,480 --> 00:25:23,510
computational efficiency. And for this I
have borrowed this part of the logic

1288
00:25:23,510 --> 00:25:23,520
have borrowed this part of the logic
 

1289
00:25:23,520 --> 00:25:27,029
have borrowed this part of the logic
from Andre Karpati's nano GPT. This can

1290
00:25:27,029 --> 00:25:27,039
from Andre Karpati's nano GPT. This can
 

1291
00:25:27,039 --> 00:25:30,230
from Andre Karpati's nano GPT. This can
be a good time to mention that Andre

1292
00:25:30,230 --> 00:25:30,240
be a good time to mention that Andre
 

1293
00:25:30,240 --> 00:25:33,430
be a good time to mention that Andre
Karpath's nano GPT code has inspired a

1294
00:25:33,430 --> 00:25:33,440
Karpath's nano GPT code has inspired a
 

1295
00:25:33,440 --> 00:25:35,350
Karpath's nano GPT code has inspired a
lot of portions of this small language

1296
00:25:35,350 --> 00:25:35,360
lot of portions of this small language
 

1297
00:25:35,360 --> 00:25:37,590
lot of portions of this small language
model tutorial and I really want to

1298
00:25:37,590 --> 00:25:37,600
model tutorial and I really want to
 

1299
00:25:37,600 --> 00:25:39,070
model tutorial and I really want to
thank

1300
00:25:39,070 --> 00:25:39,080
thank
 

1301
00:25:39,080 --> 00:25:41,669
thank
um thank him for putting out such

1302
00:25:41,669 --> 00:25:41,679
um thank him for putting out such
 

1303
00:25:41,679 --> 00:25:44,149
um thank him for putting out such
incredible content out there from which

1304
00:25:44,149 --> 00:25:44,159
incredible content out there from which
 

1305
00:25:44,159 --> 00:25:46,310
incredible content out there from which
all of us are benefiting. So I've tried

1306
00:25:46,310 --> 00:25:46,320
all of us are benefiting. So I've tried
 

1307
00:25:46,320 --> 00:25:48,149
all of us are benefiting. So I've tried
to take that repository and put it into

1308
00:25:48,149 --> 00:25:48,159
to take that repository and put it into
 

1309
00:25:48,159 --> 00:25:50,190
to take that repository and put it into
a format which is much more easily

1310
00:25:50,190 --> 00:25:50,200
a format which is much more easily
 

1311
00:25:50,200 --> 00:25:52,310
a format which is much more easily
digestible and when everything is

1312
00:25:52,310 --> 00:25:52,320
digestible and when everything is
 

1313
00:25:52,320 --> 00:25:54,310
digestible and when everything is
exposed instead of directly importing

1314
00:25:54,310 --> 00:25:54,320
exposed instead of directly importing
 

1315
00:25:54,320 --> 00:25:55,710
exposed instead of directly importing
some things from his

1316
00:25:55,710 --> 00:25:55,720
some things from his
 

1317
00:25:55,720 --> 00:25:57,990
some things from his
library. So this is what we are going to

1318
00:25:57,990 --> 00:25:58,000
library. So this is what we are going to
 

1319
00:25:58,000 --> 00:26:01,830
library. So this is what we are going to
do with respect to the uh tokenization.

1320
00:26:01,830 --> 00:26:01,840
do with respect to the uh tokenization.
 

1321
00:26:01,840 --> 00:26:03,830
do with respect to the uh tokenization.
So let me show you what exactly we are

1322
00:26:03,830 --> 00:26:03,840
So let me show you what exactly we are
 

1323
00:26:03,840 --> 00:26:06,470
So let me show you what exactly we are
going to do. Okay. So now our data set

1324
00:26:06,470 --> 00:26:06,480
going to do. Okay. So now our data set
 

1325
00:26:06,480 --> 00:26:08,470
going to do. Okay. So now our data set
looks like this right. Every row in our

1326
00:26:08,470 --> 00:26:08,480
looks like this right. Every row in our
 

1327
00:26:08,480 --> 00:26:11,310
looks like this right. Every row in our
data set essentially corresponds to a

1328
00:26:11,310 --> 00:26:11,320
data set essentially corresponds to a
 

1329
00:26:11,320 --> 00:26:13,909
data set essentially corresponds to a
story. Uh the first row corresponds to

1330
00:26:13,909 --> 00:26:13,919
story. Uh the first row corresponds to
 

1331
00:26:13,919 --> 00:26:15,510
story. Uh the first row corresponds to
the first story. The second row

1332
00:26:15,510 --> 00:26:15,520
the first story. The second row
 

1333
00:26:15,520 --> 00:26:17,269
the first story. The second row
corresponds to the second story. The

1334
00:26:17,269 --> 00:26:17,279
corresponds to the second story. The
 

1335
00:26:17,279 --> 00:26:19,350
corresponds to the second story. The
third row corresponds to the third story

1336
00:26:19,350 --> 00:26:19,360
third row corresponds to the third story
 

1337
00:26:19,360 --> 00:26:22,070
third row corresponds to the third story
etc. There are 2 million such rows for

1338
00:26:22,070 --> 00:26:22,080
etc. There are 2 million such rows for
 

1339
00:26:22,080 --> 00:26:24,310
etc. There are 2 million such rows for
training and 20,000 such rows for

1340
00:26:24,310 --> 00:26:24,320
training and 20,000 such rows for
 

1341
00:26:24,320 --> 00:26:26,310
training and 20,000 such rows for
validation. So 2 million stories for

1342
00:26:26,310 --> 00:26:26,320
validation. So 2 million stories for
 

1343
00:26:26,320 --> 00:26:28,110
validation. So 2 million stories for
training and 20,000 stories for

1344
00:26:28,110 --> 00:26:28,120
training and 20,000 stories for
 

1345
00:26:28,120 --> 00:26:30,310
training and 20,000 stories for
validation. We have already seen this in

1346
00:26:30,310 --> 00:26:30,320
validation. We have already seen this in
 

1347
00:26:30,320 --> 00:26:32,870
validation. We have already seen this in
our data set over here. 2 million 2.12

1348
00:26:32,870 --> 00:26:32,880
our data set over here. 2 million 2.12
 

1349
00:26:32,880 --> 00:26:35,669
our data set over here. 2 million 2.12
million training rows and around 22,000

1350
00:26:35,669 --> 00:26:35,679
million training rows and around 22,000
 

1351
00:26:35,679 --> 00:26:38,029
million training rows and around 22,000
validation

1352
00:26:38,029 --> 00:26:38,039
validation
 

1353
00:26:38,039 --> 00:26:40,870
validation
rows. All right. So now what we are

1354
00:26:40,870 --> 00:26:40,880
rows. All right. So now what we are
 

1355
00:26:40,880 --> 00:26:42,870
rows. All right. So now what we are
going to do is that I'm going to take a

1356
00:26:42,870 --> 00:26:42,880
going to do is that I'm going to take a
 

1357
00:26:42,880 --> 00:26:44,870
going to do is that I'm going to take a
small sample from our data set and I'm

1358
00:26:44,870 --> 00:26:44,880
small sample from our data set and I'm
 

1359
00:26:44,880 --> 00:26:46,549
small sample from our data set and I'm
going to show you how we are going to do

1360
00:26:46,549 --> 00:26:46,559
going to show you how we are going to do
 

1361
00:26:46,559 --> 00:26:49,510
going to show you how we are going to do
the tokenization. So I have pasted a

1362
00:26:49,510 --> 00:26:49,520
the tokenization. So I have pasted a
 

1363
00:26:49,520 --> 00:26:51,750
the tokenization. So I have pasted a
screenshot of this sample over here.

1364
00:26:51,750 --> 00:26:51,760
screenshot of this sample over here.
 

1365
00:26:51,760 --> 00:26:54,230
screenshot of this sample over here.
This is let's say one such story and our

1366
00:26:54,230 --> 00:26:54,240
This is let's say one such story and our
 

1367
00:26:54,240 --> 00:26:56,070
This is let's say one such story and our
goal is to do two things right. We have

1368
00:26:56,070 --> 00:26:56,080
goal is to do two things right. We have
 

1369
00:26:56,080 --> 00:26:57,990
goal is to do two things right. We have
to tokenize this data set which is we

1370
00:26:57,990 --> 00:26:58,000
to tokenize this data set which is we
 

1371
00:26:58,000 --> 00:27:00,070
to tokenize this data set which is we
need to convert it into tokens and we

1372
00:27:00,070 --> 00:27:00,080
need to convert it into tokens and we
 

1373
00:27:00,080 --> 00:27:02,310
need to convert it into tokens and we
need to store all the token ids in one

1374
00:27:02,310 --> 00:27:02,320
need to store all the token ids in one
 

1375
00:27:02,320 --> 00:27:04,470
need to store all the token ids in one
place. So remember I mentioned to you

1376
00:27:04,470 --> 00:27:04,480
place. So remember I mentioned to you
 

1377
00:27:04,480 --> 00:27:09,110
place. So remember I mentioned to you
that uh the every token will be assigned

1378
00:27:09,110 --> 00:27:09,120
that uh the every token will be assigned
 

1379
00:27:09,120 --> 00:27:11,510
that uh the every token will be assigned
a token ID. Essentially what we need to

1380
00:27:11,510 --> 00:27:11,520
a token ID. Essentially what we need to
 

1381
00:27:11,520 --> 00:27:14,230
a token ID. Essentially what we need to
do is that we need to take every single

1382
00:27:14,230 --> 00:27:14,240
do is that we need to take every single
 

1383
00:27:14,240 --> 00:27:16,549
do is that we need to take every single
story and within every single story we

1384
00:27:16,549 --> 00:27:16,559
story and within every single story we
 

1385
00:27:16,559 --> 00:27:18,549
story and within every single story we
need to convert every token into a token

1386
00:27:18,549 --> 00:27:18,559
need to convert every token into a token
 

1387
00:27:18,559 --> 00:27:21,430
need to convert every token into a token
ID and merge all of these token ids

1388
00:27:21,430 --> 00:27:21,440
ID and merge all of these token ids
 

1389
00:27:21,440 --> 00:27:24,190
ID and merge all of these token ids
together. That will be our training data

1390
00:27:24,190 --> 00:27:24,200
together. That will be our training data
 

1391
00:27:24,200 --> 00:27:28,190
together. That will be our training data
set. And the way this is done is as

1392
00:27:28,190 --> 00:27:28,200
set. And the way this is done is as
 

1393
00:27:28,200 --> 00:27:31,350
set. And the way this is done is as
follows. So first we are going to use

1394
00:27:31,350 --> 00:27:31,360
follows. So first we are going to use
 

1395
00:27:31,360 --> 00:27:34,310
follows. So first we are going to use
this GPT2 subword tokenizer. We already

1396
00:27:34,310 --> 00:27:34,320
this GPT2 subword tokenizer. We already
 

1397
00:27:34,320 --> 00:27:35,909
this GPT2 subword tokenizer. We already
saw about this that is the bite pair

1398
00:27:35,909 --> 00:27:35,919
saw about this that is the bite pair
 

1399
00:27:35,919 --> 00:27:38,710
saw about this that is the bite pair
encoding tokenizer and we are going to

1400
00:27:38,710 --> 00:27:38,720
encoding tokenizer and we are going to
 

1401
00:27:38,720 --> 00:27:41,510
encoding tokenizer and we are going to
apply this on every single story. So

1402
00:27:41,510 --> 00:27:41,520
apply this on every single story. So
 

1403
00:27:41,520 --> 00:27:43,190
apply this on every single story. So
let's say when this bite pair encoding

1404
00:27:43,190 --> 00:27:43,200
let's say when this bite pair encoding
 

1405
00:27:43,200 --> 00:27:45,590
let's say when this bite pair encoding
tokenizer is applied on this story we

1406
00:27:45,590 --> 00:27:45,600
tokenizer is applied on this story we
 

1407
00:27:45,600 --> 00:27:48,950
tokenizer is applied on this story we
get certain tokens like these. Okay. So

1408
00:27:48,950 --> 00:27:48,960
get certain tokens like these. Okay. So
 

1409
00:27:48,960 --> 00:27:53,750
get certain tokens like these. Okay. So
that can be 442 518 11 257 etc. We are

1410
00:27:53,750 --> 00:27:53,760
that can be 442 518 11 257 etc. We are
 

1411
00:27:53,760 --> 00:27:56,710
that can be 442 518 11 257 etc. We are
going to store these tokens into uh

1412
00:27:56,710 --> 00:27:56,720
going to store these tokens into uh
 

1413
00:27:56,720 --> 00:27:59,190
going to store these tokens into uh
let's say dictionary with ids. So these

1414
00:27:59,190 --> 00:27:59,200
let's say dictionary with ids. So these
 

1415
00:27:59,200 --> 00:28:01,190
let's say dictionary with ids. So these
are my ids and my length is going to be

1416
00:28:01,190 --> 00:28:01,200
are my ids and my length is going to be
 

1417
00:28:01,200 --> 00:28:03,669
are my ids and my length is going to be
30. That's the length of the token ids

1418
00:28:03,669 --> 00:28:03,679
30. That's the length of the token ids
 

1419
00:28:03,679 --> 00:28:06,389
30. That's the length of the token ids
in one story. So every story I'm going

1420
00:28:06,389 --> 00:28:06,399
in one story. So every story I'm going
 

1421
00:28:06,399 --> 00:28:07,950
in one story. So every story I'm going
to store like this. This is the first

1422
00:28:07,950 --> 00:28:07,960
to store like this. This is the first
 

1423
00:28:07,960 --> 00:28:10,470
to store like this. This is the first
story. This is the first story where I

1424
00:28:10,470 --> 00:28:10,480
story. This is the first story where I
 

1425
00:28:10,480 --> 00:28:12,789
story. This is the first story where I
store the token ids and the length of

1426
00:28:12,789 --> 00:28:12,799
store the token ids and the length of
 

1427
00:28:12,799 --> 00:28:16,310
store the token ids and the length of
how many token ids I have. So 1 2 3 4 5

1428
00:28:16,310 --> 00:28:16,320
how many token ids I have. So 1 2 3 4 5
 

1429
00:28:16,320 --> 00:28:19,590
how many token ids I have. So 1 2 3 4 5
6 7 8 9 10 11 12 13. So that's why this

1430
00:28:19,590 --> 00:28:19,600
6 7 8 9 10 11 12 13. So that's why this
 

1431
00:28:19,600 --> 00:28:22,230
6 7 8 9 10 11 12 13. So that's why this
is 13. I'm similarly going to apply

1432
00:28:22,230 --> 00:28:22,240
is 13. I'm similarly going to apply
 

1433
00:28:22,240 --> 00:28:25,510
is 13. I'm similarly going to apply
tokenization to my entire data set. So

1434
00:28:25,510 --> 00:28:25,520
tokenization to my entire data set. So
 

1435
00:28:25,520 --> 00:28:27,990
tokenization to my entire data set. So
my first story will again have ids which

1436
00:28:27,990 --> 00:28:28,000
my first story will again have ids which
 

1437
00:28:28,000 --> 00:28:30,149
my first story will again have ids which
are these and it will have this length.

1438
00:28:30,149 --> 00:28:30,159
are these and it will have this length.
 

1439
00:28:30,159 --> 00:28:32,389
are these and it will have this length.
My second story will have a set of token

1440
00:28:32,389 --> 00:28:32,399
My second story will have a set of token
 

1441
00:28:32,399 --> 00:28:34,389
My second story will have a set of token
ids which have this length. My third

1442
00:28:34,389 --> 00:28:34,399
ids which have this length. My third
 

1443
00:28:34,399 --> 00:28:36,549
ids which have this length. My third
story will have again token ids and a

1444
00:28:36,549 --> 00:28:36,559
story will have again token ids and a
 

1445
00:28:36,559 --> 00:28:38,870
story will have again token ids and a
length. Similarly my last story which is

1446
00:28:38,870 --> 00:28:38,880
length. Similarly my last story which is
 

1447
00:28:38,880 --> 00:28:41,269
length. Similarly my last story which is
now 2.12 million will have a bunch of

1448
00:28:41,269 --> 00:28:41,279
now 2.12 million will have a bunch of
 

1449
00:28:41,279 --> 00:28:44,549
now 2.12 million will have a bunch of
token ids and the length. So now what I

1450
00:28:44,549 --> 00:28:44,559
token ids and the length. So now what I
 

1451
00:28:44,559 --> 00:28:46,549
token ids and the length. So now what I
want to do is that I don't want to store

1452
00:28:46,549 --> 00:28:46,559
want to do is that I don't want to store
 

1453
00:28:46,559 --> 00:28:49,110
want to do is that I don't want to store
separate token IDs at separate places

1454
00:28:49,110 --> 00:28:49,120
separate token IDs at separate places
 

1455
00:28:49,120 --> 00:28:51,669
separate token IDs at separate places
for each story. I want to merge all of

1456
00:28:51,669 --> 00:28:51,679
for each story. I want to merge all of
 

1457
00:28:51,679 --> 00:28:53,669
for each story. I want to merge all of
these token ids together so that I can

1458
00:28:53,669 --> 00:28:53,679
these token ids together so that I can
 

1459
00:28:53,679 --> 00:28:56,389
these token ids together so that I can
store them in one place. Right? And now

1460
00:28:56,389 --> 00:28:56,399
store them in one place. Right? And now
 

1461
00:28:56,399 --> 00:28:58,230
store them in one place. Right? And now
we are going to see an efficient way of

1462
00:28:58,230 --> 00:28:58,240
we are going to see an efficient way of
 

1463
00:28:58,240 --> 00:28:59,549
we are going to see an efficient way of
doing this

1464
00:28:59,549 --> 00:28:59,559
doing this
 

1465
00:28:59,559 --> 00:29:02,070
doing this
merging. So the goal is to store all the

1466
00:29:02,070 --> 00:29:02,080
merging. So the goal is to store all the
 

1467
00:29:02,080 --> 00:29:05,669
merging. So the goal is to store all the
token ids in a single dot bin file. The

1468
00:29:05,669 --> 00:29:05,679
token ids in a single dot bin file. The
 

1469
00:29:05,679 --> 00:29:07,750
token ids in a single dot bin file. The
reason we have bin file is because we'll

1470
00:29:07,750 --> 00:29:07,760
reason we have bin file is because we'll
 

1471
00:29:07,760 --> 00:29:09,990
reason we have bin file is because we'll
store the file directly on the disk

1472
00:29:09,990 --> 00:29:10,000
store the file directly on the disk
 

1473
00:29:10,000 --> 00:29:13,110
store the file directly on the disk
instead of storing it on the RAM. That

1474
00:29:13,110 --> 00:29:13,120
instead of storing it on the RAM. That
 

1475
00:29:13,120 --> 00:29:16,149
instead of storing it on the RAM. That
avoids LA RAM overload on big data sets.

1476
00:29:16,149 --> 00:29:16,159
avoids LA RAM overload on big data sets.
 

1477
00:29:16,159 --> 00:29:18,310
avoids LA RAM overload on big data sets.
So believe me, I've tried running this

1478
00:29:18,310 --> 00:29:18,320
So believe me, I've tried running this
 

1479
00:29:18,320 --> 00:29:20,950
So believe me, I've tried running this
code without using the bin and it takes

1480
00:29:20,950 --> 00:29:20,960
code without using the bin and it takes
 

1481
00:29:20,960 --> 00:29:23,990
code without using the bin and it takes
a huge amount of time to run this and

1482
00:29:23,990 --> 00:29:24,000
a huge amount of time to run this and
 

1483
00:29:24,000 --> 00:29:26,389
a huge amount of time to run this and
the entire code becomes very slow. But

1484
00:29:26,389 --> 00:29:26,399
the entire code becomes very slow. But
 

1485
00:29:26,399 --> 00:29:28,950
the entire code becomes very slow. But
if you just do this bin, if you create a

1486
00:29:28,950 --> 00:29:28,960
if you just do this bin, if you create a
 

1487
00:29:28,960 --> 00:29:30,630
if you just do this bin, if you create a
dot bin file and if you store all the

1488
00:29:30,630 --> 00:29:30,640
dot bin file and if you store all the
 

1489
00:29:30,640 --> 00:29:33,350
dot bin file and if you store all the
token in dotbin, you essentially store

1490
00:29:33,350 --> 00:29:33,360
token in dotbin, you essentially store
 

1491
00:29:33,360 --> 00:29:35,990
token in dotbin, you essentially store
everything on the disk, right? Since

1492
00:29:35,990 --> 00:29:36,000
everything on the disk, right? Since
 

1493
00:29:36,000 --> 00:29:39,669
everything on the disk, right? Since
everything is stored on the disk, um the

1494
00:29:39,669 --> 00:29:39,679
everything is stored on the disk, um the
 

1495
00:29:39,679 --> 00:29:41,669
everything is stored on the disk, um the
data can be loaded very fast during the

1496
00:29:41,669 --> 00:29:41,679
data can be loaded very fast during the
 

1497
00:29:41,679 --> 00:29:43,350
data can be loaded very fast during the
training. You just retrieve it from the

1498
00:29:43,350 --> 00:29:43,360
training. You just retrieve it from the
 

1499
00:29:43,360 --> 00:29:46,789
training. You just retrieve it from the
disk. And secondly our RAM uh which is

1500
00:29:46,789 --> 00:29:46,799
disk. And secondly our RAM uh which is
 

1501
00:29:46,799 --> 00:29:49,590
disk. And secondly our RAM uh which is
very precious for uh faster computations

1502
00:29:49,590 --> 00:29:49,600
very precious for uh faster computations
 

1503
00:29:49,600 --> 00:29:51,750
very precious for uh faster computations
that does not get overloaded overloaded

1504
00:29:51,750 --> 00:29:51,760
that does not get overloaded overloaded
 

1505
00:29:51,760 --> 00:29:54,149
that does not get overloaded overloaded
with huge amount of data especi

1506
00:29:54,149 --> 00:29:54,159
with huge amount of data especi
 

1507
00:29:54,159 --> 00:29:55,669
with huge amount of data especi
especially since we have around 2

1508
00:29:55,669 --> 00:29:55,679
especially since we have around 2
 

1509
00:29:55,679 --> 00:29:58,230
especially since we have around 2
million rows right that is why we using

1510
00:29:58,230 --> 00:29:58,240
million rows right that is why we using
 

1511
00:29:58,240 --> 00:30:00,549
million rows right that is why we using
dot bin and another advantage of storage

1512
00:30:00,549 --> 00:30:00,559
dot bin and another advantage of storage
 

1513
00:30:00,559 --> 00:30:02,230
dot bin and another advantage of storage
on the disk is that we don't need to

1514
00:30:02,230 --> 00:30:02,240
on the disk is that we don't need to
 

1515
00:30:02,240 --> 00:30:04,389
on the disk is that we don't need to
retokenize every time it is stored on

1516
00:30:04,389 --> 00:30:04,399
retokenize every time it is stored on
 

1517
00:30:04,399 --> 00:30:06,630
retokenize every time it is stored on
the disk during multiple sessions. So

1518
00:30:06,630 --> 00:30:06,640
the disk during multiple sessions. So
 

1519
00:30:06,640 --> 00:30:08,950
the disk during multiple sessions. So
once we tokenize that's it the token ids

1520
00:30:08,950 --> 00:30:08,960
once we tokenize that's it the token ids
 

1521
00:30:08,960 --> 00:30:11,110
once we tokenize that's it the token ids
are all stored on the disk. every time

1522
00:30:11,110 --> 00:30:11,120
are all stored on the disk. every time
 

1523
00:30:11,120 --> 00:30:13,750
are all stored on the disk. every time
you restart let's say you don't need to

1524
00:30:13,750 --> 00:30:13,760
you restart let's say you don't need to
 

1525
00:30:13,760 --> 00:30:16,870
you restart let's say you don't need to
uh retokenize again so that's why we

1526
00:30:16,870 --> 00:30:16,880
uh retokenize again so that's why we
 

1527
00:30:16,880 --> 00:30:20,230
uh retokenize again so that's why we
store everything in a dot bin file right

1528
00:30:20,230 --> 00:30:20,240
store everything in a dot bin file right
 

1529
00:30:20,240 --> 00:30:22,149
store everything in a dot bin file right
uh currently we have something like this

1530
00:30:22,149 --> 00:30:22,159
uh currently we have something like this
 

1531
00:30:22,159 --> 00:30:24,549
uh currently we have something like this
for every story we have a bunch of token

1532
00:30:24,549 --> 00:30:24,559
for every story we have a bunch of token
 

1533
00:30:24,559 --> 00:30:26,070
for every story we have a bunch of token
ids and we have length of the number of

1534
00:30:26,070 --> 00:30:26,080
ids and we have length of the number of
 

1535
00:30:26,080 --> 00:30:28,070
ids and we have length of the number of
tokens so that is for story one story

1536
00:30:28,070 --> 00:30:28,080
tokens so that is for story one story
 

1537
00:30:28,080 --> 00:30:31,430
tokens so that is for story one story
two etc right so what we'll do is that

1538
00:30:31,430 --> 00:30:31,440
two etc right so what we'll do is that
 

1539
00:30:31,440 --> 00:30:33,669
two etc right so what we'll do is that
we'll create a dot bin file as I

1540
00:30:33,669 --> 00:30:33,679
we'll create a dot bin file as I
 

1541
00:30:33,679 --> 00:30:35,590
we'll create a dot bin file as I
mentioned so the dot bill file can be

1542
00:30:35,590 --> 00:30:35,600
mentioned so the dot bill file can be
 

1543
00:30:35,600 --> 00:30:38,350
mentioned so the dot bill file can be
named something like

1544
00:30:38,350 --> 00:30:38,360
named something like
 

1545
00:30:38,360 --> 00:30:40,870
named something like
trainbin we'll see this in the code also

1546
00:30:40,870 --> 00:30:40,880
trainbin we'll see this in the code also
 

1547
00:30:40,880 --> 00:30:42,630
trainbin we'll see this in the code also
we'll create a dot bin file which is

1548
00:30:42,630 --> 00:30:42,640
we'll create a dot bin file which is
 

1549
00:30:42,640 --> 00:30:45,029
we'll create a dot bin file which is
named at train.bin pin and we'll create

1550
00:30:45,029 --> 00:30:45,039
named at train.bin pin and we'll create
 

1551
00:30:45,039 --> 00:30:47,750
named at train.bin pin and we'll create
a memory mapped array. Why do we create

1552
00:30:47,750 --> 00:30:47,760
a memory mapped array. Why do we create
 

1553
00:30:47,760 --> 00:30:50,470
a memory mapped array. Why do we create
a memory mapped array? Uh because it

1554
00:30:50,470 --> 00:30:50,480
a memory mapped array? Uh because it
 

1555
00:30:50,480 --> 00:30:52,230
a memory mapped array? Uh because it
means that the file is backed onto the

1556
00:30:52,230 --> 00:30:52,240
means that the file is backed onto the
 

1557
00:30:52,240 --> 00:30:54,630
means that the file is backed onto the
disk. So as I mentioned over here, we

1558
00:30:54,630 --> 00:30:54,640
disk. So as I mentioned over here, we
 

1559
00:30:54,640 --> 00:30:57,190
disk. So as I mentioned over here, we
need to do disk storage and for that we

1560
00:30:57,190 --> 00:30:57,200
need to do disk storage and for that we
 

1561
00:30:57,200 --> 00:30:58,870
need to do disk storage and for that we
need to create an array which is memory

1562
00:30:58,870 --> 00:30:58,880
need to create an array which is memory
 

1563
00:30:58,880 --> 00:31:01,190
need to create an array which is memory
mapped which means it will get stored on

1564
00:31:01,190 --> 00:31:01,200
mapped which means it will get stored on
 

1565
00:31:01,200 --> 00:31:04,710
mapped which means it will get stored on
the disk. And uh so this train.bin file

1566
00:31:04,710 --> 00:31:04,720
the disk. And uh so this train.bin file
 

1567
00:31:04,720 --> 00:31:06,149
the disk. And uh so this train.bin file
which is there we are going to store it

1568
00:31:06,149 --> 00:31:06,159
which is there we are going to store it
 

1569
00:31:06,159 --> 00:31:08,870
which is there we are going to store it
on the disk. It looks like a numpy

1570
00:31:08,870 --> 00:31:08,880
on the disk. It looks like a numpy
 

1571
00:31:08,880 --> 00:31:13,430
on the disk. It looks like a numpy
array. uh and when we use this np.mme

1572
00:31:13,430 --> 00:31:13,440
array. uh and when we use this np.mme
 

1573
00:31:13,440 --> 00:31:15,510
array. uh and when we use this np.mme
map it means that the file is stored on

1574
00:31:15,510 --> 00:31:15,520
map it means that the file is stored on
 

1575
00:31:15,520 --> 00:31:17,110
map it means that the file is stored on
the disk and we don't have to hold

1576
00:31:17,110 --> 00:31:17,120
the disk and we don't have to hold
 

1577
00:31:17,120 --> 00:31:19,669
the disk and we don't have to hold
everything in the RAM essentially so we

1578
00:31:19,669 --> 00:31:19,679
everything in the RAM essentially so we
 

1579
00:31:19,679 --> 00:31:21,990
everything in the RAM essentially so we
create a memory mapped array and then

1580
00:31:21,990 --> 00:31:22,000
create a memory mapped array and then
 

1581
00:31:22,000 --> 00:31:24,549
create a memory mapped array and then
then what we do is that we split our

1582
00:31:24,549 --> 00:31:24,559
then what we do is that we split our
 

1583
00:31:24,559 --> 00:31:27,350
then what we do is that we split our
entire data set into batches so let's

1584
00:31:27,350 --> 00:31:27,360
entire data set into batches so let's
 

1585
00:31:27,360 --> 00:31:31,510
entire data set into batches so let's
say I have my 2.12 million stories right

1586
00:31:31,510 --> 00:31:31,520
say I have my 2.12 million stories right
 

1587
00:31:31,520 --> 00:31:34,870
say I have my 2.12 million stories right
I have my 2.12 million stories I split

1588
00:31:34,870 --> 00:31:34,880
I have my 2.12 million stories I split
 

1589
00:31:34,880 --> 00:31:37,830
I have my 2.12 million stories I split
these stories into 1024 batches so batch

1590
00:31:37,830 --> 00:31:37,840
these stories into 1024 batches so batch
 

1591
00:31:37,840 --> 00:31:40,389
these stories into 1024 batches so batch
number one batch number two etc. Let's

1592
00:31:40,389 --> 00:31:40,399
number one batch number two etc. Let's
 

1593
00:31:40,399 --> 00:31:42,310
number one batch number two etc. Let's
say badge number one has about 1,000

1594
00:31:42,310 --> 00:31:42,320
say badge number one has about 1,000
 

1595
00:31:42,320 --> 00:31:45,669
say badge number one has about 1,000
stories or 2,000 stories. Then what we

1596
00:31:45,669 --> 00:31:45,679
stories or 2,000 stories. Then what we
 

1597
00:31:45,679 --> 00:31:47,750
stories or 2,000 stories. Then what we
do is that we take the token ids from

1598
00:31:47,750 --> 00:31:47,760
do is that we take the token ids from
 

1599
00:31:47,760 --> 00:31:49,669
do is that we take the token ids from
all of those stories and put it into

1600
00:31:49,669 --> 00:31:49,679
all of those stories and put it into
 

1601
00:31:49,679 --> 00:31:52,549
all of those stories and put it into
this train.bin file. Then I go to batch

1602
00:31:52,549 --> 00:31:52,559
this train.bin file. Then I go to batch
 

1603
00:31:52,559 --> 00:31:54,549
this train.bin file. Then I go to batch
number two. I take all the token IDs

1604
00:31:54,549 --> 00:31:54,559
number two. I take all the token IDs
 

1605
00:31:54,559 --> 00:31:56,149
number two. I take all the token IDs
from batch number two and put all of

1606
00:31:56,149 --> 00:31:56,159
from batch number two and put all of
 

1607
00:31:56,159 --> 00:31:58,710
from batch number two and put all of
them into the train.bin file. Similarly,

1608
00:31:58,710 --> 00:31:58,720
them into the train.bin file. Similarly,
 

1609
00:31:58,720 --> 00:32:01,509
them into the train.bin file. Similarly,
I go to the last batch batch 1024. I

1610
00:32:01,509 --> 00:32:01,519
I go to the last batch batch 1024. I
 

1611
00:32:01,519 --> 00:32:03,430
I go to the last batch batch 1024. I
take all the token ids there and put it

1612
00:32:03,430 --> 00:32:03,440
take all the token ids there and put it
 

1613
00:32:03,440 --> 00:32:06,149
take all the token ids there and put it
into the train.bin file. Ultimately, my

1614
00:32:06,149 --> 00:32:06,159
into the train.bin file. Ultimately, my
 

1615
00:32:06,159 --> 00:32:09,110
into the train.bin file. Ultimately, my
train.bin bin file will have all uh

1616
00:32:09,110 --> 00:32:09,120
train.bin bin file will have all uh
 

1617
00:32:09,120 --> 00:32:11,029
train.bin bin file will have all uh
we'll have all the token ids from all

1618
00:32:11,029 --> 00:32:11,039
we'll have all the token ids from all
 

1619
00:32:11,039 --> 00:32:13,190
we'll have all the token ids from all
the different stories which and

1620
00:32:13,190 --> 00:32:13,200
the different stories which and
 

1621
00:32:13,200 --> 00:32:15,549
the different stories which and
everything here will be saved to the

1622
00:32:15,549 --> 00:32:15,559
everything here will be saved to the
 

1623
00:32:15,559 --> 00:32:18,230
everything here will be saved to the
disk. So let me repeat what we are doing

1624
00:32:18,230 --> 00:32:18,240
disk. So let me repeat what we are doing
 

1625
00:32:18,240 --> 00:32:20,149
disk. So let me repeat what we are doing
here. I hope until this part you have

1626
00:32:20,149 --> 00:32:20,159
here. I hope until this part you have
 

1627
00:32:20,159 --> 00:32:21,990
here. I hope until this part you have
understood where every story is broken

1628
00:32:21,990 --> 00:32:22,000
understood where every story is broken
 

1629
00:32:22,000 --> 00:32:24,470
understood where every story is broken
into a bunch of token ids and every

1630
00:32:24,470 --> 00:32:24,480
into a bunch of token ids and every
 

1631
00:32:24,480 --> 00:32:26,230
into a bunch of token ids and every
token ID has some length which is

1632
00:32:26,230 --> 00:32:26,240
token ID has some length which is
 

1633
00:32:26,240 --> 00:32:28,630
token ID has some length which is
associated with it. Ultimately what we

1634
00:32:28,630 --> 00:32:28,640
associated with it. Ultimately what we
 

1635
00:32:28,640 --> 00:32:30,149
associated with it. Ultimately what we
have to do is that we have to store all

1636
00:32:30,149 --> 00:32:30,159
have to do is that we have to store all
 

1637
00:32:30,159 --> 00:32:32,190
have to do is that we have to store all
of these token ids together in some

1638
00:32:32,190 --> 00:32:32,200
of these token ids together in some
 

1639
00:32:32,200 --> 00:32:34,710
of these token ids together in some
place. That place is going to be this

1640
00:32:34,710 --> 00:32:34,720
place. That place is going to be this
 

1641
00:32:34,720 --> 00:32:37,190
place. That place is going to be this
train.bin bin file and this file is

1642
00:32:37,190 --> 00:32:37,200
train.bin bin file and this file is
 

1643
00:32:37,200 --> 00:32:40,070
train.bin bin file and this file is
going to be on the disk instead of

1644
00:32:40,070 --> 00:32:40,080
going to be on the disk instead of
 

1645
00:32:40,080 --> 00:32:42,590
going to be on the disk instead of
storing it on the

1646
00:32:42,590 --> 00:32:42,600
storing it on the
 

1647
00:32:42,600 --> 00:32:45,269
storing it on the
RAM. So in the code what you'll see is

1648
00:32:45,269 --> 00:32:45,279
RAM. So in the code what you'll see is
 

1649
00:32:45,279 --> 00:32:46,950
RAM. So in the code what you'll see is
that you'll see the file name and then

1650
00:32:46,950 --> 00:32:46,960
that you'll see the file name and then
 

1651
00:32:46,960 --> 00:32:48,950
that you'll see the file name and then
you'll see an array array which is

1652
00:32:48,950 --> 00:32:48,960
you'll see an array array which is
 

1653
00:32:48,960 --> 00:32:51,830
you'll see an array array which is
initialized like np.mme map. So this is

1654
00:32:51,830 --> 00:32:51,840
initialized like np.mme map. So this is
 

1655
00:32:51,840 --> 00:32:55,110
initialized like np.mme map. So this is
a memory mapped array. So this array

1656
00:32:55,110 --> 00:32:55,120
a memory mapped array. So this array
 

1657
00:32:55,120 --> 00:32:57,190
a memory mapped array. So this array
eventually will contain all of my token

1658
00:32:57,190 --> 00:32:57,200
eventually will contain all of my token
 

1659
00:32:57,200 --> 00:33:00,310
eventually will contain all of my token
ids which are stored on the disk right.

1660
00:33:00,310 --> 00:33:00,320
ids which are stored on the disk right.
 

1661
00:33:00,320 --> 00:33:02,950
ids which are stored on the disk right.
um and then all and the file name where

1662
00:33:02,950 --> 00:33:02,960
um and then all and the file name where
 

1663
00:33:02,960 --> 00:33:04,549
um and then all and the file name where
eventually this array is stored that's

1664
00:33:04,549 --> 00:33:04,559
eventually this array is stored that's
 

1665
00:33:04,559 --> 00:33:06,870
eventually this array is stored that's
train.bin. So my array will contain all

1666
00:33:06,870 --> 00:33:06,880
train.bin. So my array will contain all
 

1667
00:33:06,880 --> 00:33:08,710
train.bin. So my array will contain all
the token ids and this array will be

1668
00:33:08,710 --> 00:33:08,720
the token ids and this array will be
 

1669
00:33:08,720 --> 00:33:10,830
the token ids and this array will be
stored in the file name

1670
00:33:10,830 --> 00:33:10,840
stored in the file name
 

1671
00:33:10,840 --> 00:33:12,870
stored in the file name
train.bin. That's all which is done

1672
00:33:12,870 --> 00:33:12,880
train.bin. That's all which is done
 

1673
00:33:12,880 --> 00:33:16,070
train.bin. That's all which is done
here. We uh we create a bin file. We

1674
00:33:16,070 --> 00:33:16,080
here. We uh we create a bin file. We
 

1675
00:33:16,080 --> 00:33:18,470
here. We uh we create a bin file. We
create a memory mapped array. We split

1676
00:33:18,470 --> 00:33:18,480
create a memory mapped array. We split
 

1677
00:33:18,480 --> 00:33:20,310
create a memory mapped array. We split
the data set into batches and we add

1678
00:33:20,310 --> 00:33:20,320
the data set into batches and we add
 

1679
00:33:20,320 --> 00:33:23,190
the data set into batches and we add
everything to to the train.bin file.

1680
00:33:23,190 --> 00:33:23,200
everything to to the train.bin file.
 

1681
00:33:23,200 --> 00:33:25,909
everything to to the train.bin file.
That's it. So my train.bin file now

1682
00:33:25,909 --> 00:33:25,919
That's it. So my train.bin file now
 

1683
00:33:25,919 --> 00:33:28,950
That's it. So my train.bin file now
contains all of my token ids together.

1684
00:33:28,950 --> 00:33:28,960
contains all of my token ids together.
 

1685
00:33:28,960 --> 00:33:30,549
contains all of my token ids together.
You might be wondering why do we do

1686
00:33:30,549 --> 00:33:30,559
You might be wondering why do we do
 

1687
00:33:30,559 --> 00:33:32,230
You might be wondering why do we do
these batches. The reason we do these

1688
00:33:32,230 --> 00:33:32,240
these batches. The reason we do these
 

1689
00:33:32,240 --> 00:33:34,389
these batches. The reason we do these
batches is for faster processing.

1690
00:33:34,389 --> 00:33:34,399
batches is for faster processing.
 

1691
00:33:34,399 --> 00:33:37,190
batches is for faster processing.
Because if we don't do these batches uh

1692
00:33:37,190 --> 00:33:37,200
Because if we don't do these batches uh
 

1693
00:33:37,200 --> 00:33:39,389
Because if we don't do these batches uh
then it takes a huge amount of time to

1694
00:33:39,389 --> 00:33:39,399
then it takes a huge amount of time to
 

1695
00:33:39,399 --> 00:33:41,990
then it takes a huge amount of time to
serially go through my entire data set

1696
00:33:41,990 --> 00:33:42,000
serially go through my entire data set
 

1697
00:33:42,000 --> 00:33:43,750
serially go through my entire data set
collect token ids and put it to my

1698
00:33:43,750 --> 00:33:43,760
collect token ids and put it to my
 

1699
00:33:43,760 --> 00:33:45,310
collect token ids and put it to my
train.bin

1700
00:33:45,310 --> 00:33:45,320
train.bin
 

1701
00:33:45,320 --> 00:33:47,750
train.bin
file. The reason I'm explaining all of

1702
00:33:47,750 --> 00:33:47,760
file. The reason I'm explaining all of
 

1703
00:33:47,760 --> 00:33:49,509
file. The reason I'm explaining all of
this to you in detail is because that's

1704
00:33:49,509 --> 00:33:49,519
this to you in detail is because that's
 

1705
00:33:49,519 --> 00:33:51,190
this to you in detail is because that's
exactly what we are going to see in the

1706
00:33:51,190 --> 00:33:51,200
exactly what we are going to see in the
 

1707
00:33:51,200 --> 00:33:53,350
exactly what we are going to see in the
code right now. So if you go to the code

1708
00:33:53,350 --> 00:33:53,360
code right now. So if you go to the code
 

1709
00:33:53,360 --> 00:33:55,350
code right now. So if you go to the code
and if you see step number two here is

1710
00:33:55,350 --> 00:33:55,360
and if you see step number two here is
 

1711
00:33:55,360 --> 00:33:57,029
and if you see step number two here is
where we are going to tokenize the data

1712
00:33:57,029 --> 00:33:57,039
where we are going to tokenize the data
 

1713
00:33:57,039 --> 00:33:59,430
where we are going to tokenize the data
set and ideally this should take a huge

1714
00:33:59,430 --> 00:33:59,440
set and ideally this should take a huge
 

1715
00:33:59,440 --> 00:34:02,070
set and ideally this should take a huge
amount of time because we have 2.12

1716
00:34:02,070 --> 00:34:02,080
amount of time because we have 2.12
 

1717
00:34:02,080 --> 00:34:03,750
amount of time because we have 2.12
million stories. We have to essentially

1718
00:34:03,750 --> 00:34:03,760
million stories. We have to essentially
 

1719
00:34:03,760 --> 00:34:06,230
million stories. We have to essentially
convert all of those stories into token

1720
00:34:06,230 --> 00:34:06,240
convert all of those stories into token
 

1721
00:34:06,240 --> 00:34:08,710
convert all of those stories into token
ids and append all the token ids

1722
00:34:08,710 --> 00:34:08,720
ids and append all the token ids
 

1723
00:34:08,720 --> 00:34:10,790
ids and append all the token ids
together. But this version of the code

1724
00:34:10,790 --> 00:34:10,800
together. But this version of the code
 

1725
00:34:10,800 --> 00:34:12,109
together. But this version of the code
is actually very

1726
00:34:12,109 --> 00:34:12,119
is actually very
 

1727
00:34:12,119 --> 00:34:14,550
is actually very
fast. The first thing which we are doing

1728
00:34:14,550 --> 00:34:14,560
fast. The first thing which we are doing
 

1729
00:34:14,560 --> 00:34:17,310
fast. The first thing which we are doing
is we have we are getting encoding for

1730
00:34:17,310 --> 00:34:17,320
is we have we are getting encoding for
 

1731
00:34:17,320 --> 00:34:21,190
is we have we are getting encoding for
GPT2. So this tick token is the library.

1732
00:34:21,190 --> 00:34:21,200
GPT2. So this tick token is the library.
 

1733
00:34:21,200 --> 00:34:22,669
GPT2. So this tick token is the library.
If you see tick

1734
00:34:22,669 --> 00:34:22,679
If you see tick
 

1735
00:34:22,679 --> 00:34:25,829
If you see tick
token GitHub, you'll see that this is a

1736
00:34:25,829 --> 00:34:25,839
token GitHub, you'll see that this is a
 

1737
00:34:25,839 --> 00:34:28,869
token GitHub, you'll see that this is a
library which is open as library and we

1738
00:34:28,869 --> 00:34:28,879
library which is open as library and we
 

1739
00:34:28,879 --> 00:34:31,349
library which is open as library and we
can get tokenizers from here and we can

1740
00:34:31,349 --> 00:34:31,359
can get tokenizers from here and we can
 

1741
00:34:31,359 --> 00:34:33,750
can get tokenizers from here and we can
get them for different GPT2 model GPT

1742
00:34:33,750 --> 00:34:33,760
get them for different GPT2 model GPT
 

1743
00:34:33,760 --> 00:34:35,829
get them for different GPT2 model GPT
models. So here we are getting the

1744
00:34:35,829 --> 00:34:35,839
models. So here we are getting the
 

1745
00:34:35,839 --> 00:34:38,710
models. So here we are getting the
tokenizer for GPT2 and first thing what

1746
00:34:38,710 --> 00:34:38,720
tokenizer for GPT2 and first thing what
 

1747
00:34:38,720 --> 00:34:40,069
tokenizer for GPT2 and first thing what
we are doing is that we are going

1748
00:34:40,069 --> 00:34:40,079
we are doing is that we are going
 

1749
00:34:40,079 --> 00:34:42,310
we are doing is that we are going
through all of my stories and every

1750
00:34:42,310 --> 00:34:42,320
through all of my stories and every
 

1751
00:34:42,320 --> 00:34:43,990
through all of my stories and every
story we are converting it into a

1752
00:34:43,990 --> 00:34:44,000
story we are converting it into a
 

1753
00:34:44,000 --> 00:34:46,629
story we are converting it into a
dictionary of ids and length. So as you

1754
00:34:46,629 --> 00:34:46,639
dictionary of ids and length. So as you
 

1755
00:34:46,639 --> 00:34:49,190
dictionary of ids and length. So as you
can see over here every story is getting

1756
00:34:49,190 --> 00:34:49,200
can see over here every story is getting
 

1757
00:34:49,200 --> 00:34:51,349
can see over here every story is getting
converted into ids and length. We saw

1758
00:34:51,349 --> 00:34:51,359
converted into ids and length. We saw
 

1759
00:34:51,359 --> 00:34:52,669
converted into ids and length. We saw
this over here

1760
00:34:52,669 --> 00:34:52,679
this over here
 

1761
00:34:52,679 --> 00:34:54,950
this over here
also. That's the first step which is

1762
00:34:54,950 --> 00:34:54,960
also. That's the first step which is
 

1763
00:34:54,960 --> 00:34:57,030
also. That's the first step which is
happening in this process. Right? Then

1764
00:34:57,030 --> 00:34:57,040
happening in this process. Right? Then
 

1765
00:34:57,040 --> 00:34:59,510
happening in this process. Right? Then
what we are doing is that

1766
00:34:59,510 --> 00:34:59,520
what we are doing is that
 

1767
00:34:59,520 --> 00:35:01,270
what we are doing is that
uh here you see we are creating a file

1768
00:35:01,270 --> 00:35:01,280
uh here you see we are creating a file
 

1769
00:35:01,280 --> 00:35:03,190
uh here you see we are creating a file
name which will be created for training

1770
00:35:03,190 --> 00:35:03,200
name which will be created for training
 

1771
00:35:03,200 --> 00:35:05,190
name which will be created for training
as well as for validation. For the

1772
00:35:05,190 --> 00:35:05,200
as well as for validation. For the
 

1773
00:35:05,200 --> 00:35:07,109
as well as for validation. For the
training data we'll create a file called

1774
00:35:07,109 --> 00:35:07,119
training data we'll create a file called
 

1775
00:35:07,119 --> 00:35:10,310
training data we'll create a file called
train.bin bin and we'll create an array

1776
00:35:10,310 --> 00:35:10,320
train.bin bin and we'll create an array
 

1777
00:35:10,320 --> 00:35:12,630
train.bin bin and we'll create an array
which is a memory mapped array and we'll

1778
00:35:12,630 --> 00:35:12,640
which is a memory mapped array and we'll
 

1779
00:35:12,640 --> 00:35:15,470
which is a memory mapped array and we'll
divide the entire data set into 1024

1780
00:35:15,470 --> 00:35:15,480
divide the entire data set into 1024
 

1781
00:35:15,480 --> 00:35:18,150
divide the entire data set into 1024
batches. Now what we'll do is that for

1782
00:35:18,150 --> 00:35:18,160
batches. Now what we'll do is that for
 

1783
00:35:18,160 --> 00:35:20,870
batches. Now what we'll do is that for
each batch we'll collect the token ids

1784
00:35:20,870 --> 00:35:20,880
each batch we'll collect the token ids
 

1785
00:35:20,880 --> 00:35:23,829
each batch we'll collect the token ids
and we will store them into this array.

1786
00:35:23,829 --> 00:35:23,839
and we will store them into this array.
 

1787
00:35:23,839 --> 00:35:26,390
and we will store them into this array.
That is where so every batch will

1788
00:35:26,390 --> 00:35:26,400
That is where so every batch will
 

1789
00:35:26,400 --> 00:35:28,310
That is where so every batch will
collect the token ids as has been shown

1790
00:35:28,310 --> 00:35:28,320
collect the token ids as has been shown
 

1791
00:35:28,320 --> 00:35:29,990
collect the token ids as has been shown
in this part. Every batch will collect

1792
00:35:29,990 --> 00:35:30,000
in this part. Every batch will collect
 

1793
00:35:30,000 --> 00:35:31,750
in this part. Every batch will collect
the token ids and store it into the

1794
00:35:31,750 --> 00:35:31,760
the token ids and store it into the
 

1795
00:35:31,760 --> 00:35:34,230
the token ids and store it into the
array. This is what is happening in each

1796
00:35:34,230 --> 00:35:34,240
array. This is what is happening in each
 

1797
00:35:34,240 --> 00:35:36,710
array. This is what is happening in each
iteration of the for loop here. And then

1798
00:35:36,710 --> 00:35:36,720
iteration of the for loop here. And then
 

1799
00:35:36,720 --> 00:35:38,790
iteration of the for loop here. And then
we just go through different batches.

1800
00:35:38,790 --> 00:35:38,800
we just go through different batches.
 

1801
00:35:38,800 --> 00:35:41,190
we just go through different batches.
And as we go through different batches,

1802
00:35:41,190 --> 00:35:41,200
And as we go through different batches,
 

1803
00:35:41,200 --> 00:35:43,190
And as we go through different batches,
every batch will give us a bunch of

1804
00:35:43,190 --> 00:35:43,200
every batch will give us a bunch of
 

1805
00:35:43,200 --> 00:35:45,430
every batch will give us a bunch of
token ids. All of those we'll put into

1806
00:35:45,430 --> 00:35:45,440
token ids. All of those we'll put into
 

1807
00:35:45,440 --> 00:35:46,829
token ids. All of those we'll put into
this uh

1808
00:35:46,829 --> 00:35:46,839
this uh
 

1809
00:35:46,839 --> 00:35:49,670
this uh
array. And finally, array.flush means

1810
00:35:49,670 --> 00:35:49,680
array. And finally, array.flush means
 

1811
00:35:49,680 --> 00:35:52,069
array. And finally, array.flush means
that we store everything on the disk.

1812
00:35:52,069 --> 00:35:52,079
that we store everything on the disk.
 

1813
00:35:52,079 --> 00:35:54,230
that we store everything on the disk.
That's the last command. Array. Which

1814
00:35:54,230 --> 00:35:54,240
That's the last command. Array. Which
 

1815
00:35:54,240 --> 00:35:55,910
That's the last command. Array. Which
means we save everything to the disk

1816
00:35:55,910 --> 00:35:55,920
means we save everything to the disk
 

1817
00:35:55,920 --> 00:35:59,430
means we save everything to the disk
ultimately. So now this array will be

1818
00:35:59,430 --> 00:35:59,440
ultimately. So now this array will be
 

1819
00:35:59,440 --> 00:36:02,030
ultimately. So now this array will be
then uh stored in my file which is

1820
00:36:02,030 --> 00:36:02,040
then uh stored in my file which is
 

1821
00:36:02,040 --> 00:36:04,470
then uh stored in my file which is
train.bin. File name is train.bin for

1822
00:36:04,470 --> 00:36:04,480
train.bin. File name is train.bin for
 

1823
00:36:04,480 --> 00:36:06,230
train.bin. File name is train.bin for
the training data. And I have my

1824
00:36:06,230 --> 00:36:06,240
the training data. And I have my
 

1825
00:36:06,240 --> 00:36:07,870
the training data. And I have my
validation data in

1826
00:36:07,870 --> 00:36:07,880
validation data in
 

1827
00:36:07,880 --> 00:36:10,630
validation data in
validation.bin. So now after this after

1828
00:36:10,630 --> 00:36:10,640
validation.bin. So now after this after
 

1829
00:36:10,640 --> 00:36:13,030
validation.bin. So now after this after
this point in the code, the train.bin

1830
00:36:13,030 --> 00:36:13,040
this point in the code, the train.bin
 

1831
00:36:13,040 --> 00:36:16,230
this point in the code, the train.bin
file essentially it contains a huge list

1832
00:36:16,230 --> 00:36:16,240
file essentially it contains a huge list
 

1833
00:36:16,240 --> 00:36:18,710
file essentially it contains a huge list
of token ids. It's a huge file because I

1834
00:36:18,710 --> 00:36:18,720
of token ids. It's a huge file because I
 

1835
00:36:18,720 --> 00:36:21,670
of token ids. It's a huge file because I
have 22 million stories, right? And if

1836
00:36:21,670 --> 00:36:21,680
have 22 million stories, right? And if
 

1837
00:36:21,680 --> 00:36:23,750
have 22 million stories, right? And if
you just imagine for now one word has

1838
00:36:23,750 --> 00:36:23,760
you just imagine for now one word has
 

1839
00:36:23,760 --> 00:36:24,910
you just imagine for now one word has
one

1840
00:36:24,910 --> 00:36:24,920
one
 

1841
00:36:24,920 --> 00:36:27,510
one
token. Let's imagine one word has one

1842
00:36:27,510 --> 00:36:27,520
token. Let's imagine one word has one
 

1843
00:36:27,520 --> 00:36:29,270
token. Let's imagine one word has one
token, right? And if you see every word

1844
00:36:29,270 --> 00:36:29,280
token, right? And if you see every word
 

1845
00:36:29,280 --> 00:36:32,069
token, right? And if you see every word
here, every every story here has at

1846
00:36:32,069 --> 00:36:32,079
here, every every story here has at
 

1847
00:36:32,079 --> 00:36:35,230
here, every every story here has at
least 1,

1848
00:36:35,230 --> 00:36:35,240

 

1849
00:36:35,240 --> 00:36:38,230

two, let's say 60 words. Every story at

1850
00:36:38,230 --> 00:36:38,240
two, let's say 60 words. Every story at
 

1851
00:36:38,240 --> 00:36:40,230
two, let's say 60 words. Every story at
least has 50 words and have 2 million

1852
00:36:40,230 --> 00:36:40,240
least has 50 words and have 2 million
 

1853
00:36:40,240 --> 00:36:42,150
least has 50 words and have 2 million
stories, right? So that means there are

1854
00:36:42,150 --> 00:36:42,160
stories, right? So that means there are
 

1855
00:36:42,160 --> 00:36:44,390
stories, right? So that means there are
around 100 million words in the training

1856
00:36:44,390 --> 00:36:44,400
around 100 million words in the training
 

1857
00:36:44,400 --> 00:36:47,109
around 100 million words in the training
data. So the trainbin might have around

1858
00:36:47,109 --> 00:36:47,119
data. So the trainbin might have around
 

1859
00:36:47,119 --> 00:36:50,950
data. So the trainbin might have around
100 million tokens, token ids rather.

1860
00:36:50,950 --> 00:36:50,960
100 million tokens, token ids rather.
 

1861
00:36:50,960 --> 00:36:52,670
100 million tokens, token ids rather.
and my

1862
00:36:52,670 --> 00:36:52,680
and my
 

1863
00:36:52,680 --> 00:36:56,790
and my
validation my validation is I think uh

1864
00:36:56,790 --> 00:36:56,800
validation my validation is I think uh
 

1865
00:36:56,800 --> 00:37:00,390
validation my validation is I think uh
22,000 right so that is around 100th of

1866
00:37:00,390 --> 00:37:00,400
22,000 right so that is around 100th of
 

1867
00:37:00,400 --> 00:37:03,430
22,000 right so that is around 100th of
this so my validation will have around 1

1868
00:37:03,430 --> 00:37:03,440
this so my validation will have around 1
 

1869
00:37:03,440 --> 00:37:05,870
this so my validation will have around 1
million token ids which is still

1870
00:37:05,870 --> 00:37:05,880
million token ids which is still
 

1871
00:37:05,880 --> 00:37:08,470
million token ids which is still
significant so the trainbin will have a

1872
00:37:08,470 --> 00:37:08,480
significant so the trainbin will have a
 

1873
00:37:08,480 --> 00:37:10,870
significant so the trainbin will have a
list of all the token ids of my data and

1874
00:37:10,870 --> 00:37:10,880
list of all the token ids of my data and
 

1875
00:37:10,880 --> 00:37:12,710
list of all the token ids of my data and
my validation dobbin will have all the

1876
00:37:12,710 --> 00:37:12,720
my validation dobbin will have all the
 

1877
00:37:12,720 --> 00:37:15,430
my validation dobbin will have all the
list of token ids in my data set and

1878
00:37:15,430 --> 00:37:15,440
list of token ids in my data set and
 

1879
00:37:15,440 --> 00:37:17,430
list of token ids in my data set and
again why are we doing this step because

1880
00:37:17,430 --> 00:37:17,440
again why are we doing this step because
 

1881
00:37:17,440 --> 00:37:21,270
again why are we doing this step because
we need to convert my data which is in a

1882
00:37:21,270 --> 00:37:21,280
we need to convert my data which is in a
 

1883
00:37:21,280 --> 00:37:24,069
we need to convert my data which is in a
text format into a numerical format and

1884
00:37:24,069 --> 00:37:24,079
text format into a numerical format and
 

1885
00:37:24,079 --> 00:37:25,550
text format into a numerical format and
the first step of doing that is

1886
00:37:25,550 --> 00:37:25,560
the first step of doing that is
 

1887
00:37:25,560 --> 00:37:27,670
the first step of doing that is
tokenization. So first we have to decide

1888
00:37:27,670 --> 00:37:27,680
tokenization. So first we have to decide
 

1889
00:37:27,680 --> 00:37:29,349
tokenization. So first we have to decide
the tokenizer which we are going to use

1890
00:37:29,349 --> 00:37:29,359
the tokenizer which we are going to use
 

1891
00:37:29,359 --> 00:37:30,910
the tokenizer which we are going to use
and that's going to be the bite pair

1892
00:37:30,910 --> 00:37:30,920
and that's going to be the bite pair
 

1893
00:37:30,920 --> 00:37:32,950
and that's going to be the bite pair
encoding and we are going to use a

1894
00:37:32,950 --> 00:37:32,960
encoding and we are going to use a
 

1895
00:37:32,960 --> 00:37:35,190
encoding and we are going to use a
subword tokenizer. So every subword will

1896
00:37:35,190 --> 00:37:35,200
subword tokenizer. So every subword will
 

1897
00:37:35,200 --> 00:37:37,910
subword tokenizer. So every subword will
be one token ID. All the token ids will

1898
00:37:37,910 --> 00:37:37,920
be one token ID. All the token ids will
 

1899
00:37:37,920 --> 00:37:39,670
be one token ID. All the token ids will
be appended together. That's what we are

1900
00:37:39,670 --> 00:37:39,680
be appended together. That's what we are
 

1901
00:37:39,680 --> 00:37:42,310
be appended together. That's what we are
doing in a simple manner. But in this

1902
00:37:42,310 --> 00:37:42,320
doing in a simple manner. But in this
 

1903
00:37:42,320 --> 00:37:44,550
doing in a simple manner. But in this
code we are just trying to improve the

1904
00:37:44,550 --> 00:37:44,560
code we are just trying to improve the
 

1905
00:37:44,560 --> 00:37:46,310
code we are just trying to improve the
computational efficiency by storing

1906
00:37:46,310 --> 00:37:46,320
computational efficiency by storing
 

1907
00:37:46,320 --> 00:37:48,230
computational efficiency by storing
everything on the disk. That's why we

1908
00:37:48,230 --> 00:37:48,240
everything on the disk. That's why we
 

1909
00:37:48,240 --> 00:37:50,550
everything on the disk. That's why we
need a memory mapped array over here. If

1910
00:37:50,550 --> 00:37:50,560
need a memory mapped array over here. If
 

1911
00:37:50,560 --> 00:37:52,390
need a memory mapped array over here. If
this part is a bit confusing to you,

1912
00:37:52,390 --> 00:37:52,400
this part is a bit confusing to you,
 

1913
00:37:52,400 --> 00:37:55,190
this part is a bit confusing to you,
don't worry. Just remember that after

1914
00:37:55,190 --> 00:37:55,200
don't worry. Just remember that after
 

1915
00:37:55,200 --> 00:37:57,190
don't worry. Just remember that after
this part, after this code is executed,

1916
00:37:57,190 --> 00:37:57,200
this part, after this code is executed,
 

1917
00:37:57,200 --> 00:37:58,870
this part, after this code is executed,
we have tokenized the data set into

1918
00:37:58,870 --> 00:37:58,880
we have tokenized the data set into
 

1919
00:37:58,880 --> 00:38:01,670
we have tokenized the data set into
token ids. We have created a file called

1920
00:38:01,670 --> 00:38:01,680
token ids. We have created a file called
 

1921
00:38:01,680 --> 00:38:04,990
token ids. We have created a file called
train.bin. And this should be

1922
00:38:04,990 --> 00:38:05,000
train.bin. And this should be
 

1923
00:38:05,000 --> 00:38:07,270
train.bin. And this should be
validation.bin. We have created a file

1924
00:38:07,270 --> 00:38:07,280
validation.bin. We have created a file
 

1925
00:38:07,280 --> 00:38:10,790
validation.bin. We have created a file
called train.bin and uh validation.bin

1926
00:38:10,790 --> 00:38:10,800
called train.bin and uh validation.bin
 

1927
00:38:10,800 --> 00:38:13,270
called train.bin and uh validation.bin
where all the token ids are stored. And

1928
00:38:13,270 --> 00:38:13,280
where all the token ids are stored. And
 

1929
00:38:13,280 --> 00:38:15,190
where all the token ids are stored. And
we make sure that token ids are stored

1930
00:38:15,190 --> 00:38:15,200
we make sure that token ids are stored
 

1931
00:38:15,200 --> 00:38:17,349
we make sure that token ids are stored
on a disk rather than on the RAM. for

1932
00:38:17,349 --> 00:38:17,359
on a disk rather than on the RAM. for
 

1933
00:38:17,359 --> 00:38:19,430
on a disk rather than on the RAM. for
efficient computations. These three are

1934
00:38:19,430 --> 00:38:19,440
efficient computations. These three are
 

1935
00:38:19,440 --> 00:38:21,030
efficient computations. These three are
the most important things to remember

1936
00:38:21,030 --> 00:38:21,040
the most important things to remember
 

1937
00:38:21,040 --> 00:38:22,750
the most important things to remember
from step number

1938
00:38:22,750 --> 00:38:22,760
from step number
 

1939
00:38:22,760 --> 00:38:25,349
from step number
two. So until this point we have our

1940
00:38:25,349 --> 00:38:25,359
two. So until this point we have our
 

1941
00:38:25,359 --> 00:38:26,870
two. So until this point we have our
data set, we have converted it into

1942
00:38:26,870 --> 00:38:26,880
data set, we have converted it into
 

1943
00:38:26,880 --> 00:38:29,109
data set, we have converted it into
token ids. We have a train.bin file and

1944
00:38:29,109 --> 00:38:29,119
token ids. We have a train.bin file and
 

1945
00:38:29,119 --> 00:38:31,510
token ids. We have a train.bin file and
we have a validation.bin file. I hope

1946
00:38:31,510 --> 00:38:31,520
we have a validation.bin file. I hope
 

1947
00:38:31,520 --> 00:38:33,670
we have a validation.bin file. I hope
all of us have understood until this

1948
00:38:33,670 --> 00:38:33,680
all of us have understood until this
 

1949
00:38:33,680 --> 00:38:35,710
all of us have understood until this
point that is the step two which is data

1950
00:38:35,710 --> 00:38:35,720
point that is the step two which is data
 

1951
00:38:35,720 --> 00:38:37,750
point that is the step two which is data
prep-processing. Now we are coming to

1952
00:38:37,750 --> 00:38:37,760
prep-processing. Now we are coming to
 

1953
00:38:37,760 --> 00:38:39,430
prep-processing. Now we are coming to
step number three which is creating

1954
00:38:39,430 --> 00:38:39,440
step number three which is creating
 

1955
00:38:39,440 --> 00:38:42,390
step number three which is creating
input output pairs from the data set.

1956
00:38:42,390 --> 00:38:42,400
input output pairs from the data set.
 

1957
00:38:42,400 --> 00:38:45,030
input output pairs from the data set.
The reason we need to create input and

1958
00:38:45,030 --> 00:38:45,040
The reason we need to create input and
 

1959
00:38:45,040 --> 00:38:48,069
The reason we need to create input and
output pairs is because ultimately the

1960
00:38:48,069 --> 00:38:48,079
output pairs is because ultimately the
 

1961
00:38:48,079 --> 00:38:50,069
output pairs is because ultimately the
loss function which we will define for

1962
00:38:50,069 --> 00:38:50,079
loss function which we will define for
 

1963
00:38:50,079 --> 00:38:52,150
loss function which we will define for
training the language model will be

1964
00:38:52,150 --> 00:38:52,160
training the language model will be
 

1965
00:38:52,160 --> 00:38:54,150
training the language model will be
based on these input and the output

1966
00:38:54,150 --> 00:38:54,160
based on these input and the output
 

1967
00:38:54,160 --> 00:38:57,190
based on these input and the output
pairs. When we have regression tasks or

1968
00:38:57,190 --> 00:38:57,200
pairs. When we have regression tasks or
 

1969
00:38:57,200 --> 00:38:58,870
pairs. When we have regression tasks or
classification tasks in machine

1970
00:38:58,870 --> 00:38:58,880
classification tasks in machine
 

1971
00:38:58,880 --> 00:39:00,950
classification tasks in machine
learning, we are usually provided with

1972
00:39:00,950 --> 00:39:00,960
learning, we are usually provided with
 

1973
00:39:00,960 --> 00:39:03,589
learning, we are usually provided with
the labels of the correct answer. Right?

1974
00:39:03,589 --> 00:39:03,599
the labels of the correct answer. Right?
 

1975
00:39:03,599 --> 00:39:06,310
the labels of the correct answer. Right?
But in the case of language modeling, we

1976
00:39:06,310 --> 00:39:06,320
But in the case of language modeling, we
 

1977
00:39:06,320 --> 00:39:08,150
But in the case of language modeling, we
are just provided with the data set

1978
00:39:08,150 --> 00:39:08,160
are just provided with the data set
 

1979
00:39:08,160 --> 00:39:10,310
are just provided with the data set
which looks like this. But we are not

1980
00:39:10,310 --> 00:39:10,320
which looks like this. But we are not
 

1981
00:39:10,320 --> 00:39:12,710
which looks like this. But we are not
essentially told that what is the

1982
00:39:12,710 --> 00:39:12,720
essentially told that what is the
 

1983
00:39:12,720 --> 00:39:15,829
essentially told that what is the
correct answer, what is supposed to be

1984
00:39:15,829 --> 00:39:15,839
correct answer, what is supposed to be
 

1985
00:39:15,839 --> 00:39:17,349
correct answer, what is supposed to be
the right answer generated by the

1986
00:39:17,349 --> 00:39:17,359
the right answer generated by the
 

1987
00:39:17,359 --> 00:39:20,150
the right answer generated by the
language model etc. We have to create

1988
00:39:20,150 --> 00:39:20,160
language model etc. We have to create
 

1989
00:39:20,160 --> 00:39:22,069
language model etc. We have to create
these input and output pairs from the

1990
00:39:22,069 --> 00:39:22,079
these input and output pairs from the
 

1991
00:39:22,079 --> 00:39:24,710
these input and output pairs from the
data set. To understand how to create

1992
00:39:24,710 --> 00:39:24,720
data set. To understand how to create
 

1993
00:39:24,720 --> 00:39:27,510
data set. To understand how to create
input and output pairs, first it's very

1994
00:39:27,510 --> 00:39:27,520
input and output pairs, first it's very
 

1995
00:39:27,520 --> 00:39:29,270
input and output pairs, first it's very
important for all of us to understand

1996
00:39:29,270 --> 00:39:29,280
important for all of us to understand
 

1997
00:39:29,280 --> 00:39:31,109
important for all of us to understand
what's the purpose of language modeling

1998
00:39:31,109 --> 00:39:31,119
what's the purpose of language modeling
 

1999
00:39:31,119 --> 00:39:32,630
what's the purpose of language modeling
itself.

2000
00:39:32,630 --> 00:39:32,640
itself.
 

2001
00:39:32,640 --> 00:39:35,589
itself.
So when large language models were uh

2002
00:39:35,589 --> 00:39:35,599
So when large language models were uh
 

2003
00:39:35,599 --> 00:39:37,750
So when large language models were uh
constructed and when they became really

2004
00:39:37,750 --> 00:39:37,760
constructed and when they became really
 

2005
00:39:37,760 --> 00:39:40,069
constructed and when they became really
popular, what they actually did was they

2006
00:39:40,069 --> 00:39:40,079
popular, what they actually did was they
 

2007
00:39:40,079 --> 00:39:42,069
popular, what they actually did was they
took in the input as a sequence of

2008
00:39:42,069 --> 00:39:42,079
took in the input as a sequence of
 

2009
00:39:42,079 --> 00:39:44,630
took in the input as a sequence of
tokens or a sequence of words and then

2010
00:39:44,630 --> 00:39:44,640
tokens or a sequence of words and then
 

2011
00:39:44,640 --> 00:39:46,630
tokens or a sequence of words and then
the output which they generated was

2012
00:39:46,630 --> 00:39:46,640
the output which they generated was
 

2013
00:39:46,640 --> 00:39:48,829
the output which they generated was
essentially the next token prediction

2014
00:39:48,829 --> 00:39:48,839
essentially the next token prediction
 

2015
00:39:48,839 --> 00:39:51,270
essentially the next token prediction
task. So what is the next token

2016
00:39:51,270 --> 00:39:51,280
task. So what is the next token
 

2017
00:39:51,280 --> 00:39:53,190
task. So what is the next token
prediction task? It's essentially just

2018
00:39:53,190 --> 00:39:53,200
prediction task? It's essentially just
 

2019
00:39:53,200 --> 00:39:54,790
prediction task? It's essentially just
looking at the sequence of words which

2020
00:39:54,790 --> 00:39:54,800
looking at the sequence of words which
 

2021
00:39:54,800 --> 00:39:56,710
looking at the sequence of words which
has come until now and then predicting

2022
00:39:56,710 --> 00:39:56,720
has come until now and then predicting
 

2023
00:39:56,720 --> 00:39:59,190
has come until now and then predicting
the next token. So if you think of the

2024
00:39:59,190 --> 00:39:59,200
the next token. So if you think of the
 

2025
00:39:59,200 --> 00:40:01,589
the next token. So if you think of the
language model as the engine, the engine

2026
00:40:01,589 --> 00:40:01,599
language model as the engine, the engine
 

2027
00:40:01,599 --> 00:40:03,829
language model as the engine, the engine
receives the fuel and then the car moves

2028
00:40:03,829 --> 00:40:03,839
receives the fuel and then the car moves
 

2029
00:40:03,839 --> 00:40:06,069
receives the fuel and then the car moves
forward. Right? Similar to that, the

2030
00:40:06,069 --> 00:40:06,079
forward. Right? Similar to that, the
 

2031
00:40:06,079 --> 00:40:07,990
forward. Right? Similar to that, the
equivalent analogy to the language model

2032
00:40:07,990 --> 00:40:08,000
equivalent analogy to the language model
 

2033
00:40:08,000 --> 00:40:10,470
equivalent analogy to the language model
is that the sequence of tokens which

2034
00:40:10,470 --> 00:40:10,480
is that the sequence of tokens which
 

2035
00:40:10,480 --> 00:40:13,430
is that the sequence of tokens which
goes to the input that is the fuel and

2036
00:40:13,430 --> 00:40:13,440
goes to the input that is the fuel and
 

2037
00:40:13,440 --> 00:40:15,829
goes to the input that is the fuel and
the next token prediction that is the

2038
00:40:15,829 --> 00:40:15,839
the next token prediction that is the
 

2039
00:40:15,839 --> 00:40:17,510
the next token prediction that is the
movement which is the output which we

2040
00:40:17,510 --> 00:40:17,520
movement which is the output which we
 

2041
00:40:17,520 --> 00:40:18,790
movement which is the output which we
want.

2042
00:40:18,790 --> 00:40:18,800
want.
 

2043
00:40:18,800 --> 00:40:20,790
want.
What is so incredible and also beautiful

2044
00:40:20,790 --> 00:40:20,800
What is so incredible and also beautiful
 

2045
00:40:20,800 --> 00:40:23,030
What is so incredible and also beautiful
about language modeling is that although

2046
00:40:23,030 --> 00:40:23,040
about language modeling is that although
 

2047
00:40:23,040 --> 00:40:25,270
about language modeling is that although
we teach the model or train the model to

2048
00:40:25,270 --> 00:40:25,280
we teach the model or train the model to
 

2049
00:40:25,280 --> 00:40:27,510
we teach the model or train the model to
just predict the next token at a time,

2050
00:40:27,510 --> 00:40:27,520
just predict the next token at a time,
 

2051
00:40:27,520 --> 00:40:29,910
just predict the next token at a time,
it somehow understands the structure and

2052
00:40:29,910 --> 00:40:29,920
it somehow understands the structure and
 

2053
00:40:29,920 --> 00:40:32,069
it somehow understands the structure and
the meaning of human language within

2054
00:40:32,069 --> 00:40:32,079
the meaning of human language within
 

2055
00:40:32,079 --> 00:40:34,710
the meaning of human language within
that process almost as a byproduct of

2056
00:40:34,710 --> 00:40:34,720
that process almost as a byproduct of
 

2057
00:40:34,720 --> 00:40:36,550
that process almost as a byproduct of
that process.

2058
00:40:36,550 --> 00:40:36,560
that process.
 

2059
00:40:36,560 --> 00:40:38,390
that process.
And one more way to understand next

2060
00:40:38,390 --> 00:40:38,400
And one more way to understand next
 

2061
00:40:38,400 --> 00:40:40,150
And one more way to understand next
token prediction is that if you go to

2062
00:40:40,150 --> 00:40:40,160
token prediction is that if you go to
 

2063
00:40:40,160 --> 00:40:42,069
token prediction is that if you go to
chat GPT right and if you say something

2064
00:40:42,069 --> 00:40:42,079
chat GPT right and if you say something
 

2065
00:40:42,079 --> 00:40:45,710
chat GPT right and if you say something
like uh make a

2066
00:40:45,710 --> 00:40:45,720
like uh make a
 

2067
00:40:45,720 --> 00:40:49,349
like uh make a
travel plan for Italy you'll see that

2068
00:40:49,349 --> 00:40:49,359
travel plan for Italy you'll see that
 

2069
00:40:49,359 --> 00:40:51,990
travel plan for Italy you'll see that
chat GPT response and the response is

2070
00:40:51,990 --> 00:40:52,000
chat GPT response and the response is
 

2071
00:40:52,000 --> 00:40:54,150
chat GPT response and the response is
presented as a detailed paragraph but

2072
00:40:54,150 --> 00:40:54,160
presented as a detailed paragraph but
 

2073
00:40:54,160 --> 00:40:55,910
presented as a detailed paragraph but
actually just one token which is

2074
00:40:55,910 --> 00:40:55,920
actually just one token which is
 

2075
00:40:55,920 --> 00:40:57,510
actually just one token which is
predicted at a time and then it's

2076
00:40:57,510 --> 00:40:57,520
predicted at a time and then it's
 

2077
00:40:57,520 --> 00:40:59,750
predicted at a time and then it's
appended to each other. So at every

2078
00:40:59,750 --> 00:40:59,760
appended to each other. So at every
 

2079
00:40:59,760 --> 00:41:02,230
appended to each other. So at every
single instance only the next token or

2080
00:41:02,230 --> 00:41:02,240
single instance only the next token or
 

2081
00:41:02,240 --> 00:41:04,470
single instance only the next token or
the next word is predicted. You'll

2082
00:41:04,470 --> 00:41:04,480
the next word is predicted. You'll
 

2083
00:41:04,480 --> 00:41:06,230
the next word is predicted. You'll
notice that as the lecture goes along,

2084
00:41:06,230 --> 00:41:06,240
notice that as the lecture goes along,
 

2085
00:41:06,240 --> 00:41:08,670
notice that as the lecture goes along,
I'm using the word token and the word in

2086
00:41:08,670 --> 00:41:08,680
I'm using the word token and the word in
 

2087
00:41:08,680 --> 00:41:10,950
I'm using the word token and the word in
interchangeably. Although that's not the

2088
00:41:10,950 --> 00:41:10,960
interchangeably. Although that's not the
 

2089
00:41:10,960 --> 00:41:12,589
interchangeably. Although that's not the
case. We are using the subword

2090
00:41:12,589 --> 00:41:12,599
case. We are using the subword
 

2091
00:41:12,599 --> 00:41:14,950
case. We are using the subword
tokenization. So, usually one word is

2092
00:41:14,950 --> 00:41:14,960
tokenization. So, usually one word is
 

2093
00:41:14,960 --> 00:41:17,510
tokenization. So, usually one word is
not one token, but we'll use that for

2094
00:41:17,510 --> 00:41:17,520
not one token, but we'll use that for
 

2095
00:41:17,520 --> 00:41:19,430
not one token, but we'll use that for
the sake of simplicity. It's just better

2096
00:41:19,430 --> 00:41:19,440
the sake of simplicity. It's just better
 

2097
00:41:19,440 --> 00:41:20,670
the sake of simplicity. It's just better
for

2098
00:41:20,670 --> 00:41:20,680
for
 

2099
00:41:20,680 --> 00:41:23,190
for
explanation. Now, let me tell you how

2100
00:41:23,190 --> 00:41:23,200
explanation. Now, let me tell you how
 

2101
00:41:23,200 --> 00:41:25,190
explanation. Now, let me tell you how
these input and output pairs are

2102
00:41:25,190 --> 00:41:25,200
these input and output pairs are
 

2103
00:41:25,200 --> 00:41:27,510
these input and output pairs are
actually created from the data set. Here

2104
00:41:27,510 --> 00:41:27,520
actually created from the data set. Here
 

2105
00:41:27,520 --> 00:41:29,589
actually created from the data set. Here
I have taken one single story from the

2106
00:41:29,589 --> 00:41:29,599
I have taken one single story from the
 

2107
00:41:29,599 --> 00:41:32,230
I have taken one single story from the
tiny stories and let me tell you how the

2108
00:41:32,230 --> 00:41:32,240
tiny stories and let me tell you how the
 

2109
00:41:32,240 --> 00:41:35,109
tiny stories and let me tell you how the
input pairs are created. First to create

2110
00:41:35,109 --> 00:41:35,119
input pairs are created. First to create
 

2111
00:41:35,119 --> 00:41:37,030
input pairs are created. First to create
the input pairs we have to essentially

2112
00:41:37,030 --> 00:41:37,040
the input pairs we have to essentially
 

2113
00:41:37,040 --> 00:41:39,270
the input pairs we have to essentially
decide two things. First thing which we

2114
00:41:39,270 --> 00:41:39,280
decide two things. First thing which we
 

2115
00:41:39,280 --> 00:41:41,829
decide two things. First thing which we
have to decide is the context size. The

2116
00:41:41,829 --> 00:41:41,839
have to decide is the context size. The
 

2117
00:41:41,839 --> 00:41:44,470
have to decide is the context size. The
context size is the length of words or

2118
00:41:44,470 --> 00:41:44,480
context size is the length of words or
 

2119
00:41:44,480 --> 00:41:46,390
context size is the length of words or
tokens which my language model is

2120
00:41:46,390 --> 00:41:46,400
tokens which my language model is
 

2121
00:41:46,400 --> 00:41:48,790
tokens which my language model is
looking at at one time before it

2122
00:41:48,790 --> 00:41:48,800
looking at at one time before it
 

2123
00:41:48,800 --> 00:41:51,349
looking at at one time before it
predicts the next token. Or put in

2124
00:41:51,349 --> 00:41:51,359
predicts the next token. Or put in
 

2125
00:41:51,359 --> 00:41:53,430
predicts the next token. Or put in
another way, it is the maximum number of

2126
00:41:53,430 --> 00:41:53,440
another way, it is the maximum number of
 

2127
00:41:53,440 --> 00:41:55,510
another way, it is the maximum number of
tokens my language model can look at

2128
00:41:55,510 --> 00:41:55,520
tokens my language model can look at
 

2129
00:41:55,520 --> 00:41:57,230
tokens my language model can look at
before predicting the next

2130
00:41:57,230 --> 00:41:57,240
before predicting the next
 

2131
00:41:57,240 --> 00:42:00,230
before predicting the next
token. And here I'm deciding the context

2132
00:42:00,230 --> 00:42:00,240
token. And here I'm deciding the context
 

2133
00:42:00,240 --> 00:42:02,870
token. And here I'm deciding the context
size to be equal to four. What that does

2134
00:42:02,870 --> 00:42:02,880
size to be equal to four. What that does
 

2135
00:42:02,880 --> 00:42:05,030
size to be equal to four. What that does
is that based on the context size, I

2136
00:42:05,030 --> 00:42:05,040
is that based on the context size, I
 

2137
00:42:05,040 --> 00:42:08,510
is that based on the context size, I
will divide my entire data set into

2138
00:42:08,510 --> 00:42:08,520
will divide my entire data set into
 

2139
00:42:08,520 --> 00:42:11,109
will divide my entire data set into
chunks and the length of those chunks

2140
00:42:11,109 --> 00:42:11,119
chunks and the length of those chunks
 

2141
00:42:11,119 --> 00:42:12,870
chunks and the length of those chunks
will be equivalent to the context size.

2142
00:42:12,870 --> 00:42:12,880
will be equivalent to the context size.
 

2143
00:42:12,880 --> 00:42:15,030
will be equivalent to the context size.
In this case, it is equal to four. So

2144
00:42:15,030 --> 00:42:15,040
In this case, it is equal to four. So
 

2145
00:42:15,040 --> 00:42:16,630
In this case, it is equal to four. So
this is my first chunk. This is my

2146
00:42:16,630 --> 00:42:16,640
this is my first chunk. This is my
 

2147
00:42:16,640 --> 00:42:19,190
this is my first chunk. This is my
second chunk x2. This is my third chunk

2148
00:42:19,190 --> 00:42:19,200
second chunk x2. This is my third chunk
 

2149
00:42:19,200 --> 00:42:21,150
second chunk x2. This is my third chunk
which is x3. And this is my four chunk

2150
00:42:21,150 --> 00:42:21,160
which is x3. And this is my four chunk
 

2151
00:42:21,160 --> 00:42:23,990
which is x3. And this is my four chunk
X4. Similarly, the whole data set will

2152
00:42:23,990 --> 00:42:24,000
X4. Similarly, the whole data set will
 

2153
00:42:24,000 --> 00:42:26,390
X4. Similarly, the whole data set will
be broken down into these chunks. And

2154
00:42:26,390 --> 00:42:26,400
be broken down into these chunks. And
 

2155
00:42:26,400 --> 00:42:28,230
be broken down into these chunks. And
you'll see that each chunk essentially

2156
00:42:28,230 --> 00:42:28,240
you'll see that each chunk essentially
 

2157
00:42:28,240 --> 00:42:30,710
you'll see that each chunk essentially
consists of four words or four tokens

2158
00:42:30,710 --> 00:42:30,720
consists of four words or four tokens
 

2159
00:42:30,720 --> 00:42:32,550
consists of four words or four tokens
because the context size is equal to

2160
00:42:32,550 --> 00:42:32,560
because the context size is equal to
 

2161
00:42:32,560 --> 00:42:34,870
because the context size is equal to
four. Now although I'm showing words

2162
00:42:34,870 --> 00:42:34,880
four. Now although I'm showing words
 

2163
00:42:34,880 --> 00:42:36,870
four. Now although I'm showing words
over here to do keep in mind that we

2164
00:42:36,870 --> 00:42:36,880
over here to do keep in mind that we
 

2165
00:42:36,880 --> 00:42:39,109
over here to do keep in mind that we
have created train.bin and val.bin,

2166
00:42:39,109 --> 00:42:39,119
have created train.bin and val.bin,
 

2167
00:42:39,119 --> 00:42:42,390
have created train.bin and val.bin,
right? validation.bin. So what the model

2168
00:42:42,390 --> 00:42:42,400
right? validation.bin. So what the model
 

2169
00:42:42,400 --> 00:42:44,470
right? validation.bin. So what the model
actually sees is just a bunch of token

2170
00:42:44,470 --> 00:42:44,480
actually sees is just a bunch of token
 

2171
00:42:44,480 --> 00:42:48,069
actually sees is just a bunch of token
ids like this.

2172
00:42:48,069 --> 00:42:48,079

 

2173
00:42:48,079 --> 00:42:50,309

So it creates the inputs based on these

2174
00:42:50,309 --> 00:42:50,319
So it creates the inputs based on these
 

2175
00:42:50,319 --> 00:42:52,630
So it creates the inputs based on these
token ids and not the words. I just

2176
00:42:52,630 --> 00:42:52,640
token ids and not the words. I just
 

2177
00:42:52,640 --> 00:42:55,109
token ids and not the words. I just
wanted to clarify this because the model

2178
00:42:55,109 --> 00:42:55,119
wanted to clarify this because the model
 

2179
00:42:55,119 --> 00:42:57,030
wanted to clarify this because the model
after this stage has no idea of the

2180
00:42:57,030 --> 00:42:57,040
after this stage has no idea of the
 

2181
00:42:57,040 --> 00:42:59,430
after this stage has no idea of the
words. It just looks at token ids.

2182
00:42:59,430 --> 00:42:59,440
words. It just looks at token ids.
 

2183
00:42:59,440 --> 00:43:02,150
words. It just looks at token ids.
Sometimes we we forget about that

2184
00:43:02,150 --> 00:43:02,160
Sometimes we we forget about that
 

2185
00:43:02,160 --> 00:43:04,069
Sometimes we we forget about that
because as humans it's so natural for us

2186
00:43:04,069 --> 00:43:04,079
because as humans it's so natural for us
 

2187
00:43:04,079 --> 00:43:06,390
because as humans it's so natural for us
to look at words. But just remember that

2188
00:43:06,390 --> 00:43:06,400
to look at words. But just remember that
 

2189
00:43:06,400 --> 00:43:08,150
to look at words. But just remember that
when the model is creating the input

2190
00:43:08,150 --> 00:43:08,160
when the model is creating the input
 

2191
00:43:08,160 --> 00:43:10,870
when the model is creating the input
output pairs it just looks at token ids.

2192
00:43:10,870 --> 00:43:10,880
output pairs it just looks at token ids.
 

2193
00:43:10,880 --> 00:43:12,870
output pairs it just looks at token ids.
So the entire data set is broken down

2194
00:43:12,870 --> 00:43:12,880
So the entire data set is broken down
 

2195
00:43:12,880 --> 00:43:15,670
So the entire data set is broken down
into these input pairs whose size or the

2196
00:43:15,670 --> 00:43:15,680
into these input pairs whose size or the
 

2197
00:43:15,680 --> 00:43:18,950
into these input pairs whose size or the
chunk size is equal to four. The second

2198
00:43:18,950 --> 00:43:18,960
chunk size is equal to four. The second
 

2199
00:43:18,960 --> 00:43:22,390
chunk size is equal to four. The second
quantity which I need to decide before I

2200
00:43:22,390 --> 00:43:22,400
quantity which I need to decide before I
 

2201
00:43:22,400 --> 00:43:24,309
quantity which I need to decide before I
create these input output pairs is the

2202
00:43:24,309 --> 00:43:24,319
create these input output pairs is the
 

2203
00:43:24,319 --> 00:43:27,349
create these input output pairs is the
batch size. Batch size is essentially

2204
00:43:27,349 --> 00:43:27,359
batch size. Batch size is essentially
 

2205
00:43:27,359 --> 00:43:29,510
batch size. Batch size is essentially
when you're training language models, we

2206
00:43:29,510 --> 00:43:29,520
when you're training language models, we
 

2207
00:43:29,520 --> 00:43:31,670
when you're training language models, we
don't process all the input and output

2208
00:43:31,670 --> 00:43:31,680
don't process all the input and output
 

2209
00:43:31,680 --> 00:43:34,630
don't process all the input and output
data at one time. The data is usually

2210
00:43:34,630 --> 00:43:34,640
data at one time. The data is usually
 

2211
00:43:34,640 --> 00:43:38,150
data at one time. The data is usually
processed in batches. So first you uh

2212
00:43:38,150 --> 00:43:38,160
processed in batches. So first you uh
 

2213
00:43:38,160 --> 00:43:40,550
processed in batches. So first you uh
take the output of the first batch,

2214
00:43:40,550 --> 00:43:40,560
take the output of the first batch,
 

2215
00:43:40,560 --> 00:43:42,309
take the output of the first batch,
calculate the loss, propagate it

2216
00:43:42,309 --> 00:43:42,319
calculate the loss, propagate it
 

2217
00:43:42,319 --> 00:43:43,910
calculate the loss, propagate it
backwards, update up update the

2218
00:43:43,910 --> 00:43:43,920
backwards, update up update the
 

2219
00:43:43,920 --> 00:43:45,910
backwards, update up update the
parameters. Then you calculate the

2220
00:43:45,910 --> 00:43:45,920
parameters. Then you calculate the
 

2221
00:43:45,920 --> 00:43:47,750
parameters. Then you calculate the
output of the second batch, calculate

2222
00:43:47,750 --> 00:43:47,760
output of the second batch, calculate
 

2223
00:43:47,760 --> 00:43:49,589
output of the second batch, calculate
the loss, propagate it backwards and

2224
00:43:49,589 --> 00:43:49,599
the loss, propagate it backwards and
 

2225
00:43:49,599 --> 00:43:51,990
the loss, propagate it backwards and
update the parameters. The reason this

2226
00:43:51,990 --> 00:43:52,000
update the parameters. The reason this
 

2227
00:43:52,000 --> 00:43:53,829
update the parameters. The reason this
is done is that if you have no batches,

2228
00:43:53,829 --> 00:43:53,839
is done is that if you have no batches,
 

2229
00:43:53,839 --> 00:43:55,589
is done is that if you have no batches,
it would take a huge amount of time to

2230
00:43:55,589 --> 00:43:55,599
it would take a huge amount of time to
 

2231
00:43:55,599 --> 00:43:57,190
it would take a huge amount of time to
update the parameters because you have

2232
00:43:57,190 --> 00:43:57,200
update the parameters because you have
 

2233
00:43:57,200 --> 00:43:59,589
update the parameters because you have
to go through all of these all of the

2234
00:43:59,589 --> 00:43:59,599
to go through all of these all of the
 

2235
00:43:59,599 --> 00:44:02,069
to go through all of these all of the
entire data before updating the

2236
00:44:02,069 --> 00:44:02,079
entire data before updating the
 

2237
00:44:02,079 --> 00:44:05,109
entire data before updating the
parameters. So having batches makes it

2238
00:44:05,109 --> 00:44:05,119
parameters. So having batches makes it
 

2239
00:44:05,119 --> 00:44:07,109
parameters. So having batches makes it
much more convenient to perform the

2240
00:44:07,109 --> 00:44:07,119
much more convenient to perform the
 

2241
00:44:07,119 --> 00:44:09,670
much more convenient to perform the
training procedure. So here I'm deciding

2242
00:44:09,670 --> 00:44:09,680
training procedure. So here I'm deciding
 

2243
00:44:09,680 --> 00:44:12,150
training procedure. So here I'm deciding
a batch size equal to five. Once you

2244
00:44:12,150 --> 00:44:12,160
a batch size equal to five. Once you
 

2245
00:44:12,160 --> 00:44:13,910
a batch size equal to five. Once you
decide the context size and the batch

2246
00:44:13,910 --> 00:44:13,920
decide the context size and the batch
 

2247
00:44:13,920 --> 00:44:15,990
decide the context size and the batch
size, you can create input matrix

2248
00:44:15,990 --> 00:44:16,000
size, you can create input matrix
 

2249
00:44:16,000 --> 00:44:18,309
size, you can create input matrix
matrices such as this. So this is called

2250
00:44:18,309 --> 00:44:18,319
matrices such as this. So this is called
 

2251
00:44:18,319 --> 00:44:21,589
matrices such as this. So this is called
as my first input batch. So this is now

2252
00:44:21,589 --> 00:44:21,599
as my first input batch. So this is now
 

2253
00:44:21,599 --> 00:44:24,470
as my first input batch. So this is now
first this is my first input batch. The

2254
00:44:24,470 --> 00:44:24,480
first this is my first input batch. The
 

2255
00:44:24,480 --> 00:44:26,470
first this is my first input batch. The
reason this batch has number of rows

2256
00:44:26,470 --> 00:44:26,480
reason this batch has number of rows
 

2257
00:44:26,480 --> 00:44:29,510
reason this batch has number of rows
equal to four x1 x2 x3 x4 is because the

2258
00:44:29,510 --> 00:44:29,520
equal to four x1 x2 x3 x4 is because the
 

2259
00:44:29,520 --> 00:44:32,150
equal to four x1 x2 x3 x4 is because the
batch size is equal to four. there will

2260
00:44:32,150 --> 00:44:32,160
batch size is equal to four. there will
 

2261
00:44:32,160 --> 00:44:35,190
batch size is equal to four. there will
be four um there will be four sequences

2262
00:44:35,190 --> 00:44:35,200
be four um there will be four sequences
 

2263
00:44:35,200 --> 00:44:38,390
be four um there will be four sequences
in each batch and in each sequence so

2264
00:44:38,390 --> 00:44:38,400
in each batch and in each sequence so
 

2265
00:44:38,400 --> 00:44:40,230
in each batch and in each sequence so
for example if you look at X1 the number

2266
00:44:40,230 --> 00:44:40,240
for example if you look at X1 the number
 

2267
00:44:40,240 --> 00:44:42,390
for example if you look at X1 the number
of columns is equal to four why because

2268
00:44:42,390 --> 00:44:42,400
of columns is equal to four why because
 

2269
00:44:42,400 --> 00:44:44,630
of columns is equal to four why because
the context size is equal to four so

2270
00:44:44,630 --> 00:44:44,640
the context size is equal to four so
 

2271
00:44:44,640 --> 00:44:46,470
the context size is equal to four so
what I'm just doing is I'm taking these

2272
00:44:46,470 --> 00:44:46,480
what I'm just doing is I'm taking these
 

2273
00:44:46,480 --> 00:44:48,230
what I'm just doing is I'm taking these
four and stacking them on top of each

2274
00:44:48,230 --> 00:44:48,240
four and stacking them on top of each
 

2275
00:44:48,240 --> 00:44:50,309
four and stacking them on top of each
other that's how we create the first

2276
00:44:50,309 --> 00:44:50,319
other that's how we create the first
 

2277
00:44:50,319 --> 00:44:52,550
other that's how we create the first
input batch so similarly I can go

2278
00:44:52,550 --> 00:44:52,560
input batch so similarly I can go
 

2279
00:44:52,560 --> 00:44:54,309
input batch so similarly I can go
through my entire data set and create

2280
00:44:54,309 --> 00:44:54,319
through my entire data set and create
 

2281
00:44:54,319 --> 00:44:56,630
through my entire data set and create
multiple input batches that's how my

2282
00:44:56,630 --> 00:44:56,640
multiple input batches that's how my
 

2283
00:44:56,640 --> 00:44:59,270
multiple input batches that's how my
input batches are created now let me

2284
00:44:59,270 --> 00:44:59,280
input batches are created now let me
 

2285
00:44:59,280 --> 00:45:01,750
input batches are created now let me
talk a bit about how the output batches

2286
00:45:01,750 --> 00:45:01,760
talk a bit about how the output batches
 

2287
00:45:01,760 --> 00:45:03,829
talk a bit about how the output batches
are created or how the output pairs of

2288
00:45:03,829 --> 00:45:03,839
are created or how the output pairs of
 

2289
00:45:03,839 --> 00:45:07,790
are created or how the output pairs of
data are created. So let me rub this so

2290
00:45:07,790 --> 00:45:07,800
data are created. So let me rub this so
 

2291
00:45:07,800 --> 00:45:10,550
data are created. So let me rub this so
that I'll show you how the output is

2292
00:45:10,550 --> 00:45:10,560
that I'll show you how the output is
 

2293
00:45:10,560 --> 00:45:12,870
that I'll show you how the output is
created for the first input. Right? So

2294
00:45:12,870 --> 00:45:12,880
created for the first input. Right? So
 

2295
00:45:12,880 --> 00:45:14,950
created for the first input. Right? So
if you take a look at this first input

2296
00:45:14,950 --> 00:45:14,960
if you take a look at this first input
 

2297
00:45:14,960 --> 00:45:17,349
if you take a look at this first input
one day a little girl and I'm ignoring

2298
00:45:17,349 --> 00:45:17,359
one day a little girl and I'm ignoring
 

2299
00:45:17,359 --> 00:45:18,950
one day a little girl and I'm ignoring
this comma here for the sake of

2300
00:45:18,950 --> 00:45:18,960
this comma here for the sake of
 

2301
00:45:18,960 --> 00:45:21,430
this comma here for the sake of
simplicity. The output is just going to

2302
00:45:21,430 --> 00:45:21,440
simplicity. The output is just going to
 

2303
00:45:21,440 --> 00:45:23,270
simplicity. The output is just going to
be the input shifted to the right hand

2304
00:45:23,270 --> 00:45:23,280
be the input shifted to the right hand
 

2305
00:45:23,280 --> 00:45:25,510
be the input shifted to the right hand
side by one.

2306
00:45:25,510 --> 00:45:25,520
side by one.
 

2307
00:45:25,520 --> 00:45:27,510
side by one.
So the output of this first input

2308
00:45:27,510 --> 00:45:27,520
So the output of this first input
 

2309
00:45:27,520 --> 00:45:28,829
So the output of this first input
sequence is

2310
00:45:28,829 --> 00:45:28,839
sequence is
 

2311
00:45:28,839 --> 00:45:34,630
sequence is
this day a little girl um yeah a day a

2312
00:45:34,630 --> 00:45:34,640
this day a little girl um yeah a day a
 

2313
00:45:34,640 --> 00:45:36,470
this day a little girl um yeah a day a
little girl named that's going to be the

2314
00:45:36,470 --> 00:45:36,480
little girl named that's going to be the
 

2315
00:45:36,480 --> 00:45:39,589
little girl named that's going to be the
output sequence

2316
00:45:39,589 --> 00:45:39,599
output sequence
 

2317
00:45:39,599 --> 00:45:43,030
output sequence
um and if you look at for example if I

2318
00:45:43,030 --> 00:45:43,040
um and if you look at for example if I
 

2319
00:45:43,040 --> 00:45:45,030
um and if you look at for example if I
do it over here actually here the first

2320
00:45:45,030 --> 00:45:45,040
do it over here actually here the first
 

2321
00:45:45,040 --> 00:45:47,030
do it over here actually here the first
input should be of a sequence size equal

2322
00:45:47,030 --> 00:45:47,040
input should be of a sequence size equal
 

2323
00:45:47,040 --> 00:45:49,670
input should be of a sequence size equal
to four right so I should probably so

2324
00:45:49,670 --> 00:45:49,680
to four right so I should probably so
 

2325
00:45:49,680 --> 00:45:52,829
to four right so I should probably so
this is my first input and

2326
00:45:52,829 --> 00:45:52,839
this is my first input and
 

2327
00:45:52,839 --> 00:45:55,589
this is my first input and
uh my first output is going going to be

2328
00:45:55,589 --> 00:45:55,599
uh my first output is going going to be
 

2329
00:45:55,599 --> 00:45:57,270
uh my first output is going going to be
the first input shifted to the right

2330
00:45:57,270 --> 00:45:57,280
the first input shifted to the right
 

2331
00:45:57,280 --> 00:45:58,950
the first input shifted to the right
hand side by one. So this is going to be

2332
00:45:58,950 --> 00:45:58,960
hand side by one. So this is going to be
 

2333
00:45:58,960 --> 00:46:02,670
hand side by one. So this is going to be
my first output. Similarly, if my second

2334
00:46:02,670 --> 00:46:02,680
my first output. Similarly, if my second
 

2335
00:46:02,680 --> 00:46:05,510
my first output. Similarly, if my second
input, if my second input is let's say

2336
00:46:05,510 --> 00:46:05,520
input, if my second input is let's say
 

2337
00:46:05,520 --> 00:46:08,230
input, if my second input is let's say
this, which is x3, then the output

2338
00:46:08,230 --> 00:46:08,240
this, which is x3, then the output
 

2339
00:46:08,240 --> 00:46:10,550
this, which is x3, then the output
corresponding to this is just x3 shifted

2340
00:46:10,550 --> 00:46:10,560
corresponding to this is just x3 shifted
 

2341
00:46:10,560 --> 00:46:12,309
corresponding to this is just x3 shifted
to the right by one. So that is going to

2342
00:46:12,309 --> 00:46:12,319
to the right by one. So that is going to
 

2343
00:46:12,319 --> 00:46:15,349
to the right by one. So that is going to
be my y3. Now this is how the output

2344
00:46:15,349 --> 00:46:15,359
be my y3. Now this is how the output
 

2345
00:46:15,359 --> 00:46:17,430
be my y3. Now this is how the output
pairs are created. So if you look at

2346
00:46:17,430 --> 00:46:17,440
pairs are created. So if you look at
 

2347
00:46:17,440 --> 00:46:21,270
pairs are created. So if you look at
every input pair if x1 is 1 11 15 and 24

2348
00:46:21,270 --> 00:46:21,280
every input pair if x1 is 1 11 15 and 24
 

2349
00:46:21,280 --> 00:46:23,670
every input pair if x1 is 1 11 15 and 24
the output corresponding to that is just

2350
00:46:23,670 --> 00:46:23,680
the output corresponding to that is just
 

2351
00:46:23,680 --> 00:46:25,589
the output corresponding to that is just
the input shifted to the right hand side

2352
00:46:25,589 --> 00:46:25,599
the input shifted to the right hand side
 

2353
00:46:25,599 --> 00:46:30,589
the input shifted to the right hand side
by one. So if you see 1 11 15

2354
00:46:30,589 --> 00:46:30,599
by one. So if you see 1 11 15
 

2355
00:46:30,599 --> 00:46:33,349
by one. So if you see 1 11 15
24 11 etc. Let's say this is how the

2356
00:46:33,349 --> 00:46:33,359
24 11 etc. Let's say this is how the
 

2357
00:46:33,359 --> 00:46:35,910
24 11 etc. Let's say this is how the
token ids are arranged right in my

2358
00:46:35,910 --> 00:46:35,920
token ids are arranged right in my
 

2359
00:46:35,920 --> 00:46:38,309
token ids are arranged right in my
train.bin. If my first input is this

2360
00:46:38,309 --> 00:46:38,319
train.bin. If my first input is this
 

2361
00:46:38,319 --> 00:46:41,349
train.bin. If my first input is this
which is x1 my first output is just this

2362
00:46:41,349 --> 00:46:41,359
which is x1 my first output is just this
 

2363
00:46:41,359 --> 00:46:43,270
which is x1 my first output is just this
input shifted to the right hand side by

2364
00:46:43,270 --> 00:46:43,280
input shifted to the right hand side by
 

2365
00:46:43,280 --> 00:46:46,870
input shifted to the right hand side by
one. So that will be

2366
00:46:46,870 --> 00:46:46,880
one. So that will be
 

2367
00:46:46,880 --> 00:46:50,230
one. So that will be
uh 11 15 24 and 11. That's exactly what

2368
00:46:50,230 --> 00:46:50,240
uh 11 15 24 and 11. That's exactly what
 

2369
00:46:50,240 --> 00:46:51,829
uh 11 15 24 and 11. That's exactly what
is shown over here. So you see they have

2370
00:46:51,829 --> 00:46:51,839
is shown over here. So you see they have
 

2371
00:46:51,839 --> 00:46:54,790
is shown over here. So you see they have
an overlap of three over here. That is

2372
00:46:54,790 --> 00:46:54,800
an overlap of three over here. That is
 

2373
00:46:54,800 --> 00:46:57,030
an overlap of three over here. That is
how every output is constructed from

2374
00:46:57,030 --> 00:46:57,040
how every output is constructed from
 

2375
00:46:57,040 --> 00:46:58,790
how every output is constructed from
every input. So essentially once you

2376
00:46:58,790 --> 00:46:58,800
every input. So essentially once you
 

2377
00:46:58,800 --> 00:47:01,430
every input. So essentially once you
have x1, x2, x3, x4, the output tensor

2378
00:47:01,430 --> 00:47:01,440
have x1, x2, x3, x4, the output tensor
 

2379
00:47:01,440 --> 00:47:03,829
have x1, x2, x3, x4, the output tensor
or the output batch number one. This is

2380
00:47:03,829 --> 00:47:03,839
or the output batch number one. This is
 

2381
00:47:03,839 --> 00:47:06,309
or the output batch number one. This is
my output batch one. It's quite easy to

2382
00:47:06,309 --> 00:47:06,319
my output batch one. It's quite easy to
 

2383
00:47:06,319 --> 00:47:09,430
my output batch one. It's quite easy to
construct because it is just the input

2384
00:47:09,430 --> 00:47:09,440
construct because it is just the input
 

2385
00:47:09,440 --> 00:47:12,230
construct because it is just the input
inputs x1 x2 x3 x4 shifted to the right

2386
00:47:12,230 --> 00:47:12,240
inputs x1 x2 x3 x4 shifted to the right
 

2387
00:47:12,240 --> 00:47:15,510
inputs x1 x2 x3 x4 shifted to the right
hand side by one. So imagine all the

2388
00:47:15,510 --> 00:47:15,520
hand side by one. So imagine all the
 

2389
00:47:15,520 --> 00:47:17,829
hand side by one. So imagine all the
data set imagine sliding over the data

2390
00:47:17,829 --> 00:47:17,839
data set imagine sliding over the data
 

2391
00:47:17,839 --> 00:47:20,390
data set imagine sliding over the data
set in chunks of four. That will be my

2392
00:47:20,390 --> 00:47:20,400
set in chunks of four. That will be my
 

2393
00:47:20,400 --> 00:47:22,470
set in chunks of four. That will be my
input and just moving those chunks to

2394
00:47:22,470 --> 00:47:22,480
input and just moving those chunks to
 

2395
00:47:22,480 --> 00:47:23,750
input and just moving those chunks to
the right hand side by one and

2396
00:47:23,750 --> 00:47:23,760
the right hand side by one and
 

2397
00:47:23,760 --> 00:47:25,430
the right hand side by one and
aggregating them together in batches.

2398
00:47:25,430 --> 00:47:25,440
aggregating them together in batches.
 

2399
00:47:25,440 --> 00:47:28,230
aggregating them together in batches.
That's going to be my output. Now if you

2400
00:47:28,230 --> 00:47:28,240
That's going to be my output. Now if you
 

2401
00:47:28,240 --> 00:47:30,950
That's going to be my output. Now if you
look at let's look at the first input

2402
00:47:30,950 --> 00:47:30,960
look at let's look at the first input
 

2403
00:47:30,960 --> 00:47:35,030
look at let's look at the first input
and the first output right. So my x1 is

2404
00:47:35,030 --> 00:47:35,040
and the first output right. So my x1 is
 

2405
00:47:35,040 --> 00:47:40,870
and the first output right. So my x1 is
1 11 15 and 24 and my y1 is essentially

2406
00:47:40,870 --> 00:47:40,880
1 11 15 and 24 and my y1 is essentially
 

2407
00:47:40,880 --> 00:47:46,630
1 11 15 and 24 and my y1 is essentially
11 15 24 and 11. Okay. And let's assume

2408
00:47:46,630 --> 00:47:46,640
11 15 24 and 11. Okay. And let's assume
 

2409
00:47:46,640 --> 00:47:48,870
11 15 24 and 11. Okay. And let's assume
some 1 day a little. So let's say this

2410
00:47:48,870 --> 00:47:48,880
some 1 day a little. So let's say this
 

2411
00:47:48,880 --> 00:47:50,190
some 1 day a little. So let's say this
corresponds

2412
00:47:50,190 --> 00:47:50,200
corresponds
 

2413
00:47:50,200 --> 00:47:53,950
corresponds
to one day a

2414
00:47:53,950 --> 00:47:53,960
to one day a
 

2415
00:47:53,960 --> 00:47:57,950
to one day a
little and this corresponds to day

2416
00:47:57,950 --> 00:47:57,960
little and this corresponds to day
 

2417
00:47:57,960 --> 00:48:02,230
little and this corresponds to day
a little girl. Now you might be thinking

2418
00:48:02,230 --> 00:48:02,240
a little girl. Now you might be thinking
 

2419
00:48:02,240 --> 00:48:04,230
a little girl. Now you might be thinking
that if this is my input and this is the

2420
00:48:04,230 --> 00:48:04,240
that if this is my input and this is the
 

2421
00:48:04,240 --> 00:48:05,750
that if this is my input and this is the
output

2422
00:48:05,750 --> 00:48:05,760
output
 

2423
00:48:05,760 --> 00:48:08,230
output
uh what exactly are the input output

2424
00:48:08,230 --> 00:48:08,240
uh what exactly are the input output
 

2425
00:48:08,240 --> 00:48:11,270
uh what exactly are the input output
pairs over here. So it turns out or what

2426
00:48:11,270 --> 00:48:11,280
pairs over here. So it turns out or what
 

2427
00:48:11,280 --> 00:48:13,430
pairs over here. So it turns out or what
are we really predicting here? Remember

2428
00:48:13,430 --> 00:48:13,440
are we really predicting here? Remember
 

2429
00:48:13,440 --> 00:48:16,710
are we really predicting here? Remember
I told you that language models are just

2430
00:48:16,710 --> 00:48:16,720
I told you that language models are just
 

2431
00:48:16,720 --> 00:48:19,270
I told you that language models are just
next token prediction. So it turns out

2432
00:48:19,270 --> 00:48:19,280
next token prediction. So it turns out
 

2433
00:48:19,280 --> 00:48:22,069
next token prediction. So it turns out
that in one input output pair like this

2434
00:48:22,069 --> 00:48:22,079
that in one input output pair like this
 

2435
00:48:22,079 --> 00:48:24,069
that in one input output pair like this
there are multiple next token prediction

2436
00:48:24,069 --> 00:48:24,079
there are multiple next token prediction
 

2437
00:48:24,079 --> 00:48:27,030
there are multiple next token prediction
tasks. So if one is the input then day

2438
00:48:27,030 --> 00:48:27,040
tasks. So if one is the input then day
 

2439
00:48:27,040 --> 00:48:29,430
tasks. So if one is the input then day
should be the output. If one day is the

2440
00:48:29,430 --> 00:48:29,440
should be the output. If one day is the
 

2441
00:48:29,440 --> 00:48:31,910
should be the output. If one day is the
input, then should be the output. If one

2442
00:48:31,910 --> 00:48:31,920
input, then should be the output. If one
 

2443
00:48:31,920 --> 00:48:34,150
input, then should be the output. If one
day is the input, a little should be the

2444
00:48:34,150 --> 00:48:34,160
day is the input, a little should be the
 

2445
00:48:34,160 --> 00:48:36,390
day is the input, a little should be the
output. And if one day a little is the

2446
00:48:36,390 --> 00:48:36,400
output. And if one day a little is the
 

2447
00:48:36,400 --> 00:48:38,470
output. And if one day a little is the
input, then girl should be the output.

2448
00:48:38,470 --> 00:48:38,480
input, then girl should be the output.
 

2449
00:48:38,480 --> 00:48:40,630
input, then girl should be the output.
We train the model on all these four

2450
00:48:40,630 --> 00:48:40,640
We train the model on all these four
 

2451
00:48:40,640 --> 00:48:43,030
We train the model on all these four
tasks. So when you look at one input

2452
00:48:43,030 --> 00:48:43,040
tasks. So when you look at one input
 

2453
00:48:43,040 --> 00:48:45,589
tasks. So when you look at one input
output pair, it just it's not just one

2454
00:48:45,589 --> 00:48:45,599
output pair, it just it's not just one
 

2455
00:48:45,599 --> 00:48:47,670
output pair, it just it's not just one
input and prediction. It's not just one

2456
00:48:47,670 --> 00:48:47,680
input and prediction. It's not just one
 

2457
00:48:47,680 --> 00:48:49,750
input and prediction. It's not just one
prediction tasks. There are four

2458
00:48:49,750 --> 00:48:49,760
prediction tasks. There are four
 

2459
00:48:49,760 --> 00:48:51,870
prediction tasks. There are four
prediction tasks which are happening

2460
00:48:51,870 --> 00:48:51,880
prediction tasks which are happening
 

2461
00:48:51,880 --> 00:48:56,790
prediction tasks which are happening
here. So let me write down this um these

2462
00:48:56,790 --> 00:48:56,800
here. So let me write down this um these
 

2463
00:48:56,800 --> 00:48:59,750
here. So let me write down this um these
four tasks. So one is the input and day

2464
00:48:59,750 --> 00:48:59,760
four tasks. So one is the input and day
 

2465
00:48:59,760 --> 00:49:02,150
four tasks. So one is the input and day
should be the output. One day is the

2466
00:49:02,150 --> 00:49:02,160
should be the output. One day is the
 

2467
00:49:02,160 --> 00:49:06,390
should be the output. One day is the
input. O should be the output. One day O

2468
00:49:06,390 --> 00:49:06,400
input. O should be the output. One day O
 

2469
00:49:06,400 --> 00:49:09,990
input. O should be the output. One day O
is the input, little is the output and

2470
00:49:09,990 --> 00:49:10,000
is the input, little is the output and
 

2471
00:49:10,000 --> 00:49:13,109
is the input, little is the output and
one day or little is the input and girl

2472
00:49:13,109 --> 00:49:13,119
one day or little is the input and girl
 

2473
00:49:13,119 --> 00:49:15,430
one day or little is the input and girl
is the output. So remember this column

2474
00:49:15,430 --> 00:49:15,440
is the output. So remember this column
 

2475
00:49:15,440 --> 00:49:17,990
is the output. So remember this column
is my input and this column is my

2476
00:49:17,990 --> 00:49:18,000
is my input and this column is my
 

2477
00:49:18,000 --> 00:49:20,790
is my input and this column is my
target. So when my model is initially in

2478
00:49:20,790 --> 00:49:20,800
target. So when my model is initially in
 

2479
00:49:20,800 --> 00:49:23,510
target. So when my model is initially in
my model is initialized its output which

2480
00:49:23,510 --> 00:49:23,520
my model is initialized its output which
 

2481
00:49:23,520 --> 00:49:25,109
my model is initialized its output which
it gives will be something completely

2482
00:49:25,109 --> 00:49:25,119
it gives will be something completely
 

2483
00:49:25,119 --> 00:49:27,349
it gives will be something completely
different than the target. What we have

2484
00:49:27,349 --> 00:49:27,359
different than the target. What we have
 

2485
00:49:27,359 --> 00:49:29,390
different than the target. What we have
to do is that the model

2486
00:49:29,390 --> 00:49:29,400
to do is that the model
 

2487
00:49:29,400 --> 00:49:32,390
to do is that the model
output has to be as close to the target

2488
00:49:32,390 --> 00:49:32,400
output has to be as close to the target
 

2489
00:49:32,400 --> 00:49:34,150
output has to be as close to the target
as possible. That is what we have to

2490
00:49:34,150 --> 00:49:34,160
as possible. That is what we have to
 

2491
00:49:34,160 --> 00:49:36,790
as possible. That is what we have to
ensure in the back propagation. So when

2492
00:49:36,790 --> 00:49:36,800
ensure in the back propagation. So when
 

2493
00:49:36,800 --> 00:49:38,950
ensure in the back propagation. So when
you look at these input and target pairs

2494
00:49:38,950 --> 00:49:38,960
you look at these input and target pairs
 

2495
00:49:38,960 --> 00:49:41,190
you look at these input and target pairs
for every row there are four input

2496
00:49:41,190 --> 00:49:41,200
for every row there are four input
 

2497
00:49:41,200 --> 00:49:42,870
for every row there are four input
output prediction tasks right and

2498
00:49:42,870 --> 00:49:42,880
output prediction tasks right and
 

2499
00:49:42,880 --> 00:49:44,549
output prediction tasks right and
similarly there are four such rows in

2500
00:49:44,549 --> 00:49:44,559
similarly there are four such rows in
 

2501
00:49:44,559 --> 00:49:45,470
similarly there are four such rows in
every

2502
00:49:45,470 --> 00:49:45,480
every
 

2503
00:49:45,480 --> 00:49:48,390
every
batch. So we are essentially training

2504
00:49:48,390 --> 00:49:48,400
batch. So we are essentially training
 

2505
00:49:48,400 --> 00:49:50,710
batch. So we are essentially training
the model to predict the next token at

2506
00:49:50,710 --> 00:49:50,720
the model to predict the next token at
 

2507
00:49:50,720 --> 00:49:53,270
the model to predict the next token at
every stage of the process. So if you

2508
00:49:53,270 --> 00:49:53,280
every stage of the process. So if you
 

2509
00:49:53,280 --> 00:49:55,910
every stage of the process. So if you
start looking at this paragraph from now

2510
00:49:55,910 --> 00:49:55,920
start looking at this paragraph from now
 

2511
00:49:55,920 --> 00:49:57,670
start looking at this paragraph from now
from the start itself what we are

2512
00:49:57,670 --> 00:49:57,680
from the start itself what we are
 

2513
00:49:57,680 --> 00:49:59,270
from the start itself what we are
essentially doing by creating these

2514
00:49:59,270 --> 00:49:59,280
essentially doing by creating these
 

2515
00:49:59,280 --> 00:50:02,150
essentially doing by creating these
input output pairs is that when one is

2516
00:50:02,150 --> 00:50:02,160
input output pairs is that when one is
 

2517
00:50:02,160 --> 00:50:04,230
input output pairs is that when one is
the input day is the output we teaching

2518
00:50:04,230 --> 00:50:04,240
the input day is the output we teaching
 

2519
00:50:04,240 --> 00:50:06,309
the input day is the output we teaching
the model that when one day is the input

2520
00:50:06,309 --> 00:50:06,319
the model that when one day is the input
 

2521
00:50:06,319 --> 00:50:09,109
the model that when one day is the input
o is the output when one day is the

2522
00:50:09,109 --> 00:50:09,119
o is the output when one day is the
 

2523
00:50:09,119 --> 00:50:11,589
o is the output when one day is the
input little is the output when one day

2524
00:50:11,589 --> 00:50:11,599
input little is the output when one day
 

2525
00:50:11,599 --> 00:50:13,589
input little is the output when one day
a little is the input girl is the output

2526
00:50:13,589 --> 00:50:13,599
a little is the input girl is the output
 

2527
00:50:13,599 --> 00:50:16,870
a little is the input girl is the output
that's x1 y1 training then we go to x2

2528
00:50:16,870 --> 00:50:16,880
that's x1 y1 training then we go to x2
 

2529
00:50:16,880 --> 00:50:20,069
that's x1 y1 training then we go to x2
y2 training so x2 is now girl named lily

2530
00:50:20,069 --> 00:50:20,079
y2 training so x2 is now girl named lily
 

2531
00:50:20,079 --> 00:50:22,309
y2 training so x2 is now girl named lily
found right so when girl is the input

2532
00:50:22,309 --> 00:50:22,319
found right so when girl is the input
 

2533
00:50:22,319 --> 00:50:24,230
found right so when girl is the input
named is the output. When girl named is

2534
00:50:24,230 --> 00:50:24,240
named is the output. When girl named is
 

2535
00:50:24,240 --> 00:50:26,069
named is the output. When girl named is
the input, li is the output. When girl

2536
00:50:26,069 --> 00:50:26,079
the input, li is the output. When girl
 

2537
00:50:26,079 --> 00:50:28,150
the input, li is the output. When girl
named li is the output, find found is

2538
00:50:28,150 --> 00:50:28,160
named li is the output, find found is
 

2539
00:50:28,160 --> 00:50:30,309
named li is the output, find found is
the output. So essentially we're

2540
00:50:30,309 --> 00:50:30,319
the output. So essentially we're
 

2541
00:50:30,319 --> 00:50:32,230
the output. So essentially we're
training the model at every stage to

2542
00:50:32,230 --> 00:50:32,240
training the model at every stage to
 

2543
00:50:32,240 --> 00:50:34,630
training the model at every stage to
predict the next token and we are doing

2544
00:50:34,630 --> 00:50:34,640
predict the next token and we are doing
 

2545
00:50:34,640 --> 00:50:36,630
predict the next token and we are doing
that through creation of this input and

2546
00:50:36,630 --> 00:50:36,640
that through creation of this input and
 

2547
00:50:36,640 --> 00:50:37,710
that through creation of this input and
output

2548
00:50:37,710 --> 00:50:37,720
output
 

2549
00:50:37,720 --> 00:50:40,390
output
batches. I hope this part is very clear

2550
00:50:40,390 --> 00:50:40,400
batches. I hope this part is very clear
 

2551
00:50:40,400 --> 00:50:42,390
batches. I hope this part is very clear
to you. It's very important to visualize

2552
00:50:42,390 --> 00:50:42,400
to you. It's very important to visualize
 

2553
00:50:42,400 --> 00:50:44,230
to you. It's very important to visualize
this part before we go to the model

2554
00:50:44,230 --> 00:50:44,240
this part before we go to the model
 

2555
00:50:44,240 --> 00:50:46,870
this part before we go to the model
architecture itself because here is

2556
00:50:46,870 --> 00:50:46,880
architecture itself because here is
 

2557
00:50:46,880 --> 00:50:49,190
architecture itself because here is
where all the confusion in language

2558
00:50:49,190 --> 00:50:49,200
where all the confusion in language
 

2559
00:50:49,200 --> 00:50:50,950
where all the confusion in language
modeling starts. You have to know how

2560
00:50:50,950 --> 00:50:50,960
modeling starts. You have to know how
 

2561
00:50:50,960 --> 00:50:53,109
modeling starts. You have to know how
the input and target are created. And

2562
00:50:53,109 --> 00:50:53,119
the input and target are created. And
 

2563
00:50:53,119 --> 00:50:55,430
the input and target are created. And
let me give you a short summary now of

2564
00:50:55,430 --> 00:50:55,440
let me give you a short summary now of
 

2565
00:50:55,440 --> 00:50:56,950
let me give you a short summary now of
this whole process. Let's say we have

2566
00:50:56,950 --> 00:50:56,960
this whole process. Let's say we have
 

2567
00:50:56,960 --> 00:50:58,270
this whole process. Let's say we have
this data set,

2568
00:50:58,270 --> 00:50:58,280
this data set,
 

2569
00:50:58,280 --> 00:51:00,710
this data set,
right? Let's say we have this data set.

2570
00:51:00,710 --> 00:51:00,720
right? Let's say we have this data set.
 

2571
00:51:00,720 --> 00:51:02,309
right? Let's say we have this data set.
The first thing which is done is that I

2572
00:51:02,309 --> 00:51:02,319
The first thing which is done is that I
 

2573
00:51:02,319 --> 00:51:04,309
The first thing which is done is that I
decide my context size. If it's equal to

2574
00:51:04,309 --> 00:51:04,319
decide my context size. If it's equal to
 

2575
00:51:04,319 --> 00:51:06,710
decide my context size. If it's equal to
four, I create input chunks which are

2576
00:51:06,710 --> 00:51:06,720
four, I create input chunks which are
 

2577
00:51:06,720 --> 00:51:08,470
four, I create input chunks which are
like this. So I divide the whole data

2578
00:51:08,470 --> 00:51:08,480
like this. So I divide the whole data
 

2579
00:51:08,480 --> 00:51:10,710
like this. So I divide the whole data
set into chunks of four. That will be

2580
00:51:10,710 --> 00:51:10,720
set into chunks of four. That will be
 

2581
00:51:10,720 --> 00:51:14,390
set into chunks of four. That will be
x1, x2, x3, x4 etc. And then along with

2582
00:51:14,390 --> 00:51:14,400
x1, x2, x3, x4 etc. And then along with
 

2583
00:51:14,400 --> 00:51:17,030
x1, x2, x3, x4 etc. And then along with
this I essentially shift to shift this

2584
00:51:17,030 --> 00:51:17,040
this I essentially shift to shift this
 

2585
00:51:17,040 --> 00:51:19,430
this I essentially shift to shift this
to the right hand side by one. So that

2586
00:51:19,430 --> 00:51:19,440
to the right hand side by one. So that
 

2587
00:51:19,440 --> 00:51:22,230
to the right hand side by one. So that
is how I create my output. That is how I

2588
00:51:22,230 --> 00:51:22,240
is how I create my output. That is how I
 

2589
00:51:22,240 --> 00:51:24,470
is how I create my output. That is how I
create my output chunks. Right? So I

2590
00:51:24,470 --> 00:51:24,480
create my output chunks. Right? So I
 

2591
00:51:24,480 --> 00:51:26,549
create my output chunks. Right? So I
have my input chunks. I have my output

2592
00:51:26,549 --> 00:51:26,559
have my input chunks. I have my output
 

2593
00:51:26,559 --> 00:51:28,390
have my input chunks. I have my output
chunks. Then I divide my input chunks

2594
00:51:28,390 --> 00:51:28,400
chunks. Then I divide my input chunks
 

2595
00:51:28,400 --> 00:51:30,309
chunks. Then I divide my input chunks
into batches. I divide my output chunks

2596
00:51:30,309 --> 00:51:30,319
into batches. I divide my output chunks
 

2597
00:51:30,319 --> 00:51:32,790
into batches. I divide my output chunks
into batches. If you look at one batch

2598
00:51:32,790 --> 00:51:32,800
into batches. If you look at one batch
 

2599
00:51:32,800 --> 00:51:35,670
into batches. If you look at one batch
closely, there will be four in X1, Y1,

2600
00:51:35,670 --> 00:51:35,680
closely, there will be four in X1, Y1,
 

2601
00:51:35,680 --> 00:51:39,030
closely, there will be four in X1, Y1,
X2, Y2, X3, Y3 and X4, Y4. If you

2602
00:51:39,030 --> 00:51:39,040
X2, Y2, X3, Y3 and X4, Y4. If you
 

2603
00:51:39,040 --> 00:51:41,670
X2, Y2, X3, Y3 and X4, Y4. If you
analyze each row here, X1 and Y1, there

2604
00:51:41,670 --> 00:51:41,680
analyze each row here, X1 and Y1, there
 

2605
00:51:41,680 --> 00:51:44,470
analyze each row here, X1 and Y1, there
are essentially four input output tasks

2606
00:51:44,470 --> 00:51:44,480
are essentially four input output tasks
 

2607
00:51:44,480 --> 00:51:46,230
are essentially four input output tasks
here which we are training the model.

2608
00:51:46,230 --> 00:51:46,240
here which we are training the model.
 

2609
00:51:46,240 --> 00:51:48,150
here which we are training the model.
And in each task we train the model to

2610
00:51:48,150 --> 00:51:48,160
And in each task we train the model to
 

2611
00:51:48,160 --> 00:51:50,230
And in each task we train the model to
predict the next token. So when the

2612
00:51:50,230 --> 00:51:50,240
predict the next token. So when the
 

2613
00:51:50,240 --> 00:51:52,630
predict the next token. So when the
whole training proceeds the model knows

2614
00:51:52,630 --> 00:51:52,640
whole training proceeds the model knows
 

2615
00:51:52,640 --> 00:51:54,870
whole training proceeds the model knows
that whatever is the input let's say if

2616
00:51:54,870 --> 00:51:54,880
that whatever is the input let's say if
 

2617
00:51:54,880 --> 00:51:56,309
that whatever is the input let's say if
this is the input the model is trained

2618
00:51:56,309 --> 00:51:56,319
this is the input the model is trained
 

2619
00:51:56,319 --> 00:51:59,270
this is the input the model is trained
to predict the next token as the output.

2620
00:51:59,270 --> 00:51:59,280
to predict the next token as the output.
 

2621
00:51:59,280 --> 00:52:01,109
to predict the next token as the output.
So here you might be thinking that this

2622
00:52:01,109 --> 00:52:01,119
So here you might be thinking that this
 

2623
00:52:01,119 --> 00:52:04,230
So here you might be thinking that this
is is that is that the only training

2624
00:52:04,230 --> 00:52:04,240
is is that is that the only training
 

2625
00:52:04,240 --> 00:52:05,990
is is that is that the only training
objective in language models and it

2626
00:52:05,990 --> 00:52:06,000
objective in language models and it
 

2627
00:52:06,000 --> 00:52:07,910
objective in language models and it
turns out that the training objective is

2628
00:52:07,910 --> 00:52:07,920
turns out that the training objective is
 

2629
00:52:07,920 --> 00:52:09,910
turns out that the training objective is
as simple as this. We are just

2630
00:52:09,910 --> 00:52:09,920
as simple as this. We are just
 

2631
00:52:09,920 --> 00:52:12,549
as simple as this. We are just
predicting the next token at a time. And

2632
00:52:12,549 --> 00:52:12,559
predicting the next token at a time. And
 

2633
00:52:12,559 --> 00:52:14,950
predicting the next token at a time. And
what's quite amazing is that how does

2634
00:52:14,950 --> 00:52:14,960
what's quite amazing is that how does
 

2635
00:52:14,960 --> 00:52:17,990
what's quite amazing is that how does
the model really know language from this

2636
00:52:17,990 --> 00:52:18,000
the model really know language from this
 

2637
00:52:18,000 --> 00:52:20,549
the model really know language from this
simple training? How does it understand?

2638
00:52:20,549 --> 00:52:20,559
simple training? How does it understand?
 

2639
00:52:20,559 --> 00:52:22,390
simple training? How does it understand?
So if there's a sentence such as let's

2640
00:52:22,390 --> 00:52:22,400
So if there's a sentence such as let's
 

2641
00:52:22,400 --> 00:52:25,630
So if there's a sentence such as let's
say blue

2642
00:52:25,630 --> 00:52:25,640

 

2643
00:52:25,640 --> 00:52:29,710

electrons, blue electrons eat

2644
00:52:29,710 --> 00:52:29,720
electrons, blue electrons eat
 

2645
00:52:29,720 --> 00:52:32,069
electrons, blue electrons eat
fish. Let's say there is a sentence like

2646
00:52:32,069 --> 00:52:32,079
fish. Let's say there is a sentence like
 

2647
00:52:32,079 --> 00:52:34,109
fish. Let's say there is a sentence like
this, right? Which is blue electrons eat

2648
00:52:34,109 --> 00:52:34,119
this, right? Which is blue electrons eat
 

2649
00:52:34,119 --> 00:52:36,630
this, right? Which is blue electrons eat
fish. How does the model actually know

2650
00:52:36,630 --> 00:52:36,640
fish. How does the model actually know
 

2651
00:52:36,640 --> 00:52:38,390
fish. How does the model actually know
that it should not predict a sentence

2652
00:52:38,390 --> 00:52:38,400
that it should not predict a sentence
 

2653
00:52:38,400 --> 00:52:40,470
that it should not predict a sentence
such as this? Because ultimately we are

2654
00:52:40,470 --> 00:52:40,480
such as this? Because ultimately we are
 

2655
00:52:40,480 --> 00:52:41,990
such as this? Because ultimately we are
just training on the next token

2656
00:52:41,990 --> 00:52:42,000
just training on the next token
 

2657
00:52:42,000 --> 00:52:43,750
just training on the next token
prediction, right? We are not telling

2658
00:52:43,750 --> 00:52:43,760
prediction, right? We are not telling
 

2659
00:52:43,760 --> 00:52:45,349
prediction, right? We are not telling
the model anything about the world

2660
00:52:45,349 --> 00:52:45,359
the model anything about the world
 

2661
00:52:45,359 --> 00:52:47,829
the model anything about the world
around us. But through this simple task

2662
00:52:47,829 --> 00:52:47,839
around us. But through this simple task
 

2663
00:52:47,839 --> 00:52:49,670
around us. But through this simple task
of the next token prediction, the model

2664
00:52:49,670 --> 00:52:49,680
of the next token prediction, the model
 

2665
00:52:49,680 --> 00:52:51,349
of the next token prediction, the model
somehow understands that electrons

2666
00:52:51,349 --> 00:52:51,359
somehow understands that electrons
 

2667
00:52:51,359 --> 00:52:53,670
somehow understands that electrons
cannot be blue, not just that electrons

2668
00:52:53,670 --> 00:52:53,680
cannot be blue, not just that electrons
 

2669
00:52:53,680 --> 00:52:56,470
cannot be blue, not just that electrons
cannot eat fish. And so the model

2670
00:52:56,470 --> 00:52:56,480
cannot eat fish. And so the model
 

2671
00:52:56,480 --> 00:52:58,309
cannot eat fish. And so the model
understands the form of the language and

2672
00:52:58,309 --> 00:52:58,319
understands the form of the language and
 

2673
00:52:58,319 --> 00:53:00,069
understands the form of the language and
the meaning of the language as we are

2674
00:53:00,069 --> 00:53:00,079
the meaning of the language as we are
 

2675
00:53:00,079 --> 00:53:01,150
the meaning of the language as we are
showing over

2676
00:53:01,150 --> 00:53:01,160
showing over
 

2677
00:53:01,160 --> 00:53:03,589
showing over
here. Okay. So this is the main process

2678
00:53:03,589 --> 00:53:03,599
here. Okay. So this is the main process
 

2679
00:53:03,599 --> 00:53:05,510
here. Okay. So this is the main process
of creating input output pairs and this

2680
00:53:05,510 --> 00:53:05,520
of creating input output pairs and this
 

2681
00:53:05,520 --> 00:53:07,270
of creating input output pairs and this
is what we are going to exactly see in

2682
00:53:07,270 --> 00:53:07,280
is what we are going to exactly see in
 

2683
00:53:07,280 --> 00:53:09,430
is what we are going to exactly see in
the code. Now in the code you will

2684
00:53:09,430 --> 00:53:09,440
the code. Now in the code you will
 

2685
00:53:09,440 --> 00:53:12,710
the code. Now in the code you will
encounter a statement such as this ix is

2686
00:53:12,710 --> 00:53:12,720
encounter a statement such as this ix is
 

2687
00:53:12,720 --> 00:53:16,230
encounter a statement such as this ix is
equal to torch.trant int like this. So

2688
00:53:16,230 --> 00:53:16,240
equal to torch.trant int like this. So
 

2689
00:53:16,240 --> 00:53:17,990
equal to torch.trant int like this. So
what is exactly done in this statement

2690
00:53:17,990 --> 00:53:18,000
what is exactly done in this statement
 

2691
00:53:18,000 --> 00:53:20,309
what is exactly done in this statement
is that we create an id vector of length

2692
00:53:20,309 --> 00:53:20,319
is that we create an id vector of length
 

2693
00:53:20,319 --> 00:53:25,030
is that we create an id vector of length
batch size. Um so essentially what this

2694
00:53:25,030 --> 00:53:25,040
batch size. Um so essentially what this
 

2695
00:53:25,040 --> 00:53:28,069
batch size. Um so essentially what this
means is that if for every batch we

2696
00:53:28,069 --> 00:53:28,079
means is that if for every batch we
 

2697
00:53:28,079 --> 00:53:31,510
means is that if for every batch we
sample from random places. So let's say

2698
00:53:31,510 --> 00:53:31,520
sample from random places. So let's say
 

2699
00:53:31,520 --> 00:53:34,470
sample from random places. So let's say
so here I've collected x1 x2 x3 x4 in

2700
00:53:34,470 --> 00:53:34,480
so here I've collected x1 x2 x3 x4 in
 

2701
00:53:34,480 --> 00:53:36,790
so here I've collected x1 x2 x3 x4 in
together right? It can be like this is

2702
00:53:36,790 --> 00:53:36,800
together right? It can be like this is
 

2703
00:53:36,800 --> 00:53:39,190
together right? It can be like this is
x1 let's say this is x2 this is x3 and

2704
00:53:39,190 --> 00:53:39,200
x1 let's say this is x2 this is x3 and
 

2705
00:53:39,200 --> 00:53:41,990
x1 let's say this is x2 this is x3 and
this is x4 that's also fine and that's

2706
00:53:41,990 --> 00:53:42,000
this is x4 that's also fine and that's
 

2707
00:53:42,000 --> 00:53:43,589
this is x4 that's also fine and that's
how we are going to do the code. So if

2708
00:53:43,589 --> 00:53:43,599
how we are going to do the code. So if
 

2709
00:53:43,599 --> 00:53:46,549
how we are going to do the code. So if
this is x1 this is x2 x3 and x4 the

2710
00:53:46,549 --> 00:53:46,559
this is x1 this is x2 x3 and x4 the
 

2711
00:53:46,559 --> 00:53:49,030
this is x1 this is x2 x3 and x4 the
corresponding y will be of course this

2712
00:53:49,030 --> 00:53:49,040
corresponding y will be of course this
 

2713
00:53:49,040 --> 00:53:51,230
corresponding y will be of course this
shifted to the right hand side by

2714
00:53:51,230 --> 00:53:51,240
shifted to the right hand side by
 

2715
00:53:51,240 --> 00:53:54,790
shifted to the right hand side by
one. So whenever a batch is created

2716
00:53:54,790 --> 00:53:54,800
one. So whenever a batch is created
 

2717
00:53:54,800 --> 00:53:56,549
one. So whenever a batch is created
there we are going to define a function

2718
00:53:56,549 --> 00:53:56,559
there we are going to define a function
 

2719
00:53:56,559 --> 00:53:59,910
there we are going to define a function
which is called as get batch.

2720
00:53:59,910 --> 00:53:59,920
which is called as get batch.
 

2721
00:53:59,920 --> 00:54:01,670
which is called as get batch.
Whenever the function get batch is

2722
00:54:01,670 --> 00:54:01,680
Whenever the function get batch is
 

2723
00:54:01,680 --> 00:54:03,710
Whenever the function get batch is
defined, what it will do is that it will

2724
00:54:03,710 --> 00:54:03,720
defined, what it will do is that it will
 

2725
00:54:03,720 --> 00:54:06,549
defined, what it will do is that it will
create so and if the batch size is four,

2726
00:54:06,549 --> 00:54:06,559
create so and if the batch size is four,
 

2727
00:54:06,559 --> 00:54:09,270
create so and if the batch size is four,
it will take from random places four

2728
00:54:09,270 --> 00:54:09,280
it will take from random places four
 

2729
00:54:09,280 --> 00:54:11,750
it will take from random places four
vectors and create this input batch. It

2730
00:54:11,750 --> 00:54:11,760
vectors and create this input batch. It
 

2731
00:54:11,760 --> 00:54:13,349
vectors and create this input batch. It
will shift the input batch to the right

2732
00:54:13,349 --> 00:54:13,359
will shift the input batch to the right
 

2733
00:54:13,359 --> 00:54:15,750
will shift the input batch to the right
by one and create the output batch. So

2734
00:54:15,750 --> 00:54:15,760
by one and create the output batch. So
 

2735
00:54:15,760 --> 00:54:17,349
by one and create the output batch. So
that is what you'll see happening in the

2736
00:54:17,349 --> 00:54:17,359
that is what you'll see happening in the
 

2737
00:54:17,359 --> 00:54:19,750
that is what you'll see happening in the
code. That's the first thing. The second

2738
00:54:19,750 --> 00:54:19,760
code. That's the first thing. The second
 

2739
00:54:19,760 --> 00:54:21,190
code. That's the first thing. The second
thing which you'll see happening in the

2740
00:54:21,190 --> 00:54:21,200
thing which you'll see happening in the
 

2741
00:54:21,200 --> 00:54:23,030
thing which you'll see happening in the
code is we are doing something like

2742
00:54:23,030 --> 00:54:23,040
code is we are doing something like
 

2743
00:54:23,040 --> 00:54:27,270
code is we are doing something like
X.pin memory. So these input and output

2744
00:54:27,270 --> 00:54:27,280
X.pin memory. So these input and output
 

2745
00:54:27,280 --> 00:54:29,349
X.pin memory. So these input and output
tensors or matrices which we have we are

2746
00:54:29,349 --> 00:54:29,359
tensors or matrices which we have we are
 

2747
00:54:29,359 --> 00:54:31,549
tensors or matrices which we have we are
going to pin pin them to the

2748
00:54:31,549 --> 00:54:31,559
going to pin pin them to the
 

2749
00:54:31,559 --> 00:54:33,910
going to pin pin them to the
memory. So what this does is that it

2750
00:54:33,910 --> 00:54:33,920
memory. So what this does is that it
 

2751
00:54:33,920 --> 00:54:35,670
memory. So what this does is that it
locks the memory of the tensor in the

2752
00:54:35,670 --> 00:54:35,680
locks the memory of the tensor in the
 

2753
00:54:35,680 --> 00:54:38,390
locks the memory of the tensor in the
RAM and that allows faster transfer of

2754
00:54:38,390 --> 00:54:38,400
RAM and that allows faster transfer of
 

2755
00:54:38,400 --> 00:54:42,870
RAM and that allows faster transfer of
the tensor to the GPU. Uh now we are

2756
00:54:42,870 --> 00:54:42,880
the tensor to the GPU. Uh now we are
 

2757
00:54:42,880 --> 00:54:44,950
the tensor to the GPU. Uh now we are
going to transfer this input and this

2758
00:54:44,950 --> 00:54:44,960
going to transfer this input and this
 

2759
00:54:44,960 --> 00:54:47,270
going to transfer this input and this
output tensors to the GPU at some point

2760
00:54:47,270 --> 00:54:47,280
output tensors to the GPU at some point
 

2761
00:54:47,280 --> 00:54:49,109
output tensors to the GPU at some point
for calculations in the transformer

2762
00:54:49,109 --> 00:54:49,119
for calculations in the transformer
 

2763
00:54:49,119 --> 00:54:51,829
for calculations in the transformer
block. So having the pin memory locks

2764
00:54:51,829 --> 00:54:51,839
block. So having the pin memory locks
 

2765
00:54:51,839 --> 00:54:53,750
block. So having the pin memory locks
the memory of the tensor in RAM and we

2766
00:54:53,750 --> 00:54:53,760
the memory of the tensor in RAM and we
 

2767
00:54:53,760 --> 00:54:56,150
the memory of the tensor in RAM and we
can allow faster transfer of the tensor

2768
00:54:56,150 --> 00:54:56,160
can allow faster transfer of the tensor
 

2769
00:54:56,160 --> 00:54:58,549
can allow faster transfer of the tensor
to the GPU. So it's like telling our

2770
00:54:58,549 --> 00:54:58,559
to the GPU. So it's like telling our
 

2771
00:54:58,559 --> 00:55:00,309
to the GPU. So it's like telling our
operating system that hey don't move

2772
00:55:00,309 --> 00:55:00,319
operating system that hey don't move
 

2773
00:55:00,319 --> 00:55:03,030
operating system that hey don't move
this memory around my GPU would want it

2774
00:55:03,030 --> 00:55:03,040
this memory around my GPU would want it
 

2775
00:55:03,040 --> 00:55:05,750
this memory around my GPU would want it
very soon and so the memory is kind of

2776
00:55:05,750 --> 00:55:05,760
very soon and so the memory is kind of
 

2777
00:55:05,760 --> 00:55:07,589
very soon and so the memory is kind of
locked. The input and the output are

2778
00:55:07,589 --> 00:55:07,599
locked. The input and the output are
 

2779
00:55:07,599 --> 00:55:09,430
locked. The input and the output are
locked in memory so they can be

2780
00:55:09,430 --> 00:55:09,440
locked in memory so they can be
 

2781
00:55:09,440 --> 00:55:11,910
locked in memory so they can be
transferred to the GPU easily. And then

2782
00:55:11,910 --> 00:55:11,920
transferred to the GPU easily. And then
 

2783
00:55:11,920 --> 00:55:13,670
transferred to the GPU easily. And then
we are also doing this non-blocking

2784
00:55:13,670 --> 00:55:13,680
we are also doing this non-blocking
 

2785
00:55:13,680 --> 00:55:16,069
we are also doing this non-blocking
equal to true. The reason non-blocking

2786
00:55:16,069 --> 00:55:16,079
equal to true. The reason non-blocking
 

2787
00:55:16,079 --> 00:55:18,150
equal to true. The reason non-blocking
equal to true is done is because if you

2788
00:55:18,150 --> 00:55:18,160
equal to true is done is because if you
 

2789
00:55:18,160 --> 00:55:21,270
equal to true is done is because if you
normally do dot to device, it blocks the

2790
00:55:21,270 --> 00:55:21,280
normally do dot to device, it blocks the
 

2791
00:55:21,280 --> 00:55:24,710
normally do dot to device, it blocks the
CPU until the copy to the GPU finishes.

2792
00:55:24,710 --> 00:55:24,720
CPU until the copy to the GPU finishes.
 

2793
00:55:24,720 --> 00:55:26,710
CPU until the copy to the GPU finishes.
So non-blocking equal to true ensures

2794
00:55:26,710 --> 00:55:26,720
So non-blocking equal to true ensures
 

2795
00:55:26,720 --> 00:55:29,349
So non-blocking equal to true ensures
that CPU can continue to do other work

2796
00:55:29,349 --> 00:55:29,359
that CPU can continue to do other work
 

2797
00:55:29,359 --> 00:55:31,990
that CPU can continue to do other work
like next batch preparation etc. So the

2798
00:55:31,990 --> 00:55:32,000
like next batch preparation etc. So the
 

2799
00:55:32,000 --> 00:55:37,109
like next batch preparation etc. So the
CPU does not have to be blocked while uh

2800
00:55:37,109 --> 00:55:37,119
CPU does not have to be blocked while uh
 

2801
00:55:37,119 --> 00:55:39,990
CPU does not have to be blocked while uh
uh while we copy our data to the GPU. So

2802
00:55:39,990 --> 00:55:40,000
uh while we copy our data to the GPU. So
 

2803
00:55:40,000 --> 00:55:42,150
uh while we copy our data to the GPU. So
you'll see this also in the code. So let

2804
00:55:42,150 --> 00:55:42,160
you'll see this also in the code. So let
 

2805
00:55:42,160 --> 00:55:44,069
you'll see this also in the code. So let
us jump to code right now. If you scroll

2806
00:55:44,069 --> 00:55:44,079
us jump to code right now. If you scroll
 

2807
00:55:44,079 --> 00:55:46,390
us jump to code right now. If you scroll
down below to step number three, here is

2808
00:55:46,390 --> 00:55:46,400
down below to step number three, here is
 

2809
00:55:46,400 --> 00:55:48,230
down below to step number three, here is
where we create the input and the output

2810
00:55:48,230 --> 00:55:48,240
where we create the input and the output
 

2811
00:55:48,240 --> 00:55:50,230
where we create the input and the output
batches. Right? So this is the get a

2812
00:55:50,230 --> 00:55:50,240
batches. Right? So this is the get a
 

2813
00:55:50,240 --> 00:55:52,470
batches. Right? So this is the get a
batch function. Remember every get a

2814
00:55:52,470 --> 00:55:52,480
batch function. Remember every get a
 

2815
00:55:52,480 --> 00:55:55,589
batch function. Remember every get a
batch function is going to return one

2816
00:55:55,589 --> 00:55:55,599
batch function is going to return one
 

2817
00:55:55,599 --> 00:55:58,349
batch function is going to return one
input matrix and one output matrix like

2818
00:55:58,349 --> 00:55:58,359
input matrix and one output matrix like
 

2819
00:55:58,359 --> 00:56:00,950
input matrix and one output matrix like
this. Okay. So this is that first

2820
00:56:00,950 --> 00:56:00,960
this. Okay. So this is that first
 

2821
00:56:00,960 --> 00:56:02,630
this. Okay. So this is that first
statement I mentioned. So this

2822
00:56:02,630 --> 00:56:02,640
statement I mentioned. So this
 

2823
00:56:02,640 --> 00:56:04,829
statement I mentioned. So this
essentially samples four random

2824
00:56:04,829 --> 00:56:04,839
essentially samples four random
 

2825
00:56:04,839 --> 00:56:07,109
essentially samples four random
integers which essentially corresponds

2826
00:56:07,109 --> 00:56:07,119
integers which essentially corresponds
 

2827
00:56:07,119 --> 00:56:09,190
integers which essentially corresponds
to or four. I'm saying four because I'm

2828
00:56:09,190 --> 00:56:09,200
to or four. I'm saying four because I'm
 

2829
00:56:09,200 --> 00:56:11,589
to or four. I'm saying four because I'm
assuming batch size equal to four. So it

2830
00:56:11,589 --> 00:56:11,599
assuming batch size equal to four. So it
 

2831
00:56:11,599 --> 00:56:13,510
assuming batch size equal to four. So it
will correspond to randomly sampling

2832
00:56:13,510 --> 00:56:13,520
will correspond to randomly sampling
 

2833
00:56:13,520 --> 00:56:15,349
will correspond to randomly sampling
from four different places in my data

2834
00:56:15,349 --> 00:56:15,359
from four different places in my data
 

2835
00:56:15,359 --> 00:56:18,390
from four different places in my data
set. That's IX. What X essentially does

2836
00:56:18,390 --> 00:56:18,400
set. That's IX. What X essentially does
 

2837
00:56:18,400 --> 00:56:19,230
set. That's IX. What X essentially does
is

2838
00:56:19,230 --> 00:56:19,240
is
 

2839
00:56:19,240 --> 00:56:23,670
is
that it stacks it stacks together X1,

2840
00:56:23,670 --> 00:56:23,680
that it stacks it stacks together X1,
 

2841
00:56:23,680 --> 00:56:27,510
that it stacks it stacks together X1,
X2, X3 and X4. Y stacks together Y1, Y2,

2842
00:56:27,510 --> 00:56:27,520
X2, X3 and X4. Y stacks together Y1, Y2,
 

2843
00:56:27,520 --> 00:56:30,549
X2, X3 and X4. Y stacks together Y1, Y2,
Y3 and Y4. And if you look at X1 and Y1,

2844
00:56:30,549 --> 00:56:30,559
Y3 and Y4. And if you look at X1 and Y1,
 

2845
00:56:30,559 --> 00:56:32,470
Y3 and Y4. And if you look at X1 and Y1,
let's say if you see Y is just shifted

2846
00:56:32,470 --> 00:56:32,480
let's say if you see Y is just shifted
 

2847
00:56:32,480 --> 00:56:35,109
let's say if you see Y is just shifted
to the right hand side by one. So what

2848
00:56:35,109 --> 00:56:35,119
to the right hand side by one. So what
 

2849
00:56:35,119 --> 00:56:37,109
to the right hand side by one. So what
is being mentioned as block size in

2850
00:56:37,109 --> 00:56:37,119
is being mentioned as block size in
 

2851
00:56:37,119 --> 00:56:39,910
is being mentioned as block size in
here? That is the context size which we

2852
00:56:39,910 --> 00:56:39,920
here? That is the context size which we
 

2853
00:56:39,920 --> 00:56:42,309
here? That is the context size which we
have in the code and the batch size is

2854
00:56:42,309 --> 00:56:42,319
have in the code and the batch size is
 

2855
00:56:42,319 --> 00:56:44,630
have in the code and the batch size is
the same as so the block size is what we

2856
00:56:44,630 --> 00:56:44,640
the same as so the block size is what we
 

2857
00:56:44,640 --> 00:56:46,349
the same as so the block size is what we
discussed as context size on the white

2858
00:56:46,349 --> 00:56:46,359
discussed as context size on the white
 

2859
00:56:46,359 --> 00:56:50,069
discussed as context size on the white
board and uh the batch size the batch

2860
00:56:50,069 --> 00:56:50,079
board and uh the batch size the batch
 

2861
00:56:50,079 --> 00:56:51,750
board and uh the batch size the batch
size is the same as the batch size which

2862
00:56:51,750 --> 00:56:51,760
size is the same as the batch size which
 

2863
00:56:51,760 --> 00:56:53,589
size is the same as the batch size which
we had discussed. So here we are

2864
00:56:53,589 --> 00:56:53,599
we had discussed. So here we are
 

2865
00:56:53,599 --> 00:56:55,670
we had discussed. So here we are
creating the input and the output

2866
00:56:55,670 --> 00:56:55,680
creating the input and the output
 

2867
00:56:55,680 --> 00:57:00,390
creating the input and the output
batches x1 x2 x3 x4 y1 y2 y3 y4 right

2868
00:57:00,390 --> 00:57:00,400
batches x1 x2 x3 x4 y1 y2 y3 y4 right
 

2869
00:57:00,400 --> 00:57:02,470
batches x1 x2 x3 x4 y1 y2 y3 y4 right
and now here we are just pinning the x

2870
00:57:02,470 --> 00:57:02,480
and now here we are just pinning the x
 

2871
00:57:02,480 --> 00:57:04,870
and now here we are just pinning the x
and y2 memory and non-blocking equal to

2872
00:57:04,870 --> 00:57:04,880
and y2 memory and non-blocking equal to
 

2873
00:57:04,880 --> 00:57:07,030
and y2 memory and non-blocking equal to
true so that we can move to the GPU very

2874
00:57:07,030 --> 00:57:07,040
true so that we can move to the GPU very
 

2875
00:57:07,040 --> 00:57:09,190
true so that we can move to the GPU very
easily. These statements are just

2876
00:57:09,190 --> 00:57:09,200
easily. These statements are just
 

2877
00:57:09,200 --> 00:57:10,990
easily. These statements are just
written for computational

2878
00:57:10,990 --> 00:57:11,000
written for computational
 

2879
00:57:11,000 --> 00:57:13,430
written for computational
efficiency. So within these small lines

2880
00:57:13,430 --> 00:57:13,440
efficiency. So within these small lines
 

2881
00:57:13,440 --> 00:57:15,030
efficiency. So within these small lines
of code there are actually number of

2882
00:57:15,030 --> 00:57:15,040
of code there are actually number of
 

2883
00:57:15,040 --> 00:57:16,710
of code there are actually number of
things which are happening. So if you

2884
00:57:16,710 --> 00:57:16,720
things which are happening. So if you
 

2885
00:57:16,720 --> 00:57:18,630
things which are happening. So if you
just see this code block you might be

2886
00:57:18,630 --> 00:57:18,640
just see this code block you might be
 

2887
00:57:18,640 --> 00:57:20,150
just see this code block you might be
very confused by it and you won't

2888
00:57:20,150 --> 00:57:20,160
very confused by it and you won't
 

2889
00:57:20,160 --> 00:57:22,309
very confused by it and you won't
understand what's going on here. But

2890
00:57:22,309 --> 00:57:22,319
understand what's going on here. But
 

2891
00:57:22,319 --> 00:57:23,910
understand what's going on here. But
first what we are doing is that we are

2892
00:57:23,910 --> 00:57:23,920
first what we are doing is that we are
 

2893
00:57:23,920 --> 00:57:25,910
first what we are doing is that we are
doing this batches for both training

2894
00:57:25,910 --> 00:57:25,920
doing this batches for both training
 

2895
00:57:25,920 --> 00:57:28,150
doing this batches for both training
data and validation data. That's why

2896
00:57:28,150 --> 00:57:28,160
data and validation data. That's why
 

2897
00:57:28,160 --> 00:57:29,910
data and validation data. That's why
these first lines of code are there.

2898
00:57:29,910 --> 00:57:29,920
these first lines of code are there.
 

2899
00:57:29,920 --> 00:57:32,150
these first lines of code are there.
Then here what we do is that we sample

2900
00:57:32,150 --> 00:57:32,160
Then here what we do is that we sample
 

2901
00:57:32,160 --> 00:57:34,069
Then here what we do is that we sample
random integers. So if the batch size is

2902
00:57:34,069 --> 00:57:34,079
random integers. So if the batch size is
 

2903
00:57:34,079 --> 00:57:35,990
random integers. So if the batch size is
equal to four, we are going to sample

2904
00:57:35,990 --> 00:57:36,000
equal to four, we are going to sample
 

2905
00:57:36,000 --> 00:57:37,910
equal to four, we are going to sample
four random integers. They correspond to

2906
00:57:37,910 --> 00:57:37,920
four random integers. They correspond to
 

2907
00:57:37,920 --> 00:57:39,910
four random integers. They correspond to
four positions which we are going to

2908
00:57:39,910 --> 00:57:39,920
four positions which we are going to
 

2909
00:57:39,920 --> 00:57:42,789
four positions which we are going to
look at in the data set. So for example,

2910
00:57:42,789 --> 00:57:42,799
look at in the data set. So for example,
 

2911
00:57:42,799 --> 00:57:45,190
look at in the data set. So for example,
position one can be here, position two

2912
00:57:45,190 --> 00:57:45,200
position one can be here, position two
 

2913
00:57:45,200 --> 00:57:46,950
position one can be here, position two
can be here, position three can be here,

2914
00:57:46,950 --> 00:57:46,960
can be here, position three can be here,
 

2915
00:57:46,960 --> 00:57:49,349
can be here, position three can be here,
position four can be here. Wherever the

2916
00:57:49,349 --> 00:57:49,359
position four can be here. Wherever the
 

2917
00:57:49,359 --> 00:57:51,990
position four can be here. Wherever the
position is, we get x1, x2, x3, x4. From

2918
00:57:51,990 --> 00:57:52,000
position is, we get x1, x2, x3, x4. From
 

2919
00:57:52,000 --> 00:57:53,670
position is, we get x1, x2, x3, x4. From
there, we shift to the right by one and

2920
00:57:53,670 --> 00:57:53,680
there, we shift to the right by one and
 

2921
00:57:53,680 --> 00:57:57,270
there, we shift to the right by one and
get y1, y2, y3, and y4. And then we just

2922
00:57:57,270 --> 00:57:57,280
get y1, y2, y3, and y4. And then we just
 

2923
00:57:57,280 --> 00:57:59,430
get y1, y2, y3, and y4. And then we just
pin it to the memory. That's it. We are

2924
00:57:59,430 --> 00:57:59,440
pin it to the memory. That's it. We are
 

2925
00:57:59,440 --> 00:58:00,950
pin it to the memory. That's it. We are
going to see this get batch function

2926
00:58:00,950 --> 00:58:00,960
going to see this get batch function
 

2927
00:58:00,960 --> 00:58:03,190
going to see this get batch function
when we run the pre-training loop. Every

2928
00:58:03,190 --> 00:58:03,200
when we run the pre-training loop. Every
 

2929
00:58:03,200 --> 00:58:05,030
when we run the pre-training loop. Every
time we say get a batch, we are going to

2930
00:58:05,030 --> 00:58:05,040
time we say get a batch, we are going to
 

2931
00:58:05,040 --> 00:58:08,230
time we say get a batch, we are going to
get one input and output batch. Then we

2932
00:58:08,230 --> 00:58:08,240
get one input and output batch. Then we
 

2933
00:58:08,240 --> 00:58:09,230
get one input and output batch. Then we
are going to

2934
00:58:09,230 --> 00:58:09,240
are going to
 

2935
00:58:09,240 --> 00:58:11,430
are going to
compute, we are going to run the input

2936
00:58:11,430 --> 00:58:11,440
compute, we are going to run the input
 

2937
00:58:11,440 --> 00:58:13,349
compute, we are going to run the input
through the llm, get the output, then

2938
00:58:13,349 --> 00:58:13,359
through the llm, get the output, then
 

2939
00:58:13,359 --> 00:58:16,309
through the llm, get the output, then
compute the loss, back propagate. Okay.

2940
00:58:16,309 --> 00:58:16,319
compute the loss, back propagate. Okay.
 

2941
00:58:16,319 --> 00:58:18,630
compute the loss, back propagate. Okay.
So that's why get batch serves a very

2942
00:58:18,630 --> 00:58:18,640
So that's why get batch serves a very
 

2943
00:58:18,640 --> 00:58:20,109
So that's why get batch serves a very
important

2944
00:58:20,109 --> 00:58:20,119
important
 

2945
00:58:20,119 --> 00:58:22,630
important
role. Now that you have understood until

2946
00:58:22,630 --> 00:58:22,640
role. Now that you have understood until
 

2947
00:58:22,640 --> 00:58:24,549
role. Now that you have understood until
here we now move to the next part which

2948
00:58:24,549 --> 00:58:24,559
here we now move to the next part which
 

2949
00:58:24,559 --> 00:58:27,589
here we now move to the next part which
is probably the most important in this

2950
00:58:27,589 --> 00:58:27,599
is probably the most important in this
 

2951
00:58:27,599 --> 00:58:29,990
is probably the most important in this
video and that part is essentially

2952
00:58:29,990 --> 00:58:30,000
video and that part is essentially
 

2953
00:58:30,000 --> 00:58:32,950
video and that part is essentially
defining the architecture of the small

2954
00:58:32,950 --> 00:58:32,960
defining the architecture of the small
 

2955
00:58:32,960 --> 00:58:35,030
defining the architecture of the small
language model. So let's get started

2956
00:58:35,030 --> 00:58:35,040
language model. So let's get started
 

2957
00:58:35,040 --> 00:58:37,870
language model. So let's get started
with this. The small language model

2958
00:58:37,870 --> 00:58:37,880
with this. The small language model
 

2959
00:58:37,880 --> 00:58:40,950
with this. The small language model
architecture can appear a bit

2960
00:58:40,950 --> 00:58:40,960
architecture can appear a bit
 

2961
00:58:40,960 --> 00:58:43,950
architecture can appear a bit
intimidating. It can appear a bit

2962
00:58:43,950 --> 00:58:43,960
intimidating. It can appear a bit
 

2963
00:58:43,960 --> 00:58:47,069
intimidating. It can appear a bit
frightening because as you can see over

2964
00:58:47,069 --> 00:58:47,079
frightening because as you can see over
 

2965
00:58:47,079 --> 00:58:49,670
frightening because as you can see over
here there are multiple different things

2966
00:58:49,670 --> 00:58:49,680
here there are multiple different things
 

2967
00:58:49,680 --> 00:58:51,030
here there are multiple different things
which are happening in the model

2968
00:58:51,030 --> 00:58:51,040
which are happening in the model
 

2969
00:58:51,040 --> 00:58:53,589
which are happening in the model
architecture. Right? As we saw earlier

2970
00:58:53,589 --> 00:58:53,599
architecture. Right? As we saw earlier
 

2971
00:58:53,599 --> 00:58:55,670
architecture. Right? As we saw earlier
this whole architecture can be broadly

2972
00:58:55,670 --> 00:58:55,680
this whole architecture can be broadly
 

2973
00:58:55,680 --> 00:58:58,549
this whole architecture can be broadly
divided into three parts. Part number

2974
00:58:58,549 --> 00:58:58,559
divided into three parts. Part number
 

2975
00:58:58,559 --> 00:59:01,230
divided into three parts. Part number
one, part number two and part number

2976
00:59:01,230 --> 00:59:01,240
one, part number two and part number
 

2977
00:59:01,240 --> 00:59:04,430
one, part number two and part number
three. Part number one is the input

2978
00:59:04,430 --> 00:59:04,440
three. Part number one is the input
 

2979
00:59:04,440 --> 00:59:07,549
three. Part number one is the input
block. Part number two is the

2980
00:59:07,549 --> 00:59:07,559
block. Part number two is the
 

2981
00:59:07,559 --> 00:59:10,230
block. Part number two is the
processor and part number three is the

2982
00:59:10,230 --> 00:59:10,240
processor and part number three is the
 

2983
00:59:10,240 --> 00:59:12,789
processor and part number three is the
output block.

2984
00:59:12,789 --> 00:59:12,799
output block.
 

2985
00:59:12,799 --> 00:59:15,109
output block.
To truly understand the language model

2986
00:59:15,109 --> 00:59:15,119
To truly understand the language model
 

2987
00:59:15,119 --> 00:59:16,870
To truly understand the language model
architecture, we have to understand all

2988
00:59:16,870 --> 00:59:16,880
architecture, we have to understand all
 

2989
00:59:16,880 --> 00:59:19,670
architecture, we have to understand all
of these parts in detail. And I could

2990
00:59:19,670 --> 00:59:19,680
of these parts in detail. And I could
 

2991
00:59:19,680 --> 00:59:22,950
of these parts in detail. And I could
easily spend 10 to 12 hours explaining

2992
00:59:22,950 --> 00:59:22,960
easily spend 10 to 12 hours explaining
 

2993
00:59:22,960 --> 00:59:25,069
easily spend 10 to 12 hours explaining
every single aspect of this

2994
00:59:25,069 --> 00:59:25,079
every single aspect of this
 

2995
00:59:25,079 --> 00:59:28,309
every single aspect of this
architecture. But my main aim in this

2996
00:59:28,309 --> 00:59:28,319
architecture. But my main aim in this
 

2997
00:59:28,319 --> 00:59:31,430
architecture. But my main aim in this
tutorial is to give you enough

2998
00:59:31,430 --> 00:59:31,440
tutorial is to give you enough
 

2999
00:59:31,440 --> 00:59:33,990
tutorial is to give you enough
understanding of this architecture so

3000
00:59:33,990 --> 00:59:34,000
understanding of this architecture so
 

3001
00:59:34,000 --> 00:59:35,910
understanding of this architecture so
that you can understand what's going on

3002
00:59:35,910 --> 00:59:35,920
that you can understand what's going on
 

3003
00:59:35,920 --> 00:59:38,470
that you can understand what's going on
in the code. I won't have the time to go

3004
00:59:38,470 --> 00:59:38,480
in the code. I won't have the time to go
 

3005
00:59:38,480 --> 00:59:40,870
in the code. I won't have the time to go
into the details of each and every

3006
00:59:40,870 --> 00:59:40,880
into the details of each and every
 

3007
00:59:40,880 --> 00:59:43,589
into the details of each and every
component here because that would make

3008
00:59:43,589 --> 00:59:43,599
component here because that would make
 

3009
00:59:43,599 --> 00:59:46,630
component here because that would make
this tutorial very long. Uh but I will

3010
00:59:46,630 --> 00:59:46,640
this tutorial very long. Uh but I will
 

3011
00:59:46,640 --> 00:59:49,750
this tutorial very long. Uh but I will
give you enough intuition, enough u

3012
00:59:49,750 --> 00:59:49,760
give you enough intuition, enough u
 

3013
00:59:49,760 --> 00:59:51,750
give you enough intuition, enough u
writing on the whiteboard so that you

3014
00:59:51,750 --> 00:59:51,760
writing on the whiteboard so that you
 

3015
00:59:51,760 --> 00:59:54,309
writing on the whiteboard so that you
understand step by step what is going on

3016
00:59:54,309 --> 00:59:54,319
understand step by step what is going on
 

3017
00:59:54,319 --> 00:59:56,990
understand step by step what is going on
in each part. Right? So let's get

3018
00:59:56,990 --> 00:59:57,000
in each part. Right? So let's get
 

3019
00:59:57,000 --> 00:59:59,750
in each part. Right? So let's get
started. The code associated for this

3020
00:59:59,750 --> 00:59:59,760
started. The code associated for this
 

3021
00:59:59,760 --> 01:00:02,710
started. The code associated for this
part is in this uh code cell which is

3022
01:00:02,710 --> 01:00:02,720
part is in this uh code cell which is
 

3023
01:00:02,720 --> 01:00:05,109
part is in this uh code cell which is
called defining the language model

3024
01:00:05,109 --> 01:00:05,119
called defining the language model
 

3025
01:00:05,119 --> 01:00:07,670
called defining the language model
architecture. And this is the longest

3026
01:00:07,670 --> 01:00:07,680
architecture. And this is the longest
 

3027
01:00:07,680 --> 01:00:09,829
architecture. And this is the longest
code cell. So if someone looks at the

3028
01:00:09,829 --> 01:00:09,839
code cell. So if someone looks at the
 

3029
01:00:09,839 --> 01:00:11,910
code cell. So if someone looks at the
code, they might not understand

3030
01:00:11,910 --> 01:00:11,920
code, they might not understand
 

3031
01:00:11,920 --> 01:00:13,750
code, they might not understand
anything. And at that time they might

3032
01:00:13,750 --> 01:00:13,760
anything. And at that time they might
 

3033
01:00:13,760 --> 01:00:15,750
anything. And at that time they might
feel that language model is language

3034
01:00:15,750 --> 01:00:15,760
feel that language model is language
 

3035
01:00:15,760 --> 01:00:18,230
feel that language model is language
modeling is very difficult and it's not

3036
01:00:18,230 --> 01:00:18,240
modeling is very difficult and it's not
 

3037
01:00:18,240 --> 01:00:21,190
modeling is very difficult and it's not
meant for me. But it's only when you

3038
01:00:21,190 --> 01:00:21,200
meant for me. But it's only when you
 

3039
01:00:21,200 --> 01:00:23,030
meant for me. But it's only when you
start understanding the nuts and bolts

3040
01:00:23,030 --> 01:00:23,040
start understanding the nuts and bolts
 

3041
01:00:23,040 --> 01:00:25,270
start understanding the nuts and bolts
of how this code is assembled, you'll

3042
01:00:25,270 --> 01:00:25,280
of how this code is assembled, you'll
 

3043
01:00:25,280 --> 01:00:27,030
of how this code is assembled, you'll
see that everything really makes a lot

3044
01:00:27,030 --> 01:00:27,040
see that everything really makes a lot
 

3045
01:00:27,040 --> 01:00:28,829
see that everything really makes a lot
of sense. Essentially we are just

3046
01:00:28,829 --> 01:00:28,839
of sense. Essentially we are just
 

3047
01:00:28,839 --> 01:00:31,109
of sense. Essentially we are just
assembling all of these components

3048
01:00:31,109 --> 01:00:31,119
assembling all of these components
 

3049
01:00:31,119 --> 01:00:34,630
assembling all of these components
together so that when we get the input

3050
01:00:34,630 --> 01:00:34,640
together so that when we get the input
 

3051
01:00:34,640 --> 01:00:37,589
together so that when we get the input
text which is a sequence of words at the

3052
01:00:37,589 --> 01:00:37,599
text which is a sequence of words at the
 

3053
01:00:37,599 --> 01:00:39,430
text which is a sequence of words at the
end the goal is to predict the next

3054
01:00:39,430 --> 01:00:39,440
end the goal is to predict the next
 

3055
01:00:39,440 --> 01:00:41,829
end the goal is to predict the next
token right that's all we are going to

3056
01:00:41,829 --> 01:00:41,839
token right that's all we are going to
 

3057
01:00:41,839 --> 01:00:43,309
token right that's all we are going to
see in the small language model

3058
01:00:43,309 --> 01:00:43,319
see in the small language model
 

3059
01:00:43,319 --> 01:00:45,349
see in the small language model
architecture. What we are going to see

3060
01:00:45,349 --> 01:00:45,359
architecture. What we are going to see
 

3061
01:00:45,359 --> 01:00:49,030
architecture. What we are going to see
is that when we have a sequence of words

3062
01:00:49,030 --> 01:00:49,040
is that when we have a sequence of words
 

3063
01:00:49,040 --> 01:00:52,069
is that when we have a sequence of words
which is the fuel to the language model,

3064
01:00:52,069 --> 01:00:52,079
which is the fuel to the language model,
 

3065
01:00:52,079 --> 01:00:53,990
which is the fuel to the language model,
how does it predict the output which is

3066
01:00:53,990 --> 01:00:54,000
how does it predict the output which is
 

3067
01:00:54,000 --> 01:00:56,789
how does it predict the output which is
the next token. And once we get the next

3068
01:00:56,789 --> 01:00:56,799
the next token. And once we get the next
 

3069
01:00:56,799 --> 01:00:59,109
the next token. And once we get the next
token prediction, we have already seen

3070
01:00:59,109 --> 01:00:59,119
token prediction, we have already seen
 

3071
01:00:59,119 --> 01:01:01,030
token prediction, we have already seen
that we are going to find the loss

3072
01:01:01,030 --> 01:01:01,040
that we are going to find the loss
 

3073
01:01:01,040 --> 01:01:03,910
that we are going to find the loss
between the output token produced by the

3074
01:01:03,910 --> 01:01:03,920
between the output token produced by the
 

3075
01:01:03,920 --> 01:01:06,549
between the output token produced by the
language model and the target token for

3076
01:01:06,549 --> 01:01:06,559
language model and the target token for
 

3077
01:01:06,559 --> 01:01:09,990
language model and the target token for
these input output pairs input output

3078
01:01:09,990 --> 01:01:10,000
these input output pairs input output
 

3079
01:01:10,000 --> 01:01:12,950
these input output pairs input output
patch pairs which we have seen before.

3080
01:01:12,950 --> 01:01:12,960
patch pairs which we have seen before.
 

3081
01:01:12,960 --> 01:01:15,270
patch pairs which we have seen before.
So the SLM model architecture is going

3082
01:01:15,270 --> 01:01:15,280
So the SLM model architecture is going
 

3083
01:01:15,280 --> 01:01:17,910
So the SLM model architecture is going
to take some time to explain but at the

3084
01:01:17,910 --> 01:01:17,920
to take some time to explain but at the
 

3085
01:01:17,920 --> 01:01:19,670
to take some time to explain but at the
end of it remember that we are just

3086
01:01:19,670 --> 01:01:19,680
end of it remember that we are just
 

3087
01:01:19,680 --> 01:01:22,390
end of it remember that we are just
feeding in a sequence of words such as

3088
01:01:22,390 --> 01:01:22,400
feeding in a sequence of words such as
 

3089
01:01:22,400 --> 01:01:24,710
feeding in a sequence of words such as
one day a little and we have to predict

3090
01:01:24,710 --> 01:01:24,720
one day a little and we have to predict
 

3091
01:01:24,720 --> 01:01:26,990
one day a little and we have to predict
the next token which ideally should be

3092
01:01:26,990 --> 01:01:27,000
the next token which ideally should be
 

3093
01:01:27,000 --> 01:01:29,829
the next token which ideally should be
girl. So if you see this sentence one

3094
01:01:29,829 --> 01:01:29,839
girl. So if you see this sentence one
 

3095
01:01:29,839 --> 01:01:32,990
girl. So if you see this sentence one
day a little if that is the

3096
01:01:32,990 --> 01:01:33,000
day a little if that is the
 

3097
01:01:33,000 --> 01:01:35,670
day a little if that is the
input ideally the next token should be

3098
01:01:35,670 --> 01:01:35,680
input ideally the next token should be
 

3099
01:01:35,680 --> 01:01:38,630
input ideally the next token should be
girl. So we are going to see how the LLM

3100
01:01:38,630 --> 01:01:38,640
girl. So we are going to see how the LLM
 

3101
01:01:38,640 --> 01:01:40,710
girl. So we are going to see how the LLM
takes these sequence of tokens and

3102
01:01:40,710 --> 01:01:40,720
takes these sequence of tokens and
 

3103
01:01:40,720 --> 01:01:43,270
takes these sequence of tokens and
produces the next token. My approach

3104
01:01:43,270 --> 01:01:43,280
produces the next token. My approach
 

3105
01:01:43,280 --> 01:01:45,430
produces the next token. My approach
here is that first I'm going to explain

3106
01:01:45,430 --> 01:01:45,440
here is that first I'm going to explain
 

3107
01:01:45,440 --> 01:01:48,150
here is that first I'm going to explain
to you everything on a whiteboard in a

3108
01:01:48,150 --> 01:01:48,160
to you everything on a whiteboard in a
 

3109
01:01:48,160 --> 01:01:51,270
to you everything on a whiteboard in a
very sequential manner and then what we

3110
01:01:51,270 --> 01:01:51,280
very sequential manner and then what we
 

3111
01:01:51,280 --> 01:01:52,710
very sequential manner and then what we
are going to do is that then we are

3112
01:01:52,710 --> 01:01:52,720
are going to do is that then we are
 

3113
01:01:52,720 --> 01:01:55,349
are going to do is that then we are
going to try to go to the code and we

3114
01:01:55,349 --> 01:01:55,359
going to try to go to the code and we
 

3115
01:01:55,359 --> 01:01:57,270
going to try to go to the code and we
are going to map out every single thing

3116
01:01:57,270 --> 01:01:57,280
are going to map out every single thing
 

3117
01:01:57,280 --> 01:01:59,430
are going to map out every single thing
which we have seen on the whiteboard to

3118
01:01:59,430 --> 01:01:59,440
which we have seen on the whiteboard to
 

3119
01:01:59,440 --> 01:02:01,270
which we have seen on the whiteboard to
every single detailed aspect of the

3120
01:02:01,270 --> 01:02:01,280
every single detailed aspect of the
 

3121
01:02:01,280 --> 01:02:04,390
every single detailed aspect of the
code. Right? So let's get started. First

3122
01:02:04,390 --> 01:02:04,400
code. Right? So let's get started. First
 

3123
01:02:04,400 --> 01:02:06,069
code. Right? So let's get started. First
what we do is that we have already

3124
01:02:06,069 --> 01:02:06,079
what we do is that we have already
 

3125
01:02:06,079 --> 01:02:08,230
what we do is that we have already
created this input and output batches.

3126
01:02:08,230 --> 01:02:08,240
created this input and output batches.
 

3127
01:02:08,240 --> 01:02:10,309
created this input and output batches.
Correct? So let's say we are looking at

3128
01:02:10,309 --> 01:02:10,319
Correct? So let's say we are looking at
 

3129
01:02:10,319 --> 01:02:12,309
Correct? So let's say we are looking at
the first batch initially and I'm

3130
01:02:12,309 --> 01:02:12,319
the first batch initially and I'm
 

3131
01:02:12,319 --> 01:02:13,990
the first batch initially and I'm
looking at the first row of the first

3132
01:02:13,990 --> 01:02:14,000
looking at the first row of the first
 

3133
01:02:14,000 --> 01:02:16,309
looking at the first row of the first
batch which I have mentioned over here.

3134
01:02:16,309 --> 01:02:16,319
batch which I have mentioned over here.
 

3135
01:02:16,319 --> 01:02:17,990
batch which I have mentioned over here.
That's my input right now. I'm looking

3136
01:02:17,990 --> 01:02:18,000
That's my input right now. I'm looking
 

3137
01:02:18,000 --> 01:02:20,190
That's my input right now. I'm looking
at the first row of the first

3138
01:02:20,190 --> 01:02:20,200
at the first row of the first
 

3139
01:02:20,200 --> 01:02:22,870
at the first row of the first
batch and the goal is to take this input

3140
01:02:22,870 --> 01:02:22,880
batch and the goal is to take this input
 

3141
01:02:22,880 --> 01:02:24,870
batch and the goal is to take this input
and to predict the next token ID

3142
01:02:24,870 --> 01:02:24,880
and to predict the next token ID
 

3143
01:02:24,880 --> 01:02:28,069
and to predict the next token ID
whatever it might be. So now let us see

3144
01:02:28,069 --> 01:02:28,079
whatever it might be. So now let us see
 

3145
01:02:28,079 --> 01:02:30,470
whatever it might be. So now let us see
the journey through which this input

3146
01:02:30,470 --> 01:02:30,480
the journey through which this input
 

3147
01:02:30,480 --> 01:02:33,270
the journey through which this input
sequence of tokens actually go through

3148
01:02:33,270 --> 01:02:33,280
sequence of tokens actually go through
 

3149
01:02:33,280 --> 01:02:35,230
sequence of tokens actually go through
before we have to predict the next

3150
01:02:35,230 --> 01:02:35,240
before we have to predict the next
 

3151
01:02:35,240 --> 01:02:37,589
before we have to predict the next
token. The first thing which we have to

3152
01:02:37,589 --> 01:02:37,599
token. The first thing which we have to
 

3153
01:02:37,599 --> 01:02:41,829
token. The first thing which we have to
do is that although this these tokens

3154
01:02:41,829 --> 01:02:41,839
do is that although this these tokens
 

3155
01:02:41,839 --> 01:02:43,910
do is that although this these tokens
are in numerical format so technically

3156
01:02:43,910 --> 01:02:43,920
are in numerical format so technically
 

3157
01:02:43,920 --> 01:02:46,870
are in numerical format so technically
they can be processed by the computer or

3158
01:02:46,870 --> 01:02:46,880
they can be processed by the computer or
 

3159
01:02:46,880 --> 01:02:49,829
they can be processed by the computer or
the language model. We convert every

3160
01:02:49,829 --> 01:02:49,839
the language model. We convert every
 

3161
01:02:49,839 --> 01:02:52,589
the language model. We convert every
token ID into a higher dimensional

3162
01:02:52,589 --> 01:02:52,599
token ID into a higher dimensional
 

3163
01:02:52,599 --> 01:02:55,589
token ID into a higher dimensional
vector and that step is called as token

3164
01:02:55,589 --> 01:02:55,599
vector and that step is called as token
 

3165
01:02:55,599 --> 01:02:57,430
vector and that step is called as token
embedding.

3166
01:02:57,430 --> 01:02:57,440
embedding.
 

3167
01:02:57,440 --> 01:02:59,390
embedding.
The reason we do token embedding is

3168
01:02:59,390 --> 01:02:59,400
The reason we do token embedding is
 

3169
01:02:59,400 --> 01:03:02,470
The reason we do token embedding is
because when we look at language, right?

3170
01:03:02,470 --> 01:03:02,480
because when we look at language, right?
 

3171
01:03:02,480 --> 01:03:03,309
because when we look at language, right?
Let's

3172
01:03:03,309 --> 01:03:03,319
Let's
 

3173
01:03:03,319 --> 01:03:07,430
Let's
say I take a screenshot of this this

3174
01:03:07,430 --> 01:03:07,440
say I take a screenshot of this this
 

3175
01:03:07,440 --> 01:03:10,990
say I take a screenshot of this this
story and I paste it over

3176
01:03:10,990 --> 01:03:11,000
story and I paste it over
 

3177
01:03:11,000 --> 01:03:13,910
story and I paste it over
here. Now, if you look at language and

3178
01:03:13,910 --> 01:03:13,920
here. Now, if you look at language and
 

3179
01:03:13,920 --> 01:03:17,990
here. Now, if you look at language and
the way language manifests itself, every

3180
01:03:17,990 --> 01:03:18,000
the way language manifests itself, every
 

3181
01:03:18,000 --> 01:03:20,710
the way language manifests itself, every
word carries some meaning and that's

3182
01:03:20,710 --> 01:03:20,720
word carries some meaning and that's
 

3183
01:03:20,720 --> 01:03:22,870
word carries some meaning and that's
what that's what gives meaning to the

3184
01:03:22,870 --> 01:03:22,880
what that's what gives meaning to the
 

3185
01:03:22,880 --> 01:03:25,029
what that's what gives meaning to the
whole paragraph. Right? When you read a

3186
01:03:25,029 --> 01:03:25,039
whole paragraph. Right? When you read a
 

3187
01:03:25,039 --> 01:03:26,870
whole paragraph. Right? When you read a
little girl named Lily, something comes

3188
01:03:26,870 --> 01:03:26,880
little girl named Lily, something comes
 

3189
01:03:26,880 --> 01:03:29,109
little girl named Lily, something comes
to your mind because little has some

3190
01:03:29,109 --> 01:03:29,119
to your mind because little has some
 

3191
01:03:29,119 --> 01:03:31,589
to your mind because little has some
meaning, girl has some meaning, cat has

3192
01:03:31,589 --> 01:03:31,599
meaning, girl has some meaning, cat has
 

3193
01:03:31,599 --> 01:03:33,430
meaning, girl has some meaning, cat has
some meaning, dog has some meaning. Both

3194
01:03:33,430 --> 01:03:33,440
some meaning, dog has some meaning. Both
 

3195
01:03:33,440 --> 01:03:35,670
some meaning, dog has some meaning. Both
of them come under animals. So

3196
01:03:35,670 --> 01:03:35,680
of them come under animals. So
 

3197
01:03:35,680 --> 01:03:37,750
of them come under animals. So
essentially words have some sort of a

3198
01:03:37,750 --> 01:03:37,760
essentially words have some sort of a
 

3199
01:03:37,760 --> 01:03:39,069
essentially words have some sort of a
semantic

3200
01:03:39,069 --> 01:03:39,079
semantic
 

3201
01:03:39,079 --> 01:03:42,870
semantic
notion which is very important for the

3202
01:03:42,870 --> 01:03:42,880
notion which is very important for the
 

3203
01:03:42,880 --> 01:03:44,349
notion which is very important for the
language model to

3204
01:03:44,349 --> 01:03:44,359
language model to
 

3205
01:03:44,359 --> 01:03:47,270
language model to
capture. So what can we do before we

3206
01:03:47,270 --> 01:03:47,280
capture. So what can we do before we
 

3207
01:03:47,280 --> 01:03:49,750
capture. So what can we do before we
pass the input to the transformer block?

3208
01:03:49,750 --> 01:03:49,760
pass the input to the transformer block?
 

3209
01:03:49,760 --> 01:03:52,789
pass the input to the transformer block?
Can we somehow take these token ids and

3210
01:03:52,789 --> 01:03:52,799
Can we somehow take these token ids and
 

3211
01:03:52,799 --> 01:03:56,069
Can we somehow take these token ids and
possibly represent them in higher

3212
01:03:56,069 --> 01:03:56,079
possibly represent them in higher
 

3213
01:03:56,079 --> 01:03:58,630
possibly represent them in higher
dimensional vector spaces that capture

3214
01:03:58,630 --> 01:03:58,640
dimensional vector spaces that capture
 

3215
01:03:58,640 --> 01:04:01,190
dimensional vector spaces that capture
some information about the meaning. So

3216
01:04:01,190 --> 01:04:01,200
some information about the meaning. So
 

3217
01:04:01,200 --> 01:04:02,870
some information about the meaning. So
what we are going to do is that let's

3218
01:04:02,870 --> 01:04:02,880
what we are going to do is that let's
 

3219
01:04:02,880 --> 01:04:05,349
what we are going to do is that let's
say cat right that has the token ID of

3220
01:04:05,349 --> 01:04:05,359
say cat right that has the token ID of
 

3221
01:04:05,359 --> 01:04:07,910
say cat right that has the token ID of
13 and let's say we have dog that has

3222
01:04:07,910 --> 01:04:07,920
13 and let's say we have dog that has
 

3223
01:04:07,920 --> 01:04:11,109
13 and let's say we have dog that has
the token ID of 17. So I'm going to take

3224
01:04:11,109 --> 01:04:11,119
the token ID of 17. So I'm going to take
 

3225
01:04:11,119 --> 01:04:13,430
the token ID of 17. So I'm going to take
these tokens and I'm going to project

3226
01:04:13,430 --> 01:04:13,440
these tokens and I'm going to project
 

3227
01:04:13,440 --> 01:04:15,190
these tokens and I'm going to project
them into higher dimensional vector

3228
01:04:15,190 --> 01:04:15,200
them into higher dimensional vector
 

3229
01:04:15,200 --> 01:04:17,750
them into higher dimensional vector
space. For now, let's say I'm choosing a

3230
01:04:17,750 --> 01:04:17,760
space. For now, let's say I'm choosing a
 

3231
01:04:17,760 --> 01:04:20,309
space. For now, let's say I'm choosing a
two-dimensional vector space and cat

3232
01:04:20,309 --> 01:04:20,319
two-dimensional vector space and cat
 

3233
01:04:20,319 --> 01:04:24,309
two-dimensional vector space and cat
comes over here and dog comes over here.

3234
01:04:24,309 --> 01:04:24,319
comes over here and dog comes over here.
 

3235
01:04:24,319 --> 01:04:26,309
comes over here and dog comes over here.
Cat and dog are animals. So they have

3236
01:04:26,309 --> 01:04:26,319
Cat and dog are animals. So they have
 

3237
01:04:26,319 --> 01:04:28,230
Cat and dog are animals. So they have
some similarity in meaning. So they lie

3238
01:04:28,230 --> 01:04:28,240
some similarity in meaning. So they lie
 

3239
01:04:28,240 --> 01:04:30,710
some similarity in meaning. So they lie
closer to each other. But both of these

3240
01:04:30,710 --> 01:04:30,720
closer to each other. But both of these
 

3241
01:04:30,720 --> 01:04:32,470
closer to each other. But both of these
words let's say will be very different

3242
01:04:32,470 --> 01:04:32,480
words let's say will be very different
 

3243
01:04:32,480 --> 01:04:35,190
words let's say will be very different
than a chair let's say. So chair will

3244
01:04:35,190 --> 01:04:35,200
than a chair let's say. So chair will
 

3245
01:04:35,200 --> 01:04:37,190
than a chair let's say. So chair will
lie over here but chair and table will

3246
01:04:37,190 --> 01:04:37,200
lie over here but chair and table will
 

3247
01:04:37,200 --> 01:04:39,589
lie over here but chair and table will
lie very close to each other. So you see

3248
01:04:39,589 --> 01:04:39,599
lie very close to each other. So you see
 

3249
01:04:39,599 --> 01:04:41,349
lie very close to each other. So you see
what we are doing here. We are

3250
01:04:41,349 --> 01:04:41,359
what we are doing here. We are
 

3251
01:04:41,359 --> 01:04:42,910
what we are doing here. We are
projecting these

3252
01:04:42,910 --> 01:04:42,920
projecting these
 

3253
01:04:42,920 --> 01:04:46,870
projecting these
words as vectors and then we are hoping

3254
01:04:46,870 --> 01:04:46,880
words as vectors and then we are hoping
 

3255
01:04:46,880 --> 01:04:48,870
words as vectors and then we are hoping
that vectors which are closer to each

3256
01:04:48,870 --> 01:04:48,880
that vectors which are closer to each
 

3257
01:04:48,880 --> 01:04:51,029
that vectors which are closer to each
other would be similar in meaning.

3258
01:04:51,029 --> 01:04:51,039
other would be similar in meaning.
 

3259
01:04:51,039 --> 01:04:52,789
other would be similar in meaning.
Vectors which are farther apart from

3260
01:04:52,789 --> 01:04:52,799
Vectors which are farther apart from
 

3261
01:04:52,799 --> 01:04:54,069
Vectors which are farther apart from
each other would be different in

3262
01:04:54,069 --> 01:04:54,079
each other would be different in
 

3263
01:04:54,079 --> 01:04:56,630
each other would be different in
meaning. Why do we do this? Because we

3264
01:04:56,630 --> 01:04:56,640
meaning. Why do we do this? Because we
 

3265
01:04:56,640 --> 01:05:00,190
meaning. Why do we do this? Because we
capture some notion about the meaning of

3266
01:05:00,190 --> 01:05:00,200
capture some notion about the meaning of
 

3267
01:05:00,200 --> 01:05:04,470
capture some notion about the meaning of
words and that is how we can exploit the

3268
01:05:04,470 --> 01:05:04,480
words and that is how we can exploit the
 

3269
01:05:04,480 --> 01:05:06,789
words and that is how we can exploit the
properties of language when we look at

3270
01:05:06,789 --> 01:05:06,799
properties of language when we look at
 

3271
01:05:06,799 --> 01:05:08,710
properties of language when we look at
convolutional neural network. So I'm

3272
01:05:08,710 --> 01:05:08,720
convolutional neural network. So I'm
 

3273
01:05:08,720 --> 01:05:10,710
convolutional neural network. So I'm
sure all of you must have seen this

3274
01:05:10,710 --> 01:05:10,720
sure all of you must have seen this
 

3275
01:05:10,720 --> 01:05:12,789
sure all of you must have seen this
convolutional neural network explainer,

3276
01:05:12,789 --> 01:05:12,799
convolutional neural network explainer,
 

3277
01:05:12,799 --> 01:05:15,069
convolutional neural network explainer,
right? Why do we take convolutional

3278
01:05:15,069 --> 01:05:15,079
right? Why do we take convolutional
 

3279
01:05:15,079 --> 01:05:17,589
right? Why do we take convolutional
filters and why do we slide them over

3280
01:05:17,589 --> 01:05:17,599
filters and why do we slide them over
 

3281
01:05:17,599 --> 01:05:19,670
filters and why do we slide them over
the images? Because we want to exploit

3282
01:05:19,670 --> 01:05:19,680
the images? Because we want to exploit
 

3283
01:05:19,680 --> 01:05:22,390
the images? Because we want to exploit
properties in an image. The properties

3284
01:05:22,390 --> 01:05:22,400
properties in an image. The properties
 

3285
01:05:22,400 --> 01:05:24,789
properties in an image. The properties
of translational invariance, the

3286
01:05:24,789 --> 01:05:24,799
of translational invariance, the
 

3287
01:05:24,799 --> 01:05:26,870
of translational invariance, the
properties that two pixels will always

3288
01:05:26,870 --> 01:05:26,880
properties that two pixels will always
 

3289
01:05:26,880 --> 01:05:29,109
properties that two pixels will always
slightly lie closer to each other and

3290
01:05:29,109 --> 01:05:29,119
slightly lie closer to each other and
 

3291
01:05:29,119 --> 01:05:31,270
slightly lie closer to each other and
even if a cup is present here or here,

3292
01:05:31,270 --> 01:05:31,280
even if a cup is present here or here,
 

3293
01:05:31,280 --> 01:05:33,589
even if a cup is present here or here,
it will still be a cup. So there are

3294
01:05:33,589 --> 01:05:33,599
it will still be a cup. So there are
 

3295
01:05:33,599 --> 01:05:35,589
it will still be a cup. So there are
some properties inherent in the images

3296
01:05:35,589 --> 01:05:35,599
some properties inherent in the images
 

3297
01:05:35,599 --> 01:05:37,510
some properties inherent in the images
which are exploited when we use these

3298
01:05:37,510 --> 01:05:37,520
which are exploited when we use these
 

3299
01:05:37,520 --> 01:05:39,510
which are exploited when we use these
feature maps or these convolutional

3300
01:05:39,510 --> 01:05:39,520
feature maps or these convolutional
 

3301
01:05:39,520 --> 01:05:42,789
feature maps or these convolutional
layers. That's why CNN's work so well.

3302
01:05:42,789 --> 01:05:42,799
layers. That's why CNN's work so well.
 

3303
01:05:42,799 --> 01:05:45,510
layers. That's why CNN's work so well.
So if you want to exploit language which

3304
01:05:45,510 --> 01:05:45,520
So if you want to exploit language which
 

3305
01:05:45,520 --> 01:05:48,309
So if you want to exploit language which
is our data right now the best way to do

3306
01:05:48,309 --> 01:05:48,319
is our data right now the best way to do
 

3307
01:05:48,319 --> 01:05:51,069
is our data right now the best way to do
it would be to capture meaning in

3308
01:05:51,069 --> 01:05:51,079
it would be to capture meaning in
 

3309
01:05:51,079 --> 01:05:53,750
it would be to capture meaning in
vectors. So that's why what's done as

3310
01:05:53,750 --> 01:05:53,760
vectors. So that's why what's done as
 

3311
01:05:53,760 --> 01:05:56,309
vectors. So that's why what's done as
the next step is that every token ID is

3312
01:05:56,309 --> 01:05:56,319
the next step is that every token ID is
 

3313
01:05:56,319 --> 01:05:58,470
the next step is that every token ID is
converted into a highdimensional vector

3314
01:05:58,470 --> 01:05:58,480
converted into a highdimensional vector
 

3315
01:05:58,480 --> 01:06:00,950
converted into a highdimensional vector
and that's a token embedding vector. The

3316
01:06:00,950 --> 01:06:00,960
and that's a token embedding vector. The
 

3317
01:06:00,960 --> 01:06:02,710
and that's a token embedding vector. The
way this is done is that we maintain

3318
01:06:02,710 --> 01:06:02,720
way this is done is that we maintain
 

3319
01:06:02,720 --> 01:06:06,349
way this is done is that we maintain
something called as the token embedding

3320
01:06:06,349 --> 01:06:06,359
something called as the token embedding
 

3321
01:06:06,359 --> 01:06:08,789
something called as the token embedding
matrix. We maintain something called as

3322
01:06:08,789 --> 01:06:08,799
matrix. We maintain something called as
 

3323
01:06:08,799 --> 01:06:11,270
matrix. We maintain something called as
the token embedding matrix. And the

3324
01:06:11,270 --> 01:06:11,280
the token embedding matrix. And the
 

3325
01:06:11,280 --> 01:06:13,910
the token embedding matrix. And the
token embedding matrix has number of

3326
01:06:13,910 --> 01:06:13,920
token embedding matrix has number of
 

3327
01:06:13,920 --> 01:06:16,069
token embedding matrix has number of
rows equal to our vocabulary size. So

3328
01:06:16,069 --> 01:06:16,079
rows equal to our vocabulary size. So
 

3329
01:06:16,079 --> 01:06:18,789
rows equal to our vocabulary size. So
row number one, let me do this with a

3330
01:06:18,789 --> 01:06:18,799
row number one, let me do this with a
 

3331
01:06:18,799 --> 01:06:19,870
row number one, let me do this with a
different

3332
01:06:19,870 --> 01:06:19,880
different
 

3333
01:06:19,880 --> 01:06:22,630
different
color. Row number one, row number two

3334
01:06:22,630 --> 01:06:22,640
color. Row number one, row number two
 

3335
01:06:22,640 --> 01:06:24,549
color. Row number one, row number two
dot dot dot. If the vocabulary size is

3336
01:06:24,549 --> 01:06:24,559
dot dot dot. If the vocabulary size is
 

3337
01:06:24,559 --> 01:06:28,950
dot dot dot. If the vocabulary size is
50,000, then it will be 50,000. and each

3338
01:06:28,950 --> 01:06:28,960
50,000, then it will be 50,000. and each
 

3339
01:06:28,960 --> 01:06:31,750
50,000, then it will be 50,000. and each
uh each token ID in my vocabulary is now

3340
01:06:31,750 --> 01:06:31,760
uh each token ID in my vocabulary is now
 

3341
01:06:31,760 --> 01:06:34,630
uh each token ID in my vocabulary is now
encoded as a high dimensional vector. So

3342
01:06:34,630 --> 01:06:34,640
encoded as a high dimensional vector. So
 

3343
01:06:34,640 --> 01:06:37,029
encoded as a high dimensional vector. So
this can be let's say a 768. So I have

3344
01:06:37,029 --> 01:06:37,039
this can be let's say a 768. So I have
 

3345
01:06:37,039 --> 01:06:39,430
this can be let's say a 768. So I have
to choose my embedding dimension and

3346
01:06:39,430 --> 01:06:39,440
to choose my embedding dimension and
 

3347
01:06:39,440 --> 01:06:42,150
to choose my embedding dimension and
that property stays with me throughout

3348
01:06:42,150 --> 01:06:42,160
that property stays with me throughout
 

3349
01:06:42,160 --> 01:06:44,309
that property stays with me throughout
my architecture. I have to choose my

3350
01:06:44,309 --> 01:06:44,319
my architecture. I have to choose my
 

3351
01:06:44,319 --> 01:06:46,069
my architecture. I have to choose my
embedding dimensions. So if I choose my

3352
01:06:46,069 --> 01:06:46,079
embedding dimensions. So if I choose my
 

3353
01:06:46,079 --> 01:06:48,589
embedding dimensions. So if I choose my
embedding dimensions to be equal to

3354
01:06:48,589 --> 01:06:48,599
embedding dimensions to be equal to
 

3355
01:06:48,599 --> 01:06:51,589
embedding dimensions to be equal to
768, every token here is represented as

3356
01:06:51,589 --> 01:06:51,599
768, every token here is represented as
 

3357
01:06:51,599 --> 01:06:55,390
768, every token here is represented as
a 768 dimensional vector.

3358
01:06:55,390 --> 01:06:55,400
a 768 dimensional vector.
 

3359
01:06:55,400 --> 01:07:00,029
a 768 dimensional vector.
So this is also called as the embedding

3360
01:07:00,029 --> 01:07:00,039
So this is also called as the embedding
 

3361
01:07:00,039 --> 01:07:02,789
So this is also called as the embedding
dimension. So we store this token

3362
01:07:02,789 --> 01:07:02,799
dimension. So we store this token
 

3363
01:07:02,799 --> 01:07:05,349
dimension. So we store this token
embedding matrix where every token ID in

3364
01:07:05,349 --> 01:07:05,359
embedding matrix where every token ID in
 

3365
01:07:05,359 --> 01:07:07,190
embedding matrix where every token ID in
my vocabulary is represented let's say

3366
01:07:07,190 --> 01:07:07,200
my vocabulary is represented let's say
 

3367
01:07:07,200 --> 01:07:10,710
my vocabulary is represented let's say
as a 768 dimensional vector. Notice that

3368
01:07:10,710 --> 01:07:10,720
as a 768 dimensional vector. Notice that
 

3369
01:07:10,720 --> 01:07:13,670
as a 768 dimensional vector. Notice that
the 768 is an experimental choice which

3370
01:07:13,670 --> 01:07:13,680
the 768 is an experimental choice which
 

3371
01:07:13,680 --> 01:07:16,549
the 768 is an experimental choice which
we have to make at the start. And now

3372
01:07:16,549 --> 01:07:16,559
we have to make at the start. And now
 

3373
01:07:16,559 --> 01:07:19,510
we have to make at the start. And now
once we have the token embedding matrix,

3374
01:07:19,510 --> 01:07:19,520
once we have the token embedding matrix,
 

3375
01:07:19,520 --> 01:07:21,589
once we have the token embedding matrix,
how do you retrieve the embedding

3376
01:07:21,589 --> 01:07:21,599
how do you retrieve the embedding
 

3377
01:07:21,599 --> 01:07:23,589
how do you retrieve the embedding
vectors for every token ids? You

3378
01:07:23,589 --> 01:07:23,599
vectors for every token ids? You
 

3379
01:07:23,599 --> 01:07:25,349
vectors for every token ids? You
essentially just use this as a lookup

3380
01:07:25,349 --> 01:07:25,359
essentially just use this as a lookup
 

3381
01:07:25,359 --> 01:07:28,789
essentially just use this as a lookup
table. So if token ids are 1, 11, 15 and

3382
01:07:28,789 --> 01:07:28,799
table. So if token ids are 1, 11, 15 and
 

3383
01:07:28,799 --> 01:07:31,190
table. So if token ids are 1, 11, 15 and
24, the way we retrieve their embedding

3384
01:07:31,190 --> 01:07:31,200
24, the way we retrieve their embedding
 

3385
01:07:31,200 --> 01:07:33,589
24, the way we retrieve their embedding
vectors is that we just look at row

3386
01:07:33,589 --> 01:07:33,599
vectors is that we just look at row
 

3387
01:07:33,599 --> 01:07:37,349
vectors is that we just look at row
number one, we look we look at row

3388
01:07:37,349 --> 01:07:37,359
number one, we look we look at row
 

3389
01:07:37,359 --> 01:07:39,750
number one, we look we look at row
number 11, we look at row number 11, we

3390
01:07:39,750 --> 01:07:39,760
number 11, we look at row number 11, we
 

3391
01:07:39,760 --> 01:07:42,630
number 11, we look at row number 11, we
look at row number 15 and we look at row

3392
01:07:42,630 --> 01:07:42,640
look at row number 15 and we look at row
 

3393
01:07:42,640 --> 01:07:43,950
look at row number 15 and we look at row
number

3394
01:07:43,950 --> 01:07:43,960
number
 

3395
01:07:43,960 --> 01:07:46,390
number
24 and we just get the vectors

3396
01:07:46,390 --> 01:07:46,400
24 and we just get the vectors
 

3397
01:07:46,400 --> 01:07:48,190
24 and we just get the vectors
corresponding to these

3398
01:07:48,190 --> 01:07:48,200
corresponding to these
 

3399
01:07:48,200 --> 01:07:50,950
corresponding to these
rows. So the token embedding matrix

3400
01:07:50,950 --> 01:07:50,960
rows. So the token embedding matrix
 

3401
01:07:50,960 --> 01:07:52,870
rows. So the token embedding matrix
essentially serves as the lookup table.

3402
01:07:52,870 --> 01:07:52,880
essentially serves as the lookup table.
 

3403
01:07:52,880 --> 01:07:55,349
essentially serves as the lookup table.
So that when we have token ids, we can

3404
01:07:55,349 --> 01:07:55,359
So that when we have token ids, we can
 

3405
01:07:55,359 --> 01:07:57,349
So that when we have token ids, we can
just look up this token embedding matrix

3406
01:07:57,349 --> 01:07:57,359
just look up this token embedding matrix
 

3407
01:07:57,359 --> 01:07:59,190
just look up this token embedding matrix
and retrieve the corresponding token

3408
01:07:59,190 --> 01:07:59,200
and retrieve the corresponding token
 

3409
01:07:59,200 --> 01:08:02,390
and retrieve the corresponding token
embedding vector. So now this input

3410
01:08:02,390 --> 01:08:02,400
embedding vector. So now this input
 

3411
01:08:02,400 --> 01:08:04,309
embedding vector. So now this input
which we have right now is transformed

3412
01:08:04,309 --> 01:08:04,319
which we have right now is transformed
 

3413
01:08:04,319 --> 01:08:07,510
which we have right now is transformed
into this matrix over here. So I had 1

3414
01:08:07,510 --> 01:08:07,520
into this matrix over here. So I had 1
 

3415
01:08:07,520 --> 01:08:10,789
into this matrix over here. So I had 1
11 15 and 24. Right? Now every token ID

3416
01:08:10,789 --> 01:08:10,799
11 15 and 24. Right? Now every token ID
 

3417
01:08:10,799 --> 01:08:13,270
11 15 and 24. Right? Now every token ID
is represented let's say as a 768

3418
01:08:13,270 --> 01:08:13,280
is represented let's say as a 768
 

3419
01:08:13,280 --> 01:08:15,829
is represented let's say as a 768
dimensional matrix.

3420
01:08:15,829 --> 01:08:15,839
dimensional matrix.
 

3421
01:08:15,839 --> 01:08:18,229
dimensional matrix.
So now if my input let's say I'm going

3422
01:08:18,229 --> 01:08:18,239
So now if my input let's say I'm going
 

3423
01:08:18,239 --> 01:08:20,309
So now if my input let's say I'm going
with the input as let's say every effort

3424
01:08:20,309 --> 01:08:20,319
with the input as let's say every effort
 

3425
01:08:20,319 --> 01:08:23,590
with the input as let's say every effort
moves you or one day a little whatever

3426
01:08:23,590 --> 01:08:23,600
moves you or one day a little whatever
 

3427
01:08:23,600 --> 01:08:26,070
moves you or one day a little whatever
this will be a 768 dimensional vector.

3428
01:08:26,070 --> 01:08:26,080
this will be a 768 dimensional vector.
 

3429
01:08:26,080 --> 01:08:29,149
this will be a 768 dimensional vector.
So let me change this

3430
01:08:29,149 --> 01:08:29,159
So let me change this
 

3431
01:08:29,159 --> 01:08:32,110
So let me change this
actually this was

3432
01:08:32,110 --> 01:08:32,120
actually this was
 

3433
01:08:32,120 --> 01:08:36,229
actually this was
one day a little correct so now one is a

3434
01:08:36,229 --> 01:08:36,239
one day a little correct so now one is a
 

3435
01:08:36,239 --> 01:08:39,110
one day a little correct so now one is a
768 dimensional vector day is a 768

3436
01:08:39,110 --> 01:08:39,120
768 dimensional vector day is a 768
 

3437
01:08:39,120 --> 01:08:41,189
768 dimensional vector day is a 768
dimensional vector O is a 768

3438
01:08:41,189 --> 01:08:41,199
dimensional vector O is a 768
 

3439
01:08:41,199 --> 01:08:43,349
dimensional vector O is a 768
dimensional vector and little is a 768

3440
01:08:43,349 --> 01:08:43,359
dimensional vector and little is a 768
 

3441
01:08:43,359 --> 01:08:45,749
dimensional vector and little is a 768
dimensional vector. So now you may ask

3442
01:08:45,749 --> 01:08:45,759
dimensional vector. So now you may ask
 

3443
01:08:45,759 --> 01:08:48,149
dimensional vector. So now you may ask
that how do we choose these vectors? Do

3444
01:08:48,149 --> 01:08:48,159
that how do we choose these vectors? Do
 

3445
01:08:48,159 --> 01:08:50,390
that how do we choose these vectors? Do
we know the embeddings before? So are

3446
01:08:50,390 --> 01:08:50,400
we know the embeddings before? So are
 

3447
01:08:50,400 --> 01:08:51,990
we know the embeddings before? So are
the embeddings such that they already

3448
01:08:51,990 --> 01:08:52,000
the embeddings such that they already
 

3449
01:08:52,000 --> 01:08:54,789
the embeddings such that they already
capture meaning? No. All of these values

3450
01:08:54,789 --> 01:08:54,799
capture meaning? No. All of these values
 

3451
01:08:54,799 --> 01:08:57,349
capture meaning? No. All of these values
are initialized randomly. And wherever

3452
01:08:57,349 --> 01:08:57,359
are initialized randomly. And wherever
 

3453
01:08:57,359 --> 01:08:59,510
are initialized randomly. And wherever
I'm marking P, it means that these are

3454
01:08:59,510 --> 01:08:59,520
I'm marking P, it means that these are
 

3455
01:08:59,520 --> 01:09:02,070
I'm marking P, it means that these are
trainable parameters of our model. So

3456
01:09:02,070 --> 01:09:02,080
trainable parameters of our model. So
 

3457
01:09:02,080 --> 01:09:03,829
trainable parameters of our model. So
initially this whole token embedding

3458
01:09:03,829 --> 01:09:03,839
initially this whole token embedding
 

3459
01:09:03,839 --> 01:09:05,630
initially this whole token embedding
matrix is initialized as a random

3460
01:09:05,630 --> 01:09:05,640
matrix is initialized as a random
 

3461
01:09:05,640 --> 01:09:08,070
matrix is initialized as a random
matrix. So all of these rows will be

3462
01:09:08,070 --> 01:09:08,080
matrix. So all of these rows will be
 

3463
01:09:08,080 --> 01:09:10,470
matrix. So all of these rows will be
effectively normalized. And what I was

3464
01:09:10,470 --> 01:09:10,480
effectively normalized. And what I was
 

3465
01:09:10,480 --> 01:09:12,709
effectively normalized. And what I was
saying earlier was that let's say if you

3466
01:09:12,709 --> 01:09:12,719
saying earlier was that let's say if you
 

3467
01:09:12,719 --> 01:09:15,269
saying earlier was that let's say if you
have layer 1, layer 2, layer 3, layer

3468
01:09:15,269 --> 01:09:15,279
have layer 1, layer 2, layer 3, layer
 

3469
01:09:15,279 --> 01:09:18,229
have layer 1, layer 2, layer 3, layer
four and you're back propagating, right?

3470
01:09:18,229 --> 01:09:18,239
four and you're back propagating, right?
 

3471
01:09:18,239 --> 01:09:19,829
four and you're back propagating, right?
Usually what happens is that the

3472
01:09:19,829 --> 01:09:19,839
Usually what happens is that the
 

3473
01:09:19,839 --> 01:09:21,749
Usually what happens is that the
gradients in every layer depend on the

3474
01:09:21,749 --> 01:09:21,759
gradients in every layer depend on the
 

3475
01:09:21,759 --> 01:09:24,390
gradients in every layer depend on the
output of that layer. And if the output

3476
01:09:24,390 --> 01:09:24,400
output of that layer. And if the output
 

3477
01:09:24,400 --> 01:09:26,309
output of that layer. And if the output
of the layer widely fluctuates between

3478
01:09:26,309 --> 01:09:26,319
of the layer widely fluctuates between
 

3479
01:09:26,319 --> 01:09:29,189
of the layer widely fluctuates between
large and small values, then the

3480
01:09:29,189 --> 01:09:29,199
large and small values, then the
 

3481
01:09:29,199 --> 01:09:30,950
large and small values, then the
gradient can either stagnate or the

3482
01:09:30,950 --> 01:09:30,960
gradient can either stagnate or the
 

3483
01:09:30,960 --> 01:09:33,189
gradient can either stagnate or the
gradient can explode. It just becomes

3484
01:09:33,189 --> 01:09:33,199
gradient can explode. It just becomes
 

3485
01:09:33,199 --> 01:09:36,309
gradient can explode. It just becomes
very unstable training dynamics.

3486
01:09:36,309 --> 01:09:36,319
very unstable training dynamics.
 

3487
01:09:36,319 --> 01:09:38,550
very unstable training dynamics.
Also if you don't do layer normalization

3488
01:09:38,550 --> 01:09:38,560
Also if you don't do layer normalization
 

3489
01:09:38,560 --> 01:09:40,309
Also if you don't do layer normalization
the distribution of inputs which is

3490
01:09:40,309 --> 01:09:40,319
the distribution of inputs which is
 

3491
01:09:40,319 --> 01:09:41,829
the distribution of inputs which is
received at every layer that

3492
01:09:41,829 --> 01:09:41,839
received at every layer that
 

3493
01:09:41,839 --> 01:09:44,149
received at every layer that
distribution of inputs might change. For

3494
01:09:44,149 --> 01:09:44,159
distribution of inputs might change. For
 

3495
01:09:44,159 --> 01:09:46,390
distribution of inputs might change. For
example layer 1 in one iteration might

3496
01:09:46,390 --> 01:09:46,400
example layer 1 in one iteration might
 

3497
01:09:46,400 --> 01:09:49,189
example layer 1 in one iteration might
have this input distribution in another

3498
01:09:49,189 --> 01:09:49,199
have this input distribution in another
 

3499
01:09:49,199 --> 01:09:51,070
have this input distribution in another
iteration it might have a skewed

3500
01:09:51,070 --> 01:09:51,080
iteration it might have a skewed
 

3501
01:09:51,080 --> 01:09:53,349
iteration it might have a skewed
distribution. So this training becomes

3502
01:09:53,349 --> 01:09:53,359
distribution. So this training becomes
 

3503
01:09:53,359 --> 01:09:55,030
distribution. So this training becomes
hard as the distribution changes from

3504
01:09:55,030 --> 01:09:55,040
hard as the distribution changes from
 

3505
01:09:55,040 --> 01:09:57,189
hard as the distribution changes from
one iteration to the next. Layer

3506
01:09:57,189 --> 01:09:57,199
one iteration to the next. Layer
 

3507
01:09:57,199 --> 01:09:58,950
one iteration to the next. Layer
normalization prevents this problem

3508
01:09:58,950 --> 01:09:58,960
normalization prevents this problem
 

3509
01:09:58,960 --> 01:10:01,350
normalization prevents this problem
which is called internal coariate shift.

3510
01:10:01,350 --> 01:10:01,360
which is called internal coariate shift.
 

3511
01:10:01,360 --> 01:10:04,070
which is called internal coariate shift.
it it it constrains the values of all

3512
01:10:04,070 --> 01:10:04,080
it it it constrains the values of all
 

3513
01:10:04,080 --> 01:10:06,390
it it it constrains the values of all
inputs within a certain range and it

3514
01:10:06,390 --> 01:10:06,400
inputs within a certain range and it
 

3515
01:10:06,400 --> 01:10:08,070
inputs within a certain range and it
also makes sure that distribution is

3516
01:10:08,070 --> 01:10:08,080
also makes sure that distribution is
 

3517
01:10:08,080 --> 01:10:09,910
also makes sure that distribution is
kind of similar. So that just improves

3518
01:10:09,910 --> 01:10:09,920
kind of similar. So that just improves
 

3519
01:10:09,920 --> 01:10:12,229
kind of similar. So that just improves
training. So wherever you see this layer

3520
01:10:12,229 --> 01:10:12,239
training. So wherever you see this layer
 

3521
01:10:12,239 --> 01:10:14,470
training. So wherever you see this layer
normalization block, it is usually used

3522
01:10:14,470 --> 01:10:14,480
normalization block, it is usually used
 

3523
01:10:14,480 --> 01:10:17,270
normalization block, it is usually used
to improve the training performance. So

3524
01:10:17,270 --> 01:10:17,280
to improve the training performance. So
 

3525
01:10:17,280 --> 01:10:19,510
to improve the training performance. So
the first step in the attention is the

3526
01:10:19,510 --> 01:10:19,520
the first step in the attention is the
 

3527
01:10:19,520 --> 01:10:21,750
the first step in the attention is the
layer normalization. And if you go to

3528
01:10:21,750 --> 01:10:21,760
layer normalization. And if you go to
 

3529
01:10:21,760 --> 01:10:23,590
layer normalization. And if you go to
the code, you will see that there is a

3530
01:10:23,590 --> 01:10:23,600
the code, you will see that there is a
 

3531
01:10:23,600 --> 01:10:25,709
the code, you will see that there is a
separate block which we have written

3532
01:10:25,709 --> 01:10:25,719
separate block which we have written
 

3533
01:10:25,719 --> 01:10:27,990
separate block which we have written
earlier. So there is a class called

3534
01:10:27,990 --> 01:10:28,000
earlier. So there is a class called
 

3535
01:10:28,000 --> 01:10:29,910
earlier. So there is a class called
layer normalization. Since layer

3536
01:10:29,910 --> 01:10:29,920
layer normalization. Since layer
 

3537
01:10:29,920 --> 01:10:32,070
layer normalization. Since layer
normalization is utilized at many places

3538
01:10:32,070 --> 01:10:32,080
normalization is utilized at many places
 

3539
01:10:32,080 --> 01:10:34,390
normalization is utilized at many places
in the transformer block, you see that

3540
01:10:34,390 --> 01:10:34,400
in the transformer block, you see that
 

3541
01:10:34,400 --> 01:10:37,590
in the transformer block, you see that
it's introduced it's introduced here.

3542
01:10:37,590 --> 01:10:37,600
it's introduced it's introduced here.
 

3543
01:10:37,600 --> 01:10:39,430
it's introduced it's introduced here.
It's introduced here also. So it's there

3544
01:10:39,430 --> 01:10:39,440
It's introduced here also. So it's there
 

3545
01:10:39,440 --> 01:10:41,110
It's introduced here also. So it's there
at two places, right? So it's better to

3546
01:10:41,110 --> 01:10:41,120
at two places, right? So it's better to
 

3547
01:10:41,120 --> 01:10:43,830
at two places, right? So it's better to
define a class for it. And we just use

3548
01:10:43,830 --> 01:10:43,840
define a class for it. And we just use
 

3549
01:10:43,840 --> 01:10:46,310
define a class for it. And we just use
the layer norm method which is available

3550
01:10:46,310 --> 01:10:46,320
the layer norm method which is available
 

3551
01:10:46,320 --> 01:10:48,790
the layer norm method which is available
through PyTorch. It just normalizes

3552
01:10:48,790 --> 01:10:48,800
through PyTorch. It just normalizes
 

3553
01:10:48,800 --> 01:10:50,750
through PyTorch. It just normalizes
every single row,

3554
01:10:50,750 --> 01:10:50,760
every single row,
 

3555
01:10:50,760 --> 01:10:53,750
every single row,
right? Okay. So layer normalization is

3556
01:10:53,750 --> 01:10:53,760
right? Okay. So layer normalization is
 

3557
01:10:53,760 --> 01:10:56,390
right? Okay. So layer normalization is
done. The next part is the place where

3558
01:10:56,390 --> 01:10:56,400
done. The next part is the place where
 

3559
01:10:56,400 --> 01:10:58,870
done. The next part is the place where
magic happens. So magic happens here

3560
01:10:58,870 --> 01:10:58,880
magic happens. So magic happens here
 

3561
01:10:58,880 --> 01:11:02,750
magic happens. So magic happens here
because the next step is multi head

3562
01:11:02,750 --> 01:11:02,760
because the next step is multi head
 

3563
01:11:02,760 --> 01:11:05,270
because the next step is multi head
attention. Now what exactly happens in

3564
01:11:05,270 --> 01:11:05,280
attention. Now what exactly happens in
 

3565
01:11:05,280 --> 01:11:07,030
attention. Now what exactly happens in
multi head attention and why is the

3566
01:11:07,030 --> 01:11:07,040
multi head attention and why is the
 

3567
01:11:07,040 --> 01:11:09,189
multi head attention and why is the
multi head attention block needed? If

3568
01:11:09,189 --> 01:11:09,199
multi head attention block needed? If
 

3569
01:11:09,199 --> 01:11:10,790
multi head attention block needed? If
you observe closely what I have done

3570
01:11:10,790 --> 01:11:10,800
you observe closely what I have done
 

3571
01:11:10,800 --> 01:11:13,510
you observe closely what I have done
until now uh let's say we have the

3572
01:11:13,510 --> 01:11:13,520
until now uh let's say we have the
 

3573
01:11:13,520 --> 01:11:17,550
until now uh let's say we have the
tokens right one day a

3574
01:11:17,550 --> 01:11:17,560
tokens right one day a
 

3575
01:11:17,560 --> 01:11:20,310
tokens right one day a
little we have taken for each token we

3576
01:11:20,310 --> 01:11:20,320
little we have taken for each token we
 

3577
01:11:20,320 --> 01:11:22,149
little we have taken for each token we
have taken the token embeddings we have

3578
01:11:22,149 --> 01:11:22,159
have taken the token embeddings we have
 

3579
01:11:22,159 --> 01:11:24,470
have taken the token embeddings we have
added the position embeddings but until

3580
01:11:24,470 --> 01:11:24,480
added the position embeddings but until
 

3581
01:11:24,480 --> 01:11:26,630
added the position embeddings but until
now we have no information about how one

3582
01:11:26,630 --> 01:11:26,640
now we have no information about how one
 

3583
01:11:26,640 --> 01:11:28,470
now we have no information about how one
token essentially relates to the other

3584
01:11:28,470 --> 01:11:28,480
token essentially relates to the other
 

3585
01:11:28,480 --> 01:11:31,270
token essentially relates to the other
token right and that information is very

3586
01:11:31,270 --> 01:11:31,280
token right and that information is very
 

3587
01:11:31,280 --> 01:11:33,669
token right and that information is very
important for us why because consider

3588
01:11:33,669 --> 01:11:33,679
important for us why because consider
 

3589
01:11:33,679 --> 01:11:36,110
important for us why because consider
this sentence the

3590
01:11:36,110 --> 01:11:36,120
this sentence the
 

3591
01:11:36,120 --> 01:11:39,470
this sentence the
dog the dog chased the

3592
01:11:39,470 --> 01:11:39,480
dog the dog chased the
 

3593
01:11:39,480 --> 01:11:41,870
dog the dog chased the
ball,

3594
01:11:41,870 --> 01:11:41,880
ball,
 

3595
01:11:41,880 --> 01:11:46,550
ball,
it could not catch it. Let's say if we

3596
01:11:46,550 --> 01:11:46,560
it could not catch it. Let's say if we
 

3597
01:11:46,560 --> 01:11:51,030
it could not catch it. Let's say if we
have this sentence and this first it now

3598
01:11:51,030 --> 01:11:51,040
have this sentence and this first it now
 

3599
01:11:51,040 --> 01:11:53,510
have this sentence and this first it now
relates to the dog, right? But if you

3600
01:11:53,510 --> 01:11:53,520
relates to the dog, right? But if you
 

3601
01:11:53,520 --> 01:11:56,790
relates to the dog, right? But if you
look at the second it, the second it now

3602
01:11:56,790 --> 01:11:56,800
look at the second it, the second it now
 

3603
01:11:56,800 --> 01:11:59,270
look at the second it, the second it now
relates to the ball. So if I want to do

3604
01:11:59,270 --> 01:11:59,280
relates to the ball. So if I want to do
 

3605
01:11:59,280 --> 01:12:01,350
relates to the ball. So if I want to do
next token prediction and if some it

3606
01:12:01,350 --> 01:12:01,360
next token prediction and if some it
 

3607
01:12:01,360 --> 01:12:03,430
next token prediction and if some it
comes somewhere in the next sentence, I

3608
01:12:03,430 --> 01:12:03,440
comes somewhere in the next sentence, I
 

3609
01:12:03,440 --> 01:12:04,950
comes somewhere in the next sentence, I
want to know whether it belongs to the

3610
01:12:04,950 --> 01:12:04,960
want to know whether it belongs to the
 

3611
01:12:04,960 --> 01:12:07,270
want to know whether it belongs to the
dog or whether it's the cat, right? If

3612
01:12:07,270 --> 01:12:07,280
dog or whether it's the cat, right? If
 

3613
01:12:07,280 --> 01:12:09,110
dog or whether it's the cat, right? If
it's the dog, I might predict something

3614
01:12:09,110 --> 01:12:09,120
it's the dog, I might predict something
 

3615
01:12:09,120 --> 01:12:11,590
it's the dog, I might predict something
like bark. If it's the ball, I might

3616
01:12:11,590 --> 01:12:11,600
like bark. If it's the ball, I might
 

3617
01:12:11,600 --> 01:12:13,350
like bark. If it's the ball, I might
predict something like flu because the

3618
01:12:13,350 --> 01:12:13,360
predict something like flu because the
 

3619
01:12:13,360 --> 01:12:16,550
predict something like flu because the
ball can fly. But if I don't know how

3620
01:12:16,550 --> 01:12:16,560
ball can fly. But if I don't know how
 

3621
01:12:16,560 --> 01:12:20,229
ball can fly. But if I don't know how
one token relates to another token, I

3622
01:12:20,229 --> 01:12:20,239
one token relates to another token, I
 

3623
01:12:20,239 --> 01:12:22,630
one token relates to another token, I
don't have the information about how to

3624
01:12:22,630 --> 01:12:22,640
don't have the information about how to
 

3625
01:12:22,640 --> 01:12:25,110
don't have the information about how to
predict the next token. And that's what

3626
01:12:25,110 --> 01:12:25,120
predict the next token. And that's what
 

3627
01:12:25,120 --> 01:12:27,669
predict the next token. And that's what
the attention mechanism actually does.

3628
01:12:27,669 --> 01:12:27,679
the attention mechanism actually does.
 

3629
01:12:27,679 --> 01:12:30,310
the attention mechanism actually does.
The attention mechanism augments the

3630
01:12:30,310 --> 01:12:30,320
The attention mechanism augments the
 

3631
01:12:30,320 --> 01:12:32,790
The attention mechanism augments the
input embedding vector so that for every

3632
01:12:32,790 --> 01:12:32,800
input embedding vector so that for every
 

3633
01:12:32,800 --> 01:12:35,350
input embedding vector so that for every
vector now I can have information about

3634
01:12:35,350 --> 01:12:35,360
vector now I can have information about
 

3635
01:12:35,360 --> 01:12:38,070
vector now I can have information about
the neighbors. So if I look at this

3636
01:12:38,070 --> 01:12:38,080
the neighbors. So if I look at this
 

3637
01:12:38,080 --> 01:12:40,310
the neighbors. So if I look at this
input vector, I need to have information

3638
01:12:40,310 --> 01:12:40,320
input vector, I need to have information
 

3639
01:12:40,320 --> 01:12:42,470
input vector, I need to have information
that I need to pay the maximum attention

3640
01:12:42,470 --> 01:12:42,480
that I need to pay the maximum attention
 

3641
01:12:42,480 --> 01:12:45,669
that I need to pay the maximum attention
to dog. If I look at this, it I need to

3642
01:12:45,669 --> 01:12:45,679
to dog. If I look at this, it I need to
 

3643
01:12:45,679 --> 01:12:48,030
to dog. If I look at this, it I need to
pay maximum attention to

3644
01:12:48,030 --> 01:12:48,040
pay maximum attention to
 

3645
01:12:48,040 --> 01:12:51,350
pay maximum attention to
ball. So the main purpose of the

3646
01:12:51,350 --> 01:12:51,360
ball. So the main purpose of the
 

3647
01:12:51,360 --> 01:12:54,950
ball. So the main purpose of the
attention mechanism is that let's say

3648
01:12:54,950 --> 01:12:54,960
attention mechanism is that let's say
 

3649
01:12:54,960 --> 01:12:57,030
attention mechanism is that let's say
uh let's say we look at this it and

3650
01:12:57,030 --> 01:12:57,040
uh let's say we look at this it and
 

3651
01:12:57,040 --> 01:12:58,709
uh let's say we look at this it and
currently we have the input embedding

3652
01:12:58,709 --> 01:12:58,719
currently we have the input embedding
 

3653
01:12:58,719 --> 01:13:01,430
currently we have the input embedding
vector for this it. It has no clue about

3654
01:13:01,430 --> 01:13:01,440
vector for this it. It has no clue about
 

3655
01:13:01,440 --> 01:13:03,430
vector for this it. It has no clue about
all the other tokens, right? What the

3656
01:13:03,430 --> 01:13:03,440
all the other tokens, right? What the
 

3657
01:13:03,440 --> 01:13:05,350
all the other tokens, right? What the
attention mechanism does is that it

3658
01:13:05,350 --> 01:13:05,360
attention mechanism does is that it
 

3659
01:13:05,360 --> 01:13:07,189
attention mechanism does is that it
maintains the size of this it so that

3660
01:13:07,189 --> 01:13:07,199
maintains the size of this it so that
 

3661
01:13:07,199 --> 01:13:09,630
maintains the size of this it so that
it's if it's 768 the size will remain

3662
01:13:09,630 --> 01:13:09,640
it's if it's 768 the size will remain
 

3663
01:13:09,640 --> 01:13:12,470
it's if it's 768 the size will remain
768 but it will convert it into a vector

3664
01:13:12,470 --> 01:13:12,480
768 but it will convert it into a vector
 

3665
01:13:12,480 --> 01:13:14,870
768 but it will convert it into a vector
which is called as the context vector.

3666
01:13:14,870 --> 01:13:14,880
which is called as the context vector.
 

3667
01:13:14,880 --> 01:13:16,950
which is called as the context vector.
Why is it called as the context vector?

3668
01:13:16,950 --> 01:13:16,960
Why is it called as the context vector?
 

3669
01:13:16,960 --> 01:13:20,709
Why is it called as the context vector?
Because to get this context vector from

3670
01:13:20,709 --> 01:13:20,719
Because to get this context vector from
 

3671
01:13:20,719 --> 01:13:23,350
Because to get this context vector from
the input embedding vector we take into

3672
01:13:23,350 --> 01:13:23,360
the input embedding vector we take into
 

3673
01:13:23,360 --> 01:13:26,870
the input embedding vector we take into
account how it how this it relates to

3674
01:13:26,870 --> 01:13:26,880
account how it how this it relates to
 

3675
01:13:26,880 --> 01:13:28,950
account how it how this it relates to
all the other tokens. So we somehow get

3676
01:13:28,950 --> 01:13:28,960
all the other tokens. So we somehow get
 

3677
01:13:28,960 --> 01:13:31,910
all the other tokens. So we somehow get
the attention scores between it and all

3678
01:13:31,910 --> 01:13:31,920
the attention scores between it and all
 

3679
01:13:31,920 --> 01:13:33,750
the attention scores between it and all
the other tokens and we use those

3680
01:13:33,750 --> 01:13:33,760
the other tokens and we use those
 

3681
01:13:33,760 --> 01:13:35,750
the other tokens and we use those
attention scores to go from the input

3682
01:13:35,750 --> 01:13:35,760
attention scores to go from the input
 

3683
01:13:35,760 --> 01:13:37,669
attention scores to go from the input
embedding vector to the context vector

3684
01:13:37,669 --> 01:13:37,679
embedding vector to the context vector
 

3685
01:13:37,679 --> 01:13:38,510
embedding vector to the context vector
for

3686
01:13:38,510 --> 01:13:38,520
for
 

3687
01:13:38,520 --> 01:13:41,270
for
it and this has been captured here in

3688
01:13:41,270 --> 01:13:41,280
it and this has been captured here in
 

3689
01:13:41,280 --> 01:13:44,229
it and this has been captured here in
this image very nicely. Uh when we have

3690
01:13:44,229 --> 01:13:44,239
this image very nicely. Uh when we have
 

3691
01:13:44,239 --> 01:13:47,270
this image very nicely. Uh when we have
one one token what we do is that through

3692
01:13:47,270 --> 01:13:47,280
one one token what we do is that through
 

3693
01:13:47,280 --> 01:13:48,870
one one token what we do is that through
the attention mechanism through the

3694
01:13:48,870 --> 01:13:48,880
the attention mechanism through the
 

3695
01:13:48,880 --> 01:13:50,870
the attention mechanism through the
multiad attention mechanism we actually

3696
01:13:50,870 --> 01:13:50,880
multiad attention mechanism we actually
 

3697
01:13:50,880 --> 01:13:53,510
multiad attention mechanism we actually
capture the attention score between one

3698
01:13:53,510 --> 01:13:53,520
capture the attention score between one
 

3699
01:13:53,520 --> 01:13:55,990
capture the attention score between one
token and all the other tokens around

3700
01:13:55,990 --> 01:13:56,000
token and all the other tokens around
 

3701
01:13:56,000 --> 01:13:58,310
token and all the other tokens around
it. So we capture the attention score

3702
01:13:58,310 --> 01:13:58,320
it. So we capture the attention score
 

3703
01:13:58,320 --> 01:14:00,870
it. So we capture the attention score
between next and the. We capture the

3704
01:14:00,870 --> 01:14:00,880
between next and the. We capture the
 

3705
01:14:00,880 --> 01:14:02,790
between next and the. We capture the
attention score between next and day. We

3706
01:14:02,790 --> 01:14:02,800
attention score between next and day. We
 

3707
01:14:02,800 --> 01:14:04,470
attention score between next and day. We
capture the attention score between next

3708
01:14:04,470 --> 01:14:04,480
capture the attention score between next
 

3709
01:14:04,480 --> 01:14:06,630
capture the attention score between next
and is. And we capture the attention

3710
01:14:06,630 --> 01:14:06,640
and is. And we capture the attention
 

3711
01:14:06,640 --> 01:14:09,110
and is. And we capture the attention
score between next and bright. Why do we

3712
01:14:09,110 --> 01:14:09,120
score between next and bright. Why do we
 

3713
01:14:09,120 --> 01:14:11,030
score between next and bright. Why do we
capture these attention scores? The

3714
01:14:11,030 --> 01:14:11,040
capture these attention scores? The
 

3715
01:14:11,040 --> 01:14:12,709
capture these attention scores? The
reason we capture these attention scores

3716
01:14:12,709 --> 01:14:12,719
reason we capture these attention scores
 

3717
01:14:12,719 --> 01:14:15,270
reason we capture these attention scores
is because let's say I have next, right?

3718
01:14:15,270 --> 01:14:15,280
is because let's say I have next, right?
 

3719
01:14:15,280 --> 01:14:16,870
is because let's say I have next, right?
And currently we have an input vector

3720
01:14:16,870 --> 01:14:16,880
And currently we have an input vector
 

3721
01:14:16,880 --> 01:14:19,350
And currently we have an input vector
for next. Now I will use all these

3722
01:14:19,350 --> 01:14:19,360
for next. Now I will use all these
 

3723
01:14:19,360 --> 01:14:21,270
for next. Now I will use all these
values. is I will use alpha 2 1, alpha

3724
01:14:21,270 --> 01:14:21,280
values. is I will use alpha 2 1, alpha
 

3725
01:14:21,280 --> 01:14:24,470
values. is I will use alpha 2 1, alpha
22, alpha 2 3, alpha 2 4 and alpha 25 to

3726
01:14:24,470 --> 01:14:24,480
22, alpha 2 3, alpha 2 4 and alpha 25 to
 

3727
01:14:24,480 --> 01:14:26,669
22, alpha 2 3, alpha 2 4 and alpha 25 to
convert this input vector into a context

3728
01:14:26,669 --> 01:14:26,679
convert this input vector into a context
 

3729
01:14:26,679 --> 01:14:29,270
convert this input vector into a context
vector and the context vector now

3730
01:14:29,270 --> 01:14:29,280
vector and the context vector now
 

3731
01:14:29,280 --> 01:14:31,270
vector and the context vector now
remember will be much richer. Why will

3732
01:14:31,270 --> 01:14:31,280
remember will be much richer. Why will
 

3733
01:14:31,280 --> 01:14:33,030
remember will be much richer. Why will
it be much richer? Because it will have

3734
01:14:33,030 --> 01:14:33,040
it be much richer? Because it will have
 

3735
01:14:33,040 --> 01:14:35,830
it be much richer? Because it will have
information about how my word next

3736
01:14:35,830 --> 01:14:35,840
information about how my word next
 

3737
01:14:35,840 --> 01:14:37,430
information about how my word next
relates to all the other tokens

3738
01:14:37,430 --> 01:14:37,440
relates to all the other tokens
 

3739
01:14:37,440 --> 01:14:40,950
relates to all the other tokens
surrounding it. This attention mechanism

3740
01:14:40,950 --> 01:14:40,960
surrounding it. This attention mechanism
 

3741
01:14:40,960 --> 01:14:43,030
surrounding it. This attention mechanism
if it was not there language models

3742
01:14:43,030 --> 01:14:43,040
if it was not there language models
 

3743
01:14:43,040 --> 01:14:44,790
if it was not there language models
would not work the way they are working

3744
01:14:44,790 --> 01:14:44,800
would not work the way they are working
 

3745
01:14:44,800 --> 01:14:48,149
would not work the way they are working
right now. And uh what the attention

3746
01:14:48,149 --> 01:14:48,159
right now. And uh what the attention
 

3747
01:14:48,159 --> 01:14:50,070
right now. And uh what the attention
mechanism actually does is that it takes

3748
01:14:50,070 --> 01:14:50,080
mechanism actually does is that it takes
 

3749
01:14:50,080 --> 01:14:51,910
mechanism actually does is that it takes
every tokens and converts them into

3750
01:14:51,910 --> 01:14:51,920
every tokens and converts them into
 

3751
01:14:51,920 --> 01:14:55,510
every tokens and converts them into
these context vectors. So in this part

3752
01:14:55,510 --> 01:14:55,520
these context vectors. So in this part
 

3753
01:14:55,520 --> 01:14:57,590
these context vectors. So in this part
is where we start understanding more

3754
01:14:57,590 --> 01:14:57,600
is where we start understanding more
 

3755
01:14:57,600 --> 01:15:00,149
is where we start understanding more
context about the sentence. So word

3756
01:15:00,149 --> 01:15:00,159
context about the sentence. So word
 

3757
01:15:00,159 --> 01:15:02,310
context about the sentence. So word
start understanding the meaning of their

3758
01:15:02,310 --> 01:15:02,320
start understanding the meaning of their
 

3759
01:15:02,320 --> 01:15:04,870
start understanding the meaning of their
neighbors and so next word prediction

3760
01:15:04,870 --> 01:15:04,880
neighbors and so next word prediction
 

3761
01:15:04,880 --> 01:15:06,990
neighbors and so next word prediction
effectively becomes much more

3762
01:15:06,990 --> 01:15:07,000
effectively becomes much more
 

3763
01:15:07,000 --> 01:15:09,669
effectively becomes much more
better. Now what exactly happens in this

3764
01:15:09,669 --> 01:15:09,679
better. Now what exactly happens in this
 

3765
01:15:09,679 --> 01:15:11,430
better. Now what exactly happens in this
multi-ad attention block? There are

3766
01:15:11,430 --> 01:15:11,440
multi-ad attention block? There are
 

3767
01:15:11,440 --> 01:15:13,110
multi-ad attention block? There are
several steps which happen in the multi

3768
01:15:13,110 --> 01:15:13,120
several steps which happen in the multi
 

3769
01:15:13,120 --> 01:15:15,149
several steps which happen in the multi
head attention block but more

3770
01:15:15,149 --> 01:15:15,159
head attention block but more
 

3771
01:15:15,159 --> 01:15:17,750
head attention block but more
importantly the input is these input

3772
01:15:17,750 --> 01:15:17,760
importantly the input is these input
 

3773
01:15:17,760 --> 01:15:19,350
importantly the input is these input
embeddings right. So if you look at the

3774
01:15:19,350 --> 01:15:19,360
embeddings right. So if you look at the
 

3775
01:15:19,360 --> 01:15:21,430
embeddings right. So if you look at the
multi head attention block the input are

3776
01:15:21,430 --> 01:15:21,440
multi head attention block the input are
 

3777
01:15:21,440 --> 01:15:23,189
multi head attention block the input are
these same input embeddings which are

3778
01:15:23,189 --> 01:15:23,199
these same input embeddings which are
 

3779
01:15:23,199 --> 01:15:26,229
these same input embeddings which are
these after layer normalization we apply

3780
01:15:26,229 --> 01:15:26,239
these after layer normalization we apply
 

3781
01:15:26,239 --> 01:15:28,229
these after layer normalization we apply
the mask multi head attention and the

3782
01:15:28,229 --> 01:15:28,239
the mask multi head attention and the
 

3783
01:15:28,239 --> 01:15:30,630
the mask multi head attention and the
output is the dimension is the same. So

3784
01:15:30,630 --> 01:15:30,640
output is the dimension is the same. So
 

3785
01:15:30,640 --> 01:15:32,470
output is the dimension is the same. So
if you see before multi head attention

3786
01:15:32,470 --> 01:15:32,480
if you see before multi head attention
 

3787
01:15:32,480 --> 01:15:35,590
if you see before multi head attention
each each token has a dimension 768.

3788
01:15:35,590 --> 01:15:35,600
each each token has a dimension 768.
 

3789
01:15:35,600 --> 01:15:37,510
each each token has a dimension 768.
After multi head attention also they

3790
01:15:37,510 --> 01:15:37,520
After multi head attention also they
 

3791
01:15:37,520 --> 01:15:39,990
After multi head attention also they
have a dimension 768. But now each of

3792
01:15:39,990 --> 01:15:40,000
have a dimension 768. But now each of
 

3793
01:15:40,000 --> 01:15:41,830
have a dimension 768. But now each of
these vectors are context vectors which

3794
01:15:41,830 --> 01:15:41,840
these vectors are context vectors which
 

3795
01:15:41,840 --> 01:15:43,430
these vectors are context vectors which
means they have enriched information of

3796
01:15:43,430 --> 01:15:43,440
means they have enriched information of
 

3797
01:15:43,440 --> 01:15:44,669
means they have enriched information of
all their

3798
01:15:44,669 --> 01:15:44,679
all their
 

3799
01:15:44,679 --> 01:15:47,189
all their
neighbors. The way we go from input

3800
01:15:47,189 --> 01:15:47,199
neighbors. The way we go from input
 

3801
01:15:47,199 --> 01:15:49,030
neighbors. The way we go from input
vectors to context vectors is through

3802
01:15:49,030 --> 01:15:49,040
vectors to context vectors is through
 

3803
01:15:49,040 --> 01:15:50,550
vectors to context vectors is through
these matrices which are called as

3804
01:15:50,550 --> 01:15:50,560
these matrices which are called as
 

3805
01:15:50,560 --> 01:15:53,030
these matrices which are called as
queries, keys and values. Now I'm not

3806
01:15:53,030 --> 01:15:53,040
queries, keys and values. Now I'm not
 

3807
01:15:53,040 --> 01:15:54,790
queries, keys and values. Now I'm not
going to go into detail of this because

3808
01:15:54,790 --> 01:15:54,800
going to go into detail of this because
 

3809
01:15:54,800 --> 01:15:57,270
going to go into detail of this because
that will again uh tremendously increase

3810
01:15:57,270 --> 01:15:57,280
that will again uh tremendously increase
 

3811
01:15:57,280 --> 01:15:58,950
that will again uh tremendously increase
the length of this video. But let me

3812
01:15:58,950 --> 01:15:58,960
the length of this video. But let me
 

3813
01:15:58,960 --> 01:16:00,790
the length of this video. But let me
give you a quick recap of how this is

3814
01:16:00,790 --> 01:16:00,800
give you a quick recap of how this is
 

3815
01:16:00,800 --> 01:16:04,630
give you a quick recap of how this is
actually done. First what we do is that

3816
01:16:04,630 --> 01:16:04,640
actually done. First what we do is that
 

3817
01:16:04,640 --> 01:16:06,229
actually done. First what we do is that
let's say I have my input embedding

3818
01:16:06,229 --> 01:16:06,239
let's say I have my input embedding
 

3819
01:16:06,239 --> 01:16:10,350
let's say I have my input embedding
matrix now. So I'll bring this over

3820
01:16:10,350 --> 01:16:10,360
matrix now. So I'll bring this over
 

3821
01:16:10,360 --> 01:16:13,750
matrix now. So I'll bring this over
here. Uh let's say this because it's

3822
01:16:13,750 --> 01:16:13,760
here. Uh let's say this because it's
 

3823
01:16:13,760 --> 01:16:16,470
here. Uh let's say this because it's
easy to copy paste. So I'll bring this

3824
01:16:16,470 --> 01:16:16,480
easy to copy paste. So I'll bring this
 

3825
01:16:16,480 --> 01:16:19,189
easy to copy paste. So I'll bring this
over here. That's my input embedding

3826
01:16:19,189 --> 01:16:19,199
over here. That's my input embedding
 

3827
01:16:19,199 --> 01:16:21,830
over here. That's my input embedding
matrix. Let's say what is done is that

3828
01:16:21,830 --> 01:16:21,840
matrix. Let's say what is done is that
 

3829
01:16:21,840 --> 01:16:24,149
matrix. Let's say what is done is that
first we introduce some matrices which

3830
01:16:24,149 --> 01:16:24,159
first we introduce some matrices which
 

3831
01:16:24,159 --> 01:16:26,750
first we introduce some matrices which
are called as the query matrix, the key

3832
01:16:26,750 --> 01:16:26,760
are called as the query matrix, the key
 

3833
01:16:26,760 --> 01:16:29,390
are called as the query matrix, the key
matrix and the value

3834
01:16:29,390 --> 01:16:29,400
matrix and the value
 

3835
01:16:29,400 --> 01:16:32,630
matrix and the value
matrix. And the input right now is 4x

3836
01:16:32,630 --> 01:16:32,640
matrix. And the input right now is 4x
 

3837
01:16:32,640 --> 01:16:35,750
matrix. And the input right now is 4x
768.

3838
01:16:35,750 --> 01:16:35,760

 

3839
01:16:35,760 --> 01:16:38,510

Each of these will be of a size of 768x

3840
01:16:38,510 --> 01:16:38,520
Each of these will be of a size of 768x
 

3841
01:16:38,520 --> 01:16:41,189
Each of these will be of a size of 768x
768. So when you multiply input with the

3842
01:16:41,189 --> 01:16:41,199
768. So when you multiply input with the
 

3843
01:16:41,199 --> 01:16:43,189
768. So when you multiply input with the
trainable query matrix, it gives me the

3844
01:16:43,189 --> 01:16:43,199
trainable query matrix, it gives me the
 

3845
01:16:43,199 --> 01:16:46,790
trainable query matrix, it gives me the
keys matrix. And that's again 4x 768.

3846
01:16:46,790 --> 01:16:46,800
keys matrix. And that's again 4x 768.
 

3847
01:16:46,800 --> 01:16:48,550
keys matrix. And that's again 4x 768.
When you multiply the input with the

3848
01:16:48,550 --> 01:16:48,560
When you multiply the input with the
 

3849
01:16:48,560 --> 01:16:51,030
When you multiply the input with the
trainable keys matrix, sorry, this is

3850
01:16:51,030 --> 01:16:51,040
trainable keys matrix, sorry, this is
 

3851
01:16:51,040 --> 01:16:52,630
trainable keys matrix, sorry, this is
the queries matrix. Then this is the

3852
01:16:52,630 --> 01:16:52,640
the queries matrix. Then this is the
 

3853
01:16:52,640 --> 01:16:53,870
the queries matrix. Then this is the
keys

3854
01:16:53,870 --> 01:16:53,880
keys
 

3855
01:16:53,880 --> 01:16:56,870
keys
matrix and this is the values matrix.

3856
01:16:56,870 --> 01:16:56,880
matrix and this is the values matrix.
 

3857
01:16:56,880 --> 01:16:58,790
matrix and this is the values matrix.
All of this must be sounding very weird

3858
01:16:58,790 --> 01:16:58,800
All of this must be sounding very weird
 

3859
01:16:58,800 --> 01:17:00,550
All of this must be sounding very weird
because why do we introduce additional

3860
01:17:00,550 --> 01:17:00,560
because why do we introduce additional
 

3861
01:17:00,560 --> 01:17:03,110
because why do we introduce additional
training matrices? The simple reason is

3862
01:17:03,110 --> 01:17:03,120
training matrices? The simple reason is
 

3863
01:17:03,120 --> 01:17:05,590
training matrices? The simple reason is
that humans could not figure out a

3864
01:17:05,590 --> 01:17:05,600
that humans could not figure out a
 

3865
01:17:05,600 --> 01:17:07,270
that humans could not figure out a
physical representation of how to

3866
01:17:07,270 --> 01:17:07,280
physical representation of how to
 

3867
01:17:07,280 --> 01:17:10,790
physical representation of how to
capture context. So we pass all of that

3868
01:17:10,790 --> 01:17:10,800
capture context. So we pass all of that
 

3869
01:17:10,800 --> 01:17:12,790
capture context. So we pass all of that
burden of figuring out how to capture

3870
01:17:12,790 --> 01:17:12,800
burden of figuring out how to capture
 

3871
01:17:12,800 --> 01:17:15,189
burden of figuring out how to capture
context between tokens to this trainable

3872
01:17:15,189 --> 01:17:15,199
context between tokens to this trainable
 

3873
01:17:15,199 --> 01:17:16,950
context between tokens to this trainable
weight matrices. So again I'm marking

3874
01:17:16,950 --> 01:17:16,960
weight matrices. So again I'm marking
 

3875
01:17:16,960 --> 01:17:19,189
weight matrices. So again I'm marking
these with P because these are the

3876
01:17:19,189 --> 01:17:19,199
these with P because these are the
 

3877
01:17:19,199 --> 01:17:21,110
these with P because these are the
parameters which will be learned. So

3878
01:17:21,110 --> 01:17:21,120
parameters which will be learned. So
 

3879
01:17:21,120 --> 01:17:23,550
parameters which will be learned. So
let's see where all we have marked P so

3880
01:17:23,550 --> 01:17:23,560
let's see where all we have marked P so
 

3881
01:17:23,560 --> 01:17:26,310
let's see where all we have marked P so
far. We have marked P over here in the

3882
01:17:26,310 --> 01:17:26,320
far. We have marked P over here in the
 

3883
01:17:26,320 --> 01:17:27,910
far. We have marked P over here in the
token embedding in the positional

3884
01:17:27,910 --> 01:17:27,920
token embedding in the positional
 

3885
01:17:27,920 --> 01:17:31,270
token embedding in the positional
embedding. We'll mark P over here in the

3886
01:17:31,270 --> 01:17:31,280
embedding. We'll mark P over here in the
 

3887
01:17:31,280 --> 01:17:33,270
embedding. We'll mark P over here in the
mass multi and attention also because

3888
01:17:33,270 --> 01:17:33,280
mass multi and attention also because
 

3889
01:17:33,280 --> 01:17:34,790
mass multi and attention also because
these are the parameters which are

3890
01:17:34,790 --> 01:17:34,800
these are the parameters which are
 

3891
01:17:34,800 --> 01:17:36,630
these are the parameters which are
actually learned. Once we have the

3892
01:17:36,630 --> 01:17:36,640
actually learned. Once we have the
 

3893
01:17:36,640 --> 01:17:38,390
actually learned. Once we have the
queries and the key vectors, we'll take

3894
01:17:38,390 --> 01:17:38,400
queries and the key vectors, we'll take
 

3895
01:17:38,400 --> 01:17:40,350
queries and the key vectors, we'll take
a dotproduct between the queries and the

3896
01:17:40,350 --> 01:17:40,360
a dotproduct between the queries and the
 

3897
01:17:40,360 --> 01:17:43,110
a dotproduct between the queries and the
keys and that will give me the attention

3898
01:17:43,110 --> 01:17:43,120
keys and that will give me the attention
 

3899
01:17:43,120 --> 01:17:45,270
keys and that will give me the attention
score matrix. The attention score matrix

3900
01:17:45,270 --> 01:17:45,280
score matrix. The attention score matrix
 

3901
01:17:45,280 --> 01:17:48,310
score matrix. The attention score matrix
will be a 4x4 matrix now because we have

3902
01:17:48,310 --> 01:17:48,320
will be a 4x4 matrix now because we have
 

3903
01:17:48,320 --> 01:17:50,590
will be a 4x4 matrix now because we have
four tokens, right?

3904
01:17:50,590 --> 01:17:50,600
four tokens, right?
 

3905
01:17:50,600 --> 01:17:53,910
four tokens, right?
Uh and my four tokens are every effort

3906
01:17:53,910 --> 01:17:53,920
Uh and my four tokens are every effort
 

3907
01:17:53,920 --> 01:17:58,390
Uh and my four tokens are every effort
moves you or um the little girl example

3908
01:17:58,390 --> 01:17:58,400
moves you or um the little girl example
 

3909
01:17:58,400 --> 01:18:01,590
moves you or um the little girl example
which we had one day a little. So now

3910
01:18:01,590 --> 01:18:01,600
which we had one day a little. So now
 

3911
01:18:01,600 --> 01:18:05,630
which we had one day a little. So now
why do I have a 4x4 here? So if I say

3912
01:18:05,630 --> 01:18:05,640
why do I have a 4x4 here? So if I say
 

3913
01:18:05,640 --> 01:18:08,669
why do I have a 4x4 here? So if I say
one day a

3914
01:18:08,669 --> 01:18:08,679
one day a
 

3915
01:18:08,679 --> 01:18:13,430
one day a
little and one day a little. The reason

3916
01:18:13,430 --> 01:18:13,440
little and one day a little. The reason
 

3917
01:18:13,440 --> 01:18:16,070
little and one day a little. The reason
we have a 4x4 matrix here is because for

3918
01:18:16,070 --> 01:18:16,080
we have a 4x4 matrix here is because for
 

3919
01:18:16,080 --> 01:18:17,070
we have a 4x4 matrix here is because for
every

3920
01:18:17,070 --> 01:18:17,080
every
 

3921
01:18:17,080 --> 01:18:19,910
every
token, this matrix give me gives me how

3922
01:18:19,910 --> 01:18:19,920
token, this matrix give me gives me how
 

3923
01:18:19,920 --> 01:18:21,430
token, this matrix give me gives me how
much attention should I pay to the

3924
01:18:21,430 --> 01:18:21,440
much attention should I pay to the
 

3925
01:18:21,440 --> 01:18:24,149
much attention should I pay to the
nearby token. So if I look at day, this

3926
01:18:24,149 --> 01:18:24,159
nearby token. So if I look at day, this
 

3927
01:18:24,159 --> 01:18:25,910
nearby token. So if I look at day, this
value tells me how much attention is

3928
01:18:25,910 --> 01:18:25,920
value tells me how much attention is
 

3929
01:18:25,920 --> 01:18:28,229
value tells me how much attention is
between day and 1. This value tells me

3930
01:18:28,229 --> 01:18:28,239
between day and 1. This value tells me
 

3931
01:18:28,239 --> 01:18:30,550
between day and 1. This value tells me
between day and day. This value tells me

3932
01:18:30,550 --> 01:18:30,560
between day and day. This value tells me
 

3933
01:18:30,560 --> 01:18:32,709
between day and day. This value tells me
between day and this value tells me

3934
01:18:32,709 --> 01:18:32,719
between day and this value tells me
 

3935
01:18:32,719 --> 01:18:34,550
between day and this value tells me
between day and little. This is my

3936
01:18:34,550 --> 01:18:34,560
between day and little. This is my
 

3937
01:18:34,560 --> 01:18:37,110
between day and little. This is my
attention score matrix. The attention

3938
01:18:37,110 --> 01:18:37,120
attention score matrix. The attention
 

3939
01:18:37,120 --> 01:18:39,030
attention score matrix. The attention
score matrix is then converted into an

3940
01:18:39,030 --> 01:18:39,040
score matrix is then converted into an
 

3941
01:18:39,040 --> 01:18:43,430
score matrix is then converted into an
attention weights matrix.

3942
01:18:43,430 --> 01:18:43,440

 

3943
01:18:43,440 --> 01:18:45,750

And then the attention weights matrix is

3944
01:18:45,750 --> 01:18:45,760
And then the attention weights matrix is
 

3945
01:18:45,760 --> 01:18:48,350
And then the attention weights matrix is
multiplied with my

3946
01:18:48,350 --> 01:18:48,360
multiplied with my
 

3947
01:18:48,360 --> 01:18:51,270
multiplied with my
values. Um so we have key queries and

3948
01:18:51,270 --> 01:18:51,280
values. Um so we have key queries and
 

3949
01:18:51,280 --> 01:18:52,950
values. Um so we have key queries and
values right with my values. And that

3950
01:18:52,950 --> 01:18:52,960
values right with my values. And that
 

3951
01:18:52,960 --> 01:18:55,390
values right with my values. And that
gives me a context vector

3952
01:18:55,390 --> 01:18:55,400
gives me a context vector
 

3953
01:18:55,400 --> 01:18:58,070
gives me a context vector
matrix. And this context vector matrix

3954
01:18:58,070 --> 01:18:58,080
matrix. And this context vector matrix
 

3955
01:18:58,080 --> 01:19:00,709
matrix. And this context vector matrix
is of the size of 4x 768. And this is

3956
01:19:00,709 --> 01:19:00,719
is of the size of 4x 768. And this is
 

3957
01:19:00,719 --> 01:19:02,470
is of the size of 4x 768. And this is
the output of the multi head attention

3958
01:19:02,470 --> 01:19:02,480
the output of the multi head attention
 

3959
01:19:02,480 --> 01:19:05,030
the output of the multi head attention
which we have mentioned over here. I

3960
01:19:05,030 --> 01:19:05,040
which we have mentioned over here. I
 

3961
01:19:05,040 --> 01:19:07,189
which we have mentioned over here. I
know I went through this this module a

3962
01:19:07,189 --> 01:19:07,199
know I went through this this module a
 

3963
01:19:07,199 --> 01:19:09,750
know I went through this this module a
bit fast because usually I spend about

3964
01:19:09,750 --> 01:19:09,760
bit fast because usually I spend about
 

3965
01:19:09,760 --> 01:19:12,070
bit fast because usually I spend about
four to five hours explaining attention

3966
01:19:12,070 --> 01:19:12,080
four to five hours explaining attention
 

3967
01:19:12,080 --> 01:19:14,310
four to five hours explaining attention
mechanism itself but here the main

3968
01:19:14,310 --> 01:19:14,320
mechanism itself but here the main
 

3969
01:19:14,320 --> 01:19:16,229
mechanism itself but here the main
purpose is for you to see a broader

3970
01:19:16,229 --> 01:19:16,239
purpose is for you to see a broader
 

3971
01:19:16,239 --> 01:19:17,910
purpose is for you to see a broader
picture of how different blocks are

3972
01:19:17,910 --> 01:19:17,920
picture of how different blocks are
 

3973
01:19:17,920 --> 01:19:20,390
picture of how different blocks are
assembled together so that you don't

3974
01:19:20,390 --> 01:19:20,400
assembled together so that you don't
 

3975
01:19:20,400 --> 01:19:23,030
assembled together so that you don't
feel that this is something too complex

3976
01:19:23,030 --> 01:19:23,040
feel that this is something too complex
 

3977
01:19:23,040 --> 01:19:24,790
feel that this is something too complex
you should get a basic understanding of

3978
01:19:24,790 --> 01:19:24,800
you should get a basic understanding of
 

3979
01:19:24,800 --> 01:19:26,470
you should get a basic understanding of
this and later if you want you can dive

3980
01:19:26,470 --> 01:19:26,480
this and later if you want you can dive
 

3981
01:19:26,480 --> 01:19:28,550
this and later if you want you can dive
into the multiad attention mechanism a

3982
01:19:28,550 --> 01:19:28,560
into the multiad attention mechanism a
 

3983
01:19:28,560 --> 01:19:29,630
into the multiad attention mechanism a
bit further

3984
01:19:29,630 --> 01:19:29,640
bit further
 

3985
01:19:29,640 --> 01:19:32,470
bit further
also. So if you see what exactly happens

3986
01:19:32,470 --> 01:19:32,480
also. So if you see what exactly happens
 

3987
01:19:32,480 --> 01:19:34,550
also. So if you see what exactly happens
in the multi attention mechanism is all

3988
01:19:34,550 --> 01:19:34,560
in the multi attention mechanism is all
 

3989
01:19:34,560 --> 01:19:36,950
in the multi attention mechanism is all
these steps. We multiply with trainable

3990
01:19:36,950 --> 01:19:36,960
these steps. We multiply with trainable
 

3991
01:19:36,960 --> 01:19:39,510
these steps. We multiply with trainable
queries keys values. We get the query

3992
01:19:39,510 --> 01:19:39,520
queries keys values. We get the query
 

3993
01:19:39,520 --> 01:19:41,910
queries keys values. We get the query
matrix the key matrix the value matrix.

3994
01:19:41,910 --> 01:19:41,920
matrix the key matrix the value matrix.
 

3995
01:19:41,920 --> 01:19:43,669
matrix the key matrix the value matrix.
We multiply queries with the keys

3996
01:19:43,669 --> 01:19:43,679
We multiply queries with the keys
 

3997
01:19:43,679 --> 01:19:45,590
We multiply queries with the keys
transpose and we get the attention

3998
01:19:45,590 --> 01:19:45,600
transpose and we get the attention
 

3999
01:19:45,600 --> 01:19:47,510
transpose and we get the attention
scores. The attention scores are

4000
01:19:47,510 --> 01:19:47,520
scores. The attention scores are
 

4001
01:19:47,520 --> 01:19:49,430
scores. The attention scores are
normalized by square root of keys

4002
01:19:49,430 --> 01:19:49,440
normalized by square root of keys
 

4003
01:19:49,440 --> 01:19:51,910
normalized by square root of keys
dimensions and plus we apply softmax and

4004
01:19:51,910 --> 01:19:51,920
dimensions and plus we apply softmax and
 

4005
01:19:51,920 --> 01:19:53,830
dimensions and plus we apply softmax and
we get the attention weights. We

4006
01:19:53,830 --> 01:19:53,840
we get the attention weights. We
 

4007
01:19:53,840 --> 01:19:55,510
we get the attention weights. We
multiply with values and we get the

4008
01:19:55,510 --> 01:19:55,520
multiply with values and we get the
 

4009
01:19:55,520 --> 01:19:58,229
multiply with values and we get the
context vector matrix. One more thing to

4010
01:19:58,229 --> 01:19:58,239
context vector matrix. One more thing to
 

4011
01:19:58,239 --> 01:19:59,669
context vector matrix. One more thing to
note here is that there is something

4012
01:19:59,669 --> 01:19:59,679
note here is that there is something
 

4013
01:19:59,679 --> 01:20:01,830
note here is that there is something
called as causality here. Which means

4014
01:20:01,830 --> 01:20:01,840
called as causality here. Which means
 

4015
01:20:01,840 --> 01:20:05,030
called as causality here. Which means
when we look at these uh attention

4016
01:20:05,030 --> 01:20:05,040
when we look at these uh attention
 

4017
01:20:05,040 --> 01:20:07,830
when we look at these uh attention
scores, ideally every token should have

4018
01:20:07,830 --> 01:20:07,840
scores, ideally every token should have
 

4019
01:20:07,840 --> 01:20:10,390
scores, ideally every token should have
attention score between only that token

4020
01:20:10,390 --> 01:20:10,400
attention score between only that token
 

4021
01:20:10,400 --> 01:20:12,950
attention score between only that token
and the tokens which come before it. It

4022
01:20:12,950 --> 01:20:12,960
and the tokens which come before it. It
 

4023
01:20:12,960 --> 01:20:14,790
and the tokens which come before it. It
should not have attention scores between

4024
01:20:14,790 --> 01:20:14,800
should not have attention scores between
 

4025
01:20:14,800 --> 01:20:17,110
should not have attention scores between
the token and what comes after it. So

4026
01:20:17,110 --> 01:20:17,120
the token and what comes after it. So
 

4027
01:20:17,120 --> 01:20:19,270
the token and what comes after it. So
for example, if I look at day, we should

4028
01:20:19,270 --> 01:20:19,280
for example, if I look at day, we should
 

4029
01:20:19,280 --> 01:20:20,950
for example, if I look at day, we should
not have the attention score between day

4030
01:20:20,950 --> 01:20:20,960
not have the attention score between day
 

4031
01:20:20,960 --> 01:20:23,110
not have the attention score between day
and earth because comes after day,

4032
01:20:23,110 --> 01:20:23,120
and earth because comes after day,
 

4033
01:20:23,120 --> 01:20:25,350
and earth because comes after day,
right? And ideally we are predicting the

4034
01:20:25,350 --> 01:20:25,360
right? And ideally we are predicting the
 

4035
01:20:25,360 --> 01:20:26,870
right? And ideally we are predicting the
next token. So we should have no

4036
01:20:26,870 --> 01:20:26,880
next token. So we should have no
 

4037
01:20:26,880 --> 01:20:28,950
next token. So we should have no
information about what comes after the

4038
01:20:28,950 --> 01:20:28,960
information about what comes after the
 

4039
01:20:28,960 --> 01:20:31,350
information about what comes after the
next token. So essentially what is done

4040
01:20:31,350 --> 01:20:31,360
next token. So essentially what is done
 

4041
01:20:31,360 --> 01:20:33,590
next token. So essentially what is done
is that so if you look at one it should

4042
01:20:33,590 --> 01:20:33,600
is that so if you look at one it should
 

4043
01:20:33,600 --> 01:20:35,910
is that so if you look at one it should
not have information about these three.

4044
01:20:35,910 --> 01:20:35,920
not have information about these three.
 

4045
01:20:35,920 --> 01:20:37,510
not have information about these three.
If you look at day it should not have

4046
01:20:37,510 --> 01:20:37,520
If you look at day it should not have
 

4047
01:20:37,520 --> 01:20:39,669
If you look at day it should not have
information about these two. If you look

4048
01:20:39,669 --> 01:20:39,679
information about these two. If you look
 

4049
01:20:39,679 --> 01:20:41,350
information about these two. If you look
at O it should not have information

4050
01:20:41,350 --> 01:20:41,360
at O it should not have information
 

4051
01:20:41,360 --> 01:20:43,590
at O it should not have information
about this. So all the elements above

4052
01:20:43,590 --> 01:20:43,600
about this. So all the elements above
 

4053
01:20:43,600 --> 01:20:45,470
about this. So all the elements above
the diagonal are essentially put to

4054
01:20:45,470 --> 01:20:45,480
the diagonal are essentially put to
 

4055
01:20:45,480 --> 01:20:47,910
the diagonal are essentially put to
zero. And that is called as causal

4056
01:20:47,910 --> 01:20:47,920
zero. And that is called as causal
 

4057
01:20:47,920 --> 01:20:49,750
zero. And that is called as causal
attention.

4058
01:20:49,750 --> 01:20:49,760
attention.
 

4059
01:20:49,760 --> 01:20:51,430
attention.
So when you look at the code, you are

4060
01:20:51,430 --> 01:20:51,440
So when you look at the code, you are
 

4061
01:20:51,440 --> 01:20:53,750
So when you look at the code, you are
going to see all of these steps exactly

4062
01:20:53,750 --> 01:20:53,760
going to see all of these steps exactly
 

4063
01:20:53,760 --> 01:20:55,669
going to see all of these steps exactly
implemented the same way as I mentioned

4064
01:20:55,669 --> 01:20:55,679
implemented the same way as I mentioned
 

4065
01:20:55,679 --> 01:20:57,750
implemented the same way as I mentioned
over here and you will see the name

4066
01:20:57,750 --> 01:20:57,760
over here and you will see the name
 

4067
01:20:57,760 --> 01:20:59,990
over here and you will see the name
causal attention. But don't get scared

4068
01:20:59,990 --> 01:21:00,000
causal attention. But don't get scared
 

4069
01:21:00,000 --> 01:21:01,830
causal attention. But don't get scared
or intimidated by this name. It's

4070
01:21:01,830 --> 01:21:01,840
or intimidated by this name. It's
 

4071
01:21:01,840 --> 01:21:04,149
or intimidated by this name. It's
essentially because we cannot peak into

4072
01:21:04,149 --> 01:21:04,159
essentially because we cannot peak into
 

4073
01:21:04,159 --> 01:21:06,550
essentially because we cannot peak into
the future. For one given token, we can

4074
01:21:06,550 --> 01:21:06,560
the future. For one given token, we can
 

4075
01:21:06,560 --> 01:21:08,630
the future. For one given token, we can
only pay attention to that token or what

4076
01:21:08,630 --> 01:21:08,640
only pay attention to that token or what
 

4077
01:21:08,640 --> 01:21:11,350
only pay attention to that token or what
comes before it, not to the tokens which

4078
01:21:11,350 --> 01:21:11,360
comes before it, not to the tokens which
 

4079
01:21:11,360 --> 01:21:13,189
comes before it, not to the tokens which
come after it. So we put all the

4080
01:21:13,189 --> 01:21:13,199
come after it. So we put all the
 

4081
01:21:13,199 --> 01:21:15,030
come after it. So we put all the
elements above the diagonal to be equal

4082
01:21:15,030 --> 01:21:15,040
elements above the diagonal to be equal
 

4083
01:21:15,040 --> 01:21:17,510
elements above the diagonal to be equal
to zero. If you want more information

4084
01:21:17,510 --> 01:21:17,520
to zero. If you want more information
 

4085
01:21:17,520 --> 01:21:19,750
to zero. If you want more information
about the attention mechanism and if you

4086
01:21:19,750 --> 01:21:19,760
about the attention mechanism and if you
 

4087
01:21:19,760 --> 01:21:21,750
about the attention mechanism and if you
want to dive deeper, you can have a look

4088
01:21:21,750 --> 01:21:21,760
want to dive deeper, you can have a look
 

4089
01:21:21,760 --> 01:21:23,750
want to dive deeper, you can have a look
at the build LLM from scratch lecture

4090
01:21:23,750 --> 01:21:23,760
at the build LLM from scratch lecture
 

4091
01:21:23,760 --> 01:21:25,110
at the build LLM from scratch lecture
series which is a huge number of

4092
01:21:25,110 --> 01:21:25,120
series which is a huge number of
 

4093
01:21:25,120 --> 01:21:26,790
series which is a huge number of
lectures. I think I have 5 hours of

4094
01:21:26,790 --> 01:21:26,800
lectures. I think I have 5 hours of
 

4095
01:21:26,800 --> 01:21:28,830
lectures. I think I have 5 hours of
content on the attention

4096
01:21:28,830 --> 01:21:28,840
content on the attention
 

4097
01:21:28,840 --> 01:21:31,030
content on the attention
mechanism. Okay. So this is the second

4098
01:21:31,030 --> 01:21:31,040
mechanism. Okay. So this is the second
 

4099
01:21:31,040 --> 01:21:32,630
mechanism. Okay. So this is the second
block which is the multi head attention

4100
01:21:32,630 --> 01:21:32,640
block which is the multi head attention
 

4101
01:21:32,640 --> 01:21:34,390
block which is the multi head attention
and actually this is the most important

4102
01:21:34,390 --> 01:21:34,400
and actually this is the most important
 

4103
01:21:34,400 --> 01:21:36,790
and actually this is the most important
block of the transformer architecture.

4104
01:21:36,790 --> 01:21:36,800
block of the transformer architecture.
 

4105
01:21:36,800 --> 01:21:38,470
block of the transformer architecture.
So if you look at the code you'll see

4106
01:21:38,470 --> 01:21:38,480
So if you look at the code you'll see
 

4107
01:21:38,480 --> 01:21:40,390
So if you look at the code you'll see
that this is that block which is called

4108
01:21:40,390 --> 01:21:40,400
that this is that block which is called
 

4109
01:21:40,400 --> 01:21:42,790
that this is that block which is called
causal self attention and remember it is

4110
01:21:42,790 --> 01:21:42,800
causal self attention and remember it is
 

4111
01:21:42,800 --> 01:21:44,470
causal self attention and remember it is
called causal because we cannot peak

4112
01:21:44,470 --> 01:21:44,480
called causal because we cannot peak
 

4113
01:21:44,480 --> 01:21:46,390
called causal because we cannot peak
into the future and it's called self

4114
01:21:46,390 --> 01:21:46,400
into the future and it's called self
 

4115
01:21:46,400 --> 01:21:48,310
into the future and it's called self
attention because we are looking at a

4116
01:21:48,310 --> 01:21:48,320
attention because we are looking at a
 

4117
01:21:48,320 --> 01:21:50,709
attention because we are looking at a
given sentence. We are looking at a

4118
01:21:50,709 --> 01:21:50,719
given sentence. We are looking at a
 

4119
01:21:50,719 --> 01:21:52,629
given sentence. We are looking at a
given sentence and we are see computing

4120
01:21:52,629 --> 01:21:52,639
given sentence and we are see computing
 

4121
01:21:52,639 --> 01:21:54,709
given sentence and we are see computing
the attention between tokens of that

4122
01:21:54,709 --> 01:21:54,719
the attention between tokens of that
 

4123
01:21:54,719 --> 01:21:56,709
the attention between tokens of that
sentence with tokens of the same

4124
01:21:56,709 --> 01:21:56,719
sentence with tokens of the same
 

4125
01:21:56,719 --> 01:21:58,390
sentence with tokens of the same
sentence. That's why it's called self

4126
01:21:58,390 --> 01:21:58,400
sentence. That's why it's called self
 

4127
01:21:58,400 --> 01:22:01,270
sentence. That's why it's called self
attention. Okay. So now if you see what

4128
01:22:01,270 --> 01:22:01,280
attention. Okay. So now if you see what
 

4129
01:22:01,280 --> 01:22:04,629
attention. Okay. So now if you see what
is happening here. Um so here we have

4130
01:22:04,629 --> 01:22:04,639
is happening here. Um so here we have
 

4131
01:22:04,639 --> 01:22:07,550
is happening here. Um so here we have
the queries, keys and the values matrix.

4132
01:22:07,550 --> 01:22:07,560
the queries, keys and the values matrix.
 

4133
01:22:07,560 --> 01:22:10,070
the queries, keys and the values matrix.
Okay, the queries, keys and the values

4134
01:22:10,070 --> 01:22:10,080
Okay, the queries, keys and the values
 

4135
01:22:10,080 --> 01:22:11,830
Okay, the queries, keys and the values
and then we multiply the queries with

4136
01:22:11,830 --> 01:22:11,840
and then we multiply the queries with
 

4137
01:22:11,840 --> 01:22:13,910
and then we multiply the queries with
the keys transpose to get the attention

4138
01:22:13,910 --> 01:22:13,920
the keys transpose to get the attention
 

4139
01:22:13,920 --> 01:22:16,070
the keys transpose to get the attention
scores which has been shown over here.

4140
01:22:16,070 --> 01:22:16,080
scores which has been shown over here.
 

4141
01:22:16,080 --> 01:22:18,070
scores which has been shown over here.
The queries will be multiplied with the

4142
01:22:18,070 --> 01:22:18,080
The queries will be multiplied with the
 

4143
01:22:18,080 --> 01:22:19,990
The queries will be multiplied with the
keys transpose to get these attention

4144
01:22:19,990 --> 01:22:20,000
keys transpose to get these attention
 

4145
01:22:20,000 --> 01:22:22,310
keys transpose to get these attention
scores. So these are also called causal

4146
01:22:22,310 --> 01:22:22,320
scores. So these are also called causal
 

4147
01:22:22,320 --> 01:22:24,590
scores. So these are also called causal
attention

4148
01:22:24,590 --> 01:22:24,600
attention
 

4149
01:22:24,600 --> 01:22:27,590
attention
scores. Causal attention scores. So here

4150
01:22:27,590 --> 01:22:27,600
scores. Causal attention scores. So here
 

4151
01:22:27,600 --> 01:22:30,709
scores. Causal attention scores. So here
you can see uh here we get the causal

4152
01:22:30,709 --> 01:22:30,719
you can see uh here we get the causal
 

4153
01:22:30,719 --> 01:22:33,189
you can see uh here we get the causal
attention scores and then what we do

4154
01:22:33,189 --> 01:22:33,199
attention scores and then what we do
 

4155
01:22:33,199 --> 01:22:35,830
attention scores and then what we do
here you can see that the elements above

4156
01:22:35,830 --> 01:22:35,840
here you can see that the elements above
 

4157
01:22:35,840 --> 01:22:37,990
here you can see that the elements above
the diagonal are replaced with minus

4158
01:22:37,990 --> 01:22:38,000
the diagonal are replaced with minus
 

4159
01:22:38,000 --> 01:22:39,669
the diagonal are replaced with minus
infinity so that we can when we take

4160
01:22:39,669 --> 01:22:39,679
infinity so that we can when we take
 

4161
01:22:39,679 --> 01:22:42,310
infinity so that we can when we take
soft max they will be equal to zero. So

4162
01:22:42,310 --> 01:22:42,320
soft max they will be equal to zero. So
 

4163
01:22:42,320 --> 01:22:44,950
soft max they will be equal to zero. So
when we have the attention scores we

4164
01:22:44,950 --> 01:22:44,960
when we have the attention scores we
 

4165
01:22:44,960 --> 01:22:46,709
when we have the attention scores we
replace the elements above here with

4166
01:22:46,709 --> 01:22:46,719
replace the elements above here with
 

4167
01:22:46,719 --> 01:22:48,790
replace the elements above here with
negative infinity because when we take

4168
01:22:48,790 --> 01:22:48,800
negative infinity because when we take
 

4169
01:22:48,800 --> 01:22:51,030
negative infinity because when we take
soft max they'll be zero for the causal

4170
01:22:51,030 --> 01:22:51,040
soft max they'll be zero for the causal
 

4171
01:22:51,040 --> 01:22:53,270
soft max they'll be zero for the causal
attention purposes that gives us the

4172
01:22:53,270 --> 01:22:53,280
attention purposes that gives us the
 

4173
01:22:53,280 --> 01:22:55,030
attention purposes that gives us the
attention weights and remember here we

4174
01:22:55,030 --> 01:22:55,040
attention weights and remember here we
 

4175
01:22:55,040 --> 01:22:56,790
attention weights and remember here we
are also dividing by the square root of

4176
01:22:56,790 --> 01:22:56,800
are also dividing by the square root of
 

4177
01:22:56,800 --> 01:22:59,750
are also dividing by the square root of
keys dimension. So we get the attention

4178
01:22:59,750 --> 01:22:59,760
keys dimension. So we get the attention
 

4179
01:22:59,760 --> 01:23:02,070
keys dimension. So we get the attention
scores. We get the attent the mechanism

4180
01:23:02,070 --> 01:23:02,080
scores. We get the attent the mechanism
 

4181
01:23:02,080 --> 01:23:06,270
scores. We get the attent the mechanism
proceeds like this. We get the attention

4182
01:23:06,270 --> 01:23:06,280

 

4183
01:23:06,280 --> 01:23:08,790

scores. We get the attention scores. We

4184
01:23:08,790 --> 01:23:08,800
scores. We get the attention scores. We
 

4185
01:23:08,800 --> 01:23:10,550
scores. We get the attention scores. We
divide by the square root of keys

4186
01:23:10,550 --> 01:23:10,560
divide by the square root of keys
 

4187
01:23:10,560 --> 01:23:12,550
divide by the square root of keys
dimension. This is done because the

4188
01:23:12,550 --> 01:23:12,560
dimension. This is done because the
 

4189
01:23:12,560 --> 01:23:14,390
dimension. This is done because the
variance of the query is multiplied by

4190
01:23:14,390 --> 01:23:14,400
variance of the query is multiplied by
 

4191
01:23:14,400 --> 01:23:16,149
variance of the query is multiplied by
the keys. The dot product the variance

4192
01:23:16,149 --> 01:23:16,159
the keys. The dot product the variance
 

4193
01:23:16,159 --> 01:23:18,390
the keys. The dot product the variance
of that dot dot that dot product should

4194
01:23:18,390 --> 01:23:18,400
of that dot dot that dot product should
 

4195
01:23:18,400 --> 01:23:21,270
of that dot dot that dot product should
stay as low as possible. That's again to

4196
01:23:21,270 --> 01:23:21,280
stay as low as possible. That's again to
 

4197
01:23:21,280 --> 01:23:22,750
stay as low as possible. That's again to
stabilize the

4198
01:23:22,750 --> 01:23:22,760
stabilize the
 

4199
01:23:22,760 --> 01:23:25,350
stabilize the
training. So we scale by square root of

4200
01:23:25,350 --> 01:23:25,360
training. So we scale by square root of
 

4201
01:23:25,360 --> 01:23:27,750
training. So we scale by square root of
keys dimension. Then after that what we

4202
01:23:27,750 --> 01:23:27,760
keys dimension. Then after that what we
 

4203
01:23:27,760 --> 01:23:29,669
keys dimension. Then after that what we
do is that we fill the elements above

4204
01:23:29,669 --> 01:23:29,679
do is that we fill the elements above
 

4205
01:23:29,679 --> 01:23:30,830
do is that we fill the elements above
the

4206
01:23:30,830 --> 01:23:30,840
the
 

4207
01:23:30,840 --> 01:23:33,510
the
diagonal. We fill the elements above the

4208
01:23:33,510 --> 01:23:33,520
diagonal. We fill the elements above the
 

4209
01:23:33,520 --> 01:23:35,709
diagonal. We fill the elements above the
diagonal with negative

4210
01:23:35,709 --> 01:23:35,719
diagonal with negative
 

4211
01:23:35,719 --> 01:23:39,110
diagonal with negative
infinity and then we take the soft max.

4212
01:23:39,110 --> 01:23:39,120
infinity and then we take the soft max.
 

4213
01:23:39,120 --> 01:23:40,870
infinity and then we take the soft max.
What this will ensure is that it will do

4214
01:23:40,870 --> 01:23:40,880
What this will ensure is that it will do
 

4215
01:23:40,880 --> 01:23:42,950
What this will ensure is that it will do
two things. It will make sure that every

4216
01:23:42,950 --> 01:23:42,960
two things. It will make sure that every
 

4217
01:23:42,960 --> 01:23:44,870
two things. It will make sure that every
row of the attention score sums up to

4218
01:23:44,870 --> 01:23:44,880
row of the attention score sums up to
 

4219
01:23:44,880 --> 01:23:47,750
row of the attention score sums up to
one because we take soft max and since

4220
01:23:47,750 --> 01:23:47,760
one because we take soft max and since
 

4221
01:23:47,760 --> 01:23:49,510
one because we take soft max and since
we have negative infinity and when we

4222
01:23:49,510 --> 01:23:49,520
we have negative infinity and when we
 

4223
01:23:49,520 --> 01:23:51,350
we have negative infinity and when we
take that soft max all elements above

4224
01:23:51,350 --> 01:23:51,360
take that soft max all elements above
 

4225
01:23:51,360 --> 01:23:53,910
take that soft max all elements above
the diagonal will be zero.

4226
01:23:53,910 --> 01:23:53,920
the diagonal will be zero.
 

4227
01:23:53,920 --> 01:23:55,669
the diagonal will be zero.
So then the attention scores or the

4228
01:23:55,669 --> 01:23:55,679
So then the attention scores or the
 

4229
01:23:55,679 --> 01:23:57,110
So then the attention scores or the
attention weight matrix will be

4230
01:23:57,110 --> 01:23:57,120
attention weight matrix will be
 

4231
01:23:57,120 --> 01:23:58,590
attention weight matrix will be
something like this. All this will be

4232
01:23:58,590 --> 01:23:58,600
something like this. All this will be
 

4233
01:23:58,600 --> 01:24:02,229
something like this. All this will be
zero and whatever is remaining every row

4234
01:24:02,229 --> 01:24:02,239
zero and whatever is remaining every row
 

4235
01:24:02,239 --> 01:24:03,750
zero and whatever is remaining every row
of whatever is remaining will

4236
01:24:03,750 --> 01:24:03,760
of whatever is remaining will
 

4237
01:24:03,760 --> 01:24:06,709
of whatever is remaining will
essentially sum up to one. So then you

4238
01:24:06,709 --> 01:24:06,719
essentially sum up to one. So then you
 

4239
01:24:06,719 --> 01:24:08,470
essentially sum up to one. So then you
can make quantitative statements like

4240
01:24:08,470 --> 01:24:08,480
can make quantitative statements like
 

4241
01:24:08,480 --> 01:24:10,870
can make quantitative statements like
when this is my query how much attention

4242
01:24:10,870 --> 01:24:10,880
when this is my query how much attention
 

4243
01:24:10,880 --> 01:24:13,070
when this is my query how much attention
should I pay to each of the

4244
01:24:13,070 --> 01:24:13,080
should I pay to each of the
 

4245
01:24:13,080 --> 01:24:15,750
should I pay to each of the
tokens. So all of that is happening over

4246
01:24:15,750 --> 01:24:15,760
tokens. So all of that is happening over
 

4247
01:24:15,760 --> 01:24:17,990
tokens. So all of that is happening over
here. We first take the queries

4248
01:24:17,990 --> 01:24:18,000
here. We first take the queries
 

4249
01:24:18,000 --> 01:24:19,990
here. We first take the queries
multiplied by keys transpose divide by

4250
01:24:19,990 --> 01:24:20,000
multiplied by keys transpose divide by
 

4251
01:24:20,000 --> 01:24:22,550
multiplied by keys transpose divide by
square root of keys dimension. Fill the

4252
01:24:22,550 --> 01:24:22,560
square root of keys dimension. Fill the
 

4253
01:24:22,560 --> 01:24:23,990
square root of keys dimension. Fill the
elements above the diagonal with

4254
01:24:23,990 --> 01:24:24,000
elements above the diagonal with
 

4255
01:24:24,000 --> 01:24:27,350
elements above the diagonal with
negative infinity. Take soft max. Uh and

4256
01:24:27,350 --> 01:24:27,360
negative infinity. Take soft max. Uh and
 

4257
01:24:27,360 --> 01:24:29,189
negative infinity. Take soft max. Uh and
then there is also a layer of dropout

4258
01:24:29,189 --> 01:24:29,199
then there is also a layer of dropout
 

4259
01:24:29,199 --> 01:24:30,950
then there is also a layer of dropout
which we can apply over here. Again

4260
01:24:30,950 --> 01:24:30,960
which we can apply over here. Again
 

4261
01:24:30,960 --> 01:24:32,390
which we can apply over here. Again
dropout is applied to improve

4262
01:24:32,390 --> 01:24:32,400
dropout is applied to improve
 

4263
01:24:32,400 --> 01:24:34,629
dropout is applied to improve
generalization performance. And the

4264
01:24:34,629 --> 01:24:34,639
generalization performance. And the
 

4265
01:24:34,639 --> 01:24:36,629
generalization performance. And the
final step as I've mentioned over here

4266
01:24:36,629 --> 01:24:36,639
final step as I've mentioned over here
 

4267
01:24:36,639 --> 01:24:38,390
final step as I've mentioned over here
is that we take the attention weights

4268
01:24:38,390 --> 01:24:38,400
is that we take the attention weights
 

4269
01:24:38,400 --> 01:24:40,790
is that we take the attention weights
and we multiply it with the values

4270
01:24:40,790 --> 01:24:40,800
and we multiply it with the values
 

4271
01:24:40,800 --> 01:24:42,629
and we multiply it with the values
values matrix. The attention weights is

4272
01:24:42,629 --> 01:24:42,639
values matrix. The attention weights is
 

4273
01:24:42,639 --> 01:24:44,310
values matrix. The attention weights is
multiplied with the values matrix and we

4274
01:24:44,310 --> 01:24:44,320
multiplied with the values matrix and we
 

4275
01:24:44,320 --> 01:24:46,590
multiplied with the values matrix and we
get the context vector matrix. That's

4276
01:24:46,590 --> 01:24:46,600
get the context vector matrix. That's
 

4277
01:24:46,600 --> 01:24:48,189
get the context vector matrix. That's
it.

4278
01:24:48,189 --> 01:24:48,199
it.
 

4279
01:24:48,199 --> 01:24:51,189
it.
Um and then uh once we get the context

4280
01:24:51,189 --> 01:24:51,199
Um and then uh once we get the context
 

4281
01:24:51,199 --> 01:24:54,229
Um and then uh once we get the context
vector matrix then we can add one more

4282
01:24:54,229 --> 01:24:54,239
vector matrix then we can add one more
 

4283
01:24:54,239 --> 01:24:57,110
vector matrix then we can add one more
layer of dropout after that and we can

4284
01:24:57,110 --> 01:24:57,120
layer of dropout after that and we can
 

4285
01:24:57,120 --> 01:24:59,590
layer of dropout after that and we can
also add an output projection layer. So

4286
01:24:59,590 --> 01:24:59,600
also add an output projection layer. So
 

4287
01:24:59,600 --> 01:25:01,750
also add an output projection layer. So
a final neural network is added at the

4288
01:25:01,750 --> 01:25:01,760
a final neural network is added at the
 

4289
01:25:01,760 --> 01:25:03,830
a final neural network is added at the
output. We'll come to that in just a

4290
01:25:03,830 --> 01:25:03,840
output. We'll come to that in just a
 

4291
01:25:03,840 --> 01:25:06,950
output. We'll come to that in just a
moment. But for now just remember that

4292
01:25:06,950 --> 01:25:06,960
moment. But for now just remember that
 

4293
01:25:06,960 --> 01:25:09,510
moment. But for now just remember that
in the causal self attention block until

4294
01:25:09,510 --> 01:25:09,520
in the causal self attention block until
 

4295
01:25:09,520 --> 01:25:12,070
in the causal self attention block until
here we have the context vector matrix.

4296
01:25:12,070 --> 01:25:12,080
here we have the context vector matrix.
 

4297
01:25:12,080 --> 01:25:13,910
here we have the context vector matrix.
There is one more output projection

4298
01:25:13,910 --> 01:25:13,920
There is one more output projection
 

4299
01:25:13,920 --> 01:25:15,669
There is one more output projection
layer which is a small neural network is

4300
01:25:15,669 --> 01:25:15,679
layer which is a small neural network is
 

4301
01:25:15,679 --> 01:25:19,750
layer which is a small neural network is
added at the end of the so once we get

4302
01:25:19,750 --> 01:25:19,760
added at the end of the so once we get
 

4303
01:25:19,760 --> 01:25:21,910
added at the end of the so once we get
this context vector matrix a small

4304
01:25:21,910 --> 01:25:21,920
this context vector matrix a small
 

4305
01:25:21,920 --> 01:25:23,590
this context vector matrix a small
neural network is added which kind of

4306
01:25:23,590 --> 01:25:23,600
neural network is added which kind of
 

4307
01:25:23,600 --> 01:25:25,510
neural network is added which kind of
preserves the shape so it still remains

4308
01:25:25,510 --> 01:25:25,520
preserves the shape so it still remains
 

4309
01:25:25,520 --> 01:25:29,669
preserves the shape so it still remains
4x 768 and this is not usually required

4310
01:25:29,669 --> 01:25:29,679
4x 768 and this is not usually required
 

4311
01:25:29,679 --> 01:25:31,590
4x 768 and this is not usually required
but sometimes the neural network is

4312
01:25:31,590 --> 01:25:31,600
but sometimes the neural network is
 

4313
01:25:31,600 --> 01:25:33,910
but sometimes the neural network is
there so it's an optional neural network

4314
01:25:33,910 --> 01:25:33,920
there so it's an optional neural network
 

4315
01:25:33,920 --> 01:25:35,350
there so it's an optional neural network
which can be added as an output

4316
01:25:35,350 --> 01:25:35,360
which can be added as an output
 

4317
01:25:35,360 --> 01:25:37,590
which can be added as an output
projection layer that's what's mentioned

4318
01:25:37,590 --> 01:25:37,600
projection layer that's what's mentioned
 

4319
01:25:37,600 --> 01:25:39,110
projection layer that's what's mentioned
over here and then one more layer of

4320
01:25:39,110 --> 01:25:39,120
over here and then one more layer of
 

4321
01:25:39,120 --> 01:25:41,830
over here and then one more layer of
dropout is actually applied

4322
01:25:41,830 --> 01:25:41,840
dropout is actually applied
 

4323
01:25:41,840 --> 01:25:44,229
dropout is actually applied
So this is what happens in the causal

4324
01:25:44,229 --> 01:25:44,239
So this is what happens in the causal
 

4325
01:25:44,239 --> 01:25:45,910
So this is what happens in the causal
attention block. And I know that I have

4326
01:25:45,910 --> 01:25:45,920
attention block. And I know that I have
 

4327
01:25:45,920 --> 01:25:47,430
attention block. And I know that I have
gone a bit fast through the block

4328
01:25:47,430 --> 01:25:47,440
gone a bit fast through the block
 

4329
01:25:47,440 --> 01:25:49,750
gone a bit fast through the block
through this block. But if you have

4330
01:25:49,750 --> 01:25:49,760
through this block. But if you have
 

4331
01:25:49,760 --> 01:25:51,510
through this block. But if you have
understood this much, it means you have

4332
01:25:51,510 --> 01:25:51,520
understood this much, it means you have
 

4333
01:25:51,520 --> 01:25:53,350
understood this much, it means you have
understood the basics of the attention

4334
01:25:53,350 --> 01:25:53,360
understood the basics of the attention
 

4335
01:25:53,360 --> 01:25:55,510
understood the basics of the attention
mechanism. Remember what the attention

4336
01:25:55,510 --> 01:25:55,520
mechanism. Remember what the attention
 

4337
01:25:55,520 --> 01:25:57,110
mechanism. Remember what the attention
mechanism is trying to do is that we

4338
01:25:57,110 --> 01:25:57,120
mechanism is trying to do is that we
 

4339
01:25:57,120 --> 01:25:58,790
mechanism is trying to do is that we
take the input embedding matrix and

4340
01:25:58,790 --> 01:25:58,800
take the input embedding matrix and
 

4341
01:25:58,800 --> 01:26:00,709
take the input embedding matrix and
convert it into a context vector matrix.

4342
01:26:00,709 --> 01:26:00,719
convert it into a context vector matrix.
 

4343
01:26:00,719 --> 01:26:03,030
convert it into a context vector matrix.
That's it. And these trainable weight

4344
01:26:03,030 --> 01:26:03,040
That's it. And these trainable weight
 

4345
01:26:03,040 --> 01:26:06,470
That's it. And these trainable weight
matrices are uh devised or we have added

4346
01:26:06,470 --> 01:26:06,480
matrices are uh devised or we have added
 

4347
01:26:06,480 --> 01:26:08,229
matrices are uh devised or we have added
them because we could not come up with a

4348
01:26:08,229 --> 01:26:08,239
them because we could not come up with a
 

4349
01:26:08,239 --> 01:26:09,990
them because we could not come up with a
closed form expression for how to

4350
01:26:09,990 --> 01:26:10,000
closed form expression for how to
 

4351
01:26:10,000 --> 01:26:12,229
closed form expression for how to
capture context. So we offload that to

4352
01:26:12,229 --> 01:26:12,239
capture context. So we offload that to
 

4353
01:26:12,239 --> 01:26:13,990
capture context. So we offload that to
the trainable weight matrices and we

4354
01:26:13,990 --> 01:26:14,000
the trainable weight matrices and we
 

4355
01:26:14,000 --> 01:26:16,189
the trainable weight matrices and we
hope that they will do the job for

4356
01:26:16,189 --> 01:26:16,199
hope that they will do the job for
 

4357
01:26:16,199 --> 01:26:19,830
hope that they will do the job for
us. Okay. So this is the mask multi head

4358
01:26:19,830 --> 01:26:19,840
us. Okay. So this is the mask multi head
 

4359
01:26:19,840 --> 01:26:22,070
us. Okay. So this is the mask multi head
attention until this point we get the

4360
01:26:22,070 --> 01:26:22,080
attention until this point we get the
 

4361
01:26:22,080 --> 01:26:24,709
attention until this point we get the
context vector for every single token.

4362
01:26:24,709 --> 01:26:24,719
context vector for every single token.
 

4363
01:26:24,719 --> 01:26:27,669
context vector for every single token.
Then you see in the transformer block if

4364
01:26:27,669 --> 01:26:27,679
Then you see in the transformer block if
 

4365
01:26:27,679 --> 01:26:29,590
Then you see in the transformer block if
let me rub this one so that we have a

4366
01:26:29,590 --> 01:26:29,600
let me rub this one so that we have a
 

4367
01:26:29,600 --> 01:26:32,669
let me rub this one so that we have a
clearer view of what all steps are

4368
01:26:32,669 --> 01:26:32,679
clearer view of what all steps are
 

4369
01:26:32,679 --> 01:26:35,189
clearer view of what all steps are
happening. Until now what all have we

4370
01:26:35,189 --> 01:26:35,199
happening. Until now what all have we
 

4371
01:26:35,199 --> 01:26:36,709
happening. Until now what all have we
covered in the transformer block? We

4372
01:26:36,709 --> 01:26:36,719
covered in the transformer block? We
 

4373
01:26:36,719 --> 01:26:38,470
covered in the transformer block? We
have covered the layer normalization and

4374
01:26:38,470 --> 01:26:38,480
have covered the layer normalization and
 

4375
01:26:38,480 --> 01:26:40,950
have covered the layer normalization and
multi head attention. Now after this

4376
01:26:40,950 --> 01:26:40,960
multi head attention. Now after this
 

4377
01:26:40,960 --> 01:26:42,390
multi head attention. Now after this
point there is one more layer of

4378
01:26:42,390 --> 01:26:42,400
point there is one more layer of
 

4379
01:26:42,400 --> 01:26:44,270
point there is one more layer of
dropout. There is one more layer of

4380
01:26:44,270 --> 01:26:44,280
dropout. There is one more layer of
 

4381
01:26:44,280 --> 01:26:47,189
dropout. There is one more layer of
dropout. So we randomly mask out some

4382
01:26:47,189 --> 01:26:47,199
dropout. So we randomly mask out some
 

4383
01:26:47,199 --> 01:26:48,950
dropout. So we randomly mask out some
elements to zero again to improve

4384
01:26:48,950 --> 01:26:48,960
elements to zero again to improve
 

4385
01:26:48,960 --> 01:26:51,510
elements to zero again to improve
generalization performance. So that is

4386
01:26:51,510 --> 01:26:51,520
generalization performance. So that is
 

4387
01:26:51,520 --> 01:26:54,550
generalization performance. So that is
this dropout layer. And then you'll see

4388
01:26:54,550 --> 01:26:54,560
this dropout layer. And then you'll see
 

4389
01:26:54,560 --> 01:26:56,790
this dropout layer. And then you'll see
this symbol over here, right? That

4390
01:26:56,790 --> 01:26:56,800
this symbol over here, right? That
 

4391
01:26:56,800 --> 01:26:58,550
this symbol over here, right? That
symbol is a shortcut connection or a

4392
01:26:58,550 --> 01:26:58,560
symbol is a shortcut connection or a
 

4393
01:26:58,560 --> 01:27:00,790
symbol is a shortcut connection or a
skip connection. And the reason we have

4394
01:27:00,790 --> 01:27:00,800
skip connection. And the reason we have
 

4395
01:27:00,800 --> 01:27:02,790
skip connection. And the reason we have
shortcut connections is because we give

4396
01:27:02,790 --> 01:27:02,800
shortcut connections is because we give
 

4397
01:27:02,800 --> 01:27:04,550
shortcut connections is because we give
another alternative path for the

4398
01:27:04,550 --> 01:27:04,560
another alternative path for the
 

4399
01:27:04,560 --> 01:27:07,189
another alternative path for the
gradient to flow. Shortcut connections

4400
01:27:07,189 --> 01:27:07,199
gradient to flow. Shortcut connections
 

4401
01:27:07,199 --> 01:27:08,790
gradient to flow. Shortcut connections
are added to prevent the vanishing

4402
01:27:08,790 --> 01:27:08,800
are added to prevent the vanishing
 

4403
01:27:08,800 --> 01:27:10,550
are added to prevent the vanishing
gradient problem when multiple layers

4404
01:27:10,550 --> 01:27:10,560
gradient problem when multiple layers
 

4405
01:27:10,560 --> 01:27:11,870
gradient problem when multiple layers
are chained

4406
01:27:11,870 --> 01:27:11,880
are chained
 

4407
01:27:11,880 --> 01:27:14,310
are chained
together. So whenever we have shortcut

4408
01:27:14,310 --> 01:27:14,320
together. So whenever we have shortcut
 

4409
01:27:14,320 --> 01:27:15,830
together. So whenever we have shortcut
connection here, it means that there is

4410
01:27:15,830 --> 01:27:15,840
connection here, it means that there is
 

4411
01:27:15,840 --> 01:27:18,310
connection here, it means that there is
a path which has been created. So if you

4412
01:27:18,310 --> 01:27:18,320
a path which has been created. So if you
 

4413
01:27:18,320 --> 01:27:21,189
a path which has been created. So if you
see now yeah whenever you have this

4414
01:27:21,189 --> 01:27:21,199
see now yeah whenever you have this
 

4415
01:27:21,199 --> 01:27:23,189
see now yeah whenever you have this
shortcut connection it means that there

4416
01:27:23,189 --> 01:27:23,199
shortcut connection it means that there
 

4417
01:27:23,199 --> 01:27:26,830
shortcut connection it means that there
is a shortcut between uh this path and

4418
01:27:26,830 --> 01:27:26,840
is a shortcut between uh this path and
 

4419
01:27:26,840 --> 01:27:29,830
is a shortcut between uh this path and
this whenever you see there is the

4420
01:27:29,830 --> 01:27:29,840
this whenever you see there is the
 

4421
01:27:29,840 --> 01:27:31,350
this whenever you see there is the
shortcut connection there is a shortcut

4422
01:27:31,350 --> 01:27:31,360
shortcut connection there is a shortcut
 

4423
01:27:31,360 --> 01:27:32,870
shortcut connection there is a shortcut
between this path and this which means

4424
01:27:32,870 --> 01:27:32,880
between this path and this which means
 

4425
01:27:32,880 --> 01:27:34,870
between this path and this which means
the gradient can also flow flow from

4426
01:27:34,870 --> 01:27:34,880
the gradient can also flow flow from
 

4427
01:27:34,880 --> 01:27:37,030
the gradient can also flow flow from
here. What it means that there is a

4428
01:27:37,030 --> 01:27:37,040
here. What it means that there is a
 

4429
01:27:37,040 --> 01:27:39,590
here. What it means that there is a
shortcut it means that the input of this

4430
01:27:39,590 --> 01:27:39,600
shortcut it means that the input of this
 

4431
01:27:39,600 --> 01:27:42,229
shortcut it means that the input of this
layer is added to the output here. So

4432
01:27:42,229 --> 01:27:42,239
layer is added to the output here. So
 

4433
01:27:42,239 --> 01:27:44,149
layer is added to the output here. So
this input is added to this output and

4434
01:27:44,149 --> 01:27:44,159
this input is added to this output and
 

4435
01:27:44,159 --> 01:27:45,910
this input is added to this output and
that serves as the input to the next

4436
01:27:45,910 --> 01:27:45,920
that serves as the input to the next
 

4437
01:27:45,920 --> 01:27:48,310
that serves as the input to the next
layer. Similarly, what happens here is

4438
01:27:48,310 --> 01:27:48,320
layer. Similarly, what happens here is
 

4439
01:27:48,320 --> 01:27:50,550
layer. Similarly, what happens here is
that this input is added to this output

4440
01:27:50,550 --> 01:27:50,560
that this input is added to this output
 

4441
01:27:50,560 --> 01:27:52,950
that this input is added to this output
and that's the plus summation over here.

4442
01:27:52,950 --> 01:27:52,960
and that's the plus summation over here.
 

4443
01:27:52,960 --> 01:27:54,990
and that's the plus summation over here.
So, a simple operation like this

4444
01:27:54,990 --> 01:27:55,000
So, a simple operation like this
 

4445
01:27:55,000 --> 01:27:57,590
So, a simple operation like this
prevents the vanishing gradient problem

4446
01:27:57,590 --> 01:27:57,600
prevents the vanishing gradient problem
 

4447
01:27:57,600 --> 01:27:59,430
prevents the vanishing gradient problem
by giving the gradient an alternative

4448
01:27:59,430 --> 01:27:59,440
by giving the gradient an alternative
 

4449
01:27:59,440 --> 01:28:01,830
by giving the gradient an alternative
path to flow. You can think of this as

4450
01:28:01,830 --> 01:28:01,840
path to flow. You can think of this as
 

4451
01:28:01,840 --> 01:28:03,950
path to flow. You can think of this as
similar to the ResNet

4452
01:28:03,950 --> 01:28:03,960
similar to the ResNet
 

4453
01:28:03,960 --> 01:28:06,229
similar to the ResNet
architecture. Okay. So, there is a there

4454
01:28:06,229 --> 01:28:06,239
architecture. Okay. So, there is a there
 

4455
01:28:06,239 --> 01:28:08,229
architecture. Okay. So, there is a there
is a shortcut connection here and there

4456
01:28:08,229 --> 01:28:08,239
is a shortcut connection here and there
 

4457
01:28:08,239 --> 01:28:09,910
is a shortcut connection here and there
is also a shortcut connection at the end

4458
01:28:09,910 --> 01:28:09,920
is also a shortcut connection at the end
 

4459
01:28:09,920 --> 01:28:12,149
is also a shortcut connection at the end
of the transformer block. There are two

4460
01:28:12,149 --> 01:28:12,159
of the transformer block. There are two
 

4461
01:28:12,159 --> 01:28:13,669
of the transformer block. There are two
shortcut connections. There is one

4462
01:28:13,669 --> 01:28:13,679
shortcut connections. There is one
 

4463
01:28:13,679 --> 01:28:15,350
shortcut connections. There is one
shortcut connection here and there is

4464
01:28:15,350 --> 01:28:15,360
shortcut connection here and there is
 

4465
01:28:15,360 --> 01:28:17,910
shortcut connection here and there is
one shortcut connection here. Okay.

4466
01:28:17,910 --> 01:28:17,920
one shortcut connection here. Okay.
 

4467
01:28:17,920 --> 01:28:19,990
one shortcut connection here. Okay.
After this first shortcut connection, we

4468
01:28:19,990 --> 01:28:20,000
After this first shortcut connection, we
 

4469
01:28:20,000 --> 01:28:22,709
After this first shortcut connection, we
have another layer of After this first

4470
01:28:22,709 --> 01:28:22,719
have another layer of After this first
 

4471
01:28:22,719 --> 01:28:24,950
have another layer of After this first
shortcut connection, we have another

4472
01:28:24,950 --> 01:28:24,960
shortcut connection, we have another
 

4473
01:28:24,960 --> 01:28:27,270
shortcut connection, we have another
layer normalization. So then again we

4474
01:28:27,270 --> 01:28:27,280
layer normalization. So then again we
 

4475
01:28:27,280 --> 01:28:29,110
layer normalization. So then again we
ensure that for the context vectors

4476
01:28:29,110 --> 01:28:29,120
ensure that for the context vectors
 

4477
01:28:29,120 --> 01:28:31,350
ensure that for the context vectors
which we have the mean is equal to zero

4478
01:28:31,350 --> 01:28:31,360
which we have the mean is equal to zero
 

4479
01:28:31,360 --> 01:28:33,270
which we have the mean is equal to zero
and the variance is equal to one. That

4480
01:28:33,270 --> 01:28:33,280
and the variance is equal to one. That
 

4481
01:28:33,280 --> 01:28:35,590
and the variance is equal to one. That
is very important that we apply layer

4482
01:28:35,590 --> 01:28:35,600
is very important that we apply layer
 

4483
01:28:35,600 --> 01:28:38,550
is very important that we apply layer
normalization once more. And after layer

4484
01:28:38,550 --> 01:28:38,560
normalization once more. And after layer
 

4485
01:28:38,560 --> 01:28:40,470
normalization once more. And after layer
normalization is applied, there is one

4486
01:28:40,470 --> 01:28:40,480
normalization is applied, there is one
 

4487
01:28:40,480 --> 01:28:42,310
normalization is applied, there is one
more crucial layer and that's the feed

4488
01:28:42,310 --> 01:28:42,320
more crucial layer and that's the feed
 

4489
01:28:42,320 --> 01:28:44,629
more crucial layer and that's the feed
forward neural network. Now something

4490
01:28:44,629 --> 01:28:44,639
forward neural network. Now something
 

4491
01:28:44,639 --> 01:28:46,629
forward neural network. Now something
very interesting happens in this feed

4492
01:28:46,629 --> 01:28:46,639
very interesting happens in this feed
 

4493
01:28:46,639 --> 01:28:49,430
very interesting happens in this feed
forward neural network. This feed

4494
01:28:49,430 --> 01:28:49,440
forward neural network. This feed
 

4495
01:28:49,440 --> 01:28:51,030
forward neural network. This feed
forward neural network actually looks

4496
01:28:51,030 --> 01:28:51,040
forward neural network actually looks
 

4497
01:28:51,040 --> 01:28:53,030
forward neural network actually looks
something like this. So let's say we

4498
01:28:53,030 --> 01:28:53,040
something like this. So let's say we
 

4499
01:28:53,040 --> 01:28:54,790
something like this. So let's say we
focus on one input. Right? Now it has

4500
01:28:54,790 --> 01:28:54,800
focus on one input. Right? Now it has
 

4501
01:28:54,800 --> 01:28:56,750
focus on one input. Right? Now it has
768 dimensions.

4502
01:28:56,750 --> 01:28:56,760
768 dimensions.
 

4503
01:28:56,760 --> 01:28:59,189
768 dimensions.
Right? This input will go through my

4504
01:28:59,189 --> 01:28:59,199
Right? This input will go through my
 

4505
01:28:59,199 --> 01:29:01,270
Right? This input will go through my
feed forward neural network. So I have

4506
01:29:01,270 --> 01:29:01,280
feed forward neural network. So I have
 

4507
01:29:01,280 --> 01:29:03,990
feed forward neural network. So I have
an input with 768 dimensions. The hidden

4508
01:29:03,990 --> 01:29:04,000
an input with 768 dimensions. The hidden
 

4509
01:29:04,000 --> 01:29:08,790
an input with 768 dimensions. The hidden
layer has dimensions 4 * 768 and that I

4510
01:29:08,790 --> 01:29:08,800
layer has dimensions 4 * 768 and that I
 

4511
01:29:08,800 --> 01:29:11,149
layer has dimensions 4 * 768 and that I
think is

4512
01:29:11,149 --> 01:29:11,159
think is
 

4513
01:29:11,159 --> 01:29:15,990
think is
uh uh 302 3072. So the hidden layer has

4514
01:29:15,990 --> 01:29:16,000
uh uh 302 3072. So the hidden layer has
 

4515
01:29:16,000 --> 01:29:19,189
uh uh 302 3072. So the hidden layer has
3072 dimensions and my output layer

4516
01:29:19,189 --> 01:29:19,199
3072 dimensions and my output layer
 

4517
01:29:19,199 --> 01:29:22,149
3072 dimensions and my output layer
again has 768 dimensions. So I call this

4518
01:29:22,149 --> 01:29:22,159
again has 768 dimensions. So I call this
 

4519
01:29:22,159 --> 01:29:24,550
again has 768 dimensions. So I call this
an expansion compression neural network

4520
01:29:24,550 --> 01:29:24,560
an expansion compression neural network
 

4521
01:29:24,560 --> 01:29:26,390
an expansion compression neural network
where the neural network retains the

4522
01:29:26,390 --> 01:29:26,400
where the neural network retains the
 

4523
01:29:26,400 --> 01:29:28,550
where the neural network retains the
size of the input. So every input which

4524
01:29:28,550 --> 01:29:28,560
size of the input. So every input which
 

4525
01:29:28,560 --> 01:29:30,790
size of the input. So every input which
passes through the neural network when

4526
01:29:30,790 --> 01:29:30,800
passes through the neural network when
 

4527
01:29:30,800 --> 01:29:32,310
passes through the neural network when
it comes through the output it will have

4528
01:29:32,310 --> 01:29:32,320
it comes through the output it will have
 

4529
01:29:32,320 --> 01:29:36,470
it comes through the output it will have
the same size 768. But remember that due

4530
01:29:36,470 --> 01:29:36,480
the same size 768. But remember that due
 

4531
01:29:36,480 --> 01:29:38,709
the same size 768. But remember that due
to these expansion and contraction and

4532
01:29:38,709 --> 01:29:38,719
to these expansion and contraction and
 

4533
01:29:38,719 --> 01:29:40,390
to these expansion and contraction and
due to this additional trainable

4534
01:29:40,390 --> 01:29:40,400
due to this additional trainable
 

4535
01:29:40,400 --> 01:29:42,790
due to this additional trainable
parameters it allows the language model

4536
01:29:42,790 --> 01:29:42,800
parameters it allows the language model
 

4537
01:29:42,800 --> 01:29:45,270
parameters it allows the language model
to explore a much richer space. That's

4538
01:29:45,270 --> 01:29:45,280
to explore a much richer space. That's
 

4539
01:29:45,280 --> 01:29:46,950
to explore a much richer space. That's
what's done frequently in deep learning.

4540
01:29:46,950 --> 01:29:46,960
what's done frequently in deep learning.
 

4541
01:29:46,960 --> 01:29:48,550
what's done frequently in deep learning.
Right? If something does not work in a

4542
01:29:48,550 --> 01:29:48,560
Right? If something does not work in a
 

4543
01:29:48,560 --> 01:29:50,629
Right? If something does not work in a
low dimensional space, you project

4544
01:29:50,629 --> 01:29:50,639
low dimensional space, you project
 

4545
01:29:50,639 --> 01:29:52,390
low dimensional space, you project
everything to a higher dimensional space

4546
01:29:52,390 --> 01:29:52,400
everything to a higher dimensional space
 

4547
01:29:52,400 --> 01:29:54,390
everything to a higher dimensional space
so that you can capture additional

4548
01:29:54,390 --> 01:29:54,400
so that you can capture additional
 

4549
01:29:54,400 --> 01:29:56,270
so that you can capture additional
insights. you can capture more

4550
01:29:56,270 --> 01:29:56,280
insights. you can capture more
 

4551
01:29:56,280 --> 01:29:58,390
insights. you can capture more
nonlinearities. Your function can learn

4552
01:29:58,390 --> 01:29:58,400
nonlinearities. Your function can learn
 

4553
01:29:58,400 --> 01:30:00,790
nonlinearities. Your function can learn
new things. So it turns out that without

4554
01:30:00,790 --> 01:30:00,800
new things. So it turns out that without
 

4555
01:30:00,800 --> 01:30:03,030
new things. So it turns out that without
this neural network, the language model

4556
01:30:03,030 --> 01:30:03,040
this neural network, the language model
 

4557
01:30:03,040 --> 01:30:05,510
this neural network, the language model
does not learn about the context

4558
01:30:05,510 --> 01:30:05,520
does not learn about the context
 

4559
01:30:05,520 --> 01:30:07,350
does not learn about the context
patterns in the underlying data and it

4560
01:30:07,350 --> 01:30:07,360
patterns in the underlying data and it
 

4561
01:30:07,360 --> 01:30:09,590
patterns in the underlying data and it
cannot answer queries that well. But

4562
01:30:09,590 --> 01:30:09,600
cannot answer queries that well. But
 

4563
01:30:09,600 --> 01:30:11,110
cannot answer queries that well. But
addition of this neural network

4564
01:30:11,110 --> 01:30:11,120
addition of this neural network
 

4565
01:30:11,120 --> 01:30:12,790
addition of this neural network
completely changes the performance of

4566
01:30:12,790 --> 01:30:12,800
completely changes the performance of
 

4567
01:30:12,800 --> 01:30:15,110
completely changes the performance of
the language model and that's why this

4568
01:30:15,110 --> 01:30:15,120
the language model and that's why this
 

4569
01:30:15,120 --> 01:30:16,950
the language model and that's why this
neural network is introduced at this

4570
01:30:16,950 --> 01:30:16,960
neural network is introduced at this
 

4571
01:30:16,960 --> 01:30:19,910
neural network is introduced at this
stage of uh uh this stage of the

4572
01:30:19,910 --> 01:30:19,920
stage of uh uh this stage of the
 

4573
01:30:19,920 --> 01:30:24,390
stage of uh uh this stage of the
transformer block where uh the input. So

4574
01:30:24,390 --> 01:30:24,400
transformer block where uh the input. So
 

4575
01:30:24,400 --> 01:30:28,070
transformer block where uh the input. So
now we have four rows right? So 4x 768

4576
01:30:28,070 --> 01:30:28,080
now we have four rows right? So 4x 768
 

4577
01:30:28,080 --> 01:30:29,510
now we have four rows right? So 4x 768
when it passes through the neural

4578
01:30:29,510 --> 01:30:29,520
when it passes through the neural
 

4579
01:30:29,520 --> 01:30:31,189
when it passes through the neural
network and when it comes out it's again

4580
01:30:31,189 --> 01:30:31,199
network and when it comes out it's again
 

4581
01:30:31,199 --> 01:30:34,390
network and when it comes out it's again
4x 768 but through this process we make

4582
01:30:34,390 --> 01:30:34,400
4x 768 but through this process we make
 

4583
01:30:34,400 --> 01:30:37,510
4x 768 but through this process we make
the uh additional we have additional

4584
01:30:37,510 --> 01:30:37,520
the uh additional we have additional
 

4585
01:30:37,520 --> 01:30:39,189
the uh additional we have additional
parameters so that the language model

4586
01:30:39,189 --> 01:30:39,199
parameters so that the language model
 

4587
01:30:39,199 --> 01:30:42,070
parameters so that the language model
can explore a much richer space. Now one

4588
01:30:42,070 --> 01:30:42,080
can explore a much richer space. Now one
 

4589
01:30:42,080 --> 01:30:44,310
can explore a much richer space. Now one
more thing to notice is that usually the

4590
01:30:44,310 --> 01:30:44,320
more thing to notice is that usually the
 

4591
01:30:44,320 --> 01:30:45,910
more thing to notice is that usually the
activation functions which we have in

4592
01:30:45,910 --> 01:30:45,920
activation functions which we have in
 

4593
01:30:45,920 --> 01:30:49,669
activation functions which we have in
neural networks are ru tanh etc. But the

4594
01:30:49,669 --> 01:30:49,679
neural networks are ru tanh etc. But the
 

4595
01:30:49,679 --> 01:30:51,950
neural networks are ru tanh etc. But the
activation function here is

4596
01:30:51,950 --> 01:30:51,960
activation function here is
 

4597
01:30:51,960 --> 01:30:54,629
activation function here is
JLU and that's a different function than

4598
01:30:54,629 --> 01:30:54,639
JLU and that's a different function than
 

4599
01:30:54,639 --> 01:30:57,110
JLU and that's a different function than
RLU because the RLU is this right? It's

4600
01:30:57,110 --> 01:30:57,120
RLU because the RLU is this right? It's
 

4601
01:30:57,120 --> 01:30:59,990
RLU because the RLU is this right? It's
it looks something like this. But a JU

4602
01:30:59,990 --> 01:31:00,000
it looks something like this. But a JU
 

4603
01:31:00,000 --> 01:31:01,950
it looks something like this. But a JU
actually looks something

4604
01:31:01,950 --> 01:31:01,960
actually looks something
 

4605
01:31:01,960 --> 01:31:05,629
actually looks something
like it looks something like

4606
01:31:05,629 --> 01:31:05,639
like it looks something like
 

4607
01:31:05,639 --> 01:31:09,510
like it looks something like
this. So at higher X's when X becomes

4608
01:31:09,510 --> 01:31:09,520
this. So at higher X's when X becomes
 

4609
01:31:09,520 --> 01:31:12,470
this. So at higher X's when X becomes
high it becomes very similar to Y= X. So

4610
01:31:12,470 --> 01:31:12,480
high it becomes very similar to Y= X. So
 

4611
01:31:12,480 --> 01:31:15,350
high it becomes very similar to Y= X. So
it approximates ALOU but at negative XS

4612
01:31:15,350 --> 01:31:15,360
it approximates ALOU but at negative XS
 

4613
01:31:15,360 --> 01:31:17,270
it approximates ALOU but at negative XS
it does not directly become zero. But

4614
01:31:17,270 --> 01:31:17,280
it does not directly become zero. But
 

4615
01:31:17,280 --> 01:31:19,110
it does not directly become zero. But
it's smooth. It's differentiable at x

4616
01:31:19,110 --> 01:31:19,120
it's smooth. It's differentiable at x
 

4617
01:31:19,120 --> 01:31:21,750
it's smooth. It's differentiable at x
equal to0 and it slowly becomes zero. So

4618
01:31:21,750 --> 01:31:21,760
equal to0 and it slowly becomes zero. So
 

4619
01:31:21,760 --> 01:31:24,030
equal to0 and it slowly becomes zero. So
generally this has shown to have good

4620
01:31:24,030 --> 01:31:24,040
generally this has shown to have good
 

4621
01:31:24,040 --> 01:31:26,390
generally this has shown to have good
results. Again this is an experimental

4622
01:31:26,390 --> 01:31:26,400
results. Again this is an experimental
 

4623
01:31:26,400 --> 01:31:28,229
results. Again this is an experimental
choice. There is no real need to only

4624
01:31:28,229 --> 01:31:28,239
choice. There is no real need to only
 

4625
01:31:28,239 --> 01:31:31,270
choice. There is no real need to only
stick with this. But as a hyperparameter

4626
01:31:31,270 --> 01:31:31,280
stick with this. But as a hyperparameter
 

4627
01:31:31,280 --> 01:31:33,270
stick with this. But as a hyperparameter
researchers have got very good results

4628
01:31:33,270 --> 01:31:33,280
researchers have got very good results
 

4629
01:31:33,280 --> 01:31:35,110
researchers have got very good results
with language modeling if they use the

4630
01:31:35,110 --> 01:31:35,120
with language modeling if they use the
 

4631
01:31:35,120 --> 01:31:38,709
with language modeling if they use the
JLU activation function. Okay. So that's

4632
01:31:38,709 --> 01:31:38,719
JLU activation function. Okay. So that's
 

4633
01:31:38,719 --> 01:31:40,550
JLU activation function. Okay. So that's
the activation function used in the feed

4634
01:31:40,550 --> 01:31:40,560
the activation function used in the feed
 

4635
01:31:40,560 --> 01:31:42,790
the activation function used in the feed
forward neural network. And once we come

4636
01:31:42,790 --> 01:31:42,800
forward neural network. And once we come
 

4637
01:31:42,800 --> 01:31:44,950
forward neural network. And once we come
outside the feed forward neural network,

4638
01:31:44,950 --> 01:31:44,960
outside the feed forward neural network,
 

4639
01:31:44,960 --> 01:31:47,430
outside the feed forward neural network,
there is another dropout layer over

4640
01:31:47,430 --> 01:31:47,440
there is another dropout layer over
 

4641
01:31:47,440 --> 01:31:49,590
there is another dropout layer over
here. So then the output of the feed

4642
01:31:49,590 --> 01:31:49,600
here. So then the output of the feed
 

4643
01:31:49,600 --> 01:31:51,189
here. So then the output of the feed
forward neural network, some of these

4644
01:31:51,189 --> 01:31:51,199
forward neural network, some of these
 

4645
01:31:51,199 --> 01:31:53,189
forward neural network, some of these
outputs are again randomly turned off to

4646
01:31:53,189 --> 01:31:53,199
outputs are again randomly turned off to
 

4647
01:31:53,199 --> 01:31:55,590
outputs are again randomly turned off to
zero. As you can see over here, some of

4648
01:31:55,590 --> 01:31:55,600
zero. As you can see over here, some of
 

4649
01:31:55,600 --> 01:31:58,030
zero. As you can see over here, some of
these outputs are randomly turned off to

4650
01:31:58,030 --> 01:31:58,040
these outputs are randomly turned off to
 

4651
01:31:58,040 --> 01:32:00,470
these outputs are randomly turned off to
zero to improve generalization

4652
01:32:00,470 --> 01:32:00,480
zero to improve generalization
 

4653
01:32:00,480 --> 01:32:02,550
zero to improve generalization
performance. And then we have one more

4654
01:32:02,550 --> 01:32:02,560
performance. And then we have one more
 

4655
01:32:02,560 --> 01:32:05,750
performance. And then we have one more
shortcut connection here. So the input

4656
01:32:05,750 --> 01:32:05,760
shortcut connection here. So the input
 

4657
01:32:05,760 --> 01:32:08,070
shortcut connection here. So the input
of this is added to the output of this

4658
01:32:08,070 --> 01:32:08,080
of this is added to the output of this
 

4659
01:32:08,080 --> 01:32:10,629
of this is added to the output of this
block. Again the shortcut connection

4660
01:32:10,629 --> 01:32:10,639
block. Again the shortcut connection
 

4661
01:32:10,639 --> 01:32:12,390
block. Again the shortcut connection
provides another path for the gradient

4662
01:32:12,390 --> 01:32:12,400
provides another path for the gradient
 

4663
01:32:12,400 --> 01:32:14,229
provides another path for the gradient
to flow thus preventing the vanishing

4664
01:32:14,229 --> 01:32:14,239
to flow thus preventing the vanishing
 

4665
01:32:14,239 --> 01:32:15,629
to flow thus preventing the vanishing
gradient

4666
01:32:15,629 --> 01:32:15,639
gradient
 

4667
01:32:15,639 --> 01:32:18,550
gradient
problem. So now actually let us see the

4668
01:32:18,550 --> 01:32:18,560
problem. So now actually let us see the
 

4669
01:32:18,560 --> 01:32:20,070
problem. So now actually let us see the
building blocks which we have assembled

4670
01:32:20,070 --> 01:32:20,080
building blocks which we have assembled
 

4671
01:32:20,080 --> 01:32:21,910
building blocks which we have assembled
so far. We have the layer normalization

4672
01:32:21,910 --> 01:32:21,920
so far. We have the layer normalization
 

4673
01:32:21,920 --> 01:32:24,990
so far. We have the layer normalization
block. We saw how the

4674
01:32:24,990 --> 01:32:25,000
block. We saw how the
 

4675
01:32:25,000 --> 01:32:27,910
block. We saw how the
um causal attention block is there.

4676
01:32:27,910 --> 01:32:27,920
um causal attention block is there.
 

4677
01:32:27,920 --> 01:32:29,430
um causal attention block is there.
There is one more class which we

4678
01:32:29,430 --> 01:32:29,440
There is one more class which we
 

4679
01:32:29,440 --> 01:32:31,189
There is one more class which we
specially create for this feed forward

4680
01:32:31,189 --> 01:32:31,199
specially create for this feed forward
 

4681
01:32:31,199 --> 01:32:33,590
specially create for this feed forward
neural network. So here you can see we

4682
01:32:33,590 --> 01:32:33,600
neural network. So here you can see we
 

4683
01:32:33,600 --> 01:32:35,430
neural network. So here you can see we
have a feed forward neural network over

4684
01:32:35,430 --> 01:32:35,440
have a feed forward neural network over
 

4685
01:32:35,440 --> 01:32:38,550
have a feed forward neural network over
here. uh this is the first layer and

4686
01:32:38,550 --> 01:32:38,560
here. uh this is the first layer and
 

4687
01:32:38,560 --> 01:32:41,750
here. uh this is the first layer and
that input goes into four times the

4688
01:32:41,750 --> 01:32:41,760
that input goes into four times the
 

4689
01:32:41,760 --> 01:32:43,510
that input goes into four times the
embedding dimension. So we have an input

4690
01:32:43,510 --> 01:32:43,520
embedding dimension. So we have an input
 

4691
01:32:43,520 --> 01:32:45,189
embedding dimension. So we have an input
with the embedding dimension as we saw

4692
01:32:45,189 --> 01:32:45,199
with the embedding dimension as we saw
 

4693
01:32:45,199 --> 01:32:48,149
with the embedding dimension as we saw
over here that was 768. The input goes

4694
01:32:48,149 --> 01:32:48,159
over here that was 768. The input goes
 

4695
01:32:48,159 --> 01:32:49,750
over here that was 768. The input goes
into a hidden layer with four times

4696
01:32:49,750 --> 01:32:49,760
into a hidden layer with four times
 

4697
01:32:49,760 --> 01:32:51,830
into a hidden layer with four times
embedding dimension that's the expansion

4698
01:32:51,830 --> 01:32:51,840
embedding dimension that's the expansion
 

4699
01:32:51,840 --> 01:32:54,070
embedding dimension that's the expansion
layer. Then we have the JU activation

4700
01:32:54,070 --> 01:32:54,080
layer. Then we have the JU activation
 

4701
01:32:54,080 --> 01:32:56,390
layer. Then we have the JU activation
function which we saw and then there is

4702
01:32:56,390 --> 01:32:56,400
function which we saw and then there is
 

4703
01:32:56,400 --> 01:32:57,390
function which we saw and then there is
a

4704
01:32:57,390 --> 01:32:57,400
a
 

4705
01:32:57,400 --> 01:32:59,910
a
uh there is a compression layer over

4706
01:32:59,910 --> 01:32:59,920
uh there is a compression layer over
 

4707
01:32:59,920 --> 01:33:02,550
uh there is a compression layer over
here. So that is again projecting the 4

4708
01:33:02,550 --> 01:33:02,560
here. So that is again projecting the 4
 

4709
01:33:02,560 --> 01:33:04,750
here. So that is again projecting the 4
into 768 back into

4710
01:33:04,750 --> 01:33:04,760
into 768 back into
 

4711
01:33:04,760 --> 01:33:07,590
into 768 back into
768 followed with a dropout. So the

4712
01:33:07,590 --> 01:33:07,600
768 followed with a dropout. So the
 

4713
01:33:07,600 --> 01:33:09,350
768 followed with a dropout. So the
neural network is again followed with a

4714
01:33:09,350 --> 01:33:09,360
neural network is again followed with a
 

4715
01:33:09,360 --> 01:33:11,110
neural network is again followed with a
dropout as we have seen over here. The

4716
01:33:11,110 --> 01:33:11,120
dropout as we have seen over here. The
 

4717
01:33:11,120 --> 01:33:13,030
dropout as we have seen over here. The
neural network layer follows a dropout

4718
01:33:13,030 --> 01:33:13,040
neural network layer follows a dropout
 

4719
01:33:13,040 --> 01:33:15,030
neural network layer follows a dropout
layer. So we call this class as

4720
01:33:15,030 --> 01:33:15,040
layer. So we call this class as
 

4721
01:33:15,040 --> 01:33:18,229
layer. So we call this class as
multi-layer perceptron. Okay. So here if

4722
01:33:18,229 --> 01:33:18,239
multi-layer perceptron. Okay. So here if
 

4723
01:33:18,239 --> 01:33:19,910
multi-layer perceptron. Okay. So here if
you see the code we assemble three

4724
01:33:19,910 --> 01:33:19,920
you see the code we assemble three
 

4725
01:33:19,920 --> 01:33:21,430
you see the code we assemble three
classes. We assemble the layer

4726
01:33:21,430 --> 01:33:21,440
classes. We assemble the layer
 

4727
01:33:21,440 --> 01:33:23,830
classes. We assemble the layer
normalization class. We assemble the

4728
01:33:23,830 --> 01:33:23,840
normalization class. We assemble the
 

4729
01:33:23,840 --> 01:33:26,470
normalization class. We assemble the
causal attention class and we assemble

4730
01:33:26,470 --> 01:33:26,480
causal attention class and we assemble
 

4731
01:33:26,480 --> 01:33:28,950
causal attention class and we assemble
the multi-layer perceptron class. Once

4732
01:33:28,950 --> 01:33:28,960
the multi-layer perceptron class. Once
 

4733
01:33:28,960 --> 01:33:30,629
the multi-layer perceptron class. Once
we have these three classes, we can

4734
01:33:30,629 --> 01:33:30,639
we have these three classes, we can
 

4735
01:33:30,639 --> 01:33:32,310
we have these three classes, we can
technically assemble the entire

4736
01:33:32,310 --> 01:33:32,320
technically assemble the entire
 

4737
01:33:32,320 --> 01:33:35,950
technically assemble the entire
transformer block like this because we

4738
01:33:35,950 --> 01:33:35,960
transformer block like this because we
 

4739
01:33:35,960 --> 01:33:39,669
transformer block like this because we
have let me rub this. Why can we assem

4740
01:33:39,669 --> 01:33:39,679
have let me rub this. Why can we assem
 

4741
01:33:39,679 --> 01:33:42,470
have let me rub this. Why can we assem
Why can we assemble this entire block?

4742
01:33:42,470 --> 01:33:42,480
Why can we assemble this entire block?
 

4743
01:33:42,480 --> 01:33:44,070
Why can we assemble this entire block?
Because now we have everything we need,

4744
01:33:44,070 --> 01:33:44,080
Because now we have everything we need,
 

4745
01:33:44,080 --> 01:33:46,510
Because now we have everything we need,
right? We have the class for the layer

4746
01:33:46,510 --> 01:33:46,520
right? We have the class for the layer
 

4747
01:33:46,520 --> 01:33:48,709
right? We have the class for the layer
normalization. So I'm marking this over

4748
01:33:48,709 --> 01:33:48,719
normalization. So I'm marking this over
 

4749
01:33:48,719 --> 01:33:50,790
normalization. So I'm marking this over
here one. We have a layer normalization

4750
01:33:50,790 --> 01:33:50,800
here one. We have a layer normalization
 

4751
01:33:50,800 --> 01:33:54,149
here one. We have a layer normalization
class. We have a multi attention class

4752
01:33:54,149 --> 01:33:54,159
class. We have a multi attention class
 

4753
01:33:54,159 --> 01:33:56,470
class. We have a multi attention class
with dropout. And we have uh the feed

4754
01:33:56,470 --> 01:33:56,480
with dropout. And we have uh the feed
 

4755
01:33:56,480 --> 01:33:58,390
with dropout. And we have uh the feed
forward neural network we drop out

4756
01:33:58,390 --> 01:33:58,400
forward neural network we drop out
 

4757
01:33:58,400 --> 01:33:59,910
forward neural network we drop out
everything we have. So now we just

4758
01:33:59,910 --> 01:33:59,920
everything we have. So now we just
 

4759
01:33:59,920 --> 01:34:01,550
everything we have. So now we just
assemble these different classes

4760
01:34:01,550 --> 01:34:01,560
assemble these different classes
 

4761
01:34:01,560 --> 01:34:04,629
assemble these different classes
together. And that's why that's what's

4762
01:34:04,629 --> 01:34:04,639
together. And that's why that's what's
 

4763
01:34:04,639 --> 01:34:07,350
together. And that's why that's what's
done in this class called block. So the

4764
01:34:07,350 --> 01:34:07,360
done in this class called block. So the
 

4765
01:34:07,360 --> 01:34:10,750
done in this class called block. So the
class called block is my transformer

4766
01:34:10,750 --> 01:34:10,760
class called block is my transformer
 

4767
01:34:10,760 --> 01:34:13,350
class called block is my transformer
block is my transformer block. And if

4768
01:34:13,350 --> 01:34:13,360
block is my transformer block. And if
 

4769
01:34:13,360 --> 01:34:15,270
block is my transformer block. And if
you see closely what's happening here,

4770
01:34:15,270 --> 01:34:15,280
you see closely what's happening here,
 

4771
01:34:15,280 --> 01:34:17,990
you see closely what's happening here,
we first have a layer normalization as

4772
01:34:17,990 --> 01:34:18,000
we first have a layer normalization as
 

4773
01:34:18,000 --> 01:34:20,149
we first have a layer normalization as
we saw here with number one. That's the

4774
01:34:20,149 --> 01:34:20,159
we saw here with number one. That's the
 

4775
01:34:20,159 --> 01:34:22,470
we saw here with number one. That's the
layer normalization. Then we have the

4776
01:34:22,470 --> 01:34:22,480
layer normalization. Then we have the
 

4777
01:34:22,480 --> 01:34:24,470
layer normalization. Then we have the
multi head attention with drop out. So

4778
01:34:24,470 --> 01:34:24,480
multi head attention with drop out. So
 

4779
01:34:24,480 --> 01:34:26,470
multi head attention with drop out. So
if you see the causal attention by

4780
01:34:26,470 --> 01:34:26,480
if you see the causal attention by
 

4781
01:34:26,480 --> 01:34:28,310
if you see the causal attention by
default towards the end the dropout is

4782
01:34:28,310 --> 01:34:28,320
default towards the end the dropout is
 

4783
01:34:28,320 --> 01:34:29,149
default towards the end the dropout is
already

4784
01:34:29,149 --> 01:34:29,159
already
 

4785
01:34:29,159 --> 01:34:31,990
already
implemented. So this is number two the

4786
01:34:31,990 --> 01:34:32,000
implemented. So this is number two the
 

4787
01:34:32,000 --> 01:34:34,470
implemented. So this is number two the
multi head attention with dropout that's

4788
01:34:34,470 --> 01:34:34,480
multi head attention with dropout that's
 

4789
01:34:34,480 --> 01:34:37,270
multi head attention with dropout that's
number two that's done right now. Then

4790
01:34:37,270 --> 01:34:37,280
number two that's done right now. Then
 

4791
01:34:37,280 --> 01:34:38,669
number two that's done right now. Then
we have

4792
01:34:38,669 --> 01:34:38,679
we have
 

4793
01:34:38,679 --> 01:34:41,510
we have
uh uh then we have another layer

4794
01:34:41,510 --> 01:34:41,520
uh uh then we have another layer
 

4795
01:34:41,520 --> 01:34:43,750
uh uh then we have another layer
normalization. If you see over here we

4796
01:34:43,750 --> 01:34:43,760
normalization. If you see over here we
 

4797
01:34:43,760 --> 01:34:46,470
normalization. If you see over here we
have another layer normalization that's

4798
01:34:46,470 --> 01:34:46,480
have another layer normalization that's
 

4799
01:34:46,480 --> 01:34:49,030
have another layer normalization that's
again marked as one. And then finally we

4800
01:34:49,030 --> 01:34:49,040
again marked as one. And then finally we
 

4801
01:34:49,040 --> 01:34:50,790
again marked as one. And then finally we
have the feed forward neural network

4802
01:34:50,790 --> 01:34:50,800
have the feed forward neural network
 

4803
01:34:50,800 --> 01:34:52,950
have the feed forward neural network
with feed forward neural network with

4804
01:34:52,950 --> 01:34:52,960
with feed forward neural network with
 

4805
01:34:52,960 --> 01:34:54,709
with feed forward neural network with
the dropout. So that's marked as three

4806
01:34:54,709 --> 01:34:54,719
the dropout. So that's marked as three
 

4807
01:34:54,719 --> 01:34:56,629
the dropout. So that's marked as three
over here. So that's how we have

4808
01:34:56,629 --> 01:34:56,639
over here. So that's how we have
 

4809
01:34:56,639 --> 01:34:58,550
over here. So that's how we have
assembled the entire transformer block

4810
01:34:58,550 --> 01:34:58,560
assembled the entire transformer block
 

4811
01:34:58,560 --> 01:35:00,390
assembled the entire transformer block
in just three lines of code or four

4812
01:35:00,390 --> 01:35:00,400
in just three lines of code or four
 

4813
01:35:00,400 --> 01:35:02,470
in just three lines of code or four
lines of code because we have defined

4814
01:35:02,470 --> 01:35:02,480
lines of code because we have defined
 

4815
01:35:02,480 --> 01:35:05,430
lines of code because we have defined
classes earlier. The ln one is the first

4816
01:35:05,430 --> 01:35:05,440
classes earlier. The ln one is the first
 

4817
01:35:05,440 --> 01:35:07,590
classes earlier. The ln one is the first
layer normalization. ln2 is the second

4818
01:35:07,590 --> 01:35:07,600
layer normalization. ln2 is the second
 

4819
01:35:07,600 --> 01:35:09,750
layer normalization. ln2 is the second
layer normalization. This is the causal

4820
01:35:09,750 --> 01:35:09,760
layer normalization. This is the causal
 

4821
01:35:09,760 --> 01:35:11,270
layer normalization. This is the causal
attention block and here we have the

4822
01:35:11,270 --> 01:35:11,280
attention block and here we have the
 

4823
01:35:11,280 --> 01:35:14,470
attention block and here we have the
multi-layer perceptron block. Okay. And

4824
01:35:14,470 --> 01:35:14,480
multi-layer perceptron block. Okay. And
 

4825
01:35:14,480 --> 01:35:16,310
multi-layer perceptron block. Okay. And
if you see the shortcut connections are

4826
01:35:16,310 --> 01:35:16,320
if you see the shortcut connections are
 

4827
01:35:16,320 --> 01:35:20,310
if you see the shortcut connections are
these. So the input to the so if you see

4828
01:35:20,310 --> 01:35:20,320
these. So the input to the so if you see
 

4829
01:35:20,320 --> 01:35:22,870
these. So the input to the so if you see
attention right uh the self.attension

4830
01:35:22,870 --> 01:35:22,880
attention right uh the self.attension
 

4831
01:35:22,880 --> 01:35:24,950
attention right uh the self.attension
attention. That's where the first uh

4832
01:35:24,950 --> 01:35:24,960
attention. That's where the first uh
 

4833
01:35:24,960 --> 01:35:26,629
attention. That's where the first uh
shortcut connection exists. So the

4834
01:35:26,629 --> 01:35:26,639
shortcut connection exists. So the
 

4835
01:35:26,639 --> 01:35:30,550
shortcut connection exists. So the
output of this is added to the input.

4836
01:35:30,550 --> 01:35:30,560
output of this is added to the input.
 

4837
01:35:30,560 --> 01:35:32,790
output of this is added to the input.
That's what this line represents. And

4838
01:35:32,790 --> 01:35:32,800
That's what this line represents. And
 

4839
01:35:32,800 --> 01:35:35,030
That's what this line represents. And
the second shortcut connection is after

4840
01:35:35,030 --> 01:35:35,040
the second shortcut connection is after
 

4841
01:35:35,040 --> 01:35:37,510
the second shortcut connection is after
the feed forward neural network over

4842
01:35:37,510 --> 01:35:37,520
the feed forward neural network over
 

4843
01:35:37,520 --> 01:35:40,070
the feed forward neural network over
here. So the output of this is added to

4844
01:35:40,070 --> 01:35:40,080
here. So the output of this is added to
 

4845
01:35:40,080 --> 01:35:43,350
here. So the output of this is added to
the input of this. That's what this

4846
01:35:43,350 --> 01:35:43,360
the input of this. That's what this
 

4847
01:35:43,360 --> 01:35:44,750
the input of this. That's what this
that's what this line actually

4848
01:35:44,750 --> 01:35:44,760
that's what this line actually
 

4849
01:35:44,760 --> 01:35:47,189
that's what this line actually
represents. So this transformer block

4850
01:35:47,189 --> 01:35:47,199
represents. So this transformer block
 

4851
01:35:47,199 --> 01:35:49,270
represents. So this transformer block
code here contains the entire code for

4852
01:35:49,270 --> 01:35:49,280
code here contains the entire code for
 

4853
01:35:49,280 --> 01:35:51,750
code here contains the entire code for
the full transformer block. So these

4854
01:35:51,750 --> 01:35:51,760
the full transformer block. So these
 

4855
01:35:51,760 --> 01:35:53,750
the full transformer block. So these
eight lines of code is where all the

4856
01:35:53,750 --> 01:35:53,760
eight lines of code is where all the
 

4857
01:35:53,760 --> 01:35:55,310
eight lines of code is where all the
magic really

4858
01:35:55,310 --> 01:35:55,320
magic really
 

4859
01:35:55,320 --> 01:35:59,510
magic really
happens. And remember that in GPT there

4860
01:35:59,510 --> 01:35:59,520
happens. And remember that in GPT there
 

4861
01:35:59,520 --> 01:36:01,270
happens. And remember that in GPT there
is not one transformer block. There are

4862
01:36:01,270 --> 01:36:01,280
is not one transformer block. There are
 

4863
01:36:01,280 --> 01:36:03,350
is not one transformer block. There are
multiple transformer blocks which we

4864
01:36:03,350 --> 01:36:03,360
multiple transformer blocks which we
 

4865
01:36:03,360 --> 01:36:06,070
multiple transformer blocks which we
have to decide. So until now in the code

4866
01:36:06,070 --> 01:36:06,080
have to decide. So until now in the code
 

4867
01:36:06,080 --> 01:36:08,149
have to decide. So until now in the code
if you see we have reached until this

4868
01:36:08,149 --> 01:36:08,159
if you see we have reached until this
 

4869
01:36:08,159 --> 01:36:10,229
if you see we have reached until this
stage right where we saw the token

4870
01:36:10,229 --> 01:36:10,239
stage right where we saw the token
 

4871
01:36:10,239 --> 01:36:12,310
stage right where we saw the token
embedding plus the position embedding

4872
01:36:12,310 --> 01:36:12,320
embedding plus the position embedding
 

4873
01:36:12,320 --> 01:36:14,229
embedding plus the position embedding
these were passed as an input to the

4874
01:36:14,229 --> 01:36:14,239
these were passed as an input to the
 

4875
01:36:14,239 --> 01:36:17,030
these were passed as an input to the
transformer block. Okay. But now we have

4876
01:36:17,030 --> 01:36:17,040
transformer block. Okay. But now we have
 

4877
01:36:17,040 --> 01:36:19,990
transformer block. Okay. But now we have
multiple transformer blocks. So if you

4878
01:36:19,990 --> 01:36:20,000
multiple transformer blocks. So if you
 

4879
01:36:20,000 --> 01:36:23,709
multiple transformer blocks. So if you
see

4880
01:36:23,709 --> 01:36:23,719

 

4881
01:36:23,719 --> 01:36:28,229

uh here uh yeah h right so we have

4882
01:36:28,229 --> 01:36:28,239
uh here uh yeah h right so we have
 

4883
01:36:28,239 --> 01:36:29,990
uh here uh yeah h right so we have
multiple transformer blocks which is

4884
01:36:29,990 --> 01:36:30,000
multiple transformer blocks which is
 

4885
01:36:30,000 --> 01:36:31,590
multiple transformer blocks which is
given by the number of layers. So the

4886
01:36:31,590 --> 01:36:31,600
given by the number of layers. So the
 

4887
01:36:31,600 --> 01:36:33,189
given by the number of layers. So the
number of layers tells us how many

4888
01:36:33,189 --> 01:36:33,199
number of layers tells us how many
 

4889
01:36:33,199 --> 01:36:35,830
number of layers tells us how many
transformer blocks we have. What our

4890
01:36:35,830 --> 01:36:35,840
transformer blocks we have. What our
 

4891
01:36:35,840 --> 01:36:37,510
transformer blocks we have. What our
input does is that it has to pass

4892
01:36:37,510 --> 01:36:37,520
input does is that it has to pass
 

4893
01:36:37,520 --> 01:36:40,390
input does is that it has to pass
through all of these transformer blocks.

4894
01:36:40,390 --> 01:36:40,400
through all of these transformer blocks.
 

4895
01:36:40,400 --> 01:36:41,830
through all of these transformer blocks.
It has to pass through all of these

4896
01:36:41,830 --> 01:36:41,840
It has to pass through all of these
 

4897
01:36:41,840 --> 01:36:43,590
It has to pass through all of these
transformer blocks. So whatever I have

4898
01:36:43,590 --> 01:36:43,600
transformer blocks. So whatever I have
 

4899
01:36:43,600 --> 01:36:46,070
transformer blocks. So whatever I have
shown over here, whatever I have shown

4900
01:36:46,070 --> 01:36:46,080
shown over here, whatever I have shown
 

4901
01:36:46,080 --> 01:36:47,990
shown over here, whatever I have shown
over here, right? There are actually

4902
01:36:47,990 --> 01:36:48,000
over here, right? There are actually
 

4903
01:36:48,000 --> 01:36:51,870
over here, right? There are actually
multiple transformer blocks over

4904
01:36:51,870 --> 01:36:51,880

 

4905
01:36:51,880 --> 01:36:54,470

here. There is not just one transformer

4906
01:36:54,470 --> 01:36:54,480
here. There is not just one transformer
 

4907
01:36:54,480 --> 01:36:56,390
here. There is not just one transformer
block. So one transformer block has this

4908
01:36:56,390 --> 01:36:56,400
block. So one transformer block has this
 

4909
01:36:56,400 --> 01:36:57,990
block. So one transformer block has this
entire architecture, right? These might

4910
01:36:57,990 --> 01:36:58,000
entire architecture, right? These might
 

4911
01:36:58,000 --> 01:37:00,950
entire architecture, right? These might
be 12 or 24 such blocks. So my input has

4912
01:37:00,950 --> 01:37:00,960
be 12 or 24 such blocks. So my input has
 

4913
01:37:00,960 --> 01:37:02,390
be 12 or 24 such blocks. So my input has
to pass through the first transformer

4914
01:37:02,390 --> 01:37:02,400
to pass through the first transformer
 

4915
01:37:02,400 --> 01:37:03,910
to pass through the first transformer
block, then through the second, then the

4916
01:37:03,910 --> 01:37:03,920
block, then through the second, then the
 

4917
01:37:03,920 --> 01:37:06,390
block, then through the second, then the
third, right up till the very end. And

4918
01:37:06,390 --> 01:37:06,400
third, right up till the very end. And
 

4919
01:37:06,400 --> 01:37:08,629
third, right up till the very end. And
then and only then does it exit out of

4920
01:37:08,629 --> 01:37:08,639
then and only then does it exit out of
 

4921
01:37:08,639 --> 01:37:10,550
then and only then does it exit out of
the transformer and then it has to come

4922
01:37:10,550 --> 01:37:10,560
the transformer and then it has to come
 

4923
01:37:10,560 --> 01:37:13,189
the transformer and then it has to come
to the output block. So if you see over

4924
01:37:13,189 --> 01:37:13,199
to the output block. So if you see over
 

4925
01:37:13,199 --> 01:37:14,950
to the output block. So if you see over
here what we have written over here is

4926
01:37:14,950 --> 01:37:14,960
here what we have written over here is
 

4927
01:37:14,960 --> 01:37:16,870
here what we have written over here is
that the input which was the token plus

4928
01:37:16,870 --> 01:37:16,880
that the input which was the token plus
 

4929
01:37:16,880 --> 01:37:18,950
that the input which was the token plus
the positional embedding has to go

4930
01:37:18,950 --> 01:37:18,960
the positional embedding has to go
 

4931
01:37:18,960 --> 01:37:20,390
the positional embedding has to go
through all of these multiple

4932
01:37:20,390 --> 01:37:20,400
through all of these multiple
 

4933
01:37:20,400 --> 01:37:22,550
through all of these multiple
transformer blocks and then it comes out

4934
01:37:22,550 --> 01:37:22,560
transformer blocks and then it comes out
 

4935
01:37:22,560 --> 01:37:25,270
transformer blocks and then it comes out
of the processor and this is that step

4936
01:37:25,270 --> 01:37:25,280
of the processor and this is that step
 

4937
01:37:25,280 --> 01:37:27,510
of the processor and this is that step
in which we have the output defined. So

4938
01:37:27,510 --> 01:37:27,520
in which we have the output defined. So
 

4939
01:37:27,520 --> 01:37:29,709
in which we have the output defined. So
I'll come to that in a

4940
01:37:29,709 --> 01:37:29,719
I'll come to that in a
 

4941
01:37:29,719 --> 01:37:32,030
I'll come to that in a
moment.

4942
01:37:32,030 --> 01:37:32,040
moment.
 

4943
01:37:32,040 --> 01:37:35,790
moment.
Um all right so the output has been

4944
01:37:35,790 --> 01:37:35,800
Um all right so the output has been
 

4945
01:37:35,800 --> 01:37:38,470
Um all right so the output has been
defined actually the output has been

4946
01:37:38,470 --> 01:37:38,480
defined actually the output has been
 

4947
01:37:38,480 --> 01:37:40,870
defined actually the output has been
defined over here and also over here. So

4948
01:37:40,870 --> 01:37:40,880
defined over here and also over here. So
 

4949
01:37:40,880 --> 01:37:42,629
defined over here and also over here. So
I'll come to that in a moment. But until

4950
01:37:42,629 --> 01:37:42,639
I'll come to that in a moment. But until
 

4951
01:37:42,639 --> 01:37:44,390
I'll come to that in a moment. But until
this point just remember that we have

4952
01:37:44,390 --> 01:37:44,400
this point just remember that we have
 

4953
01:37:44,400 --> 01:37:46,070
this point just remember that we have
gone through the multiple transformer

4954
01:37:46,070 --> 01:37:46,080
gone through the multiple transformer
 

4955
01:37:46,080 --> 01:37:48,870
gone through the multiple transformer
blocks and then we have the output. What

4956
01:37:48,870 --> 01:37:48,880
blocks and then we have the output. What
 

4957
01:37:48,880 --> 01:37:50,629
blocks and then we have the output. What
is the output size after going through

4958
01:37:50,629 --> 01:37:50,639
is the output size after going through
 

4959
01:37:50,639 --> 01:37:52,950
is the output size after going through
multiple transformer blocks? After the

4960
01:37:52,950 --> 01:37:52,960
multiple transformer blocks? After the
 

4961
01:37:52,960 --> 01:37:55,030
multiple transformer blocks? After the
first transformer block, the output size

4962
01:37:55,030 --> 01:37:55,040
first transformer block, the output size
 

4963
01:37:55,040 --> 01:37:57,030
first transformer block, the output size
remains the same as the input. Right?

4964
01:37:57,030 --> 01:37:57,040
remains the same as the input. Right?
 

4965
01:37:57,040 --> 01:37:59,109
remains the same as the input. Right?
When we went inside the transformer

4966
01:37:59,109 --> 01:37:59,119
When we went inside the transformer
 

4967
01:37:59,119 --> 01:38:01,830
When we went inside the transformer
block, remember the size was four rows

4968
01:38:01,830 --> 01:38:01,840
block, remember the size was four rows
 

4969
01:38:01,840 --> 01:38:04,470
block, remember the size was four rows
and 768 columns. Even after applying

4970
01:38:04,470 --> 01:38:04,480
and 768 columns. Even after applying
 

4971
01:38:04,480 --> 01:38:06,870
and 768 columns. Even after applying
dropout, that's the same size. When we

4972
01:38:06,870 --> 01:38:06,880
dropout, that's the same size. When we
 

4973
01:38:06,880 --> 01:38:08,950
dropout, that's the same size. When we
come out of multiple transformer blocks,

4974
01:38:08,950 --> 01:38:08,960
come out of multiple transformer blocks,
 

4975
01:38:08,960 --> 01:38:12,350
come out of multiple transformer blocks,
the size remains the same. That's 4 *

4976
01:38:12,350 --> 01:38:12,360
the size remains the same. That's 4 *
 

4977
01:38:12,360 --> 01:38:15,270
the size remains the same. That's 4 *
768. Okay. Now we have come out of the

4978
01:38:15,270 --> 01:38:15,280
768. Okay. Now we have come out of the
 

4979
01:38:15,280 --> 01:38:17,270
768. Okay. Now we have come out of the
multiple transformer blocks and finally

4980
01:38:17,270 --> 01:38:17,280
multiple transformer blocks and finally
 

4981
01:38:17,280 --> 01:38:20,390
multiple transformer blocks and finally
we'll go to this last third block which

4982
01:38:20,390 --> 01:38:20,400
we'll go to this last third block which
 

4983
01:38:20,400 --> 01:38:23,350
we'll go to this last third block which
is the output layer. Now you see here

4984
01:38:23,350 --> 01:38:23,360
is the output layer. Now you see here
 

4985
01:38:23,360 --> 01:38:24,870
is the output layer. Now you see here
there are two things which happen. First

4986
01:38:24,870 --> 01:38:24,880
there are two things which happen. First
 

4987
01:38:24,880 --> 01:38:27,070
there are two things which happen. First
we have to pass through another layer of

4988
01:38:27,070 --> 01:38:27,080
we have to pass through another layer of
 

4989
01:38:27,080 --> 01:38:29,109
we have to pass through another layer of
normalization and then we have to pass

4990
01:38:29,109 --> 01:38:29,119
normalization and then we have to pass
 

4991
01:38:29,119 --> 01:38:31,270
normalization and then we have to pass
through the output layer. So let me

4992
01:38:31,270 --> 01:38:31,280
through the output layer. So let me
 

4993
01:38:31,280 --> 01:38:33,109
through the output layer. So let me
explain step by step what is happening

4994
01:38:33,109 --> 01:38:33,119
explain step by step what is happening
 

4995
01:38:33,119 --> 01:38:35,270
explain step by step what is happening
over here. When we come out of the

4996
01:38:35,270 --> 01:38:35,280
over here. When we come out of the
 

4997
01:38:35,280 --> 01:38:36,870
over here. When we come out of the
transformer block, we have this size

4998
01:38:36,870 --> 01:38:36,880
transformer block, we have this size
 

4999
01:38:36,880 --> 01:38:40,470
transformer block, we have this size
right which is 4 * 768. Then we will do

5000
01:38:40,470 --> 01:38:40,480
right which is 4 * 768. Then we will do
 

5001
01:38:40,480 --> 01:38:42,470
right which is 4 * 768. Then we will do
one more layer of normalization. And

5002
01:38:42,470 --> 01:38:42,480
one more layer of normalization. And
 

5003
01:38:42,480 --> 01:38:44,709
one more layer of normalization. And
here the same thing happens. Essentially

5004
01:38:44,709 --> 01:38:44,719
here the same thing happens. Essentially
 

5005
01:38:44,719 --> 01:38:46,470
here the same thing happens. Essentially
we subtract the mean divide by the

5006
01:38:46,470 --> 01:38:46,480
we subtract the mean divide by the
 

5007
01:38:46,480 --> 01:38:48,229
we subtract the mean divide by the
square root of variance. So now the mean

5008
01:38:48,229 --> 01:38:48,239
square root of variance. So now the mean
 

5009
01:38:48,239 --> 01:38:50,229
square root of variance. So now the mean
is equal to zero. Variance is equal to 1

5010
01:38:50,229 --> 01:38:50,239
is equal to zero. Variance is equal to 1
 

5011
01:38:50,239 --> 01:38:53,590
is equal to zero. Variance is equal to 1
for every row. Okay. And then we pass

5012
01:38:53,590 --> 01:38:53,600
for every row. Okay. And then we pass
 

5013
01:38:53,600 --> 01:38:57,990
for every row. Okay. And then we pass
this 4 * 768 matrix into an output head.

5014
01:38:57,990 --> 01:38:58,000
this 4 * 768 matrix into an output head.
 

5015
01:38:58,000 --> 01:38:59,990
this 4 * 768 matrix into an output head.
What this output head does is that this

5016
01:38:59,990 --> 01:39:00,000
What this output head does is that this
 

5017
01:39:00,000 --> 01:39:03,109
What this output head does is that this
output head converts every vector is now

5018
01:39:03,109 --> 01:39:03,119
output head converts every vector is now
 

5019
01:39:03,119 --> 01:39:06,149
output head converts every vector is now
of a size of 768. Right? It converts

5020
01:39:06,149 --> 01:39:06,159
of a size of 768. Right? It converts
 

5021
01:39:06,159 --> 01:39:09,550
of a size of 768. Right? It converts
every vector into the vocabulary size.

5022
01:39:09,550 --> 01:39:09,560
every vector into the vocabulary size.
 

5023
01:39:09,560 --> 01:39:12,390
every vector into the vocabulary size.
Now it converts every vector into the

5024
01:39:12,390 --> 01:39:12,400
Now it converts every vector into the
 

5025
01:39:12,400 --> 01:39:14,709
Now it converts every vector into the
vocabulary size. And the way it does is

5026
01:39:14,709 --> 01:39:14,719
vocabulary size. And the way it does is
 

5027
01:39:14,719 --> 01:39:17,030
vocabulary size. And the way it does is
that by passing every vector through a

5028
01:39:17,030 --> 01:39:17,040
that by passing every vector through a
 

5029
01:39:17,040 --> 01:39:19,990
that by passing every vector through a
neural network. So if this size is of 4x

5030
01:39:19,990 --> 01:39:20,000
neural network. So if this size is of 4x
 

5031
01:39:20,000 --> 01:39:22,229
neural network. So if this size is of 4x
768, I will have a neural network of

5032
01:39:22,229 --> 01:39:22,239
768, I will have a neural network of
 

5033
01:39:22,239 --> 01:39:24,870
768, I will have a neural network of
size 768 multiplied by my vocabulary

5034
01:39:24,870 --> 01:39:24,880
size 768 multiplied by my vocabulary
 

5035
01:39:24,880 --> 01:39:26,870
size 768 multiplied by my vocabulary
size.

5036
01:39:26,870 --> 01:39:26,880
size.
 

5037
01:39:26,880 --> 01:39:28,709
size.
So when this input passes through this

5038
01:39:28,709 --> 01:39:28,719
So when this input passes through this
 

5039
01:39:28,719 --> 01:39:31,590
So when this input passes through this
neural network, it will be of a size of

5040
01:39:31,590 --> 01:39:31,600
neural network, it will be of a size of
 

5041
01:39:31,600 --> 01:39:34,750
neural network, it will be of a size of
4 into vocabulary

5042
01:39:34,750 --> 01:39:34,760
4 into vocabulary
 

5043
01:39:34,760 --> 01:39:37,590
4 into vocabulary
size. So now take a look here. Every

5044
01:39:37,590 --> 01:39:37,600
size. So now take a look here. Every
 

5045
01:39:37,600 --> 01:39:41,109
size. So now take a look here. Every
token now will have 50257 or my

5046
01:39:41,109 --> 01:39:41,119
token now will have 50257 or my
 

5047
01:39:41,119 --> 01:39:43,830
token now will have 50257 or my
vocabulary size if my vocabulary size is

5048
01:39:43,830 --> 01:39:43,840
vocabulary size if my vocabulary size is
 

5049
01:39:43,840 --> 01:39:45,590
vocabulary size if my vocabulary size is
something every token will have those

5050
01:39:45,590 --> 01:39:45,600
something every token will have those
 

5051
01:39:45,600 --> 01:39:46,750
something every token will have those
many number of

5052
01:39:46,750 --> 01:39:46,760
many number of
 

5053
01:39:46,760 --> 01:39:49,590
many number of
columns. Uh so every token has these

5054
01:39:49,590 --> 01:39:49,600
columns. Uh so every token has these
 

5055
01:39:49,600 --> 01:39:51,030
columns. Uh so every token has these
many number of columns which are equal

5056
01:39:51,030 --> 01:39:51,040
many number of columns which are equal
 

5057
01:39:51,040 --> 01:39:53,750
many number of columns which are equal
to the vocabulary size.

5058
01:39:53,750 --> 01:39:53,760
to the vocabulary size.
 

5059
01:39:53,760 --> 01:39:56,189
to the vocabulary size.
This is called as the logit

5060
01:39:56,189 --> 01:39:56,199
This is called as the logit
 

5061
01:39:56,199 --> 01:39:58,790
This is called as the logit
stensor. So if we just have one batch,

5062
01:39:58,790 --> 01:39:58,800
stensor. So if we just have one batch,
 

5063
01:39:58,800 --> 01:40:01,030
stensor. So if we just have one batch,
the size of this logit stensor will be

5064
01:40:01,030 --> 01:40:01,040
the size of this logit stensor will be
 

5065
01:40:01,040 --> 01:40:03,189
the size of this logit stensor will be
four multiplied by the vocabulary size.

5066
01:40:03,189 --> 01:40:03,199
four multiplied by the vocabulary size.
 

5067
01:40:03,199 --> 01:40:04,790
four multiplied by the vocabulary size.
If we have two batches, it will be 2

5068
01:40:04,790 --> 01:40:04,800
If we have two batches, it will be 2
 

5069
01:40:04,800 --> 01:40:07,669
If we have two batches, it will be 2
mult* 4 multiplied by vocabulary size.

5070
01:40:07,669 --> 01:40:07,679
mult* 4 multiplied by vocabulary size.
 

5071
01:40:07,679 --> 01:40:09,590
mult* 4 multiplied by vocabulary size.
Don't worry about multiple batches for

5072
01:40:09,590 --> 01:40:09,600
Don't worry about multiple batches for
 

5073
01:40:09,600 --> 01:40:11,189
Don't worry about multiple batches for
now because we are just looking at one

5074
01:40:11,189 --> 01:40:11,199
now because we are just looking at one
 

5075
01:40:11,199 --> 01:40:14,470
now because we are just looking at one
batch currently. So every effort moves

5076
01:40:14,470 --> 01:40:14,480
batch currently. So every effort moves
 

5077
01:40:14,480 --> 01:40:16,550
batch currently. So every effort moves
you. Let's say these are the four tokens

5078
01:40:16,550 --> 01:40:16,560
you. Let's say these are the four tokens
 

5079
01:40:16,560 --> 01:40:18,629
you. Let's say these are the four tokens
and we have the number of columns equal

5080
01:40:18,629 --> 01:40:18,639
and we have the number of columns equal
 

5081
01:40:18,639 --> 01:40:21,030
and we have the number of columns equal
to my vocabulary size. Now this is

5082
01:40:21,030 --> 01:40:21,040
to my vocabulary size. Now this is
 

5083
01:40:21,040 --> 01:40:22,709
to my vocabulary size. Now this is
exactly what is done in the code, right?

5084
01:40:22,709 --> 01:40:22,719
exactly what is done in the code, right?
 

5085
01:40:22,719 --> 01:40:24,709
exactly what is done in the code, right?
So here we have come out of the multiple

5086
01:40:24,709 --> 01:40:24,719
So here we have come out of the multiple
 

5087
01:40:24,719 --> 01:40:27,030
So here we have come out of the multiple
transformer blocks. What is done here is

5088
01:40:27,030 --> 01:40:27,040
transformer blocks. What is done here is
 

5089
01:40:27,040 --> 01:40:30,070
transformer blocks. What is done here is
that if you see lnf it is another layer

5090
01:40:30,070 --> 01:40:30,080
that if you see lnf it is another layer
 

5091
01:40:30,080 --> 01:40:31,990
that if you see lnf it is another layer
of normalization. So we apply the

5092
01:40:31,990 --> 01:40:32,000
of normalization. So we apply the
 

5093
01:40:32,000 --> 01:40:34,709
of normalization. So we apply the
normalization layer here. That is what

5094
01:40:34,709 --> 01:40:34,719
normalization layer here. That is what
 

5095
01:40:34,719 --> 01:40:37,910
normalization layer here. That is what
is given in the

5096
01:40:37,910 --> 01:40:37,920
is given in the
 

5097
01:40:37,920 --> 01:40:40,390
is given in the
um if you see the output block. Yeah the

5098
01:40:40,390 --> 01:40:40,400
um if you see the output block. Yeah the
 

5099
01:40:40,400 --> 01:40:42,310
um if you see the output block. Yeah the
we have one more layer of normalization

5100
01:40:42,310 --> 01:40:42,320
we have one more layer of normalization
 

5101
01:40:42,320 --> 01:40:45,270
we have one more layer of normalization
here right that's the first layer of

5102
01:40:45,270 --> 01:40:45,280
here right that's the first layer of
 

5103
01:40:45,280 --> 01:40:48,550
here right that's the first layer of
normalization. And then if you see what

5104
01:40:48,550 --> 01:40:48,560
normalization. And then if you see what
 

5105
01:40:48,560 --> 01:40:51,430
normalization. And then if you see what
is done here is selflm head. Now this LM

5106
01:40:51,430 --> 01:40:51,440
is done here is selflm head. Now this LM
 

5107
01:40:51,440 --> 01:40:54,470
is done here is selflm head. Now this LM
head if you see that's a neural network

5108
01:40:54,470 --> 01:40:54,480
head if you see that's a neural network
 

5109
01:40:54,480 --> 01:40:57,030
head if you see that's a neural network
with the input of number of embeddings

5110
01:40:57,030 --> 01:40:57,040
with the input of number of embeddings
 

5111
01:40:57,040 --> 01:40:58,950
with the input of number of embeddings
and the output is equal to my vocabulary

5112
01:40:58,950 --> 01:40:58,960
and the output is equal to my vocabulary
 

5113
01:40:58,960 --> 01:41:01,350
and the output is equal to my vocabulary
size right now. So that's my neural

5114
01:41:01,350 --> 01:41:01,360
size right now. So that's my neural
 

5115
01:41:01,360 --> 01:41:03,270
size right now. So that's my neural
network with the number of rows equal to

5116
01:41:03,270 --> 01:41:03,280
network with the number of rows equal to
 

5117
01:41:03,280 --> 01:41:06,550
network with the number of rows equal to
n embedding and the number of uh columns

5118
01:41:06,550 --> 01:41:06,560
n embedding and the number of uh columns
 

5119
01:41:06,560 --> 01:41:08,990
n embedding and the number of uh columns
which is equal to my vocabulary

5120
01:41:08,990 --> 01:41:09,000
which is equal to my vocabulary
 

5121
01:41:09,000 --> 01:41:12,709
which is equal to my vocabulary
size. All right good. So this is the lm

5122
01:41:12,709 --> 01:41:12,719
size. All right good. So this is the lm
 

5123
01:41:12,719 --> 01:41:14,629
size. All right good. So this is the lm
head which is the output head now which

5124
01:41:14,629 --> 01:41:14,639
head which is the output head now which
 

5125
01:41:14,639 --> 01:41:15,950
head which is the output head now which
my

5126
01:41:15,950 --> 01:41:15,960
my
 

5127
01:41:15,960 --> 01:41:18,629
my
u which is the final output of my

5128
01:41:18,629 --> 01:41:18,639
u which is the final output of my
 

5129
01:41:18,639 --> 01:41:20,790
u which is the final output of my
transformer block. So this is my logits

5130
01:41:20,790 --> 01:41:20,800
transformer block. So this is my logits
 

5131
01:41:20,800 --> 01:41:23,270
transformer block. So this is my logits
matrix and this logits matrix is now

5132
01:41:23,270 --> 01:41:23,280
matrix and this logits matrix is now
 

5133
01:41:23,280 --> 01:41:25,270
matrix and this logits matrix is now
going to be used for the next token

5134
01:41:25,270 --> 01:41:25,280
going to be used for the next token
 

5135
01:41:25,280 --> 01:41:27,910
going to be used for the next token
prediction task. How is my logits matrix

5136
01:41:27,910 --> 01:41:27,920
prediction task. How is my logits matrix
 

5137
01:41:27,920 --> 01:41:29,590
prediction task. How is my logits matrix
going to be used for the next token

5138
01:41:29,590 --> 01:41:29,600
going to be used for the next token
 

5139
01:41:29,600 --> 01:41:31,750
going to be used for the next token
prediction task? If you carefully look

5140
01:41:31,750 --> 01:41:31,760
prediction task? If you carefully look
 

5141
01:41:31,760 --> 01:41:34,470
prediction task? If you carefully look
at the logits matrix right now every

5142
01:41:34,470 --> 01:41:34,480
at the logits matrix right now every
 

5143
01:41:34,480 --> 01:41:36,390
at the logits matrix right now every
logits matrix has size equal to the

5144
01:41:36,390 --> 01:41:36,400
logits matrix has size equal to the
 

5145
01:41:36,400 --> 01:41:38,990
logits matrix has size equal to the
vocabulary right. So

5146
01:41:38,990 --> 01:41:39,000
vocabulary right. So
 

5147
01:41:39,000 --> 01:41:44,070
vocabulary right. So
uh and as I told you every batch every

5148
01:41:44,070 --> 01:41:44,080
uh and as I told you every batch every
 

5149
01:41:44,080 --> 01:41:45,390
uh and as I told you every batch every
batch

5150
01:41:45,390 --> 01:41:45,400
batch
 

5151
01:41:45,400 --> 01:41:48,390
batch
has multiple input target prediction

5152
01:41:48,390 --> 01:41:48,400
has multiple input target prediction
 

5153
01:41:48,400 --> 01:41:51,350
has multiple input target prediction
tasks. So if you see this let's take

5154
01:41:51,350 --> 01:41:51,360
tasks. So if you see this let's take
 

5155
01:41:51,360 --> 01:41:53,950
tasks. So if you see this let's take
this same example right one day a

5156
01:41:53,950 --> 01:41:53,960
this same example right one day a
 

5157
01:41:53,960 --> 01:41:56,550
this same example right one day a
little when we come out of the entire

5158
01:41:56,550 --> 01:41:56,560
little when we come out of the entire
 

5159
01:41:56,560 --> 01:41:59,030
little when we come out of the entire
transformer architecture

5160
01:41:59,030 --> 01:41:59,040
transformer architecture
 

5161
01:41:59,040 --> 01:42:00,629
transformer architecture
uh when we come out of the entire

5162
01:42:00,629 --> 01:42:00,639
uh when we come out of the entire
 

5163
01:42:00,639 --> 01:42:02,310
uh when we come out of the entire
transformer architecture we have

5164
01:42:02,310 --> 01:42:02,320
transformer architecture we have
 

5165
01:42:02,320 --> 01:42:07,950
transformer architecture we have
something like this 1 day a

5166
01:42:07,950 --> 01:42:07,960
something like this 1 day a
 

5167
01:42:07,960 --> 01:42:12,070
something like this 1 day a
little and this is now 50 to 57 or my

5168
01:42:12,070 --> 01:42:12,080
little and this is now 50 to 57 or my
 

5169
01:42:12,080 --> 01:42:14,070
little and this is now 50 to 57 or my
vocabulary size let's say all of these

5170
01:42:14,070 --> 01:42:14,080
vocabulary size let's say all of these
 

5171
01:42:14,080 --> 01:42:17,229
vocabulary size let's say all of these
are now equal to my vocabulary

5172
01:42:17,229 --> 01:42:17,239
are now equal to my vocabulary
 

5173
01:42:17,239 --> 01:42:20,149
are now equal to my vocabulary
size All of these are now equal to my

5174
01:42:20,149 --> 01:42:20,159
size All of these are now equal to my
 

5175
01:42:20,159 --> 01:42:22,350
size All of these are now equal to my
vocabulary

5176
01:42:22,350 --> 01:42:22,360
vocabulary
 

5177
01:42:22,360 --> 01:42:25,830
vocabulary
size. Okay. So the way the next token is

5178
01:42:25,830 --> 01:42:25,840
size. Okay. So the way the next token is
 

5179
01:42:25,840 --> 01:42:28,070
size. Okay. So the way the next token is
predicted is that when one is the input,

5180
01:42:28,070 --> 01:42:28,080
predicted is that when one is the input,
 

5181
01:42:28,080 --> 01:42:30,470
predicted is that when one is the input,
I look at that token index which has the

5182
01:42:30,470 --> 01:42:30,480
I look at that token index which has the
 

5183
01:42:30,480 --> 01:42:32,790
I look at that token index which has the
highest probability. If that is equal to

5184
01:42:32,790 --> 01:42:32,800
highest probability. If that is equal to
 

5185
01:42:32,800 --> 01:42:35,149
highest probability. If that is equal to
40, I go to my

5186
01:42:35,149 --> 01:42:35,159
40, I go to my
 

5187
01:42:35,159 --> 01:42:38,070
40, I go to my
vocabulary and I look for token ID 40

5188
01:42:38,070 --> 01:42:38,080
vocabulary and I look for token ID 40
 

5189
01:42:38,080 --> 01:42:40,629
vocabulary and I look for token ID 40
and I see the token associated with it.

5190
01:42:40,629 --> 01:42:40,639
and I see the token associated with it.
 

5191
01:42:40,639 --> 01:42:44,750
and I see the token associated with it.
So ideally if my model is trained, the

5192
01:42:44,750 --> 01:42:44,760
So ideally if my model is trained, the
 

5193
01:42:44,760 --> 01:42:47,350
So ideally if my model is trained, the
output should be the same as the target.

5194
01:42:47,350 --> 01:42:47,360
output should be the same as the target.
 

5195
01:42:47,360 --> 01:42:49,510
output should be the same as the target.
Now the target when one is the input day

5196
01:42:49,510 --> 01:42:49,520
Now the target when one is the input day
 

5197
01:42:49,520 --> 01:42:51,669
Now the target when one is the input day
is the output right that's the target

5198
01:42:51,669 --> 01:42:51,679
is the output right that's the target
 

5199
01:42:51,679 --> 01:42:53,590
is the output right that's the target
but maybe token 40 corresponds to

5200
01:42:53,590 --> 01:42:53,600
but maybe token 40 corresponds to
 

5201
01:42:53,600 --> 01:42:55,709
but maybe token 40 corresponds to
something like random thing like

5202
01:42:55,709 --> 01:42:55,719
something like random thing like
 

5203
01:42:55,719 --> 01:42:58,870
something like random thing like
the now when one when day when I look at

5204
01:42:58,870 --> 01:42:58,880
the now when one when day when I look at
 

5205
01:42:58,880 --> 01:43:00,629
the now when one when day when I look at
day I again look at that thing with the

5206
01:43:00,629 --> 01:43:00,639
day I again look at that thing with the
 

5207
01:43:00,639 --> 01:43:02,470
day I again look at that thing with the
maximum probability the index with the

5208
01:43:02,470 --> 01:43:02,480
maximum probability the index with the
 

5209
01:43:02,480 --> 01:43:04,950
maximum probability the index with the
maximum probability when one day is the

5210
01:43:04,950 --> 01:43:04,960
maximum probability when one day is the
 

5211
01:43:04,960 --> 01:43:07,750
maximum probability when one day is the
input o is the output but maybe I'll get

5212
01:43:07,750 --> 01:43:07,760
input o is the output but maybe I'll get
 

5213
01:43:07,760 --> 01:43:09,149
input o is the output but maybe I'll get
something like

5214
01:43:09,149 --> 01:43:09,159
something like
 

5215
01:43:09,159 --> 01:43:12,390
something like
flag then oh I look at the index which

5216
01:43:12,390 --> 01:43:12,400
flag then oh I look at the index which
 

5217
01:43:12,400 --> 01:43:14,070
flag then oh I look at the index which
corresponds to the maximum probability

5218
01:43:14,070 --> 01:43:14,080
corresponds to the maximum probability
 

5219
01:43:14,080 --> 01:43:16,550
corresponds to the maximum probability
if that index is 60 maybe that

5220
01:43:16,550 --> 01:43:16,560
if that index is 60 maybe that
 

5221
01:43:16,560 --> 01:43:18,149
if that index is 60 maybe that
corresponds responds to something like

5222
01:43:18,149 --> 01:43:18,159
corresponds responds to something like
 

5223
01:43:18,159 --> 01:43:22,310
corresponds responds to something like
uh O but my target is little and when I

5224
01:43:22,310 --> 01:43:22,320
uh O but my target is little and when I
 

5225
01:43:22,320 --> 01:43:24,310
uh O but my target is little and when I
look at little and I look at the index

5226
01:43:24,310 --> 01:43:24,320
look at little and I look at the index
 

5227
01:43:24,320 --> 01:43:26,070
look at little and I look at the index
with the maximum probability maybe that

5228
01:43:26,070 --> 01:43:26,080
with the maximum probability maybe that
 

5229
01:43:26,080 --> 01:43:28,629
with the maximum probability maybe that
corresponds to person but ideally my

5230
01:43:28,629 --> 01:43:28,639
corresponds to person but ideally my
 

5231
01:43:28,639 --> 01:43:31,750
corresponds to person but ideally my
target is girl so now I have my outputs

5232
01:43:31,750 --> 01:43:31,760
target is girl so now I have my outputs
 

5233
01:43:31,760 --> 01:43:33,270
target is girl so now I have my outputs
and I have my targets and then I'll

5234
01:43:33,270 --> 01:43:33,280
and I have my targets and then I'll
 

5235
01:43:33,280 --> 01:43:35,030
and I have my targets and then I'll
calculate the loss function between the

5236
01:43:35,030 --> 01:43:35,040
calculate the loss function between the
 

5237
01:43:35,040 --> 01:43:36,350
calculate the loss function between the
output and the

5238
01:43:36,350 --> 01:43:36,360
output and the
 

5239
01:43:36,360 --> 01:43:39,430
output and the
target this is how the logit tensor the

5240
01:43:39,430 --> 01:43:39,440
target this is how the logit tensor the
 

5241
01:43:39,440 --> 01:43:41,669
target this is how the logit tensor the
logits the output logits matrix which I

5242
01:43:41,669 --> 01:43:41,679
logits the output logits matrix which I
 

5243
01:43:41,679 --> 01:43:43,590
logits the output logits matrix which I
get after coming out of the transformer

5244
01:43:43,590 --> 01:43:43,600
get after coming out of the transformer
 

5245
01:43:43,600 --> 01:43:45,669
get after coming out of the transformer
block this is how it is used for the

5246
01:43:45,669 --> 01:43:45,679
block this is how it is used for the
 

5247
01:43:45,679 --> 01:43:48,629
block this is how it is used for the
next token prediction task. So, so one

5248
01:43:48,629 --> 01:43:48,639
next token prediction task. So, so one
 

5249
01:43:48,639 --> 01:43:50,550
next token prediction task. So, so one
when one batch is passed, let's say one

5250
01:43:50,550 --> 01:43:50,560
when one batch is passed, let's say one
 

5251
01:43:50,560 --> 01:43:52,310
when one batch is passed, let's say one
batch is passed, right? Which is one day

5252
01:43:52,310 --> 01:43:52,320
batch is passed, right? Which is one day
 

5253
01:43:52,320 --> 01:43:54,950
batch is passed, right? Which is one day
a little when it comes out of all of

5254
01:43:54,950 --> 01:43:54,960
a little when it comes out of all of
 

5255
01:43:54,960 --> 01:43:56,629
a little when it comes out of all of
these three steps, when it comes out of

5256
01:43:56,629 --> 01:43:56,639
these three steps, when it comes out of
 

5257
01:43:56,639 --> 01:43:59,189
these three steps, when it comes out of
the input processor and output, it has

5258
01:43:59,189 --> 01:43:59,199
the input processor and output, it has
 

5259
01:43:59,199 --> 01:44:01,350
the input processor and output, it has
this logits and within that there are

5260
01:44:01,350 --> 01:44:01,360
this logits and within that there are
 

5261
01:44:01,360 --> 01:44:03,189
this logits and within that there are
multiple input output tasks. There are

5262
01:44:03,189 --> 01:44:03,199
multiple input output tasks. There are
 

5263
01:44:03,199 --> 01:44:05,109
multiple input output tasks. There are
four input output tasks which needs to

5264
01:44:05,109 --> 01:44:05,119
four input output tasks which needs to
 

5265
01:44:05,119 --> 01:44:07,830
four input output tasks which needs to
be predicted in each batch and this is

5266
01:44:07,830 --> 01:44:07,840
be predicted in each batch and this is
 

5267
01:44:07,840 --> 01:44:10,310
be predicted in each batch and this is
exactly how these tasks are done. So now

5268
01:44:10,310 --> 01:44:10,320
exactly how these tasks are done. So now
 

5269
01:44:10,320 --> 01:44:12,709
exactly how these tasks are done. So now
there are four output target pairs,

5270
01:44:12,709 --> 01:44:12,719
there are four output target pairs,
 

5271
01:44:12,719 --> 01:44:14,550
there are four output target pairs,
right? we find a loss function based on

5272
01:44:14,550 --> 01:44:14,560
right? we find a loss function based on
 

5273
01:44:14,560 --> 01:44:16,709
right? we find a loss function based on
all of these four and it's this loss

5274
01:44:16,709 --> 01:44:16,719
all of these four and it's this loss
 

5275
01:44:16,719 --> 01:44:19,189
all of these four and it's this loss
which eventually needs to be minimized

5276
01:44:19,189 --> 01:44:19,199
which eventually needs to be minimized
 

5277
01:44:19,199 --> 01:44:21,669
which eventually needs to be minimized
in my training routine. So whenever one

5278
01:44:21,669 --> 01:44:21,679
in my training routine. So whenever one
 

5279
01:44:21,679 --> 01:44:23,510
in my training routine. So whenever one
batch is passed there is a loss which is

5280
01:44:23,510 --> 01:44:23,520
batch is passed there is a loss which is
 

5281
01:44:23,520 --> 01:44:26,149
batch is passed there is a loss which is
an accumulation of let's say if this is

5282
01:44:26,149 --> 01:44:26,159
an accumulation of let's say if this is
 

5283
01:44:26,159 --> 01:44:30,390
an accumulation of let's say if this is
L1 this is L2 this is L3 and L4 for

5284
01:44:30,390 --> 01:44:30,400
L1 this is L2 this is L3 and L4 for
 

5285
01:44:30,400 --> 01:44:33,590
L1 this is L2 this is L3 and L4 for
every batch my loss is L1 + L2 plus L3

5286
01:44:33,590 --> 01:44:33,600
every batch my loss is L1 + L2 plus L3
 

5287
01:44:33,600 --> 01:44:35,149
every batch my loss is L1 + L2 plus L3
plus

5288
01:44:35,149 --> 01:44:35,159
plus
 

5289
01:44:35,159 --> 01:44:38,229
plus
L4 so I get my loss for all the batches

5290
01:44:38,229 --> 01:44:38,239
L4 so I get my loss for all the batches
 

5291
01:44:38,239 --> 01:44:40,390
L4 so I get my loss for all the batches
then I do the back propagation etc and

5292
01:44:40,390 --> 01:44:40,400
then I do the back propagation etc and
 

5293
01:44:40,400 --> 01:44:41,830
then I do the back propagation etc and
that's how the training actually

5294
01:44:41,830 --> 01:44:41,840
that's how the training actually
 

5295
01:44:41,840 --> 01:44:47,390
that's how the training actually
proceeds

5296
01:44:47,390 --> 01:44:47,400

 

5297
01:44:47,400 --> 01:44:50,709

um yeah so until now what I've shown you

5298
01:44:50,709 --> 01:44:50,719
um yeah so until now what I've shown you
 

5299
01:44:50,719 --> 01:44:54,310
um yeah so until now what I've shown you
is exactly how the

5300
01:44:54,310 --> 01:44:54,320
is exactly how the
 

5301
01:44:54,320 --> 01:44:56,310
is exactly how the
uh how the transformer architecture is

5302
01:44:56,310 --> 01:44:56,320
uh how the transformer architecture is
 

5303
01:44:56,320 --> 01:44:58,149
uh how the transformer architecture is
assembled and let's see some details in

5304
01:44:58,149 --> 01:44:58,159
assembled and let's see some details in
 

5305
01:44:58,159 --> 01:44:59,750
assembled and let's see some details in
the code right now which we might have

5306
01:44:59,750 --> 01:44:59,760
the code right now which we might have
 

5307
01:44:59,760 --> 01:45:03,109
the code right now which we might have
missed and we'll cover some of that uh

5308
01:45:03,109 --> 01:45:03,119
missed and we'll cover some of that uh
 

5309
01:45:03,119 --> 01:45:04,790
missed and we'll cover some of that uh
in the future or in the rest of the

5310
01:45:04,790 --> 01:45:04,800
in the future or in the rest of the
 

5311
01:45:04,800 --> 01:45:06,390
in the future or in the rest of the
modules as well. So in the forward

5312
01:45:06,390 --> 01:45:06,400
modules as well. So in the forward
 

5313
01:45:06,400 --> 01:45:07,990
modules as well. So in the forward
method now we have token embeddings,

5314
01:45:07,990 --> 01:45:08,000
method now we have token embeddings,
 

5315
01:45:08,000 --> 01:45:09,510
method now we have token embeddings,
positional embeddings, we add them

5316
01:45:09,510 --> 01:45:09,520
positional embeddings, we add them
 

5317
01:45:09,520 --> 01:45:12,070
positional embeddings, we add them
together, we apply dropout, pass them

5318
01:45:12,070 --> 01:45:12,080
together, we apply dropout, pass them
 

5319
01:45:12,080 --> 01:45:14,070
together, we apply dropout, pass them
through through the transformer block.

5320
01:45:14,070 --> 01:45:14,080
through through the transformer block.
 

5321
01:45:14,080 --> 01:45:16,149
through through the transformer block.
In each transformer block which we have

5322
01:45:16,149 --> 01:45:16,159
In each transformer block which we have
 

5323
01:45:16,159 --> 01:45:17,590
In each transformer block which we have
there are multiple things which are

5324
01:45:17,590 --> 01:45:17,600
there are multiple things which are
 

5325
01:45:17,600 --> 01:45:19,910
there are multiple things which are
happening and that's defined in this

5326
01:45:19,910 --> 01:45:19,920
happening and that's defined in this
 

5327
01:45:19,920 --> 01:45:22,390
happening and that's defined in this
block. There is a layer normalization, a

5328
01:45:22,390 --> 01:45:22,400
block. There is a layer normalization, a
 

5329
01:45:22,400 --> 01:45:24,310
block. There is a layer normalization, a
causal self attention, another layer

5330
01:45:24,310 --> 01:45:24,320
causal self attention, another layer
 

5331
01:45:24,320 --> 01:45:26,310
causal self attention, another layer
normalization, a feed forward neural

5332
01:45:26,310 --> 01:45:26,320
normalization, a feed forward neural
 

5333
01:45:26,320 --> 01:45:29,629
normalization, a feed forward neural
network. So we go through all of these

5334
01:45:29,629 --> 01:45:29,639
network. So we go through all of these
 

5335
01:45:29,639 --> 01:45:32,629
network. So we go through all of these
blocks. Uh we go through all of these in

5336
01:45:32,629 --> 01:45:32,639
blocks. Uh we go through all of these in
 

5337
01:45:32,639 --> 01:45:34,390
blocks. Uh we go through all of these in
the in one transformer block. And

5338
01:45:34,390 --> 01:45:34,400
the in one transformer block. And
 

5339
01:45:34,400 --> 01:45:35,910
the in one transformer block. And
similarly there are multiple such

5340
01:45:35,910 --> 01:45:35,920
similarly there are multiple such
 

5341
01:45:35,920 --> 01:45:38,470
similarly there are multiple such
transformer blocks. When our input

5342
01:45:38,470 --> 01:45:38,480
transformer blocks. When our input
 

5343
01:45:38,480 --> 01:45:40,470
transformer blocks. When our input
embeddings come out of this they are now

5344
01:45:40,470 --> 01:45:40,480
embeddings come out of this they are now
 

5345
01:45:40,480 --> 01:45:42,470
embeddings come out of this they are now
context embeddings. And when they come

5346
01:45:42,470 --> 01:45:42,480
context embeddings. And when they come
 

5347
01:45:42,480 --> 01:45:44,070
context embeddings. And when they come
out of the transformer block when they

5348
01:45:44,070 --> 01:45:44,080
out of the transformer block when they
 

5349
01:45:44,080 --> 01:45:46,070
out of the transformer block when they
go through multiple transformer blocks

5350
01:45:46,070 --> 01:45:46,080
go through multiple transformer blocks
 

5351
01:45:46,080 --> 01:45:48,870
go through multiple transformer blocks
we pass them through one more uh layer

5352
01:45:48,870 --> 01:45:48,880
we pass them through one more uh layer
 

5353
01:45:48,880 --> 01:45:51,189
we pass them through one more uh layer
normalization layer and then we pass

5354
01:45:51,189 --> 01:45:51,199
normalization layer and then we pass
 

5355
01:45:51,199 --> 01:45:53,350
normalization layer and then we pass
them through the output neural network

5356
01:45:53,350 --> 01:45:53,360
them through the output neural network
 

5357
01:45:53,360 --> 01:45:55,510
them through the output neural network
which is also called as the output head.

5358
01:45:55,510 --> 01:45:55,520
which is also called as the output head.
 

5359
01:45:55,520 --> 01:45:57,590
which is also called as the output head.
That gives us the logits matrix. The

5360
01:45:57,590 --> 01:45:57,600
That gives us the logits matrix. The
 

5361
01:45:57,600 --> 01:46:00,070
That gives us the logits matrix. The
logits matrix when we look at the logit

5362
01:46:00,070 --> 01:46:00,080
logits matrix when we look at the logit
 

5363
01:46:00,080 --> 01:46:01,910
logits matrix when we look at the logit
matrix for every row we look at that

5364
01:46:01,910 --> 01:46:01,920
matrix for every row we look at that
 

5365
01:46:01,920 --> 01:46:03,750
matrix for every row we look at that
token ID which corresponds to the next

5366
01:46:03,750 --> 01:46:03,760
token ID which corresponds to the next
 

5367
01:46:03,760 --> 01:46:05,830
token ID which corresponds to the next
token prediction because that has the

5368
01:46:05,830 --> 01:46:05,840
token prediction because that has the
 

5369
01:46:05,840 --> 01:46:08,390
token prediction because that has the
maximum probability and then I compare

5370
01:46:08,390 --> 01:46:08,400
maximum probability and then I compare
 

5371
01:46:08,400 --> 01:46:10,189
maximum probability and then I compare
it with my actual

5372
01:46:10,189 --> 01:46:10,199
it with my actual
 

5373
01:46:10,199 --> 01:46:12,310
it with my actual
targets. That's how the loss is

5374
01:46:12,310 --> 01:46:12,320
targets. That's how the loss is
 

5375
01:46:12,320 --> 01:46:16,470
targets. That's how the loss is
calculated. So when I showed this L1, L2

5376
01:46:16,470 --> 01:46:16,480
calculated. So when I showed this L1, L2
 

5377
01:46:16,480 --> 01:46:19,590
calculated. So when I showed this L1, L2
um when I showed this L1, L2, L3 and L4

5378
01:46:19,590 --> 01:46:19,600
um when I showed this L1, L2, L3 and L4
 

5379
01:46:19,600 --> 01:46:21,669
um when I showed this L1, L2, L3 and L4
here actually that's the cross entropy

5380
01:46:21,669 --> 01:46:21,679
here actually that's the cross entropy
 

5381
01:46:21,679 --> 01:46:23,750
here actually that's the cross entropy
loss which is calculated and we'll see

5382
01:46:23,750 --> 01:46:23,760
loss which is calculated and we'll see
 

5383
01:46:23,760 --> 01:46:26,709
loss which is calculated and we'll see
that in a lot more detail in uh in the

5384
01:46:26,709 --> 01:46:26,719
that in a lot more detail in uh in the
 

5385
01:46:26,719 --> 01:46:28,550
that in a lot more detail in uh in the
next section actually. So the next

5386
01:46:28,550 --> 01:46:28,560
next section actually. So the next
 

5387
01:46:28,560 --> 01:46:30,310
next section actually. So the next
section after this point is going to be

5388
01:46:30,310 --> 01:46:30,320
section after this point is going to be
 

5389
01:46:30,320 --> 01:46:31,830
section after this point is going to be
the loss function. So we are going to

5390
01:46:31,830 --> 01:46:31,840
the loss function. So we are going to
 

5391
01:46:31,840 --> 01:46:33,550
the loss function. So we are going to
see that in a bit of

5392
01:46:33,550 --> 01:46:33,560
see that in a bit of
 

5393
01:46:33,560 --> 01:46:35,830
see that in a bit of
detail. So until now you would have

5394
01:46:35,830 --> 01:46:35,840
detail. So until now you would have
 

5395
01:46:35,840 --> 01:46:37,590
detail. So until now you would have
understood everything in this forward

5396
01:46:37,590 --> 01:46:37,600
understood everything in this forward
 

5397
01:46:37,600 --> 01:46:39,910
understood everything in this forward
method and in this class GPT. You have

5398
01:46:39,910 --> 01:46:39,920
method and in this class GPT. You have
 

5399
01:46:39,920 --> 01:46:42,870
method and in this class GPT. You have
understood class GPT, class block, class

5400
01:46:42,870 --> 01:46:42,880
understood class GPT, class block, class
 

5401
01:46:42,880 --> 01:46:46,550
understood class GPT, class block, class
MLP and class uh causal attention. There

5402
01:46:46,550 --> 01:46:46,560
MLP and class uh causal attention. There
 

5403
01:46:46,560 --> 01:46:48,390
MLP and class uh causal attention. There
are just some initializations which are

5404
01:46:48,390 --> 01:46:48,400
are just some initializations which are
 

5405
01:46:48,400 --> 01:46:50,390
are just some initializations which are
done at several places in this code

5406
01:46:50,390 --> 01:46:50,400
done at several places in this code
 

5407
01:46:50,400 --> 01:46:52,790
done at several places in this code
which I want to explain to you. So for

5408
01:46:52,790 --> 01:46:52,800
which I want to explain to you. So for
 

5409
01:46:52,800 --> 01:46:54,310
which I want to explain to you. So for
example, if you see there are these

5410
01:46:54,310 --> 01:46:54,320
example, if you see there are these
 

5411
01:46:54,320 --> 01:46:56,390
example, if you see there are these
initializations which are done. So

5412
01:46:56,390 --> 01:46:56,400
initializations which are done. So
 

5413
01:46:56,400 --> 01:46:58,709
initializations which are done. So
different modules have different

5414
01:46:58,709 --> 01:46:58,719
different modules have different
 

5415
01:46:58,719 --> 01:47:00,790
different modules have different
trainable parameters which are kind of

5416
01:47:00,790 --> 01:47:00,800
trainable parameters which are kind of
 

5417
01:47:00,800 --> 01:47:02,390
trainable parameters which are kind of
initialized in slightly different

5418
01:47:02,390 --> 01:47:02,400
initialized in slightly different
 

5419
01:47:02,400 --> 01:47:05,709
initialized in slightly different
manner. So for example, if you see over

5420
01:47:05,709 --> 01:47:05,719
manner. So for example, if you see over
 

5421
01:47:05,719 --> 01:47:08,070
manner. So for example, if you see over
here, here is a list of all the

5422
01:47:08,070 --> 01:47:08,080
here, here is a list of all the
 

5423
01:47:08,080 --> 01:47:09,910
here, here is a list of all the
trainable parameters which are usually

5424
01:47:09,910 --> 01:47:09,920
trainable parameters which are usually
 

5425
01:47:09,920 --> 01:47:11,910
trainable parameters which are usually
there, right? So the token embedding

5426
01:47:11,910 --> 01:47:11,920
there, right? So the token embedding
 

5427
01:47:11,920 --> 01:47:15,189
there, right? So the token embedding
layer has trainable parameters. The

5428
01:47:15,189 --> 01:47:15,199
layer has trainable parameters. The
 

5429
01:47:15,199 --> 01:47:17,470
layer has trainable parameters. The
positional embedding layer has trainable

5430
01:47:17,470 --> 01:47:17,480
positional embedding layer has trainable
 

5431
01:47:17,480 --> 01:47:19,830
positional embedding layer has trainable
parameters. Those are initialized as a

5432
01:47:19,830 --> 01:47:19,840
parameters. Those are initialized as a
 

5433
01:47:19,840 --> 01:47:22,310
parameters. Those are initialized as a
gshian with normal distribution and

5434
01:47:22,310 --> 01:47:22,320
gshian with normal distribution and
 

5435
01:47:22,320 --> 01:47:23,790
gshian with normal distribution and
standard deviation

5436
01:47:23,790 --> 01:47:23,800
standard deviation
 

5437
01:47:23,800 --> 01:47:26,390
standard deviation
0.02. Then the attention block has the

5438
01:47:26,390 --> 01:47:26,400
0.02. Then the attention block has the
 

5439
01:47:26,400 --> 01:47:28,629
0.02. Then the attention block has the
query keys and the value matrix. They

5440
01:47:28,629 --> 01:47:28,639
query keys and the value matrix. They
 

5441
01:47:28,639 --> 01:47:30,550
query keys and the value matrix. They
are also initialized as a goshian each

5442
01:47:30,550 --> 01:47:30,560
are also initialized as a goshian each
 

5443
01:47:30,560 --> 01:47:32,709
are also initialized as a goshian each
of these matrices. Then the attention

5444
01:47:32,709 --> 01:47:32,719
of these matrices. Then the attention
 

5445
01:47:32,719 --> 01:47:34,709
of these matrices. Then the attention
block has an output projection. Remember

5446
01:47:34,709 --> 01:47:34,719
block has an output projection. Remember
 

5447
01:47:34,719 --> 01:47:37,350
block has an output projection. Remember
I said that's optional but you can keep

5448
01:47:37,350 --> 01:47:37,360
I said that's optional but you can keep
 

5449
01:47:37,360 --> 01:47:39,189
I said that's optional but you can keep
that on. And that's initialized in a

5450
01:47:39,189 --> 01:47:39,199
that on. And that's initialized in a
 

5451
01:47:39,199 --> 01:47:41,270
that on. And that's initialized in a
slightly different manner just so that

5452
01:47:41,270 --> 01:47:41,280
slightly different manner just so that
 

5453
01:47:41,280 --> 01:47:43,030
slightly different manner just so that
the training process becomes a bit more

5454
01:47:43,030 --> 01:47:43,040
the training process becomes a bit more
 

5455
01:47:43,040 --> 01:47:46,470
the training process becomes a bit more
optimal. We divide it uh by square root

5456
01:47:46,470 --> 01:47:46,480
optimal. We divide it uh by square root
 

5457
01:47:46,480 --> 01:47:48,310
optimal. We divide it uh by square root
of two into the number of transform

5458
01:47:48,310 --> 01:47:48,320
of two into the number of transform
 

5459
01:47:48,320 --> 01:47:51,350
of two into the number of transform
transformer blocks which are there. Then

5460
01:47:51,350 --> 01:47:51,360
transformer blocks which are there. Then
 

5461
01:47:51,360 --> 01:47:53,830
transformer blocks which are there. Then
this is the feed forward neural network.

5462
01:47:53,830 --> 01:47:53,840
this is the feed forward neural network.
 

5463
01:47:53,840 --> 01:47:56,550
this is the feed forward neural network.
Uh the first layer which goes from 768

5464
01:47:56,550 --> 01:47:56,560
Uh the first layer which goes from 768
 

5465
01:47:56,560 --> 01:47:59,510
Uh the first layer which goes from 768
to 4x 768. We initialize the weights as

5466
01:47:59,510 --> 01:47:59,520
to 4x 768. We initialize the weights as
 

5467
01:47:59,520 --> 01:48:02,070
to 4x 768. We initialize the weights as
a goshian. And the output layer which is

5468
01:48:02,070 --> 01:48:02,080
a goshian. And the output layer which is
 

5469
01:48:02,080 --> 01:48:03,590
a goshian. And the output layer which is
also the projection layer. It's called

5470
01:48:03,590 --> 01:48:03,600
also the projection layer. It's called
 

5471
01:48:03,600 --> 01:48:05,870
also the projection layer. It's called
as the projection layer. Uh that's

5472
01:48:05,870 --> 01:48:05,880
as the projection layer. Uh that's
 

5473
01:48:05,880 --> 01:48:09,189
as the projection layer. Uh that's
initialized by a gshian. But we divide

5474
01:48:09,189 --> 01:48:09,199
initialized by a gshian. But we divide
 

5475
01:48:09,199 --> 01:48:11,990
initialized by a gshian. But we divide
uh 0.02 by again square root of 2 into

5476
01:48:11,990 --> 01:48:12,000
uh 0.02 by again square root of 2 into
 

5477
01:48:12,000 --> 01:48:13,669
uh 0.02 by again square root of 2 into
the number of transformer blocks which

5478
01:48:13,669 --> 01:48:13,679
the number of transformer blocks which
 

5479
01:48:13,679 --> 01:48:15,270
the number of transformer blocks which
is if it's 12 then it will be square

5480
01:48:15,270 --> 01:48:15,280
is if it's 12 then it will be square
 

5481
01:48:15,280 --> 01:48:17,990
is if it's 12 then it will be square
root of 24. Then the final layer

5482
01:48:17,990 --> 01:48:18,000
root of 24. Then the final layer
 

5483
01:48:18,000 --> 01:48:20,510
root of 24. Then the final layer
normalization again that has

5484
01:48:20,510 --> 01:48:20,520
normalization again that has
 

5485
01:48:20,520 --> 01:48:23,750
normalization again that has
no no initializations. and the final

5486
01:48:23,750 --> 01:48:23,760
no no initializations. and the final
 

5487
01:48:23,760 --> 01:48:25,669
no no initializations. and the final
output head which gets me my logits

5488
01:48:25,669 --> 01:48:25,679
output head which gets me my logits
 

5489
01:48:25,679 --> 01:48:27,990
output head which gets me my logits
matrix that is initialized with a gshian

5490
01:48:27,990 --> 01:48:28,000
matrix that is initialized with a gshian
 

5491
01:48:28,000 --> 01:48:31,310
matrix that is initialized with a gshian
of mean zero and standard deviation of

5492
01:48:31,310 --> 01:48:31,320
of mean zero and standard deviation of
 

5493
01:48:31,320 --> 01:48:35,109
of mean zero and standard deviation of
0.02. Now whenever now when you run this

5494
01:48:35,109 --> 01:48:35,119
0.02. Now whenever now when you run this
 

5495
01:48:35,119 --> 01:48:37,270
0.02. Now whenever now when you run this
code cell when you run this whole code

5496
01:48:37,270 --> 01:48:37,280
code cell when you run this whole code
 

5497
01:48:37,280 --> 01:48:39,590
code cell when you run this whole code
cell ultimately we are going to only use

5498
01:48:39,590 --> 01:48:39,600
cell ultimately we are going to only use
 

5499
01:48:39,600 --> 01:48:42,390
cell ultimately we are going to only use
class GPT which is my model and for that

5500
01:48:42,390 --> 01:48:42,400
class GPT which is my model and for that
 

5501
01:48:42,400 --> 01:48:44,950
class GPT which is my model and for that
we have to define a GPT configuration.

5502
01:48:44,950 --> 01:48:44,960
we have to define a GPT configuration.
 

5503
01:48:44,960 --> 01:48:46,709
we have to define a GPT configuration.
So the configuration which we are going

5504
01:48:46,709 --> 01:48:46,719
So the configuration which we are going
 

5505
01:48:46,719 --> 01:48:50,149
So the configuration which we are going
to use is vocabulary size of 50257.

5506
01:48:50,149 --> 01:48:50,159
to use is vocabulary size of 50257.
 

5507
01:48:50,159 --> 01:48:52,590
to use is vocabulary size of 50257.
Um then I'm going to use a block

5508
01:48:52,590 --> 01:48:52,600
Um then I'm going to use a block
 

5509
01:48:52,600 --> 01:48:56,629
Um then I'm going to use a block
size block size of 128 the embedding

5510
01:48:56,629 --> 01:48:56,639
size block size of 128 the embedding
 

5511
01:48:56,639 --> 01:49:00,470
size block size of 128 the embedding
dimensions of around 384. Um that's the

5512
01:49:00,470 --> 01:49:00,480
dimensions of around 384. Um that's the
 

5513
01:49:00,480 --> 01:49:02,910
dimensions of around 384. Um that's the
embedding size which we are going to

5514
01:49:02,910 --> 01:49:02,920
embedding size which we are going to
 

5515
01:49:02,920 --> 01:49:07,030
embedding size which we are going to
use and uh the dropout rate of 0.1. So

5516
01:49:07,030 --> 01:49:07,040
use and uh the dropout rate of 0.1. So
 

5517
01:49:07,040 --> 01:49:08,709
use and uh the dropout rate of 0.1. So
number of layers and number of heads

5518
01:49:08,709 --> 01:49:08,719
number of layers and number of heads
 

5519
01:49:08,719 --> 01:49:10,709
number of layers and number of heads
there is a distinction between these.

5520
01:49:10,709 --> 01:49:10,719
there is a distinction between these.
 

5521
01:49:10,719 --> 01:49:12,629
there is a distinction between these.
Number of layers is the number of

5522
01:49:12,629 --> 01:49:12,639
Number of layers is the number of
 

5523
01:49:12,639 --> 01:49:15,030
Number of layers is the number of
transformer blocks which I have. So

5524
01:49:15,030 --> 01:49:15,040
transformer blocks which I have. So
 

5525
01:49:15,040 --> 01:49:16,550
transformer blocks which I have. So
remember here I told you that

5526
01:49:16,550 --> 01:49:16,560
remember here I told you that
 

5527
01:49:16,560 --> 01:49:18,870
remember here I told you that
transformer a number of transformers

5528
01:49:18,870 --> 01:49:18,880
transformer a number of transformers
 

5529
01:49:18,880 --> 01:49:22,950
transformer a number of transformers
will be uh added here. So here we have

5530
01:49:22,950 --> 01:49:22,960
will be uh added here. So here we have
 

5531
01:49:22,960 --> 01:49:24,790
will be uh added here. So here we have
to decide the number of transformers and

5532
01:49:24,790 --> 01:49:24,800
to decide the number of transformers and
 

5533
01:49:24,800 --> 01:49:26,709
to decide the number of transformers and
that's equal to six. So here I'll just

5534
01:49:26,709 --> 01:49:26,719
that's equal to six. So here I'll just
 

5535
01:49:26,719 --> 01:49:29,790
that's equal to six. So here I'll just
mention

5536
01:49:29,790 --> 01:49:29,800

 

5537
01:49:29,800 --> 01:49:34,030

uh n layer equal to number of

5538
01:49:34,030 --> 01:49:34,040
uh n layer equal to number of
 

5539
01:49:34,040 --> 01:49:36,790
uh n layer equal to number of
transformers. Okay number of transformer

5540
01:49:36,790 --> 01:49:36,800
transformers. Okay number of transformer
 

5541
01:49:36,800 --> 01:49:39,030
transformers. Okay number of transformer
blocks and n heads is the number of

5542
01:49:39,030 --> 01:49:39,040
blocks and n heads is the number of
 

5543
01:49:39,040 --> 01:49:41,750
blocks and n heads is the number of
attention heads in each block.

5544
01:49:41,750 --> 01:49:41,760
attention heads in each block.
 

5545
01:49:41,760 --> 01:49:43,830
attention heads in each block.
So we did not talk about attention heads

5546
01:49:43,830 --> 01:49:43,840
So we did not talk about attention heads
 

5547
01:49:43,840 --> 01:49:45,590
So we did not talk about attention heads
today. But essentially when multi head

5548
01:49:45,590 --> 01:49:45,600
today. But essentially when multi head
 

5549
01:49:45,600 --> 01:49:47,430
today. But essentially when multi head
attention is computed. There are

5550
01:49:47,430 --> 01:49:47,440
attention is computed. There are
 

5551
01:49:47,440 --> 01:49:49,510
attention is computed. There are
multiple attention heads each computing

5552
01:49:49,510 --> 01:49:49,520
multiple attention heads each computing
 

5553
01:49:49,520 --> 01:49:51,910
multiple attention heads each computing
a separate attention score so that we

5554
01:49:51,910 --> 01:49:51,920
a separate attention score so that we
 

5555
01:49:51,920 --> 01:49:53,669
a separate attention score so that we
can capture multiple perspectives and

5556
01:49:53,669 --> 01:49:53,679
can capture multiple perspectives and
 

5557
01:49:53,679 --> 01:49:55,990
can capture multiple perspectives and
that's equal to six. So you can think of

5558
01:49:55,990 --> 01:49:56,000
that's equal to six. So you can think of
 

5559
01:49:56,000 --> 01:49:58,149
that's equal to six. So you can think of
this fact that in each transformer block

5560
01:49:58,149 --> 01:49:58,159
this fact that in each transformer block
 

5561
01:49:58,159 --> 01:50:00,229
this fact that in each transformer block
there is a multiate attention and the

5562
01:50:00,229 --> 01:50:00,239
there is a multiate attention and the
 

5563
01:50:00,239 --> 01:50:02,629
there is a multiate attention and the
number of attention blocks which we have

5564
01:50:02,629 --> 01:50:02,639
number of attention blocks which we have
 

5565
01:50:02,639 --> 01:50:05,629
number of attention blocks which we have
are now equal to

5566
01:50:05,629 --> 01:50:05,639
are now equal to
 

5567
01:50:05,639 --> 01:50:08,149
are now equal to
six. And here also we have six

5568
01:50:08,149 --> 01:50:08,159
six. And here also we have six
 

5569
01:50:08,159 --> 01:50:09,750
six. And here also we have six
transformer blocks. Right? So let me

5570
01:50:09,750 --> 01:50:09,760
transformer blocks. Right? So let me
 

5571
01:50:09,760 --> 01:50:12,310
transformer blocks. Right? So let me
replace this with actually six since we

5572
01:50:12,310 --> 01:50:12,320
replace this with actually six since we
 

5573
01:50:12,320 --> 01:50:14,629
replace this with actually six since we
have six transformer. So six transformer

5574
01:50:14,629 --> 01:50:14,639
have six transformer. So six transformer
 

5575
01:50:14,639 --> 01:50:16,550
have six transformer. So six transformer
blocks each have six attention heads. So

5576
01:50:16,550 --> 01:50:16,560
blocks each have six attention heads. So
 

5577
01:50:16,560 --> 01:50:18,990
blocks each have six attention heads. So
overall there will be 36 attention

5578
01:50:18,990 --> 01:50:19,000
overall there will be 36 attention
 

5579
01:50:19,000 --> 01:50:21,510
overall there will be 36 attention
heads. So this is the configuration the

5580
01:50:21,510 --> 01:50:21,520
heads. So this is the configuration the
 

5581
01:50:21,520 --> 01:50:23,350
heads. So this is the configuration the
n embedding is the embedding dimension

5582
01:50:23,350 --> 01:50:23,360
n embedding is the embedding dimension
 

5583
01:50:23,360 --> 01:50:25,430
n embedding is the embedding dimension
which we have right. So at the start I

5584
01:50:25,430 --> 01:50:25,440
which we have right. So at the start I
 

5585
01:50:25,440 --> 01:50:27,030
which we have right. So at the start I
told you that we have to decide an

5586
01:50:27,030 --> 01:50:27,040
told you that we have to decide an
 

5587
01:50:27,040 --> 01:50:28,709
told you that we have to decide an
embedding dimension. That embedding

5588
01:50:28,709 --> 01:50:28,719
embedding dimension. That embedding
 

5589
01:50:28,719 --> 01:50:32,229
embedding dimension. That embedding
dimension is now going to be 384. Okay.

5590
01:50:32,229 --> 01:50:32,239
dimension is now going to be 384. Okay.
 

5591
01:50:32,239 --> 01:50:33,990
dimension is now going to be 384. Okay.
So this is the embedding dimension which

5592
01:50:33,990 --> 01:50:34,000
So this is the embedding dimension which
 

5593
01:50:34,000 --> 01:50:37,109
So this is the embedding dimension which
is there. Um and then I just define my

5594
01:50:37,109 --> 01:50:37,119
is there. Um and then I just define my
 

5595
01:50:37,119 --> 01:50:39,590
is there. Um and then I just define my
model as GPT. So this GPT which is

5596
01:50:39,590 --> 01:50:39,600
model as GPT. So this GPT which is
 

5597
01:50:39,600 --> 01:50:42,310
model as GPT. So this GPT which is
mentioned here is the GPT class which we

5598
01:50:42,310 --> 01:50:42,320
mentioned here is the GPT class which we
 

5599
01:50:42,320 --> 01:50:46,830
mentioned here is the GPT class which we
have defined over here. My GPT class

5600
01:50:46,830 --> 01:50:46,840
have defined over here. My GPT class
 

5601
01:50:46,840 --> 01:50:49,750
have defined over here. My GPT class
um this GPT class which takes my input

5602
01:50:49,750 --> 01:50:49,760
um this GPT class which takes my input
 

5603
01:50:49,760 --> 01:50:53,350
um this GPT class which takes my input
embedding matrix and produces um a loss

5604
01:50:53,350 --> 01:50:53,360
embedding matrix and produces um a loss
 

5605
01:50:53,360 --> 01:50:55,510
embedding matrix and produces um a loss
which is based on the logit's output and

5606
01:50:55,510 --> 01:50:55,520
which is based on the logit's output and
 

5607
01:50:55,520 --> 01:50:57,390
which is based on the logit's output and
my target

5608
01:50:57,390 --> 01:50:57,400
my target
 

5609
01:50:57,400 --> 01:51:00,229
my target
outputs. So this is the model GPT and I

5610
01:51:00,229 --> 01:51:00,239
outputs. So this is the model GPT and I
 

5611
01:51:00,239 --> 01:51:01,910
outputs. So this is the model GPT and I
have to specify this configuration as

5612
01:51:01,910 --> 01:51:01,920
have to specify this configuration as
 

5613
01:51:01,920 --> 01:51:05,030
have to specify this configuration as
has been mentioned over here. Now what I

5614
01:51:05,030 --> 01:51:05,040
has been mentioned over here. Now what I
 

5615
01:51:05,040 --> 01:51:06,629
has been mentioned over here. Now what I
I'm going to do is that in the next

5616
01:51:06,629 --> 01:51:06,639
I'm going to do is that in the next
 

5617
01:51:06,639 --> 01:51:08,390
I'm going to do is that in the next
section I'm going to talk a bit more

5618
01:51:08,390 --> 01:51:08,400
section I'm going to talk a bit more
 

5619
01:51:08,400 --> 01:51:10,070
section I'm going to talk a bit more
about the loss because that's very

5620
01:51:10,070 --> 01:51:10,080
about the loss because that's very
 

5621
01:51:10,080 --> 01:51:12,310
about the loss because that's very
important to understand. In this section

5622
01:51:12,310 --> 01:51:12,320
important to understand. In this section
 

5623
01:51:12,320 --> 01:51:14,070
important to understand. In this section
four, the section four was the most

5624
01:51:14,070 --> 01:51:14,080
four, the section four was the most
 

5625
01:51:14,080 --> 01:51:15,590
four, the section four was the most
important section where we had to

5626
01:51:15,590 --> 01:51:15,600
important section where we had to
 

5627
01:51:15,600 --> 01:51:17,990
important section where we had to
understand the different steps of how

5628
01:51:17,990 --> 01:51:18,000
understand the different steps of how
 

5629
01:51:18,000 --> 01:51:20,629
understand the different steps of how
essentially the input the input uh

5630
01:51:20,629 --> 01:51:20,639
essentially the input the input uh
 

5631
01:51:20,639 --> 01:51:22,870
essentially the input the input uh
sequence goes through the entire

5632
01:51:22,870 --> 01:51:22,880
sequence goes through the entire
 

5633
01:51:22,880 --> 01:51:24,709
sequence goes through the entire
transformer architecture. the input the

5634
01:51:24,709 --> 01:51:24,719
transformer architecture. the input the
 

5635
01:51:24,719 --> 01:51:26,870
transformer architecture. the input the
processor and the output and at the

5636
01:51:26,870 --> 01:51:26,880
processor and the output and at the
 

5637
01:51:26,880 --> 01:51:29,990
processor and the output and at the
output we get this logits we get this

5638
01:51:29,990 --> 01:51:30,000
output we get this logits we get this
 

5639
01:51:30,000 --> 01:51:32,310
output we get this logits we get this
logits matrix and that can be used for

5640
01:51:32,310 --> 01:51:32,320
logits matrix and that can be used for
 

5641
01:51:32,320 --> 01:51:34,390
logits matrix and that can be used for
next token prediction task and compare

5642
01:51:34,390 --> 01:51:34,400
next token prediction task and compare
 

5643
01:51:34,400 --> 01:51:36,229
next token prediction task and compare
with the target next tokens and that's

5644
01:51:36,229 --> 01:51:36,239
with the target next tokens and that's
 

5645
01:51:36,239 --> 01:51:37,950
with the target next tokens and that's
how the loss function is

5646
01:51:37,950 --> 01:51:37,960
how the loss function is
 

5647
01:51:37,960 --> 01:51:40,310
how the loss function is
determined this is the whole workflow of

5648
01:51:40,310 --> 01:51:40,320
determined this is the whole workflow of
 

5649
01:51:40,320 --> 01:51:42,870
determined this is the whole workflow of
how the LLM architecture operates and I

5650
01:51:42,870 --> 01:51:42,880
how the LLM architecture operates and I
 

5651
01:51:42,880 --> 01:51:45,109
how the LLM architecture operates and I
hope you have understood how it maps

5652
01:51:45,109 --> 01:51:45,119
hope you have understood how it maps
 

5653
01:51:45,119 --> 01:51:47,669
hope you have understood how it maps
exactly to the code I really encourage

5654
01:51:47,669 --> 01:51:47,679
exactly to the code I really encourage
 

5655
01:51:47,679 --> 01:51:49,589
exactly to the code I really encourage
you to take a look at the code in in

5656
01:51:49,589 --> 01:51:49,599
you to take a look at the code in in
 

5657
01:51:49,599 --> 01:51:51,350
you to take a look at the code in in
much more detail after going through

5658
01:51:51,350 --> 01:51:51,360
much more detail after going through
 

5659
01:51:51,360 --> 01:51:53,430
much more detail after going through
this lecture but I've given you enough

5660
01:51:53,430 --> 01:51:53,440
this lecture but I've given you enough
 

5661
01:51:53,440 --> 01:51:55,189
this lecture but I've given you enough
starting point so that you'll understand

5662
01:51:55,189 --> 01:51:55,199
starting point so that you'll understand
 

5663
01:51:55,199 --> 01:51:56,390
starting point so that you'll understand
many different things which are

5664
01:51:56,390 --> 01:51:56,400
many different things which are
 

5665
01:51:56,400 --> 01:51:59,109
many different things which are
happening in the code. Now what we'll do

5666
01:51:59,109 --> 01:51:59,119
happening in the code. Now what we'll do
 

5667
01:51:59,119 --> 01:52:01,030
happening in the code. Now what we'll do
is that we'll come to the next step

5668
01:52:01,030 --> 01:52:01,040
is that we'll come to the next step
 

5669
01:52:01,040 --> 01:52:03,830
is that we'll come to the next step
which is defining the loss function. It

5670
01:52:03,830 --> 01:52:03,840
which is defining the loss function. It
 

5671
01:52:03,840 --> 01:52:06,310
which is defining the loss function. It
is very important for us to understand

5672
01:52:06,310 --> 01:52:06,320
is very important for us to understand
 

5673
01:52:06,320 --> 01:52:08,149
is very important for us to understand
what exactly happens in the small

5674
01:52:08,149 --> 01:52:08,159
what exactly happens in the small
 

5675
01:52:08,159 --> 01:52:10,870
what exactly happens in the small
language model loss function because

5676
01:52:10,870 --> 01:52:10,880
language model loss function because
 

5677
01:52:10,880 --> 01:52:14,070
language model loss function because
here is where we'll ultimately try to

5678
01:52:14,070 --> 01:52:14,080
here is where we'll ultimately try to
 

5679
01:52:14,080 --> 01:52:16,149
here is where we'll ultimately try to
tell the language model that look you're

5680
01:52:16,149 --> 01:52:16,159
tell the language model that look you're
 

5681
01:52:16,159 --> 01:52:18,550
tell the language model that look you're
making a mistake and you need to improve

5682
01:52:18,550 --> 01:52:18,560
making a mistake and you need to improve
 

5683
01:52:18,560 --> 01:52:21,189
making a mistake and you need to improve
upon this. Right? We have already looked

5684
01:52:21,189 --> 01:52:21,199
upon this. Right? We have already looked
 

5685
01:52:21,199 --> 01:52:23,510
upon this. Right? We have already looked
a bit at the loss in the previous part

5686
01:52:23,510 --> 01:52:23,520
a bit at the loss in the previous part
 

5687
01:52:23,520 --> 01:52:26,870
a bit at the loss in the previous part
where we saw that once the logits matrix

5688
01:52:26,870 --> 01:52:26,880
where we saw that once the logits matrix
 

5689
01:52:26,880 --> 01:52:28,310
where we saw that once the logits matrix
is obtained. Let's say here we get the

5690
01:52:28,310 --> 01:52:28,320
is obtained. Let's say here we get the
 

5691
01:52:28,320 --> 01:52:30,950
is obtained. Let's say here we get the
logits matrix. Right? After this point,

5692
01:52:30,950 --> 01:52:30,960
logits matrix. Right? After this point,
 

5693
01:52:30,960 --> 01:52:34,070
logits matrix. Right? After this point,
we look at every token and we look at

5694
01:52:34,070 --> 01:52:34,080
we look at every token and we look at
 

5695
01:52:34,080 --> 01:52:36,310
we look at every token and we look at
the token ID which corresponds to the

5696
01:52:36,310 --> 01:52:36,320
the token ID which corresponds to the
 

5697
01:52:36,320 --> 01:52:38,790
the token ID which corresponds to the
highest probability and we decode the

5698
01:52:38,790 --> 01:52:38,800
highest probability and we decode the
 

5699
01:52:38,800 --> 01:52:41,189
highest probability and we decode the
output corresponding to that token ID.

5700
01:52:41,189 --> 01:52:41,199
output corresponding to that token ID.
 

5701
01:52:41,199 --> 01:52:43,030
output corresponding to that token ID.
We compare it with the target and that's

5702
01:52:43,030 --> 01:52:43,040
We compare it with the target and that's
 

5703
01:52:43,040 --> 01:52:44,589
We compare it with the target and that's
how we get the loss

5704
01:52:44,589 --> 01:52:44,599
how we get the loss
 

5705
01:52:44,599 --> 01:52:46,790
how we get the loss
function. But now we are going to look

5706
01:52:46,790 --> 01:52:46,800
function. But now we are going to look
 

5707
01:52:46,800 --> 01:52:49,669
function. But now we are going to look
at this same step in a bit more detail

5708
01:52:49,669 --> 01:52:49,679
at this same step in a bit more detail
 

5709
01:52:49,679 --> 01:52:51,910
at this same step in a bit more detail
so that you can get an exact clarity of

5710
01:52:51,910 --> 01:52:51,920
so that you can get an exact clarity of
 

5711
01:52:51,920 --> 01:52:53,669
so that you can get an exact clarity of
what exactly is happening in the loss

5712
01:52:53,669 --> 01:52:53,679
what exactly is happening in the loss
 

5713
01:52:53,679 --> 01:52:56,149
what exactly is happening in the loss
function. So let's say we have the

5714
01:52:56,149 --> 01:52:56,159
function. So let's say we have the
 

5715
01:52:56,159 --> 01:52:57,830
function. So let's say we have the
batches which look something like this,

5716
01:52:57,830 --> 01:52:57,840
batches which look something like this,
 

5717
01:52:57,840 --> 01:52:59,669
batches which look something like this,
right? So this is my first batch and

5718
01:52:59,669 --> 01:52:59,679
right? So this is my first batch and
 

5719
01:52:59,679 --> 01:53:02,229
right? So this is my first batch and
let's say that these four tokens are my

5720
01:53:02,229 --> 01:53:02,239
let's say that these four tokens are my
 

5721
01:53:02,239 --> 01:53:05,270
let's say that these four tokens are my
input X1 and these are my target which

5722
01:53:05,270 --> 01:53:05,280
input X1 and these are my target which
 

5723
01:53:05,280 --> 01:53:08,070
input X1 and these are my target which
is equal to Y1. Okay. So what we'll

5724
01:53:08,070 --> 01:53:08,080
is equal to Y1. Okay. So what we'll
 

5725
01:53:08,080 --> 01:53:10,550
is equal to Y1. Okay. So what we'll
first do is that we'll first take X1

5726
01:53:10,550 --> 01:53:10,560
first do is that we'll first take X1
 

5727
01:53:10,560 --> 01:53:12,470
first do is that we'll first take X1
which is this and we'll pass it through

5728
01:53:12,470 --> 01:53:12,480
which is this and we'll pass it through
 

5729
01:53:12,480 --> 01:53:14,870
which is this and we'll pass it through
the transformer model. We'll pass it

5730
01:53:14,870 --> 01:53:14,880
the transformer model. We'll pass it
 

5731
01:53:14,880 --> 01:53:16,629
the transformer model. We'll pass it
through the input the processor and the

5732
01:53:16,629 --> 01:53:16,639
through the input the processor and the
 

5733
01:53:16,639 --> 01:53:18,470
through the input the processor and the
output. So let me bring this over here

5734
01:53:18,470 --> 01:53:18,480
output. So let me bring this over here
 

5735
01:53:18,480 --> 01:53:20,750
output. So let me bring this over here
so that

5736
01:53:20,750 --> 01:53:20,760
so that
 

5737
01:53:20,760 --> 01:53:23,430
so that
um you recollect what we have seen in

5738
01:53:23,430 --> 01:53:23,440
um you recollect what we have seen in
 

5739
01:53:23,440 --> 01:53:25,189
um you recollect what we have seen in
the previous lecture. So let me copy

5740
01:53:25,189 --> 01:53:25,199
the previous lecture. So let me copy
 

5741
01:53:25,199 --> 01:53:28,470
the previous lecture. So let me copy
paste this and u I'll bring it over

5742
01:53:28,470 --> 01:53:28,480
paste this and u I'll bring it over
 

5743
01:53:28,480 --> 01:53:31,030
paste this and u I'll bring it over
here. Okay. So what we are going to do

5744
01:53:31,030 --> 01:53:31,040
here. Okay. So what we are going to do
 

5745
01:53:31,040 --> 01:53:32,870
here. Okay. So what we are going to do
is that we have this input text right

5746
01:53:32,870 --> 01:53:32,880
is that we have this input text right
 

5747
01:53:32,880 --> 01:53:35,270
is that we have this input text right
which is I had always thought. I had

5748
01:53:35,270 --> 01:53:35,280
which is I had always thought. I had
 

5749
01:53:35,280 --> 01:53:37,510
which is I had always thought. I had
always thought we'll pass it through

5750
01:53:37,510 --> 01:53:37,520
always thought we'll pass it through
 

5751
01:53:37,520 --> 01:53:40,149
always thought we'll pass it through
first the input layer then it will go

5752
01:53:40,149 --> 01:53:40,159
first the input layer then it will go
 

5753
01:53:40,159 --> 01:53:42,229
first the input layer then it will go
through the transformer block and then

5754
01:53:42,229 --> 01:53:42,239
through the transformer block and then
 

5755
01:53:42,239 --> 01:53:44,070
through the transformer block and then
when it comes out of the transformer

5756
01:53:44,070 --> 01:53:44,080
when it comes out of the transformer
 

5757
01:53:44,080 --> 01:53:46,030
when it comes out of the transformer
block we essentially have this logits

5758
01:53:46,030 --> 01:53:46,040
block we essentially have this logits
 

5759
01:53:46,040 --> 01:53:50,149
block we essentially have this logits
matrix and try to think about the

5760
01:53:50,149 --> 01:53:50,159
matrix and try to think about the
 

5761
01:53:50,159 --> 01:53:51,990
matrix and try to think about the
uh try to think about the size of this

5762
01:53:51,990 --> 01:53:52,000
uh try to think about the size of this
 

5763
01:53:52,000 --> 01:53:55,669
uh try to think about the size of this
logits matrix right? If you have four if

5764
01:53:55,669 --> 01:53:55,679
logits matrix right? If you have four if
 

5765
01:53:55,679 --> 01:53:57,350
logits matrix right? If you have four if
you have four tokens which go as an

5766
01:53:57,350 --> 01:53:57,360
you have four tokens which go as an
 

5767
01:53:57,360 --> 01:53:59,030
you have four tokens which go as an
input they are first converted into an

5768
01:53:59,030 --> 01:53:59,040
input they are first converted into an
 

5769
01:53:59,040 --> 01:54:01,109
input they are first converted into an
embedding dimension of let's say 768

5770
01:54:01,109 --> 01:54:01,119
embedding dimension of let's say 768
 

5771
01:54:01,119 --> 01:54:03,189
embedding dimension of let's say 768
which we had seen before but once they

5772
01:54:03,189 --> 01:54:03,199
which we had seen before but once they
 

5773
01:54:03,199 --> 01:54:05,430
which we had seen before but once they
come out of the transformer block the

5774
01:54:05,430 --> 01:54:05,440
come out of the transformer block the
 

5775
01:54:05,440 --> 01:54:07,189
come out of the transformer block the
logits essentially look something like

5776
01:54:07,189 --> 01:54:07,199
logits essentially look something like
 

5777
01:54:07,199 --> 01:54:09,189
logits essentially look something like
this we still have these four tokens I

5778
01:54:09,189 --> 01:54:09,199
this we still have these four tokens I
 

5779
01:54:09,199 --> 01:54:11,350
this we still have these four tokens I
had always thought but now the

5780
01:54:11,350 --> 01:54:11,360
had always thought but now the
 

5781
01:54:11,360 --> 01:54:13,950
had always thought but now the
dimensions of each of them are

5782
01:54:13,950 --> 01:54:13,960
dimensions of each of them are
 

5783
01:54:13,960 --> 01:54:16,870
dimensions of each of them are
50257 which can be the vocabulary size

5784
01:54:16,870 --> 01:54:16,880
50257 which can be the vocabulary size
 

5785
01:54:16,880 --> 01:54:19,350
50257 which can be the vocabulary size
or whatever vocabulary size we have. So

5786
01:54:19,350 --> 01:54:19,360
or whatever vocabulary size we have. So
 

5787
01:54:19,360 --> 01:54:22,070
or whatever vocabulary size we have. So
if you take a look over here, this is

5788
01:54:22,070 --> 01:54:22,080
if you take a look over here, this is
 

5789
01:54:22,080 --> 01:54:24,470
if you take a look over here, this is
what we have seen previously also the

5790
01:54:24,470 --> 01:54:24,480
what we have seen previously also the
 

5791
01:54:24,480 --> 01:54:26,950
what we have seen previously also the
size or the dimensions of every token

5792
01:54:26,950 --> 01:54:26,960
size or the dimensions of every token
 

5793
01:54:26,960 --> 01:54:29,589
size or the dimensions of every token
are now equal to the vocabulary size.

5794
01:54:29,589 --> 01:54:29,599
are now equal to the vocabulary size.
 

5795
01:54:29,599 --> 01:54:32,149
are now equal to the vocabulary size.
That's the first step which is uh which

5796
01:54:32,149 --> 01:54:32,159
That's the first step which is uh which
 

5797
01:54:32,159 --> 01:54:34,470
That's the first step which is uh which
is done to calculate the loss. Then what

5798
01:54:34,470 --> 01:54:34,480
is done to calculate the loss. Then what
 

5799
01:54:34,480 --> 01:54:36,870
is done to calculate the loss. Then what
we do after this point here is that

5800
01:54:36,870 --> 01:54:36,880
we do after this point here is that
 

5801
01:54:36,880 --> 01:54:38,709
we do after this point here is that
essentially we want to convert every

5802
01:54:38,709 --> 01:54:38,719
essentially we want to convert every
 

5803
01:54:38,719 --> 01:54:40,390
essentially we want to convert every
row. So let's say this is row number

5804
01:54:40,390 --> 01:54:40,400
row. So let's say this is row number
 

5805
01:54:40,400 --> 01:54:42,950
row. So let's say this is row number
one. Uh let's say this is row number

5806
01:54:42,950 --> 01:54:42,960
one. Uh let's say this is row number
 

5807
01:54:42,960 --> 01:54:46,310
one. Uh let's say this is row number
two. This is my row number three. And

5808
01:54:46,310 --> 01:54:46,320
two. This is my row number three. And
 

5809
01:54:46,320 --> 01:54:48,950
two. This is my row number three. And
let's say this is my row number four. I

5810
01:54:48,950 --> 01:54:48,960
let's say this is my row number four. I
 

5811
01:54:48,960 --> 01:54:51,189
let's say this is my row number four. I
want to convert every row into a vector

5812
01:54:51,189 --> 01:54:51,199
want to convert every row into a vector
 

5813
01:54:51,199 --> 01:54:53,750
want to convert every row into a vector
of probabilities which means that I want

5814
01:54:53,750 --> 01:54:53,760
of probabilities which means that I want
 

5815
01:54:53,760 --> 01:54:55,910
of probabilities which means that I want
every element I want each row to sum up

5816
01:54:55,910 --> 01:54:55,920
every element I want each row to sum up
 

5817
01:54:55,920 --> 01:54:58,070
every element I want each row to sum up
to one. So if I consider all the

5818
01:54:58,070 --> 01:54:58,080
to one. So if I consider all the
 

5819
01:54:58,080 --> 01:55:00,229
to one. So if I consider all the
elements of row one and I sum them up I

5820
01:55:00,229 --> 01:55:00,239
elements of row one and I sum them up I
 

5821
01:55:00,239 --> 01:55:02,390
elements of row one and I sum them up I
want their summation to be one so that I

5822
01:55:02,390 --> 01:55:02,400
want their summation to be one so that I
 

5823
01:55:02,400 --> 01:55:04,149
want their summation to be one so that I
can give a probabilistic meaning to

5824
01:55:04,149 --> 01:55:04,159
can give a probabilistic meaning to
 

5825
01:55:04,159 --> 01:55:04,990
can give a probabilistic meaning to
every

5826
01:55:04,990 --> 01:55:05,000
every
 

5827
01:55:05,000 --> 01:55:07,750
every
token. To do that what I do is I apply

5828
01:55:07,750 --> 01:55:07,760
token. To do that what I do is I apply
 

5829
01:55:07,760 --> 01:55:09,990
token. To do that what I do is I apply
the softmax to this logits matrix and

5830
01:55:09,990 --> 01:55:10,000
the softmax to this logits matrix and
 

5831
01:55:10,000 --> 01:55:11,870
the softmax to this logits matrix and
then essentially I get a vector of

5832
01:55:11,870 --> 01:55:11,880
then essentially I get a vector of
 

5833
01:55:11,880 --> 01:55:14,310
then essentially I get a vector of
probabilities. Remember what softmax

5834
01:55:14,310 --> 01:55:14,320
probabilities. Remember what softmax
 

5835
01:55:14,320 --> 01:55:15,830
probabilities. Remember what softmax
essentially does is that it replaces

5836
01:55:15,830 --> 01:55:15,840
essentially does is that it replaces
 

5837
01:55:15,840 --> 01:55:18,070
essentially does is that it replaces
every element with let's say e to x

5838
01:55:18,070 --> 01:55:18,080
every element with let's say e to x
 

5839
01:55:18,080 --> 01:55:20,870
every element with let's say e to x
which is that element multiplied by e to

5840
01:55:20,870 --> 01:55:20,880
which is that element multiplied by e to
 

5841
01:55:20,880 --> 01:55:24,229
which is that element multiplied by e to
x1 plus e to x2 etc for a particular

5842
01:55:24,229 --> 01:55:24,239
x1 plus e to x2 etc for a particular
 

5843
01:55:24,239 --> 01:55:26,709
x1 plus e to x2 etc for a particular
row. So it ensures that all the elements

5844
01:55:26,709 --> 01:55:26,719
row. So it ensures that all the elements
 

5845
01:55:26,719 --> 01:55:30,229
row. So it ensures that all the elements
lie within 0 to 1 and they sum to one.

5846
01:55:30,229 --> 01:55:30,239
lie within 0 to 1 and they sum to one.
 

5847
01:55:30,239 --> 01:55:33,270
lie within 0 to 1 and they sum to one.
So now every row can be interpreted in a

5848
01:55:33,270 --> 01:55:33,280
So now every row can be interpreted in a
 

5849
01:55:33,280 --> 01:55:35,109
So now every row can be interpreted in a
probabilistic sense. So you can look at

5850
01:55:35,109 --> 01:55:35,119
probabilistic sense. So you can look at
 

5851
01:55:35,119 --> 01:55:38,149
probabilistic sense. So you can look at
every row and try to find that token ID

5852
01:55:38,149 --> 01:55:38,159
every row and try to find that token ID
 

5853
01:55:38,159 --> 01:55:40,149
every row and try to find that token ID
token index which has the maximum

5854
01:55:40,149 --> 01:55:40,159
token index which has the maximum
 

5855
01:55:40,159 --> 01:55:42,550
token index which has the maximum
probability. So that will be the decoded

5856
01:55:42,550 --> 01:55:42,560
probability. So that will be the decoded
 

5857
01:55:42,560 --> 01:55:45,030
probability. So that will be the decoded
next token. So for example, if this

5858
01:55:45,030 --> 01:55:45,040
next token. So for example, if this
 

5859
01:55:45,040 --> 01:55:47,589
next token. So for example, if this
token ID has the maximum probability and

5860
01:55:47,589 --> 01:55:47,599
token ID has the maximum probability and
 

5861
01:55:47,599 --> 01:55:50,390
token ID has the maximum probability and
if this is token ID 35, that might

5862
01:55:50,390 --> 01:55:50,400
if this is token ID 35, that might
 

5863
01:55:50,400 --> 01:55:52,589
if this is token ID 35, that might
correspond to a

5864
01:55:52,589 --> 01:55:52,599
correspond to a
 

5865
01:55:52,599 --> 01:55:55,830
correspond to a
decoded subword of AC. So we go back to

5866
01:55:55,830 --> 01:55:55,840
decoded subword of AC. So we go back to
 

5867
01:55:55,840 --> 01:55:57,510
decoded subword of AC. So we go back to
the vocabulary. Now we look at that

5868
01:55:57,510 --> 01:55:57,520
the vocabulary. Now we look at that
 

5869
01:55:57,520 --> 01:56:00,149
the vocabulary. Now we look at that
token ID and get the decoded token. Of

5870
01:56:00,149 --> 01:56:00,159
token ID and get the decoded token. Of
 

5871
01:56:00,159 --> 01:56:02,070
token ID and get the decoded token. Of
course, this is my LLM output and this

5872
01:56:02,070 --> 01:56:02,080
course, this is my LLM output and this
 

5873
01:56:02,080 --> 01:56:04,390
course, this is my LLM output and this
is not the target output at all. The

5874
01:56:04,390 --> 01:56:04,400
is not the target output at all. The
 

5875
01:56:04,400 --> 01:56:06,510
is not the target output at all. The
target output which I want

5876
01:56:06,510 --> 01:56:06,520
target output which I want
 

5877
01:56:06,520 --> 01:56:10,790
target output which I want
is had always thought jack. So this is

5878
01:56:10,790 --> 01:56:10,800
is had always thought jack. So this is
 

5879
01:56:10,800 --> 01:56:13,109
is had always thought jack. So this is
my x1 and this is my y1 right. So this

5880
01:56:13,109 --> 01:56:13,119
my x1 and this is my y1 right. So this
 

5881
01:56:13,119 --> 01:56:14,430
my x1 and this is my y1 right. So this
is my target

5882
01:56:14,430 --> 01:56:14,440
is my target
 

5883
01:56:14,440 --> 01:56:17,910
is my target
output. So for these inputs ideally I

5884
01:56:17,910 --> 01:56:17,920
output. So for these inputs ideally I
 

5885
01:56:17,920 --> 01:56:20,790
output. So for these inputs ideally I
want the target output as had always

5886
01:56:20,790 --> 01:56:20,800
want the target output as had always
 

5887
01:56:20,800 --> 01:56:23,990
want the target output as had always
thought jack. Okay. And if I go to my

5888
01:56:23,990 --> 01:56:24,000
thought jack. Okay. And if I go to my
 

5889
01:56:24,000 --> 01:56:26,709
thought jack. Okay. And if I go to my
vocabulary I see that had has a token ID

5890
01:56:26,709 --> 01:56:26,719
vocabulary I see that had has a token ID
 

5891
01:56:26,719 --> 01:56:30,790
vocabulary I see that had has a token ID
of 23. Always has a token ID of 3881.

5892
01:56:30,790 --> 01:56:30,800
of 23. Always has a token ID of 3881.
 

5893
01:56:30,800 --> 01:56:33,669
of 23. Always has a token ID of 3881.
Thought has a token ID of 11223. And

5894
01:56:33,669 --> 01:56:33,679
Thought has a token ID of 11223. And
 

5895
01:56:33,679 --> 01:56:37,189
Thought has a token ID of 11223. And
Jack has a token ID of 15. So now let me

5896
01:56:37,189 --> 01:56:37,199
Jack has a token ID of 15. So now let me
 

5897
01:56:37,199 --> 01:56:39,750
Jack has a token ID of 15. So now let me
go to this uh vector matrix of

5898
01:56:39,750 --> 01:56:39,760
go to this uh vector matrix of
 

5899
01:56:39,760 --> 01:56:41,669
go to this uh vector matrix of
probabilities and let me write this here

5900
01:56:41,669 --> 01:56:41,679
probabilities and let me write this here
 

5901
01:56:41,679 --> 01:56:44,830
probabilities and let me write this here
again. I had always

5902
01:56:44,830 --> 01:56:44,840
again. I had always
 

5903
01:56:44,840 --> 01:56:47,910
again. I had always
thought and let's look at I first okay

5904
01:56:47,910 --> 01:56:47,920
thought and let's look at I first okay
 

5905
01:56:47,920 --> 01:56:50,930
thought and let's look at I first okay
let's look at I first this is that 50257

5906
01:56:50,930 --> 01:56:50,940
let's look at I first this is that 50257
 

5907
01:56:50,940 --> 01:56:52,390
let's look at I first this is that 50257
[Music]

5908
01:56:52,390 --> 01:56:52,400
[Music]
 

5909
01:56:52,400 --> 01:56:55,109
[Music]
uh dimensional vector and the maximum

5910
01:56:55,109 --> 01:56:55,119
uh dimensional vector and the maximum
 

5911
01:56:55,119 --> 01:56:57,270
uh dimensional vector and the maximum
probability currently is at index number

5912
01:56:57,270 --> 01:56:57,280
probability currently is at index number
 

5913
01:56:57,280 --> 01:57:00,870
probability currently is at index number
35 let's say or let's say index number

5914
01:57:00,870 --> 01:57:00,880
35 let's say or let's say index number
 

5915
01:57:00,880 --> 01:57:02,709
35 let's say or let's say index number
35 is here that's the maximum

5916
01:57:02,709 --> 01:57:02,719
35 is here that's the maximum
 

5917
01:57:02,719 --> 01:57:04,790
35 is here that's the maximum
probability and it corresponds to the

5918
01:57:04,790 --> 01:57:04,800
probability and it corresponds to the
 

5919
01:57:04,800 --> 01:57:08,310
probability and it corresponds to the
token output of AC and what I want is

5920
01:57:08,310 --> 01:57:08,320
token output of AC and what I want is
 

5921
01:57:08,320 --> 01:57:11,109
token output of AC and what I want is
that I want the next token to be had

5922
01:57:11,109 --> 01:57:11,119
that I want the next token to be had
 

5923
01:57:11,119 --> 01:57:13,669
that I want the next token to be had
when I is the input So I want the token

5924
01:57:13,669 --> 01:57:13,679
when I is the input So I want the token
 

5925
01:57:13,679 --> 01:57:15,910
when I is the input So I want the token
ID corresponding to 23 to have the

5926
01:57:15,910 --> 01:57:15,920
ID corresponding to 23 to have the
 

5927
01:57:15,920 --> 01:57:18,950
ID corresponding to 23 to have the
maximum probability. So what I want is

5928
01:57:18,950 --> 01:57:18,960
maximum probability. So what I want is
 

5929
01:57:18,960 --> 01:57:21,109
maximum probability. So what I want is
that I want a token ID let's say which

5930
01:57:21,109 --> 01:57:21,119
that I want a token ID let's say which
 

5931
01:57:21,119 --> 01:57:23,350
that I want a token ID let's say which
is this. I want this 23 to have the

5932
01:57:23,350 --> 01:57:23,360
is this. I want this 23 to have the
 

5933
01:57:23,360 --> 01:57:26,470
is this. I want this 23 to have the
maximum probability. So ideally what I

5934
01:57:26,470 --> 01:57:26,480
maximum probability. So ideally what I
 

5935
01:57:26,480 --> 01:57:28,310
maximum probability. So ideally what I
want to do is that I want to make sure

5936
01:57:28,310 --> 01:57:28,320
want to do is that I want to make sure
 

5937
01:57:28,320 --> 01:57:31,589
want to do is that I want to make sure
that for the token ID number 23 which is

5938
01:57:31,589 --> 01:57:31,599
that for the token ID number 23 which is
 

5939
01:57:31,599 --> 01:57:34,629
that for the token ID number 23 which is
this this probability is as close to one

5940
01:57:34,629 --> 01:57:34,639
this this probability is as close to one
 

5941
01:57:34,639 --> 01:57:36,870
this this probability is as close to one
as possible. What that will ensure is

5942
01:57:36,870 --> 01:57:36,880
as possible. What that will ensure is
 

5943
01:57:36,880 --> 01:57:38,950
as possible. What that will ensure is
that if this probability is as close to

5944
01:57:38,950 --> 01:57:38,960
that if this probability is as close to
 

5945
01:57:38,960 --> 01:57:41,830
that if this probability is as close to
one, all others will be zero and my next

5946
01:57:41,830 --> 01:57:41,840
one, all others will be zero and my next
 

5947
01:57:41,840 --> 01:57:44,510
one, all others will be zero and my next
token which is predicted would be

5948
01:57:44,510 --> 01:57:44,520
token which is predicted would be
 

5949
01:57:44,520 --> 01:57:47,430
token which is predicted would be
had. So ideally the loss function which

5950
01:57:47,430 --> 01:57:47,440
had. So ideally the loss function which
 

5951
01:57:47,440 --> 01:57:49,990
had. So ideally the loss function which
I have that is constructed only based on

5952
01:57:49,990 --> 01:57:50,000
I have that is constructed only based on
 

5953
01:57:50,000 --> 01:57:52,470
I have that is constructed only based on
this token index which is 23. So

5954
01:57:52,470 --> 01:57:52,480
this token index which is 23. So
 

5955
01:57:52,480 --> 01:57:54,709
this token index which is 23. So
currently let's say the token index 23

5956
01:57:54,709 --> 01:57:54,719
currently let's say the token index 23
 

5957
01:57:54,719 --> 01:57:57,109
currently let's say the token index 23
has a probability of around 0.1. Let's

5958
01:57:57,109 --> 01:57:57,119
has a probability of around 0.1. Let's
 

5959
01:57:57,119 --> 01:57:59,189
has a probability of around 0.1. Let's
say in the current version let's say

5960
01:57:59,189 --> 01:57:59,199
say in the current version let's say
 

5961
01:57:59,199 --> 01:58:01,629
say in the current version let's say
token index number 23 has a probability

5962
01:58:01,629 --> 01:58:01,639
token index number 23 has a probability
 

5963
01:58:01,639 --> 01:58:05,189
token index number 23 has a probability
of.1 which is not very good. I want this

5964
01:58:05,189 --> 01:58:05,199
of.1 which is not very good. I want this
 

5965
01:58:05,199 --> 01:58:07,189
of.1 which is not very good. I want this
probability to be as close to one as

5966
01:58:07,189 --> 01:58:07,199
probability to be as close to one as
 

5967
01:58:07,199 --> 01:58:09,270
probability to be as close to one as
possible and that's how I will define my

5968
01:58:09,270 --> 01:58:09,280
possible and that's how I will define my
 

5969
01:58:09,280 --> 01:58:11,910
possible and that's how I will define my
loss. I will look at the index which I

5970
01:58:11,910 --> 01:58:11,920
loss. I will look at the index which I
 

5971
01:58:11,920 --> 01:58:14,149
loss. I will look at the index which I
actually want as the next token and I

5972
01:58:14,149 --> 01:58:14,159
actually want as the next token and I
 

5973
01:58:14,159 --> 01:58:15,830
actually want as the next token and I
will make sure that the probability

5974
01:58:15,830 --> 01:58:15,840
will make sure that the probability
 

5975
01:58:15,840 --> 01:58:17,589
will make sure that the probability
corresponding corresponding to that

5976
01:58:17,589 --> 01:58:17,599
corresponding corresponding to that
 

5977
01:58:17,599 --> 01:58:20,070
corresponding corresponding to that
index is as close to one as possible.

5978
01:58:20,070 --> 01:58:20,080
index is as close to one as possible.
 

5979
01:58:20,080 --> 01:58:22,149
index is as close to one as possible.
Let's do this exercise for the second

5980
01:58:22,149 --> 01:58:22,159
Let's do this exercise for the second
 

5981
01:58:22,159 --> 01:58:24,950
Let's do this exercise for the second
row which is had. Now and let me write

5982
01:58:24,950 --> 01:58:24,960
row which is had. Now and let me write
 

5983
01:58:24,960 --> 01:58:27,030
row which is had. Now and let me write
down the

5984
01:58:27,030 --> 01:58:27,040
down the
 

5985
01:58:27,040 --> 01:58:30,589
down the
u vocabulary size

5986
01:58:30,589 --> 01:58:30,599
u vocabulary size
 

5987
01:58:30,599 --> 01:58:33,510
u vocabulary size
length vector now for had. Okay, let's

5988
01:58:33,510 --> 01:58:33,520
length vector now for had. Okay, let's
 

5989
01:58:33,520 --> 01:58:36,709
length vector now for had. Okay, let's
say this is the vector and I want to now

5990
01:58:36,709 --> 01:58:36,719
say this is the vector and I want to now
 

5991
01:58:36,719 --> 01:58:39,350
say this is the vector and I want to now
look at always because when I had is the

5992
01:58:39,350 --> 01:58:39,360
look at always because when I had is the
 

5993
01:58:39,360 --> 01:58:41,589
look at always because when I had is the
input, I want always to be the output.

5994
01:58:41,589 --> 01:58:41,599
input, I want always to be the output.
 

5995
01:58:41,599 --> 01:58:44,149
input, I want always to be the output.
So I want the token ID corresponding to

5996
01:58:44,149 --> 01:58:44,159
So I want the token ID corresponding to
 

5997
01:58:44,159 --> 01:58:47,430
So I want the token ID corresponding to
3881 to have the maximum probability. So

5998
01:58:47,430 --> 01:58:47,440
3881 to have the maximum probability. So
 

5999
01:58:47,440 --> 01:58:49,510
3881 to have the maximum probability. So
here I'm going to look at the token ID

6000
01:58:49,510 --> 01:58:49,520
here I'm going to look at the token ID
 

6001
01:58:49,520 --> 01:58:50,910
here I'm going to look at the token ID
corresponding to

6002
01:58:50,910 --> 01:58:50,920
corresponding to
 

6003
01:58:50,920 --> 01:58:53,750
corresponding to
3881 and I will look at its probability

6004
01:58:53,750 --> 01:58:53,760
3881 and I will look at its probability
 

6005
01:58:53,760 --> 01:58:55,430
3881 and I will look at its probability
and currently let's say the probability

6006
01:58:55,430 --> 01:58:55,440
and currently let's say the probability
 

6007
01:58:55,440 --> 01:58:58,390
and currently let's say the probability
is 02 which is not good. I want to take

6008
01:58:58,390 --> 01:58:58,400
is 02 which is not good. I want to take
 

6009
01:58:58,400 --> 01:59:00,390
is 02 which is not good. I want to take
this probability as close to one as

6010
01:59:00,390 --> 01:59:00,400
this probability as close to one as
 

6011
01:59:00,400 --> 01:59:02,310
this probability as close to one as
possible. So I'll define the loss

6012
01:59:02,310 --> 01:59:02,320
possible. So I'll define the loss
 

6013
01:59:02,320 --> 01:59:03,629
possible. So I'll define the loss
between these

6014
01:59:03,629 --> 01:59:03,639
between these
 

6015
01:59:03,639 --> 01:59:06,470
between these
two. So if you notice carefully what is

6016
01:59:06,470 --> 01:59:06,480
two. So if you notice carefully what is
 

6017
01:59:06,480 --> 01:59:08,550
two. So if you notice carefully what is
going on, we have these target indexes,

6018
01:59:08,550 --> 01:59:08,560
going on, we have these target indexes,
 

6019
01:59:08,560 --> 01:59:10,550
going on, we have these target indexes,
right? Which play a very important role

6020
01:59:10,550 --> 01:59:10,560
right? Which play a very important role
 

6021
01:59:10,560 --> 01:59:12,310
right? Which play a very important role
in getting my loss function. These are

6022
01:59:12,310 --> 01:59:12,320
in getting my loss function. These are
 

6023
01:59:12,320 --> 01:59:15,109
in getting my loss function. These are
my target token ids and ultimately the

6024
01:59:15,109 --> 01:59:15,119
my target token ids and ultimately the
 

6025
01:59:15,119 --> 01:59:16,950
my target token ids and ultimately the
loss function will be calculated based

6026
01:59:16,950 --> 01:59:16,960
loss function will be calculated based
 

6027
01:59:16,960 --> 01:59:19,750
loss function will be calculated based
on these token ids itself. And I want

6028
01:59:19,750 --> 01:59:19,760
on these token ids itself. And I want
 

6029
01:59:19,760 --> 01:59:21,589
on these token ids itself. And I want
the probabilities which is associated

6030
01:59:21,589 --> 01:59:21,599
the probabilities which is associated
 

6031
01:59:21,599 --> 01:59:23,669
the probabilities which is associated
with these token ids to be as close to

6032
01:59:23,669 --> 01:59:23,679
with these token ids to be as close to
 

6033
01:59:23,679 --> 01:59:25,750
with these token ids to be as close to
one as possible.

6034
01:59:25,750 --> 01:59:25,760
one as possible.
 

6035
01:59:25,760 --> 01:59:27,589
one as possible.
So what we do in the next step is that

6036
01:59:27,589 --> 01:59:27,599
So what we do in the next step is that
 

6037
01:59:27,599 --> 01:59:29,510
So what we do in the next step is that
we get the output probability for the

6038
01:59:29,510 --> 01:59:29,520
we get the output probability for the
 

6039
01:59:29,520 --> 01:59:31,750
we get the output probability for the
target token ids. Which means that in

6040
01:59:31,750 --> 01:59:31,760
target token ids. Which means that in
 

6041
01:59:31,760 --> 01:59:34,390
target token ids. Which means that in
the first row we look at token number or

6042
01:59:34,390 --> 01:59:34,400
the first row we look at token number or
 

6043
01:59:34,400 --> 01:59:35,990
the first row we look at token number or
id number 23 and get the current

6044
01:59:35,990 --> 01:59:36,000
id number 23 and get the current
 

6045
01:59:36,000 --> 01:59:38,550
id number 23 and get the current
probability. Let's say that is 0.1. In

6046
01:59:38,550 --> 01:59:38,560
probability. Let's say that is 0.1. In
 

6047
01:59:38,560 --> 01:59:41,189
probability. Let's say that is 0.1. In
the token uh in the second token had we

6048
01:59:41,189 --> 01:59:41,199
the token uh in the second token had we
 

6049
01:59:41,199 --> 01:59:43,910
the token uh in the second token had we
look at index number 3881 and get the

6050
01:59:43,910 --> 01:59:43,920
look at index number 3881 and get the
 

6051
01:59:43,920 --> 01:59:45,510
look at index number 3881 and get the
probability corresponding to that. Let's

6052
01:59:45,510 --> 01:59:45,520
probability corresponding to that. Let's
 

6053
01:59:45,520 --> 01:59:48,070
probability corresponding to that. Let's
say that's 02. Similarly we get the

6054
01:59:48,070 --> 01:59:48,080
say that's 02. Similarly we get the
 

6055
01:59:48,080 --> 01:59:50,550
say that's 02. Similarly we get the
probabilities corresponding to always

6056
01:59:50,550 --> 01:59:50,560
probabilities corresponding to always
 

6057
01:59:50,560 --> 01:59:52,790
probabilities corresponding to always
whose true answer is thought and thought

6058
01:59:52,790 --> 01:59:52,800
whose true answer is thought and thought
 

6059
01:59:52,800 --> 01:59:55,189
whose true answer is thought and thought
whose true answer is Jack.

6060
01:59:55,189 --> 01:59:55,199
whose true answer is Jack.
 

6061
01:59:55,199 --> 01:59:57,430
whose true answer is Jack.
Okay. And we get these probabilities and

6062
01:59:57,430 --> 01:59:57,440
Okay. And we get these probabilities and
 

6063
01:59:57,440 --> 02:00:01,589
Okay. And we get these probabilities and
they are P1, P2, P3 and P4. P1, P2, P3

6064
02:00:01,589 --> 02:00:01,599
they are P1, P2, P3 and P4. P1, P2, P3
 

6065
02:00:01,599 --> 02:00:03,510
they are P1, P2, P3 and P4. P1, P2, P3
and P4 are probabilities corresponding

6066
02:00:03,510 --> 02:00:03,520
and P4 are probabilities corresponding
 

6067
02:00:03,520 --> 02:00:06,149
and P4 are probabilities corresponding
to these target token ids. And remember,

6068
02:00:06,149 --> 02:00:06,159
to these target token ids. And remember,
 

6069
02:00:06,159 --> 02:00:08,390
to these target token ids. And remember,
we want these probabilities to be as

6070
02:00:08,390 --> 02:00:08,400
we want these probabilities to be as
 

6071
02:00:08,400 --> 02:00:11,189
we want these probabilities to be as
close to one as possible. That's the

6072
02:00:11,189 --> 02:00:11,199
close to one as possible. That's the
 

6073
02:00:11,199 --> 02:00:15,109
close to one as possible. That's the
main uh that's the main output which we

6074
02:00:15,109 --> 02:00:15,119
main uh that's the main output which we
 

6075
02:00:15,119 --> 02:00:16,950
main uh that's the main output which we
are chasing. And the loss function which

6076
02:00:16,950 --> 02:00:16,960
are chasing. And the loss function which
 

6077
02:00:16,960 --> 02:00:18,950
are chasing. And the loss function which
we are going to define is exactly based

6078
02:00:18,950 --> 02:00:18,960
we are going to define is exactly based
 

6079
02:00:18,960 --> 02:00:21,350
we are going to define is exactly based
on this concept that we want all of

6080
02:00:21,350 --> 02:00:21,360
on this concept that we want all of
 

6081
02:00:21,360 --> 02:00:23,270
on this concept that we want all of
these probabilities to be as close to

6082
02:00:23,270 --> 02:00:23,280
these probabilities to be as close to
 

6083
02:00:23,280 --> 02:00:25,830
these probabilities to be as close to
one as possible. And so we conduct or

6084
02:00:25,830 --> 02:00:25,840
one as possible. And so we conduct or
 

6085
02:00:25,840 --> 02:00:27,430
one as possible. And so we conduct or
and so we construct a loss which is

6086
02:00:27,430 --> 02:00:27,440
and so we construct a loss which is
 

6087
02:00:27,440 --> 02:00:29,189
and so we construct a loss which is
called as the cross entropy loss which

6088
02:00:29,189 --> 02:00:29,199
called as the cross entropy loss which
 

6089
02:00:29,199 --> 02:00:30,790
called as the cross entropy loss which
is based on the loss fun which is based

6090
02:00:30,790 --> 02:00:30,800
is based on the loss fun which is based
 

6091
02:00:30,800 --> 02:00:32,790
is based on the loss fun which is based
on the logarithm. So we take the

6092
02:00:32,790 --> 02:00:32,800
on the logarithm. So we take the
 

6093
02:00:32,800 --> 02:00:34,550
on the logarithm. So we take the
logarithm of all of this and apply

6094
02:00:34,550 --> 02:00:34,560
logarithm of all of this and apply
 

6095
02:00:34,560 --> 02:00:37,189
logarithm of all of this and apply
negative sign here. Since we want to

6096
02:00:37,189 --> 02:00:37,199
negative sign here. Since we want to
 

6097
02:00:37,199 --> 02:00:39,589
negative sign here. Since we want to
maximize all of these probabilities when

6098
02:00:39,589 --> 02:00:39,599
maximize all of these probabilities when
 

6099
02:00:39,599 --> 02:00:41,589
maximize all of these probabilities when
we take the negative sign of here

6100
02:00:41,589 --> 02:00:41,599
we take the negative sign of here
 

6101
02:00:41,599 --> 02:00:43,189
we take the negative sign of here
essentially we want to minimize these

6102
02:00:43,189 --> 02:00:43,199
essentially we want to minimize these
 

6103
02:00:43,199 --> 02:00:45,910
essentially we want to minimize these
quantities as much as possible.

6104
02:00:45,910 --> 02:00:45,920
quantities as much as possible.
 

6105
02:00:45,920 --> 02:00:48,550
quantities as much as possible.
Okay. And since the probabilities always

6106
02:00:48,550 --> 02:00:48,560
Okay. And since the probabilities always
 

6107
02:00:48,560 --> 02:00:51,430
Okay. And since the probabilities always
lie between 0 and one, these this entire

6108
02:00:51,430 --> 02:00:51,440
lie between 0 and one, these this entire
 

6109
02:00:51,440 --> 02:00:53,750
lie between 0 and one, these this entire
quantity will be minimum when P1 is as

6110
02:00:53,750 --> 02:00:53,760
quantity will be minimum when P1 is as
 

6111
02:00:53,760 --> 02:00:56,070
quantity will be minimum when P1 is as
close to one as possible, P2 is as close

6112
02:00:56,070 --> 02:00:56,080
close to one as possible, P2 is as close
 

6113
02:00:56,080 --> 02:00:58,709
close to one as possible, P2 is as close
to one as possible, P3 is as close to

6114
02:00:58,709 --> 02:00:58,719
to one as possible, P3 is as close to
 

6115
02:00:58,719 --> 02:01:00,950
to one as possible, P3 is as close to
one as possible and P4 is as close to

6116
02:01:00,950 --> 02:01:00,960
one as possible and P4 is as close to
 

6117
02:01:00,960 --> 02:01:04,990
one as possible and P4 is as close to
one as possible. This loss which is the

6118
02:01:04,990 --> 02:01:05,000
one as possible. This loss which is the
 

6119
02:01:05,000 --> 02:01:07,430
one as possible. This loss which is the
negative of the summation of the

6120
02:01:07,430 --> 02:01:07,440
negative of the summation of the
 

6121
02:01:07,440 --> 02:01:10,070
negative of the summation of the
logarithms of these probabilities is

6122
02:01:10,070 --> 02:01:10,080
logarithms of these probabilities is
 

6123
02:01:10,080 --> 02:01:11,589
logarithms of these probabilities is
also called as the negative log

6124
02:01:11,589 --> 02:01:11,599
also called as the negative log
 

6125
02:01:11,599 --> 02:01:13,510
also called as the negative log
likelihood or it's also referred to as

6126
02:01:13,510 --> 02:01:13,520
likelihood or it's also referred to as
 

6127
02:01:13,520 --> 02:01:15,750
likelihood or it's also referred to as
the cross entropy loss. The cross

6128
02:01:15,750 --> 02:01:15,760
the cross entropy loss. The cross
 

6129
02:01:15,760 --> 02:01:17,750
the cross entropy loss. The cross
entropy loss is the main loss through

6130
02:01:17,750 --> 02:01:17,760
entropy loss is the main loss through
 

6131
02:01:17,760 --> 02:01:19,589
entropy loss is the main loss through
which we are going to do back

6132
02:01:19,589 --> 02:01:19,599
which we are going to do back
 

6133
02:01:19,599 --> 02:01:22,229
which we are going to do back
propagation for language models. And

6134
02:01:22,229 --> 02:01:22,239
propagation for language models. And
 

6135
02:01:22,239 --> 02:01:24,229
propagation for language models. And
it's very important for you to visualize

6136
02:01:24,229 --> 02:01:24,239
it's very important for you to visualize
 

6137
02:01:24,239 --> 02:01:27,510
it's very important for you to visualize
how we obtain this loss. Okay. First we

6138
02:01:27,510 --> 02:01:27,520
how we obtain this loss. Okay. First we
 

6139
02:01:27,520 --> 02:01:29,950
how we obtain this loss. Okay. First we
go through this entire transformer block

6140
02:01:29,950 --> 02:01:29,960
go through this entire transformer block
 

6141
02:01:29,960 --> 02:01:33,270
go through this entire transformer block
the the input the processor and the

6142
02:01:33,270 --> 02:01:33,280
the the input the processor and the
 

6143
02:01:33,280 --> 02:01:35,510
the the input the processor and the
output. When we come out of the output

6144
02:01:35,510 --> 02:01:35,520
output. When we come out of the output
 

6145
02:01:35,520 --> 02:01:37,270
output. When we come out of the output
block of the transformer we have this

6146
02:01:37,270 --> 02:01:37,280
block of the transformer we have this
 

6147
02:01:37,280 --> 02:01:40,070
block of the transformer we have this
logits matrix right from this logits

6148
02:01:40,070 --> 02:01:40,080
logits matrix right from this logits
 

6149
02:01:40,080 --> 02:01:41,990
logits matrix right from this logits
matrix essentially we have the tokens

6150
02:01:41,990 --> 02:01:42,000
matrix essentially we have the tokens
 

6151
02:01:42,000 --> 02:01:44,790
matrix essentially we have the tokens
and we have the token ids uh which

6152
02:01:44,790 --> 02:01:44,800
and we have the token ids uh which
 

6153
02:01:44,800 --> 02:01:47,830
and we have the token ids uh which
correspond to the decoded next token. So

6154
02:01:47,830 --> 02:01:47,840
correspond to the decoded next token. So
 

6155
02:01:47,840 --> 02:01:49,510
correspond to the decoded next token. So
what we do is that we have the inputs

6156
02:01:49,510 --> 02:01:49,520
what we do is that we have the inputs
 

6157
02:01:49,520 --> 02:01:51,589
what we do is that we have the inputs
and we have the targets. So for every

6158
02:01:51,589 --> 02:01:51,599
and we have the targets. So for every
 

6159
02:01:51,599 --> 02:01:54,790
and we have the targets. So for every
input we have a set of target token ids.

6160
02:01:54,790 --> 02:01:54,800
input we have a set of target token ids.
 

6161
02:01:54,800 --> 02:01:57,430
input we have a set of target token ids.
Let's say for this input I had always I

6162
02:01:57,430 --> 02:01:57,440
Let's say for this input I had always I
 

6163
02:01:57,440 --> 02:01:59,589
Let's say for this input I had always I
had always thought the set of target

6164
02:01:59,589 --> 02:01:59,599
had always thought the set of target
 

6165
02:01:59,599 --> 02:02:02,229
had always thought the set of target
token ids was had always thought jack

6166
02:02:02,229 --> 02:02:02,239
token ids was had always thought jack
 

6167
02:02:02,239 --> 02:02:06,310
token ids was had always thought jack
that's 23 388 1 2 3 and 15. We want the

6168
02:02:06,310 --> 02:02:06,320
that's 23 388 1 2 3 and 15. We want the
 

6169
02:02:06,320 --> 02:02:07,990
that's 23 388 1 2 3 and 15. We want the
probabilities corresponding to these

6170
02:02:07,990 --> 02:02:08,000
probabilities corresponding to these
 

6171
02:02:08,000 --> 02:02:10,070
probabilities corresponding to these
token ids to be as close to one as

6172
02:02:10,070 --> 02:02:10,080
token ids to be as close to one as
 

6173
02:02:10,080 --> 02:02:11,430
token ids to be as close to one as
possible because that will be the

6174
02:02:11,430 --> 02:02:11,440
possible because that will be the
 

6175
02:02:11,440 --> 02:02:14,550
possible because that will be the
decoded next token. So in the first row

6176
02:02:14,550 --> 02:02:14,560
decoded next token. So in the first row
 

6177
02:02:14,560 --> 02:02:16,870
decoded next token. So in the first row
I want the 23rd token to have the

6178
02:02:16,870 --> 02:02:16,880
I want the 23rd token to have the
 

6179
02:02:16,880 --> 02:02:20,070
I want the 23rd token to have the
highest probability. In the second row I

6180
02:02:20,070 --> 02:02:20,080
highest probability. In the second row I
 

6181
02:02:20,080 --> 02:02:22,709
highest probability. In the second row I
want the 3881 token to have the highest

6182
02:02:22,709 --> 02:02:22,719
want the 3881 token to have the highest
 

6183
02:02:22,719 --> 02:02:25,750
want the 3881 token to have the highest
probability. In the third row, I want

6184
02:02:25,750 --> 02:02:25,760
probability. In the third row, I want
 

6185
02:02:25,760 --> 02:02:28,070
probability. In the third row, I want
the 1 22 23 token to have the highest

6186
02:02:28,070 --> 02:02:28,080
the 1 22 23 token to have the highest
 

6187
02:02:28,080 --> 02:02:30,149
the 1 22 23 token to have the highest
probability. And in the fourth row, I

6188
02:02:30,149 --> 02:02:30,159
probability. And in the fourth row, I
 

6189
02:02:30,159 --> 02:02:32,270
probability. And in the fourth row, I
want the 15th token to have the highest

6190
02:02:32,270 --> 02:02:32,280
want the 15th token to have the highest
 

6191
02:02:32,280 --> 02:02:34,629
want the 15th token to have the highest
probability. And these probabilities are

6192
02:02:34,629 --> 02:02:34,639
probability. And these probabilities are
 

6193
02:02:34,639 --> 02:02:37,910
probability. And these probabilities are
currently P1, P2, P3, and P4. I want P1,

6194
02:02:37,910 --> 02:02:37,920
currently P1, P2, P3, and P4. I want P1,
 

6195
02:02:37,920 --> 02:02:40,390
currently P1, P2, P3, and P4. I want P1,
P2, P3, P4 to be as close to one as

6196
02:02:40,390 --> 02:02:40,400
P2, P3, P4 to be as close to one as
 

6197
02:02:40,400 --> 02:02:43,510
P2, P3, P4 to be as close to one as
possible. So I want to maximize P1 + P2

6198
02:02:43,510 --> 02:02:43,520
possible. So I want to maximize P1 + P2
 

6199
02:02:43,520 --> 02:02:47,589
possible. So I want to maximize P1 + P2
plus P3 plus P4. One more way to rewrite

6200
02:02:47,589 --> 02:02:47,599
plus P3 plus P4. One more way to rewrite
 

6201
02:02:47,599 --> 02:02:48,950
plus P3 plus P4. One more way to rewrite
this same loss is if you want to

6202
02:02:48,950 --> 02:02:48,960
this same loss is if you want to
 

6203
02:02:48,960 --> 02:02:51,589
this same loss is if you want to
minimize the average of log P1 plus log

6204
02:02:51,589 --> 02:02:51,599
minimize the average of log P1 plus log
 

6205
02:02:51,599 --> 02:02:54,629
minimize the average of log P1 plus log
P2 plus log P3 plus log P4. Taking a

6206
02:02:54,629 --> 02:02:54,639
P2 plus log P3 plus log P4. Taking a
 

6207
02:02:54,639 --> 02:02:56,870
P2 plus log P3 plus log P4. Taking a
logarithm is generally much more better

6208
02:02:56,870 --> 02:02:56,880
logarithm is generally much more better
 

6209
02:02:56,880 --> 02:02:59,589
logarithm is generally much more better
because if you want to maximize P1, P2,

6210
02:02:59,589 --> 02:02:59,599
because if you want to maximize P1, P2,
 

6211
02:02:59,599 --> 02:03:02,470
because if you want to maximize P1, P2,
P3 and P4, you would construct your

6212
02:03:02,470 --> 02:03:02,480
P3 and P4, you would construct your
 

6213
02:03:02,480 --> 02:03:04,149
P3 and P4, you would construct your
probability which you want to maximize

6214
02:03:04,149 --> 02:03:04,159
probability which you want to maximize
 

6215
02:03:04,159 --> 02:03:08,790
probability which you want to maximize
as P1 into P2 into P3 into P4. Right? Um

6216
02:03:08,790 --> 02:03:08,800
as P1 into P2 into P3 into P4. Right? Um
 

6217
02:03:08,800 --> 02:03:11,109
as P1 into P2 into P3 into P4. Right? Um
and taking derivatives of products is

6218
02:03:11,109 --> 02:03:11,119
and taking derivatives of products is
 

6219
02:03:11,119 --> 02:03:13,109
and taking derivatives of products is
not very easy. So you convert this into

6220
02:03:13,109 --> 02:03:13,119
not very easy. So you convert this into
 

6221
02:03:13,119 --> 02:03:15,030
not very easy. So you convert this into
a summation by taking a logarithm of

6222
02:03:15,030 --> 02:03:15,040
a summation by taking a logarithm of
 

6223
02:03:15,040 --> 02:03:17,189
a summation by taking a logarithm of
this and taking a derivative of this

6224
02:03:17,189 --> 02:03:17,199
this and taking a derivative of this
 

6225
02:03:17,199 --> 02:03:19,270
this and taking a derivative of this
quantity during back propagation is just

6226
02:03:19,270 --> 02:03:19,280
quantity during back propagation is just
 

6227
02:03:19,280 --> 02:03:21,830
quantity during back propagation is just
easier and we apply negative sign

6228
02:03:21,830 --> 02:03:21,840
easier and we apply negative sign
 

6229
02:03:21,840 --> 02:03:24,990
easier and we apply negative sign
because it's common notation to say that

6230
02:03:24,990 --> 02:03:25,000
because it's common notation to say that
 

6231
02:03:25,000 --> 02:03:28,709
because it's common notation to say that
uh loss needs to be reduced. So ideally

6232
02:03:28,709 --> 02:03:28,719
uh loss needs to be reduced. So ideally
 

6233
02:03:28,719 --> 02:03:30,550
uh loss needs to be reduced. So ideally
we want to reduce this quantity as much

6234
02:03:30,550 --> 02:03:30,560
we want to reduce this quantity as much
 

6235
02:03:30,560 --> 02:03:33,350
we want to reduce this quantity as much
as possible so that P1 P2 P3 and P4 will

6236
02:03:33,350 --> 02:03:33,360
as possible so that P1 P2 P3 and P4 will
 

6237
02:03:33,360 --> 02:03:35,750
as possible so that P1 P2 P3 and P4 will
be as close to one as possible and

6238
02:03:35,750 --> 02:03:35,760
be as close to one as possible and
 

6239
02:03:35,760 --> 02:03:38,470
be as close to one as possible and
that's the negative log likelihood loss.

6240
02:03:38,470 --> 02:03:38,480
that's the negative log likelihood loss.
 

6241
02:03:38,480 --> 02:03:40,310
that's the negative log likelihood loss.
What I showed you right now is just for

6242
02:03:40,310 --> 02:03:40,320
What I showed you right now is just for
 

6243
02:03:40,320 --> 02:03:42,950
What I showed you right now is just for
a single input, right? But when you are

6244
02:03:42,950 --> 02:03:42,960
a single input, right? But when you are
 

6245
02:03:42,960 --> 02:03:45,270
a single input, right? But when you are
processing an entire batch together,

6246
02:03:45,270 --> 02:03:45,280
processing an entire batch together,
 

6247
02:03:45,280 --> 02:03:47,510
processing an entire batch together,
what usually happens is that this logits

6248
02:03:47,510 --> 02:03:47,520
what usually happens is that this logits
 

6249
02:03:47,520 --> 02:03:49,189
what usually happens is that this logits
matrix for multiple batches are

6250
02:03:49,189 --> 02:03:49,199
matrix for multiple batches are
 

6251
02:03:49,199 --> 02:03:52,229
matrix for multiple batches are
aggregated together and the target token

6252
02:03:52,229 --> 02:03:52,239
aggregated together and the target token
 

6253
02:03:52,239 --> 02:03:54,629
aggregated together and the target token
ids for multiple batches or inputs of

6254
02:03:54,629 --> 02:03:54,639
ids for multiple batches or inputs of
 

6255
02:03:54,639 --> 02:03:56,790
ids for multiple batches or inputs of
the same batch are aggregated together.

6256
02:03:56,790 --> 02:03:56,800
the same batch are aggregated together.
 

6257
02:03:56,800 --> 02:03:58,790
the same batch are aggregated together.
For example, if you look at one batch

6258
02:03:58,790 --> 02:03:58,800
For example, if you look at one batch
 

6259
02:03:58,800 --> 02:04:02,790
For example, if you look at one batch
now, it has four inputs, right? X1, X2,

6260
02:04:02,790 --> 02:04:02,800
now, it has four inputs, right? X1, X2,
 

6261
02:04:02,800 --> 02:04:05,470
now, it has four inputs, right? X1, X2,
X3 and X4. If the batch size is equal to

6262
02:04:05,470 --> 02:04:05,480
X3 and X4. If the batch size is equal to
 

6263
02:04:05,480 --> 02:04:08,550
X3 and X4. If the batch size is equal to
four, these four inputs are aggregated

6264
02:04:08,550 --> 02:04:08,560
four, these four inputs are aggregated
 

6265
02:04:08,560 --> 02:04:10,310
four, these four inputs are aggregated
together. Their logits are aggregated

6266
02:04:10,310 --> 02:04:10,320
together. Their logits are aggregated
 

6267
02:04:10,320 --> 02:04:12,550
together. Their logits are aggregated
together and their target token ids are

6268
02:04:12,550 --> 02:04:12,560
together and their target token ids are
 

6269
02:04:12,560 --> 02:04:15,270
together and their target token ids are
also aggregated together. And so here we

6270
02:04:15,270 --> 02:04:15,280
also aggregated together. And so here we
 

6271
02:04:15,280 --> 02:04:17,910
also aggregated together. And so here we
have P1, P2, P3 and P4. Right? So then

6272
02:04:17,910 --> 02:04:17,920
have P1, P2, P3 and P4. Right? So then
 

6273
02:04:17,920 --> 02:04:20,229
have P1, P2, P3 and P4. Right? So then
we'll have 16 such probabilities which

6274
02:04:20,229 --> 02:04:20,239
we'll have 16 such probabilities which
 

6275
02:04:20,239 --> 02:04:22,310
we'll have 16 such probabilities which
we'll need to maximize because now there

6276
02:04:22,310 --> 02:04:22,320
we'll need to maximize because now there
 

6277
02:04:22,320 --> 02:04:24,430
we'll need to maximize because now there
are four inputs and there are four

6278
02:04:24,430 --> 02:04:24,440
are four inputs and there are four
 

6279
02:04:24,440 --> 02:04:27,189
are four inputs and there are four
outputs. Uh so let me repeat this once

6280
02:04:27,189 --> 02:04:27,199
outputs. Uh so let me repeat this once
 

6281
02:04:27,199 --> 02:04:30,390
outputs. Uh so let me repeat this once
more. Let's say if you have x1 which is

6282
02:04:30,390 --> 02:04:30,400
more. Let's say if you have x1 which is
 

6283
02:04:30,400 --> 02:04:34,390
more. Let's say if you have x1 which is
1 11 15 and 23 you have x2 which is

6284
02:04:34,390 --> 02:04:34,400
1 11 15 and 23 you have x2 which is
 

6285
02:04:34,400 --> 02:04:37,030
1 11 15 and 23 you have x2 which is
again these four you have x3 which is

6286
02:04:37,030 --> 02:04:37,040
again these four you have x3 which is
 

6287
02:04:37,040 --> 02:04:38,790
again these four you have x3 which is
these four and you have x4 which is

6288
02:04:38,790 --> 02:04:38,800
these four and you have x4 which is
 

6289
02:04:38,800 --> 02:04:41,350
these four and you have x4 which is
these four that's my first batch correct

6290
02:04:41,350 --> 02:04:41,360
these four that's my first batch correct
 

6291
02:04:41,360 --> 02:04:45,149
these four that's my first batch correct
and you have y1 y2 y3 and

6292
02:04:45,149 --> 02:04:45,159
and you have y1 y2 y3 and
 

6293
02:04:45,159 --> 02:04:48,870
and you have y1 y2 y3 and
y4 the what ideally you have is that you

6294
02:04:48,870 --> 02:04:48,880
y4 the what ideally you have is that you
 

6295
02:04:48,880 --> 02:04:52,149
y4 the what ideally you have is that you
have this logits matrix for x1 which is

6296
02:04:52,149 --> 02:04:52,159
have this logits matrix for x1 which is
 

6297
02:04:52,159 --> 02:04:55,470
have this logits matrix for x1 which is
now 4 by vocap

6298
02:04:55,470 --> 02:04:55,480
now 4 by vocap
 

6299
02:04:55,480 --> 02:04:58,790
now 4 by vocap
size you have logic logs for x2 which is

6300
02:04:58,790 --> 02:04:58,800
size you have logic logs for x2 which is
 

6301
02:04:58,800 --> 02:05:01,990
size you have logic logs for x2 which is
four by vocap size. You have logits for

6302
02:05:01,990 --> 02:05:02,000
four by vocap size. You have logits for
 

6303
02:05:02,000 --> 02:05:03,990
four by vocap size. You have logits for
x3 and you have logits for x4 which is

6304
02:05:03,990 --> 02:05:04,000
x3 and you have logits for x4 which is
 

6305
02:05:04,000 --> 02:05:06,149
x3 and you have logits for x4 which is
the same size. So it actually looks

6306
02:05:06,149 --> 02:05:06,159
the same size. So it actually looks
 

6307
02:05:06,159 --> 02:05:07,229
the same size. So it actually looks
something like

6308
02:05:07,229 --> 02:05:07,239
something like
 

6309
02:05:07,239 --> 02:05:09,830
something like
this. This same vector you the same

6310
02:05:09,830 --> 02:05:09,840
this. This same vector you the same
 

6311
02:05:09,840 --> 02:05:11,830
this. This same vector you the same
matrix you can imagine but now repeated

6312
02:05:11,830 --> 02:05:11,840
matrix you can imagine but now repeated
 

6313
02:05:11,840 --> 02:05:12,990
matrix you can imagine but now repeated
four

6314
02:05:12,990 --> 02:05:13,000
four
 

6315
02:05:13,000 --> 02:05:15,430
four
times. This is for the second this is

6316
02:05:15,430 --> 02:05:15,440
times. This is for the second this is
 

6317
02:05:15,440 --> 02:05:16,790
times. This is for the second this is
for the third and this is for the

6318
02:05:16,790 --> 02:05:16,800
for the third and this is for the
 

6319
02:05:16,800 --> 02:05:18,870
for the third and this is for the
fourth. Then what you do is that you

6320
02:05:18,870 --> 02:05:18,880
fourth. Then what you do is that you
 

6321
02:05:18,880 --> 02:05:20,950
fourth. Then what you do is that you
collect all of these logits matrices

6322
02:05:20,950 --> 02:05:20,960
collect all of these logits matrices
 

6323
02:05:20,960 --> 02:05:22,629
collect all of these logits matrices
together. That's called as the

6324
02:05:22,629 --> 02:05:22,639
together. That's called as the
 

6325
02:05:22,639 --> 02:05:25,350
together. That's called as the
flattening operation. You flatten all of

6326
02:05:25,350 --> 02:05:25,360
flattening operation. You flatten all of
 

6327
02:05:25,360 --> 02:05:27,510
flattening operation. You flatten all of
these together into one big logits

6328
02:05:27,510 --> 02:05:27,520
these together into one big logits
 

6329
02:05:27,520 --> 02:05:29,830
these together into one big logits
matrix right and that will essentially

6330
02:05:29,830 --> 02:05:29,840
matrix right and that will essentially
 

6331
02:05:29,840 --> 02:05:32,870
matrix right and that will essentially
now be so if you have four four such

6332
02:05:32,870 --> 02:05:32,880
now be so if you have four four such
 

6333
02:05:32,880 --> 02:05:34,950
now be so if you have four four such
ones right so that will be 16 multiplied

6334
02:05:34,950 --> 02:05:34,960
ones right so that will be 16 multiplied
 

6335
02:05:34,960 --> 02:05:37,070
ones right so that will be 16 multiplied
by my vocap

6336
02:05:37,070 --> 02:05:37,080
by my vocap
 

6337
02:05:37,080 --> 02:05:41,189
by my vocap
size so you have 16 rows over here and

6338
02:05:41,189 --> 02:05:41,199
size so you have 16 rows over here and
 

6339
02:05:41,199 --> 02:05:43,830
size so you have 16 rows over here and
this target right my target now my

6340
02:05:43,830 --> 02:05:43,840
this target right my target now my
 

6341
02:05:43,840 --> 02:05:45,550
this target right my target now my
target will also

6342
02:05:45,550 --> 02:05:45,560
target will also
 

6343
02:05:45,560 --> 02:05:50,189
target will also
be 1 2 3 4 that's corresponding to uh

6344
02:05:50,189 --> 02:05:50,199
be 1 2 3 4 that's corresponding to uh
 

6345
02:05:50,199 --> 02:05:53,270
be 1 2 3 4 that's corresponding to uh
y1 then I will have 1 2 3 4 that's

6346
02:05:53,270 --> 02:05:53,280
y1 then I will have 1 2 3 4 that's
 

6347
02:05:53,280 --> 02:05:56,070
y1 then I will have 1 2 3 4 that's
corresponding to Y2 1 2 3 4

6348
02:05:56,070 --> 02:05:56,080
corresponding to Y2 1 2 3 4
 

6349
02:05:56,080 --> 02:05:58,390
corresponding to Y2 1 2 3 4
corresponding to Y3 and 1 2 3 4

6350
02:05:58,390 --> 02:05:58,400
corresponding to Y3 and 1 2 3 4
 

6351
02:05:58,400 --> 02:06:02,310
corresponding to Y3 and 1 2 3 4
corresponding to Y4. Okay. And as we saw

6352
02:06:02,310 --> 02:06:02,320
corresponding to Y4. Okay. And as we saw
 

6353
02:06:02,320 --> 02:06:04,870
corresponding to Y4. Okay. And as we saw
for each Y we have four probabilities

6354
02:06:04,870 --> 02:06:04,880
for each Y we have four probabilities
 

6355
02:06:04,880 --> 02:06:09,910
for each Y we have four probabilities
right? P1 P2 P3 P4. So now we have P1 P2

6356
02:06:09,910 --> 02:06:09,920
right? P1 P2 P3 P4. So now we have P1 P2
 

6357
02:06:09,920 --> 02:06:16,390
right? P1 P2 P3 P4. So now we have P1 P2
P3 P4 that's for Y1 then P5 P6 P7 P8

6358
02:06:16,390 --> 02:06:16,400
P3 P4 that's for Y1 then P5 P6 P7 P8
 

6359
02:06:16,400 --> 02:06:23,430
P3 P4 that's for Y1 then P5 P6 P7 P8
that's for my Y2. Then P9, P10, P11, P12

6360
02:06:23,430 --> 02:06:23,440
that's for my Y2. Then P9, P10, P11, P12
 

6361
02:06:23,440 --> 02:06:25,629
that's for my Y2. Then P9, P10, P11, P12
for Y3 and

6362
02:06:25,629 --> 02:06:25,639
for Y3 and
 

6363
02:06:25,639 --> 02:06:31,030
for Y3 and
P13, P14, P15 and P16. That's for my Y4.

6364
02:06:31,030 --> 02:06:31,040
P13, P14, P15 and P16. That's for my Y4.
 

6365
02:06:31,040 --> 02:06:33,109
P13, P14, P15 and P16. That's for my Y4.
Now this is my set of probabilities and

6366
02:06:33,109 --> 02:06:33,119
Now this is my set of probabilities and
 

6367
02:06:33,119 --> 02:06:35,589
Now this is my set of probabilities and
I want to make all these probabilities

6368
02:06:35,589 --> 02:06:35,599
I want to make all these probabilities
 

6369
02:06:35,599 --> 02:06:37,830
I want to make all these probabilities
as close to one as possible. So we'll

6370
02:06:37,830 --> 02:06:37,840
as close to one as possible. So we'll
 

6371
02:06:37,840 --> 02:06:40,149
as close to one as possible. So we'll
just again calculate a negative like log

6372
02:06:40,149 --> 02:06:40,159
just again calculate a negative like log
 

6373
02:06:40,159 --> 02:06:43,109
just again calculate a negative like log
likelihood for this entire batch. So

6374
02:06:43,109 --> 02:06:43,119
likelihood for this entire batch. So
 

6375
02:06:43,119 --> 02:06:45,990
likelihood for this entire batch. So
that's how one batch is processed. Okay.

6376
02:06:45,990 --> 02:06:46,000
that's how one batch is processed. Okay.
 

6377
02:06:46,000 --> 02:06:47,510
that's how one batch is processed. Okay.
So when we see the code you will see

6378
02:06:47,510 --> 02:06:47,520
So when we see the code you will see
 

6379
02:06:47,520 --> 02:06:49,830
So when we see the code you will see
some terms such as flatten etc. So

6380
02:06:49,830 --> 02:06:49,840
some terms such as flatten etc. So
 

6381
02:06:49,840 --> 02:06:52,069
some terms such as flatten etc. So
whenever this flatten is used it

6382
02:06:52,069 --> 02:06:52,079
whenever this flatten is used it
 

6383
02:06:52,079 --> 02:06:54,870
whenever this flatten is used it
essentially makes it means that my

6384
02:06:54,870 --> 02:06:54,880
essentially makes it means that my
 

6385
02:06:54,880 --> 02:06:57,270
essentially makes it means that my
multiple uh inputs are now merged

6386
02:06:57,270 --> 02:06:57,280
multiple uh inputs are now merged
 

6387
02:06:57,280 --> 02:06:59,870
multiple uh inputs are now merged
together into one big uh matrix or

6388
02:06:59,870 --> 02:06:59,880
together into one big uh matrix or
 

6389
02:06:59,880 --> 02:07:02,390
together into one big uh matrix or
tensor. So that's how the loss of the

6390
02:07:02,390 --> 02:07:02,400
tensor. So that's how the loss of the
 

6391
02:07:02,400 --> 02:07:03,870
tensor. So that's how the loss of the
small language model is actually

6392
02:07:03,870 --> 02:07:03,880
small language model is actually
 

6393
02:07:03,880 --> 02:07:07,109
small language model is actually
calculated and you'll see that when you

6394
02:07:07,109 --> 02:07:07,119
calculated and you'll see that when you
 

6395
02:07:07,119 --> 02:07:10,390
calculated and you'll see that when you
have this GPT class right when you have

6396
02:07:10,390 --> 02:07:10,400
have this GPT class right when you have
 

6397
02:07:10,400 --> 02:07:12,709
have this GPT class right when you have
the class GPT the loss is actually

6398
02:07:12,709 --> 02:07:12,719
the class GPT the loss is actually
 

6399
02:07:12,719 --> 02:07:14,709
the class GPT the loss is actually
calculated within this class itself. If

6400
02:07:14,709 --> 02:07:14,719
calculated within this class itself. If
 

6401
02:07:14,719 --> 02:07:17,189
calculated within this class itself. If
you look at the forward method until now

6402
02:07:17,189 --> 02:07:17,199
you look at the forward method until now
 

6403
02:07:17,199 --> 02:07:19,430
you look at the forward method until now
we have the logits matrix right. So here

6404
02:07:19,430 --> 02:07:19,440
we have the logits matrix right. So here
 

6405
02:07:19,440 --> 02:07:21,189
we have the logits matrix right. So here
you see what is happening we have the

6406
02:07:21,189 --> 02:07:21,199
you see what is happening we have the
 

6407
02:07:21,199 --> 02:07:23,750
you see what is happening we have the
cross entropy loss again between the

6408
02:07:23,750 --> 02:07:23,760
cross entropy loss again between the
 

6409
02:07:23,760 --> 02:07:25,589
cross entropy loss again between the
logits and between the targets. So this

6410
02:07:25,589 --> 02:07:25,599
logits and between the targets. So this
 

6411
02:07:25,599 --> 02:07:27,350
logits and between the targets. So this
is that cross entropy loss which I

6412
02:07:27,350 --> 02:07:27,360
is that cross entropy loss which I
 

6413
02:07:27,360 --> 02:07:30,870
is that cross entropy loss which I
mentioned um over here. Essentially that

6414
02:07:30,870 --> 02:07:30,880
mentioned um over here. Essentially that
 

6415
02:07:30,880 --> 02:07:33,109
mentioned um over here. Essentially that
cross entropy loss just means taking the

6416
02:07:33,109 --> 02:07:33,119
cross entropy loss just means taking the
 

6417
02:07:33,119 --> 02:07:35,270
cross entropy loss just means taking the
negative log likelihood for the

6418
02:07:35,270 --> 02:07:35,280
negative log likelihood for the
 

6419
02:07:35,280 --> 02:07:38,310
negative log likelihood for the
estimated tokens which we have. And then

6420
02:07:38,310 --> 02:07:38,320
estimated tokens which we have. And then
 

6421
02:07:38,320 --> 02:07:39,750
estimated tokens which we have. And then
we have one more function which is

6422
02:07:39,750 --> 02:07:39,760
we have one more function which is
 

6423
02:07:39,760 --> 02:07:41,910
we have one more function which is
called as the estimate loss. This

6424
02:07:41,910 --> 02:07:41,920
called as the estimate loss. This
 

6425
02:07:41,920 --> 02:07:44,790
called as the estimate loss. This
estimate loss function essentially uh

6426
02:07:44,790 --> 02:07:44,800
estimate loss function essentially uh
 

6427
02:07:44,800 --> 02:07:47,109
estimate loss function essentially uh
what it does is that it gets one batch

6428
02:07:47,109 --> 02:07:47,119
what it does is that it gets one batch
 

6429
02:07:47,119 --> 02:07:50,229
what it does is that it gets one batch
and then it estimates the uh losses for

6430
02:07:50,229 --> 02:07:50,239
and then it estimates the uh losses for
 

6431
02:07:50,239 --> 02:07:52,229
and then it estimates the uh losses for
that entire batch actually. So here you

6432
02:07:52,229 --> 02:07:52,239
that entire batch actually. So here you
 

6433
02:07:52,239 --> 02:07:55,189
that entire batch actually. So here you
can see that uh for a prescribed number

6434
02:07:55,189 --> 02:07:55,199
can see that uh for a prescribed number
 

6435
02:07:55,199 --> 02:07:58,069
can see that uh for a prescribed number
of iterations let's say we have 100 uh

6436
02:07:58,069 --> 02:07:58,079
of iterations let's say we have 100 uh
 

6437
02:07:58,079 --> 02:08:01,189
of iterations let's say we have 100 uh
we have 100 iterations right and and

6438
02:08:01,189 --> 02:08:01,199
we have 100 iterations right and and
 

6439
02:08:01,199 --> 02:08:03,270
we have 100 iterations right and and
evaluations are done after every 100

6440
02:08:03,270 --> 02:08:03,280
evaluations are done after every 100
 

6441
02:08:03,280 --> 02:08:05,189
evaluations are done after every 100
iterations. So what happens is that for

6442
02:08:05,189 --> 02:08:05,199
iterations. So what happens is that for
 

6443
02:08:05,199 --> 02:08:07,750
iterations. So what happens is that for
every iteration we get one batch and

6444
02:08:07,750 --> 02:08:07,760
every iteration we get one batch and
 

6445
02:08:07,760 --> 02:08:11,270
every iteration we get one batch and
then u essentially for every batch which

6446
02:08:11,270 --> 02:08:11,280
then u essentially for every batch which
 

6447
02:08:11,280 --> 02:08:14,870
then u essentially for every batch which
we have we get the uh we get the loss

6448
02:08:14,870 --> 02:08:14,880
we have we get the uh we get the loss
 

6449
02:08:14,880 --> 02:08:18,310
we have we get the uh we get the loss
and that's added to my losses right now.

6450
02:08:18,310 --> 02:08:18,320
and that's added to my losses right now.
 

6451
02:08:18,320 --> 02:08:22,149
and that's added to my losses right now.
Okay. And then you see this out dot

6452
02:08:22,149 --> 02:08:22,159
Okay. And then you see this out dot
 

6453
02:08:22,159 --> 02:08:24,310
Okay. And then you see this out dot
outsplit equal to losses dot mean. So

6454
02:08:24,310 --> 02:08:24,320
outsplit equal to losses dot mean. So
 

6455
02:08:24,320 --> 02:08:25,910
outsplit equal to losses dot mean. So
for all of these iterations I

6456
02:08:25,910 --> 02:08:25,920
for all of these iterations I
 

6457
02:08:25,920 --> 02:08:27,990
for all of these iterations I
essentially will take a mean of mean of

6458
02:08:27,990 --> 02:08:28,000
essentially will take a mean of mean of
 

6459
02:08:28,000 --> 02:08:30,629
essentially will take a mean of mean of
the losses.

6460
02:08:30,629 --> 02:08:30,639
the losses.
 

6461
02:08:30,639 --> 02:08:32,790
the losses.
So this estimate loss what it does is

6462
02:08:32,790 --> 02:08:32,800
So this estimate loss what it does is
 

6463
02:08:32,800 --> 02:08:35,510
So this estimate loss what it does is
that I have to run the model for a

6464
02:08:35,510 --> 02:08:35,520
that I have to run the model for a
 

6465
02:08:35,520 --> 02:08:38,310
that I have to run the model for a
prescribed uh I have to run the model

6466
02:08:38,310 --> 02:08:38,320
prescribed uh I have to run the model
 

6467
02:08:38,320 --> 02:08:41,510
prescribed uh I have to run the model
for prescribed number of iterations and

6468
02:08:41,510 --> 02:08:41,520
for prescribed number of iterations and
 

6469
02:08:41,520 --> 02:08:44,629
for prescribed number of iterations and
uh when I when I do that when I run the

6470
02:08:44,629 --> 02:08:44,639
uh when I when I do that when I run the
 

6471
02:08:44,639 --> 02:08:46,629
uh when I when I do that when I run the
model let's say for 100 iterations for

6472
02:08:46,629 --> 02:08:46,639
model let's say for 100 iterations for
 

6473
02:08:46,639 --> 02:08:48,550
model let's say for 100 iterations for
each iteration I'm getting a different

6474
02:08:48,550 --> 02:08:48,560
each iteration I'm getting a different
 

6475
02:08:48,560 --> 02:08:50,709
each iteration I'm getting a different
batch when I get a batch for an

6476
02:08:50,709 --> 02:08:50,719
batch when I get a batch for an
 

6477
02:08:50,719 --> 02:08:52,629
batch when I get a batch for an
iteration let's say x and y I get a

6478
02:08:52,629 --> 02:08:52,639
iteration let's say x and y I get a
 

6479
02:08:52,639 --> 02:08:54,950
iteration let's say x and y I get a
batch which is like what I've shown here

6480
02:08:54,950 --> 02:08:54,960
batch which is like what I've shown here
 

6481
02:08:54,960 --> 02:08:57,510
batch which is like what I've shown here
we get a batch for an iteration we pass

6482
02:08:57,510 --> 02:08:57,520
we get a batch for an iteration we pass
 

6483
02:08:57,520 --> 02:08:59,589
we get a batch for an iteration we pass
this batch through the model we pass

6484
02:08:59,589 --> 02:08:59,599
this batch through the model we pass
 

6485
02:08:59,599 --> 02:09:01,350
this batch through the model we pass
this batch through this entire model we

6486
02:09:01,350 --> 02:09:01,360
this batch through this entire model we
 

6487
02:09:01,360 --> 02:09:03,189
this batch through this entire model we
get the logits. We merge all of these

6488
02:09:03,189 --> 02:09:03,199
get the logits. We merge all of these
 

6489
02:09:03,199 --> 02:09:06,790
get the logits. We merge all of these
logits together and then we get the loss

6490
02:09:06,790 --> 02:09:06,800
logits together and then we get the loss
 

6491
02:09:06,800 --> 02:09:08,470
logits together and then we get the loss
which has been mentioned over here. We

6492
02:09:08,470 --> 02:09:08,480
which has been mentioned over here. We
 

6493
02:09:08,480 --> 02:09:10,229
which has been mentioned over here. We
get the loss the cross entropy loss

6494
02:09:10,229 --> 02:09:10,239
get the loss the cross entropy loss
 

6495
02:09:10,239 --> 02:09:12,390
get the loss the cross entropy loss
that's the loss from one batch during

6496
02:09:12,390 --> 02:09:12,400
that's the loss from one batch during
 

6497
02:09:12,400 --> 02:09:14,669
that's the loss from one batch during
one iteration. When we are doing another

6498
02:09:14,669 --> 02:09:14,679
one iteration. When we are doing another
 

6499
02:09:14,679 --> 02:09:17,589
one iteration. When we are doing another
iteration a separate batch is is given

6500
02:09:17,589 --> 02:09:17,599
iteration a separate batch is is given
 

6501
02:09:17,599 --> 02:09:20,069
iteration a separate batch is is given
to us. Remember the get a batch function

6502
02:09:20,069 --> 02:09:20,079
to us. Remember the get a batch function
 

6503
02:09:20,079 --> 02:09:21,910
to us. Remember the get a batch function
sources the inputs and the targets

6504
02:09:21,910 --> 02:09:21,920
sources the inputs and the targets
 

6505
02:09:21,920 --> 02:09:24,149
sources the inputs and the targets
randomly. So if you remember what we had

6506
02:09:24,149 --> 02:09:24,159
randomly. So if you remember what we had
 

6507
02:09:24,159 --> 02:09:26,149
randomly. So if you remember what we had
seen in get a batch there is this equal

6508
02:09:26,149 --> 02:09:26,159
seen in get a batch there is this equal
 

6509
02:09:26,159 --> 02:09:28,709
seen in get a batch there is this equal
to torch.random integer right? So it

6510
02:09:28,709 --> 02:09:28,719
to torch.random integer right? So it
 

6511
02:09:28,719 --> 02:09:30,950
to torch.random integer right? So it
takes batch batches randomly from the

6512
02:09:30,950 --> 02:09:30,960
takes batch batches randomly from the
 

6513
02:09:30,960 --> 02:09:34,310
takes batch batches randomly from the
data set and once we have one batch we

6514
02:09:34,310 --> 02:09:34,320
data set and once we have one batch we
 

6515
02:09:34,320 --> 02:09:36,310
data set and once we have one batch we
can propagate it through the model and

6516
02:09:36,310 --> 02:09:36,320
can propagate it through the model and
 

6517
02:09:36,320 --> 02:09:37,910
can propagate it through the model and
we can get the loss. That's what's

6518
02:09:37,910 --> 02:09:37,920
we can get the loss. That's what's
 

6519
02:09:37,920 --> 02:09:41,270
we can get the loss. That's what's
happening here. Okay. Uh we collect all

6520
02:09:41,270 --> 02:09:41,280
happening here. Okay. Uh we collect all
 

6521
02:09:41,280 --> 02:09:43,430
happening here. Okay. Uh we collect all
of the losses which are happening

6522
02:09:43,430 --> 02:09:43,440
of the losses which are happening
 

6523
02:09:43,440 --> 02:09:45,589
of the losses which are happening
through different iterations. And then

6524
02:09:45,589 --> 02:09:45,599
through different iterations. And then
 

6525
02:09:45,599 --> 02:09:47,350
through different iterations. And then
what we do is that we essentially take

6526
02:09:47,350 --> 02:09:47,360
what we do is that we essentially take
 

6527
02:09:47,360 --> 02:09:48,669
what we do is that we essentially take
the

6528
02:09:48,669 --> 02:09:48,679
the
 

6529
02:09:48,679 --> 02:09:50,950
the
mean. We take the mean. So let's say in

6530
02:09:50,950 --> 02:09:50,960
mean. We take the mean. So let's say in
 

6531
02:09:50,960 --> 02:09:52,870
mean. We take the mean. So let's say in
the iteration number one the loss on the

6532
02:09:52,870 --> 02:09:52,880
the iteration number one the loss on the
 

6533
02:09:52,880 --> 02:09:55,030
the iteration number one the loss on the
batch is this. Iteration number two the

6534
02:09:55,030 --> 02:09:55,040
batch is this. Iteration number two the
 

6535
02:09:55,040 --> 02:09:56,629
batch is this. Iteration number two the
loss on the batch is this. Iteration

6536
02:09:56,629 --> 02:09:56,639
loss on the batch is this. Iteration
 

6537
02:09:56,639 --> 02:09:58,069
loss on the batch is this. Iteration
number three, the loss on the passive

6538
02:09:58,069 --> 02:09:58,079
number three, the loss on the passive
 

6539
02:09:58,079 --> 02:09:59,669
number three, the loss on the passive
batch is this. For all of these

6540
02:09:59,669 --> 02:09:59,679
batch is this. For all of these
 

6541
02:09:59,679 --> 02:10:02,229
batch is this. For all of these
iterations, the loss, the overall loss

6542
02:10:02,229 --> 02:10:02,239
iterations, the loss, the overall loss
 

6543
02:10:02,239 --> 02:10:04,069
iterations, the loss, the overall loss
is the mean over let's say these 100

6544
02:10:04,069 --> 02:10:04,079
is the mean over let's say these 100
 

6545
02:10:04,079 --> 02:10:05,390
is the mean over let's say these 100
iterations

6546
02:10:05,390 --> 02:10:05,400
iterations
 

6547
02:10:05,400 --> 02:10:08,790
iterations
uh the 100 iterations which we have. So

6548
02:10:08,790 --> 02:10:08,800
uh the 100 iterations which we have. So
 

6549
02:10:08,800 --> 02:10:10,709
uh the 100 iterations which we have. So
that's the estimate loss function. This

6550
02:10:10,709 --> 02:10:10,719
that's the estimate loss function. This
 

6551
02:10:10,719 --> 02:10:12,790
that's the estimate loss function. This
function will be very useful for us when

6552
02:10:12,790 --> 02:10:12,800
function will be very useful for us when
 

6553
02:10:12,800 --> 02:10:14,790
function will be very useful for us when
we are let's say training the model and

6554
02:10:14,790 --> 02:10:14,800
we are let's say training the model and
 

6555
02:10:14,800 --> 02:10:16,870
we are let's say training the model and
we want to print out the loss after

6556
02:10:16,870 --> 02:10:16,880
we want to print out the loss after
 

6557
02:10:16,880 --> 02:10:19,669
we want to print out the loss after
every evaluation iterations. So if you

6558
02:10:19,669 --> 02:10:19,679
every evaluation iterations. So if you
 

6559
02:10:19,679 --> 02:10:21,189
every evaluation iterations. So if you
want to print out the loss after every

6560
02:10:21,189 --> 02:10:21,199
want to print out the loss after every
 

6561
02:10:21,199 --> 02:10:24,550
want to print out the loss after every
100 iterations, it it takes the mean of

6562
02:10:24,550 --> 02:10:24,560
100 iterations, it it takes the mean of
 

6563
02:10:24,560 --> 02:10:26,470
100 iterations, it it takes the mean of
all of those 100 iterations and prints

6564
02:10:26,470 --> 02:10:26,480
all of those 100 iterations and prints
 

6565
02:10:26,480 --> 02:10:28,709
all of those 100 iterations and prints
out the and prints out the mean loss for

6566
02:10:28,709 --> 02:10:28,719
out the and prints out the mean loss for
 

6567
02:10:28,719 --> 02:10:31,669
out the and prints out the mean loss for
those 100. That's what is happening in

6568
02:10:31,669 --> 02:10:31,679
those 100. That's what is happening in
 

6569
02:10:31,679 --> 02:10:34,229
those 100. That's what is happening in
this loss function calculation. So this

6570
02:10:34,229 --> 02:10:34,239
this loss function calculation. So this
 

6571
02:10:34,239 --> 02:10:35,990
this loss function calculation. So this
is how we get the loss in a language

6572
02:10:35,990 --> 02:10:36,000
is how we get the loss in a language
 

6573
02:10:36,000 --> 02:10:38,229
is how we get the loss in a language
model. We convert the data set into

6574
02:10:38,229 --> 02:10:38,239
model. We convert the data set into
 

6575
02:10:38,239 --> 02:10:40,470
model. We convert the data set into
batches. One batch is plucked out that

6576
02:10:40,470 --> 02:10:40,480
batches. One batch is plucked out that
 

6577
02:10:40,480 --> 02:10:41,950
batches. One batch is plucked out that
that's passed through

6578
02:10:41,950 --> 02:10:41,960
that's passed through
 

6579
02:10:41,960 --> 02:10:44,870
that's passed through
the that's passed through my model. then

6580
02:10:44,870 --> 02:10:44,880
the that's passed through my model. then
 

6581
02:10:44,880 --> 02:10:48,550
the that's passed through my model. then
I get the loss out of uh the target

6582
02:10:48,550 --> 02:10:48,560
I get the loss out of uh the target
 

6583
02:10:48,560 --> 02:10:49,990
I get the loss out of uh the target
probabilities and my output

6584
02:10:49,990 --> 02:10:50,000
probabilities and my output
 

6585
02:10:50,000 --> 02:10:53,030
probabilities and my output
probabilities for the entire batch and

6586
02:10:53,030 --> 02:10:53,040
probabilities for the entire batch and
 

6587
02:10:53,040 --> 02:10:55,430
probabilities for the entire batch and
then I go to the next iteration etc.

6588
02:10:55,430 --> 02:10:55,440
then I go to the next iteration etc.
 

6589
02:10:55,440 --> 02:10:58,270
then I go to the next iteration etc.
That's how the losses are estimated.

6590
02:10:58,270 --> 02:10:58,280
That's how the losses are estimated.
 

6591
02:10:58,280 --> 02:11:00,870
That's how the losses are estimated.
Okay. So until now what we have done is

6592
02:11:00,870 --> 02:11:00,880
Okay. So until now what we have done is
 

6593
02:11:00,880 --> 02:11:02,629
Okay. So until now what we have done is
we have finished step number five which

6594
02:11:02,629 --> 02:11:02,639
we have finished step number five which
 

6595
02:11:02,639 --> 02:11:05,430
we have finished step number five which
is defining and calculating the language

6596
02:11:05,430 --> 02:11:05,440
is defining and calculating the language
 

6597
02:11:05,440 --> 02:11:07,910
is defining and calculating the language
model loss function and then we now go

6598
02:11:07,910 --> 02:11:07,920
model loss function and then we now go
 

6599
02:11:07,920 --> 02:11:09,990
model loss function and then we now go
to step number six which is coming to

6600
02:11:09,990 --> 02:11:10,000
to step number six which is coming to
 

6601
02:11:10,000 --> 02:11:11,830
to step number six which is coming to
the training loop of the small language

6602
02:11:11,830 --> 02:11:11,840
the training loop of the small language
 

6603
02:11:11,840 --> 02:11:14,950
the training loop of the small language
model. So until now let's do a quick

6604
02:11:14,950 --> 02:11:14,960
model. So until now let's do a quick
 

6605
02:11:14,960 --> 02:11:17,589
model. So until now let's do a quick
recap of what we have until now. What we

6606
02:11:17,589 --> 02:11:17,599
recap of what we have until now. What we
 

6607
02:11:17,599 --> 02:11:20,069
recap of what we have until now. What we
can do is that we can take a batch.

6608
02:11:20,069 --> 02:11:20,079
can do is that we can take a batch.
 

6609
02:11:20,079 --> 02:11:22,390
can do is that we can take a batch.
Let's say this is my first batch of

6610
02:11:22,390 --> 02:11:22,400
Let's say this is my first batch of
 

6611
02:11:22,400 --> 02:11:26,470
Let's say this is my first batch of
input X1 which has let's say X1, X2, X3

6612
02:11:26,470 --> 02:11:26,480
input X1 which has let's say X1, X2, X3
 

6613
02:11:26,480 --> 02:11:29,109
input X1 which has let's say X1, X2, X3
and X4. And that's my first batch of

6614
02:11:29,109 --> 02:11:29,119
and X4. And that's my first batch of
 

6615
02:11:29,119 --> 02:11:32,950
and X4. And that's my first batch of
output Y1 which is Y1, Y2, Y3 and Y4. I

6616
02:11:32,950 --> 02:11:32,960
output Y1 which is Y1, Y2, Y3 and Y4. I
 

6617
02:11:32,960 --> 02:11:35,030
output Y1 which is Y1, Y2, Y3 and Y4. I
can take my whole batch. I can pass it

6618
02:11:35,030 --> 02:11:35,040
can take my whole batch. I can pass it
 

6619
02:11:35,040 --> 02:11:36,790
can take my whole batch. I can pass it
through the language model which

6620
02:11:36,790 --> 02:11:36,800
through the language model which
 

6621
02:11:36,800 --> 02:11:39,270
through the language model which
consists of input, processor and output.

6622
02:11:39,270 --> 02:11:39,280
consists of input, processor and output.
 

6623
02:11:39,280 --> 02:11:42,550
consists of input, processor and output.
Then I get these logits.

6624
02:11:42,550 --> 02:11:42,560
Then I get these logits.
 

6625
02:11:42,560 --> 02:11:44,709
Then I get these logits.
then I have a loss function estimation

6626
02:11:44,709 --> 02:11:44,719
then I have a loss function estimation
 

6627
02:11:44,719 --> 02:11:47,669
then I have a loss function estimation
and I get my loss. So essentially now we

6628
02:11:47,669 --> 02:11:47,679
and I get my loss. So essentially now we
 

6629
02:11:47,679 --> 02:11:50,109
and I get my loss. So essentially now we
can do one full forward pass through my

6630
02:11:50,109 --> 02:11:50,119
can do one full forward pass through my
 

6631
02:11:50,119 --> 02:11:52,709
can do one full forward pass through my
input and once we can do this full

6632
02:11:52,709 --> 02:11:52,719
input and once we can do this full
 

6633
02:11:52,719 --> 02:11:54,790
input and once we can do this full
forward pass once we get the loss then

6634
02:11:54,790 --> 02:11:54,800
forward pass once we get the loss then
 

6635
02:11:54,800 --> 02:11:57,189
forward pass once we get the loss then
the only thing which needs to be done is

6636
02:11:57,189 --> 02:11:57,199
the only thing which needs to be done is
 

6637
02:11:57,199 --> 02:11:58,950
the only thing which needs to be done is
that we need to propagate this loss

6638
02:11:58,950 --> 02:11:58,960
that we need to propagate this loss
 

6639
02:11:58,960 --> 02:12:01,910
that we need to propagate this loss
backwards through my entire architecture

6640
02:12:01,910 --> 02:12:01,920
backwards through my entire architecture
 

6641
02:12:01,920 --> 02:12:03,510
backwards through my entire architecture
so that we can then update the

6642
02:12:03,510 --> 02:12:03,520
so that we can then update the
 

6643
02:12:03,520 --> 02:12:05,990
so that we can then update the
parameters of my architecture and that's

6644
02:12:05,990 --> 02:12:06,000
parameters of my architecture and that's
 

6645
02:12:06,000 --> 02:12:08,310
parameters of my architecture and that's
when we start defining the training loop

6646
02:12:08,310 --> 02:12:08,320
when we start defining the training loop
 

6647
02:12:08,320 --> 02:12:10,950
when we start defining the training loop
okay of the small language model. Now,

6648
02:12:10,950 --> 02:12:10,960
okay of the small language model. Now,
 

6649
02:12:10,960 --> 02:12:12,629
okay of the small language model. Now,
as I mentioned at the start of this

6650
02:12:12,629 --> 02:12:12,639
as I mentioned at the start of this
 

6651
02:12:12,639 --> 02:12:14,470
as I mentioned at the start of this
tutorial, this is going to be a bit of a

6652
02:12:14,470 --> 02:12:14,480
tutorial, this is going to be a bit of a
 

6653
02:12:14,480 --> 02:12:16,390
tutorial, this is going to be a bit of a
production level tutorial because we are

6654
02:12:16,390 --> 02:12:16,400
production level tutorial because we are
 

6655
02:12:16,400 --> 02:12:19,510
production level tutorial because we are
going to do a bit more advanced things

6656
02:12:19,510 --> 02:12:19,520
going to do a bit more advanced things
 

6657
02:12:19,520 --> 02:12:22,629
going to do a bit more advanced things
so that our iterations run faster. One

6658
02:12:22,629 --> 02:12:22,639
so that our iterations run faster. One
 

6659
02:12:22,639 --> 02:12:23,910
so that our iterations run faster. One
such advanced thing which you will

6660
02:12:23,910 --> 02:12:23,920
such advanced thing which you will
 

6661
02:12:23,920 --> 02:12:25,510
such advanced thing which you will
notice in the code is that we use

6662
02:12:25,510 --> 02:12:25,520
notice in the code is that we use
 

6663
02:12:25,520 --> 02:12:28,109
notice in the code is that we use
something like

6664
02:12:28,109 --> 02:12:28,119

 

6665
02:12:28,119 --> 02:12:30,390

torch.cast. The reason this is done is

6666
02:12:30,390 --> 02:12:30,400
torch.cast. The reason this is done is
 

6667
02:12:30,400 --> 02:12:31,830
torch.cast. The reason this is done is
that it enables something which is

6668
02:12:31,830 --> 02:12:31,840
that it enables something which is
 

6669
02:12:31,840 --> 02:12:34,790
that it enables something which is
called as automatic mixed precision. So

6670
02:12:34,790 --> 02:12:34,800
called as automatic mixed precision. So
 

6671
02:12:34,800 --> 02:12:37,270
called as automatic mixed precision. So
precision corresponds to float 32, float

6672
02:12:37,270 --> 02:12:37,280
precision corresponds to float 32, float
 

6673
02:12:37,280 --> 02:12:41,030
precision corresponds to float 32, float
16 etc. Um so usually by default let's

6674
02:12:41,030 --> 02:12:41,040
16 etc. Um so usually by default let's
 

6675
02:12:41,040 --> 02:12:43,189
16 etc. Um so usually by default let's
say if everything is in float 32 that

6676
02:12:43,189 --> 02:12:43,199
say if everything is in float 32 that
 

6677
02:12:43,199 --> 02:12:45,510
say if everything is in float 32 that
takes the longest time to compute. When

6678
02:12:45,510 --> 02:12:45,520
takes the longest time to compute. When
 

6679
02:12:45,520 --> 02:12:48,550
takes the longest time to compute. When
we use automatic mix precision the model

6680
02:12:48,550 --> 02:12:48,560
we use automatic mix precision the model
 

6681
02:12:48,560 --> 02:12:51,109
we use automatic mix precision the model
converts these to float 16 so that the

6682
02:12:51,109 --> 02:12:51,119
converts these to float 16 so that the
 

6683
02:12:51,119 --> 02:12:53,589
converts these to float 16 so that the
computations become faster. Now when are

6684
02:12:53,589 --> 02:12:53,599
computations become faster. Now when are
 

6685
02:12:53,599 --> 02:12:56,069
computations become faster. Now when are
the numbers converted to float 16. Let's

6686
02:12:56,069 --> 02:12:56,079
the numbers converted to float 16. Let's
 

6687
02:12:56,079 --> 02:12:57,430
the numbers converted to float 16. Let's
say if we are doing something like

6688
02:12:57,430 --> 02:12:57,440
say if we are doing something like
 

6689
02:12:57,440 --> 02:12:59,910
say if we are doing something like
matrix multiplications in the attention

6690
02:12:59,910 --> 02:12:59,920
matrix multiplications in the attention
 

6691
02:12:59,920 --> 02:13:02,069
matrix multiplications in the attention
score calculation or we are doing a

6692
02:13:02,069 --> 02:13:02,079
score calculation or we are doing a
 

6693
02:13:02,079 --> 02:13:04,950
score calculation or we are doing a
dropout let's say then the automatic

6694
02:13:04,950 --> 02:13:04,960
dropout let's say then the automatic
 

6695
02:13:04,960 --> 02:13:06,470
dropout let's say then the automatic
mixed precision would use something like

6696
02:13:06,470 --> 02:13:06,480
mixed precision would use something like
 

6697
02:13:06,480 --> 02:13:09,030
mixed precision would use something like
a float 16 because it's safer to compute

6698
02:13:09,030 --> 02:13:09,040
a float 16 because it's safer to compute
 

6699
02:13:09,040 --> 02:13:11,189
a float 16 because it's safer to compute
in lower precision over here but let's

6700
02:13:11,189 --> 02:13:11,199
in lower precision over here but let's
 

6701
02:13:11,199 --> 02:13:12,709
in lower precision over here but let's
say if we are doing soft max where

6702
02:13:12,709 --> 02:13:12,719
say if we are doing soft max where
 

6703
02:13:12,719 --> 02:13:14,870
say if we are doing soft max where
exponential operations are involved

6704
02:13:14,870 --> 02:13:14,880
exponential operations are involved
 

6705
02:13:14,880 --> 02:13:16,390
exponential operations are involved
cross entropy loss where where

6706
02:13:16,390 --> 02:13:16,400
cross entropy loss where where
 

6707
02:13:16,400 --> 02:13:19,109
cross entropy loss where where
logarithms are involved weight updates

6708
02:13:19,109 --> 02:13:19,119
logarithms are involved weight updates
 

6709
02:13:19,119 --> 02:13:21,109
logarithms are involved weight updates
etc all of these which have some

6710
02:13:21,109 --> 02:13:21,119
etc all of these which have some
 

6711
02:13:21,119 --> 02:13:22,870
etc all of these which have some
sensitive operations and which can lead

6712
02:13:22,870 --> 02:13:22,880
sensitive operations and which can lead
 

6713
02:13:22,880 --> 02:13:24,790
sensitive operations and which can lead
to a blow up of numbers if due to

6714
02:13:24,790 --> 02:13:24,800
to a blow up of numbers if due to
 

6715
02:13:24,800 --> 02:13:27,189
to a blow up of numbers if due to
exponential ial or log operations there

6716
02:13:27,189 --> 02:13:27,199
exponential ial or log operations there
 

6717
02:13:27,199 --> 02:13:29,310
exponential ial or log operations there
it's better to go with float

6718
02:13:29,310 --> 02:13:29,320
it's better to go with float
 

6719
02:13:29,320 --> 02:13:32,390
it's better to go with float
32. So in these operations we usually go

6720
02:13:32,390 --> 02:13:32,400
32. So in these operations we usually go
 

6721
02:13:32,400 --> 02:13:35,350
32. So in these operations we usually go
ahead with float 32 but for operations

6722
02:13:35,350 --> 02:13:35,360
ahead with float 32 but for operations
 

6723
02:13:35,360 --> 02:13:37,990
ahead with float 32 but for operations
such as matrix multiplications dropout

6724
02:13:37,990 --> 02:13:38,000
such as matrix multiplications dropout
 

6725
02:13:38,000 --> 02:13:41,550
such as matrix multiplications dropout
etc float 16 is also completely fine to

6726
02:13:41,550 --> 02:13:41,560
etc float 16 is also completely fine to
 

6727
02:13:41,560 --> 02:13:44,470
etc float 16 is also completely fine to
use and this automatic mixed precision

6728
02:13:44,470 --> 02:13:44,480
use and this automatic mixed precision
 

6729
02:13:44,480 --> 02:13:46,470
use and this automatic mixed precision
helps us do that. So wherever possible

6730
02:13:46,470 --> 02:13:46,480
helps us do that. So wherever possible
 

6731
02:13:46,480 --> 02:13:50,310
helps us do that. So wherever possible
the model uses float 16 and uh if not

6732
02:13:50,310 --> 02:13:50,320
the model uses float 16 and uh if not
 

6733
02:13:50,320 --> 02:13:52,709
the model uses float 16 and uh if not
then the model sticks to float 32. So

6734
02:13:52,709 --> 02:13:52,719
then the model sticks to float 32. So
 

6735
02:13:52,719 --> 02:13:54,470
then the model sticks to float 32. So
you'll see this in the code. So when we

6736
02:13:54,470 --> 02:13:54,480
you'll see this in the code. So when we
 

6737
02:13:54,480 --> 02:13:56,550
you'll see this in the code. So when we
go to step number six which is the

6738
02:13:56,550 --> 02:13:56,560
go to step number six which is the
 

6739
02:13:56,560 --> 02:13:59,910
go to step number six which is the
training configuration you'll see that

6740
02:13:59,910 --> 02:13:59,920
training configuration you'll see that
 

6741
02:13:59,920 --> 02:14:02,460
training configuration you'll see that
here this is where we are using the

6742
02:14:02,460 --> 02:14:02,470
here this is where we are using the
 

6743
02:14:02,470 --> 02:14:03,709
here this is where we are using the
[Music]

6744
02:14:03,709 --> 02:14:03,719
[Music]
 

6745
02:14:03,719 --> 02:14:05,910
[Music]
torch.cast. So this is important to

6746
02:14:05,910 --> 02:14:05,920
torch.cast. So this is important to
 

6747
02:14:05,920 --> 02:14:08,470
torch.cast. So this is important to
understand exactly why we are uh why we

6748
02:14:08,470 --> 02:14:08,480
understand exactly why we are uh why we
 

6749
02:14:08,480 --> 02:14:13,229
understand exactly why we are uh why we
are doing this uh um

6750
02:14:13,229 --> 02:14:13,239
are doing this uh um
 

6751
02:14:13,239 --> 02:14:15,510
are doing this uh um
torch.cast. Then the second thing which

6752
02:14:15,510 --> 02:14:15,520
torch.cast. Then the second thing which
 

6753
02:14:15,520 --> 02:14:17,270
torch.cast. Then the second thing which
is important to understand is that there

6754
02:14:17,270 --> 02:14:17,280
is important to understand is that there
 

6755
02:14:17,280 --> 02:14:18,629
is important to understand is that there
is something which is called as the

6756
02:14:18,629 --> 02:14:18,639
is something which is called as the
 

6757
02:14:18,639 --> 02:14:21,669
is something which is called as the
gradient accumulation. Okay. Now when we

6758
02:14:21,669 --> 02:14:21,679
gradient accumulation. Okay. Now when we
 

6759
02:14:21,679 --> 02:14:23,750
gradient accumulation. Okay. Now when we
go to the actual code, let's say we want

6760
02:14:23,750 --> 02:14:23,760
go to the actual code, let's say we want
 

6761
02:14:23,760 --> 02:14:26,109
go to the actual code, let's say we want
to train with a batch size of

6762
02:14:26,109 --> 02:14:26,119
to train with a batch size of
 

6763
02:14:26,119 --> 02:14:30,350
to train with a batch size of
1024. Um the problem is that

6764
02:14:30,350 --> 02:14:30,360
1024. Um the problem is that
 

6765
02:14:30,360 --> 02:14:33,270
1024. Um the problem is that
1024 input and output pairs will not fit

6766
02:14:33,270 --> 02:14:33,280
1024 input and output pairs will not fit
 

6767
02:14:33,280 --> 02:14:37,109
1024 input and output pairs will not fit
in my GPU at one time. If only if my GPU

6768
02:14:37,109 --> 02:14:37,119
in my GPU at one time. If only if my GPU
 

6769
02:14:37,119 --> 02:14:40,470
in my GPU at one time. If only if my GPU
can only accept let's say 32 samples. So

6770
02:14:40,470 --> 02:14:40,480
can only accept let's say 32 samples. So
 

6771
02:14:40,480 --> 02:14:42,390
can only accept let's say 32 samples. So
what we can do is that we can set

6772
02:14:42,390 --> 02:14:42,400
what we can do is that we can set
 

6773
02:14:42,400 --> 02:14:45,390
what we can do is that we can set
gradient accumulation steps equal to

6774
02:14:45,390 --> 02:14:45,400
gradient accumulation steps equal to
 

6775
02:14:45,400 --> 02:14:48,149
gradient accumulation steps equal to
32. So what that means is that we'll run

6776
02:14:48,149 --> 02:14:48,159
32. So what that means is that we'll run
 

6777
02:14:48,159 --> 02:14:51,510
32. So what that means is that we'll run
32 iterations. Um and then we'll update

6778
02:14:51,510 --> 02:14:51,520
32 iterations. Um and then we'll update
 

6779
02:14:51,520 --> 02:14:53,750
32 iterations. Um and then we'll update
the parameters essentially after every

6780
02:14:53,750 --> 02:14:53,760
the parameters essentially after every
 

6781
02:14:53,760 --> 02:14:54,830
the parameters essentially after every
32

6782
02:14:54,830 --> 02:14:54,840
32
 

6783
02:14:54,840 --> 02:14:57,350
32
steps. Instead of updating the

6784
02:14:57,350 --> 02:14:57,360
steps. Instead of updating the
 

6785
02:14:57,360 --> 02:15:00,550
steps. Instead of updating the
parameters after every 1024 steps, what

6786
02:15:00,550 --> 02:15:00,560
parameters after every 1024 steps, what
 

6787
02:15:00,560 --> 02:15:03,430
parameters after every 1024 steps, what
we do is that the parameters are updated

6788
02:15:03,430 --> 02:15:03,440
we do is that the parameters are updated
 

6789
02:15:03,440 --> 02:15:06,310
we do is that the parameters are updated
after 32 steps. So we can run 32

6790
02:15:06,310 --> 02:15:06,320
after 32 steps. So we can run 32
 

6791
02:15:06,320 --> 02:15:08,790
after 32 steps. So we can run 32
iterations because the b the GPU fits

6792
02:15:08,790 --> 02:15:08,800
iterations because the b the GPU fits
 

6793
02:15:08,800 --> 02:15:12,629
iterations because the b the GPU fits
only batch size of 32 and then we do it

6794
02:15:12,629 --> 02:15:12,639
only batch size of 32 and then we do it
 

6795
02:15:12,639 --> 02:15:17,229
only batch size of 32 and then we do it
uh then we do it once more, right? Um so

6796
02:15:17,229 --> 02:15:17,239
uh then we do it once more, right? Um so
 

6797
02:15:17,239 --> 02:15:19,350
uh then we do it once more, right? Um so
essentially we are not doing the

6798
02:15:19,350 --> 02:15:19,360
essentially we are not doing the
 

6799
02:15:19,360 --> 02:15:23,270
essentially we are not doing the
parameter update after every 1024 steps

6800
02:15:23,270 --> 02:15:23,280
parameter update after every 1024 steps
 

6801
02:15:23,280 --> 02:15:25,830
parameter update after every 1024 steps
but we do the parameter updates after 32

6802
02:15:25,830 --> 02:15:25,840
but we do the parameter updates after 32
 

6803
02:15:25,840 --> 02:15:28,629
but we do the parameter updates after 32
steps. This is done just so that if we

6804
02:15:28,629 --> 02:15:28,639
steps. This is done just so that if we
 

6805
02:15:28,639 --> 02:15:30,229
steps. This is done just so that if we
are doing the parameter updates let's

6806
02:15:30,229 --> 02:15:30,239
are doing the parameter updates let's
 

6807
02:15:30,239 --> 02:15:32,709
are doing the parameter updates let's
say after 1024 steps it means we'll have

6808
02:15:32,709 --> 02:15:32,719
say after 1024 steps it means we'll have
 

6809
02:15:32,719 --> 02:15:36,629
say after 1024 steps it means we'll have
to compute the gradients of batch size

6810
02:15:36,629 --> 02:15:36,639
to compute the gradients of batch size
 

6811
02:15:36,639 --> 02:15:38,790
to compute the gradients of batch size
equal to 1024 and we'll need to store

6812
02:15:38,790 --> 02:15:38,800
equal to 1024 and we'll need to store
 

6813
02:15:38,800 --> 02:15:40,950
equal to 1024 and we'll need to store
all those gradients but that is not

6814
02:15:40,950 --> 02:15:40,960
all those gradients but that is not
 

6815
02:15:40,960 --> 02:15:43,589
all those gradients but that is not
possible to do um that is not possible

6816
02:15:43,589 --> 02:15:43,599
possible to do um that is not possible
 

6817
02:15:43,599 --> 02:15:45,910
possible to do um that is not possible
to do in a GPU. If my GPU fits less

6818
02:15:45,910 --> 02:15:45,920
to do in a GPU. If my GPU fits less
 

6819
02:15:45,920 --> 02:15:49,430
to do in a GPU. If my GPU fits less
number of uh samples then we can only

6820
02:15:49,430 --> 02:15:49,440
number of uh samples then we can only
 

6821
02:15:49,440 --> 02:15:51,510
number of uh samples then we can only
calculate those many gradients and then

6822
02:15:51,510 --> 02:15:51,520
calculate those many gradients and then
 

6823
02:15:51,520 --> 02:15:54,030
calculate those many gradients and then
we can essentially update the

6824
02:15:54,030 --> 02:15:54,040
we can essentially update the
 

6825
02:15:54,040 --> 02:15:56,790
we can essentially update the
parameters. So here what I'll do is that

6826
02:15:56,790 --> 02:15:56,800
parameters. So here what I'll do is that
 

6827
02:15:56,800 --> 02:16:00,229
parameters. So here what I'll do is that
let's say uh you do first forward pass.

6828
02:16:00,229 --> 02:16:00,239
let's say uh you do first forward pass.
 

6829
02:16:00,239 --> 02:16:02,550
let's say uh you do first forward pass.
Let's say you do the this entire forward

6830
02:16:02,550 --> 02:16:02,560
Let's say you do the this entire forward
 

6831
02:16:02,560 --> 02:16:04,790
Let's say you do the this entire forward
pass then you calculate the gradient.

6832
02:16:04,790 --> 02:16:04,800
pass then you calculate the gradient.
 

6833
02:16:04,800 --> 02:16:07,030
pass then you calculate the gradient.
You do the forward pass once more. You

6834
02:16:07,030 --> 02:16:07,040
You do the forward pass once more. You
 

6835
02:16:07,040 --> 02:16:08,629
You do the forward pass once more. You
calculate the gradients like this. You

6836
02:16:08,629 --> 02:16:08,639
calculate the gradients like this. You
 

6837
02:16:08,639 --> 02:16:10,470
calculate the gradients like this. You
do it for 32 times right? So there will

6838
02:16:10,470 --> 02:16:10,480
do it for 32 times right? So there will
 

6839
02:16:10,480 --> 02:16:12,149
do it for 32 times right? So there will
be 32 gradients which which have

6840
02:16:12,149 --> 02:16:12,159
be 32 gradients which which have
 

6841
02:16:12,159 --> 02:16:14,550
be 32 gradients which which have
accumulated. Then what you do is that

6842
02:16:14,550 --> 02:16:14,560
accumulated. Then what you do is that
 

6843
02:16:14,560 --> 02:16:16,310
accumulated. Then what you do is that
the final gradient which is there you

6844
02:16:16,310 --> 02:16:16,320
the final gradient which is there you
 

6845
02:16:16,320 --> 02:16:18,550
the final gradient which is there you
then take the mean you then you then

6846
02:16:18,550 --> 02:16:18,560
then take the mean you then you then
 

6847
02:16:18,560 --> 02:16:20,790
then take the mean you then you then
take the you update the parameters based

6848
02:16:20,790 --> 02:16:20,800
take the you update the parameters based
 

6849
02:16:20,800 --> 02:16:22,870
take the you update the parameters based
on that final gradient. So you don't

6850
02:16:22,870 --> 02:16:22,880
on that final gradient. So you don't
 

6851
02:16:22,880 --> 02:16:24,910
on that final gradient. So you don't
update the parameters after every

6852
02:16:24,910 --> 02:16:24,920
update the parameters after every
 

6853
02:16:24,920 --> 02:16:27,189
update the parameters after every
gradient. So think about what happens

6854
02:16:27,189 --> 02:16:27,199
gradient. So think about what happens
 

6855
02:16:27,199 --> 02:16:29,189
gradient. So think about what happens
right in every step here one batch is

6856
02:16:29,189 --> 02:16:29,199
right in every step here one batch is
 

6857
02:16:29,199 --> 02:16:31,109
right in every step here one batch is
processed. So in the first step one

6858
02:16:31,109 --> 02:16:31,119
processed. So in the first step one
 

6859
02:16:31,119 --> 02:16:33,589
processed. So in the first step one
batch comes here x1 and y1 we do the

6860
02:16:33,589 --> 02:16:33,599
batch comes here x1 and y1 we do the
 

6861
02:16:33,599 --> 02:16:35,349
batch comes here x1 and y1 we do the
forward pass. In the next step the

6862
02:16:35,349 --> 02:16:35,359
forward pass. In the next step the
 

6863
02:16:35,359 --> 02:16:38,230
forward pass. In the next step the
second the second batch is the second

6864
02:16:38,230 --> 02:16:38,240
second the second batch is the second
 

6865
02:16:38,240 --> 02:16:41,509
second the second batch is the second
batch is obtained which is x2 y2.

6866
02:16:41,509 --> 02:16:41,519
batch is obtained which is x2 y2.
 

6867
02:16:41,519 --> 02:16:43,589
batch is obtained which is x2 y2.
Similarly, we do this until let's say

6868
02:16:43,589 --> 02:16:43,599
Similarly, we do this until let's say
 

6869
02:16:43,599 --> 02:16:46,070
Similarly, we do this until let's say
gradient accumulation step is 32. We do

6870
02:16:46,070 --> 02:16:46,080
gradient accumulation step is 32. We do
 

6871
02:16:46,080 --> 02:16:48,309
gradient accumulation step is 32. We do
this 32 times and until now remember we

6872
02:16:48,309 --> 02:16:48,319
this 32 times and until now remember we
 

6873
02:16:48,319 --> 02:16:50,549
this 32 times and until now remember we
have not updated the parameters. The

6874
02:16:50,549 --> 02:16:50,559
have not updated the parameters. The
 

6875
02:16:50,559 --> 02:16:53,509
have not updated the parameters. The
parameters will be updated only after 32

6876
02:16:53,509 --> 02:16:53,519
parameters will be updated only after 32
 

6877
02:16:53,519 --> 02:16:56,469
parameters will be updated only after 32
steps are done and then I go to the next

6878
02:16:56,469 --> 02:16:56,479
steps are done and then I go to the next
 

6879
02:16:56,479 --> 02:16:59,830
steps are done and then I go to the next
etc. So although there are 1024 batches,

6880
02:16:59,830 --> 02:16:59,840
etc. So although there are 1024 batches,
 

6881
02:16:59,840 --> 02:17:01,910
etc. So although there are 1024 batches,
the parameter updates are not done after

6882
02:17:01,910 --> 02:17:01,920
the parameter updates are not done after
 

6883
02:17:01,920 --> 02:17:04,469
the parameter updates are not done after
1024 batches. They are done after 32

6884
02:17:04,469 --> 02:17:04,479
1024 batches. They are done after 32
 

6885
02:17:04,479 --> 02:17:08,389
1024 batches. They are done after 32
batches.

6886
02:17:08,389 --> 02:17:08,399

 

6887
02:17:08,399 --> 02:17:12,709

Um so one more thing

6888
02:17:12,709 --> 02:17:12,719
Um so one more thing
 

6889
02:17:12,719 --> 02:17:14,549
Um so one more thing
uh okay one more thing I want to mention

6890
02:17:14,549 --> 02:17:14,559
uh okay one more thing I want to mention
 

6891
02:17:14,559 --> 02:17:16,070
uh okay one more thing I want to mention
is that gradient accumulation is

6892
02:17:16,070 --> 02:17:16,080
is that gradient accumulation is
 

6893
02:17:16,080 --> 02:17:18,629
is that gradient accumulation is
typically done if the batch sizes are

6894
02:17:18,629 --> 02:17:18,639
typically done if the batch sizes are
 

6895
02:17:18,639 --> 02:17:21,030
typically done if the batch sizes are
quite large okay so if the batch size is

6896
02:17:21,030 --> 02:17:21,040
quite large okay so if the batch size is
 

6897
02:17:21,040 --> 02:17:23,589
quite large okay so if the batch size is
equal to 1024 that's quite large right

6898
02:17:23,589 --> 02:17:23,599
equal to 1024 that's quite large right
 

6899
02:17:23,599 --> 02:17:26,070
equal to 1024 that's quite large right
because in every batch which you have x1

6900
02:17:26,070 --> 02:17:26,080
because in every batch which you have x1
 

6901
02:17:26,080 --> 02:17:29,790
because in every batch which you have x1
will have 1024 x2 will have 1024

6902
02:17:29,790 --> 02:17:29,800
will have 1024 x2 will have 1024
 

6903
02:17:29,800 --> 02:17:32,469
will have 1024 x2 will have 1024
etc and ideally the practice is to

6904
02:17:32,469 --> 02:17:32,479
etc and ideally the practice is to
 

6905
02:17:32,479 --> 02:17:35,669
etc and ideally the practice is to
process one entire batch and then update

6906
02:17:35,669 --> 02:17:35,679
process one entire batch and then update
 

6907
02:17:35,679 --> 02:17:37,830
process one entire batch and then update
the parameters but that's not what's

6908
02:17:37,830 --> 02:17:37,840
the parameters but that's not what's
 

6909
02:17:37,840 --> 02:17:39,509
the parameters but that's not what's
done over here. If the batch is used

6910
02:17:39,509 --> 02:17:39,519
done over here. If the batch is used
 

6911
02:17:39,519 --> 02:17:41,750
done over here. If the batch is used
then only 32 are done. The parameters

6912
02:17:41,750 --> 02:17:41,760
then only 32 are done. The parameters
 

6913
02:17:41,760 --> 02:17:43,910
then only 32 are done. The parameters
are updated. Then we go to the next the

6914
02:17:43,910 --> 02:17:43,920
are updated. Then we go to the next the
 

6915
02:17:43,920 --> 02:17:47,629
are updated. Then we go to the next the
parameters are updated

6916
02:17:47,629 --> 02:17:47,639

 

6917
02:17:47,639 --> 02:17:49,709

etc.

6918
02:17:49,709 --> 02:17:49,719
etc.
 

6919
02:17:49,719 --> 02:17:53,589
etc.
Um okay so let me clarify this once more

6920
02:17:53,589 --> 02:17:53,599
Um okay so let me clarify this once more
 

6921
02:17:53,599 --> 02:17:55,429
Um okay so let me clarify this once more
because I know that this part is a bit

6922
02:17:55,429 --> 02:17:55,439
because I know that this part is a bit
 

6923
02:17:55,439 --> 02:17:57,429
because I know that this part is a bit
tricky. So I want to mention that let's

6924
02:17:57,429 --> 02:17:57,439
tricky. So I want to mention that let's
 

6925
02:17:57,439 --> 02:17:59,830
tricky. So I want to mention that let's
say uh let me do this once more. Let's

6926
02:17:59,830 --> 02:17:59,840
say uh let me do this once more. Let's
 

6927
02:17:59,840 --> 02:18:02,309
say uh let me do this once more. Let's
say this is my one batch. Okay. And my

6928
02:18:02,309 --> 02:18:02,319
say this is my one batch. Okay. And my
 

6929
02:18:02,319 --> 02:18:06,709
say this is my one batch. Okay. And my
one batch now has 1024 such samples. And

6930
02:18:06,709 --> 02:18:06,719
one batch now has 1024 such samples. And
 

6931
02:18:06,719 --> 02:18:10,270
one batch now has 1024 such samples. And
this is my x1 and this is my

6932
02:18:10,270 --> 02:18:10,280
this is my x1 and this is my
 

6933
02:18:10,280 --> 02:18:14,070
this is my x1 and this is my
y1. Okay. What gradient accumulation

6934
02:18:14,070 --> 02:18:14,080
y1. Okay. What gradient accumulation
 

6935
02:18:14,080 --> 02:18:15,589
y1. Okay. What gradient accumulation
actually does is that we first go

6936
02:18:15,589 --> 02:18:15,599
actually does is that we first go
 

6937
02:18:15,599 --> 02:18:16,669
actually does is that we first go
through this

6938
02:18:16,669 --> 02:18:16,679
through this
 

6939
02:18:16,679 --> 02:18:19,830
through this
first. It's it's gone through the entire

6940
02:18:19,830 --> 02:18:19,840
first. It's it's gone through the entire
 

6941
02:18:19,840 --> 02:18:22,549
first. It's it's gone through the entire
model. Then we update uh then we get the

6942
02:18:22,549 --> 02:18:22,559
model. Then we update uh then we get the
 

6943
02:18:22,559 --> 02:18:25,429
model. Then we update uh then we get the
loss function. Then we uh get the

6944
02:18:25,429 --> 02:18:25,439
loss function. Then we uh get the
 

6945
02:18:25,439 --> 02:18:27,110
loss function. Then we uh get the
gradients. Okay. Then we go through the

6946
02:18:27,110 --> 02:18:27,120
gradients. Okay. Then we go through the
 

6947
02:18:27,120 --> 02:18:29,110
gradients. Okay. Then we go through the
second. We get the loss function. We get

6948
02:18:29,110 --> 02:18:29,120
second. We get the loss function. We get
 

6949
02:18:29,120 --> 02:18:31,910
second. We get the loss function. We get
the gradients. Similarly, we go to 32.

6950
02:18:31,910 --> 02:18:31,920
the gradients. Similarly, we go to 32.
 

6951
02:18:31,920 --> 02:18:33,910
the gradients. Similarly, we go to 32.
This is the gradient accumulation steps.

6952
02:18:33,910 --> 02:18:33,920
This is the gradient accumulation steps.
 

6953
02:18:33,920 --> 02:18:35,509
This is the gradient accumulation steps.
And until then, we have got all the

6954
02:18:35,509 --> 02:18:35,519
And until then, we have got all the
 

6955
02:18:35,519 --> 02:18:37,589
And until then, we have got all the
gradients. Once we have reached until

6956
02:18:37,589 --> 02:18:37,599
gradients. Once we have reached until
 

6957
02:18:37,599 --> 02:18:39,509
gradients. Once we have reached until
32, that's when we update the loss

6958
02:18:39,509 --> 02:18:39,519
32, that's when we update the loss
 

6959
02:18:39,519 --> 02:18:41,669
32, that's when we update the loss
function or rather that's when we update

6960
02:18:41,669 --> 02:18:41,679
function or rather that's when we update
 

6961
02:18:41,679 --> 02:18:44,469
function or rather that's when we update
my parameters. Okay. So, the parameter

6962
02:18:44,469 --> 02:18:44,479
my parameters. Okay. So, the parameter
 

6963
02:18:44,479 --> 02:18:46,629
my parameters. Okay. So, the parameter
update is done after these gradient

6964
02:18:46,629 --> 02:18:46,639
update is done after these gradient
 

6965
02:18:46,639 --> 02:18:48,469
update is done after these gradient
accumulation steps, not after I go

6966
02:18:48,469 --> 02:18:48,479
accumulation steps, not after I go
 

6967
02:18:48,479 --> 02:18:51,669
accumulation steps, not after I go
through the entire batch. That's what uh

6968
02:18:51,669 --> 02:18:51,679
through the entire batch. That's what uh
 

6969
02:18:51,679 --> 02:18:53,309
through the entire batch. That's what uh
is the main point of gradient

6970
02:18:53,309 --> 02:18:53,319
is the main point of gradient
 

6971
02:18:53,319 --> 02:18:55,910
is the main point of gradient
accumulation. And you'll see gradient

6972
02:18:55,910 --> 02:18:55,920
accumulation. And you'll see gradient
 

6973
02:18:55,920 --> 02:18:58,790
accumulation. And you'll see gradient
accumulation in the code also. So if you

6974
02:18:58,790 --> 02:18:58,800
accumulation in the code also. So if you
 

6975
02:18:58,800 --> 02:19:00,709
accumulation in the code also. So if you
see here gradient accumulation steps

6976
02:19:00,709 --> 02:19:00,719
see here gradient accumulation steps
 

6977
02:19:00,719 --> 02:19:03,990
see here gradient accumulation steps
equal to 32 right so you're accumulating

6978
02:19:03,990 --> 02:19:04,000
equal to 32 right so you're accumulating
 

6979
02:19:04,000 --> 02:19:05,830
equal to 32 right so you're accumulating
gradients up till that point and then

6980
02:19:05,830 --> 02:19:05,840
gradients up till that point and then
 

6981
02:19:05,840 --> 02:19:08,429
gradients up till that point and then
you are updating the parameters

6982
02:19:08,429 --> 02:19:08,439
you are updating the parameters
 

6983
02:19:08,439 --> 02:19:10,309
you are updating the parameters
essentially that's the second thing

6984
02:19:10,309 --> 02:19:10,319
essentially that's the second thing
 

6985
02:19:10,319 --> 02:19:12,549
essentially that's the second thing
which is important to note in this uh

6986
02:19:12,549 --> 02:19:12,559
which is important to note in this uh
 

6987
02:19:12,559 --> 02:19:14,709
which is important to note in this uh
training loop. The third thing which is

6988
02:19:14,709 --> 02:19:14,719
training loop. The third thing which is
 

6989
02:19:14,719 --> 02:19:16,469
training loop. The third thing which is
very important to note is how we are

6990
02:19:16,469 --> 02:19:16,479
very important to note is how we are
 

6991
02:19:16,479 --> 02:19:19,030
very important to note is how we are
doing the training process itself. Okay.

6992
02:19:19,030 --> 02:19:19,040
doing the training process itself. Okay.
 

6993
02:19:19,040 --> 02:19:21,270
doing the training process itself. Okay.
So uh when we are doing the training

6994
02:19:21,270 --> 02:19:21,280
So uh when we are doing the training
 

6995
02:19:21,280 --> 02:19:23,030
So uh when we are doing the training
process there are optimizers which need

6996
02:19:23,030 --> 02:19:23,040
process there are optimizers which need
 

6997
02:19:23,040 --> 02:19:24,790
process there are optimizers which need
to be used and in this case we are going

6998
02:19:24,790 --> 02:19:24,800
to be used and in this case we are going
 

6999
02:19:24,800 --> 02:19:27,349
to be used and in this case we are going
to use the Adam optimizer. uh with

7000
02:19:27,349 --> 02:19:27,359
to use the Adam optimizer. uh with
 

7001
02:19:27,359 --> 02:19:29,429
to use the Adam optimizer. uh with
weight decay. So if you scroll down over

7002
02:19:29,429 --> 02:19:29,439
weight decay. So if you scroll down over
 

7003
02:19:29,439 --> 02:19:30,950
weight decay. So if you scroll down over
here, you'll see that we are using the

7004
02:19:30,950 --> 02:19:30,960
here, you'll see that we are using the
 

7005
02:19:30,960 --> 02:19:33,669
here, you'll see that we are using the
Adam MW optimizer that just better so

7006
02:19:33,669 --> 02:19:33,679
Adam MW optimizer that just better so
 

7007
02:19:33,679 --> 02:19:35,190
Adam MW optimizer that just better so
that we don't get stuck in a local

7008
02:19:35,190 --> 02:19:35,200
that we don't get stuck in a local
 

7009
02:19:35,200 --> 02:19:37,750
that we don't get stuck in a local
minima and then training becomes stable.

7010
02:19:37,750 --> 02:19:37,760
minima and then training becomes stable.
 

7011
02:19:37,760 --> 02:19:39,349
minima and then training becomes stable.
But one more thing which you will see in

7012
02:19:39,349 --> 02:19:39,359
But one more thing which you will see in
 

7013
02:19:39,359 --> 02:19:41,270
But one more thing which you will see in
the code is that we use a learning rate

7014
02:19:41,270 --> 02:19:41,280
the code is that we use a learning rate
 

7015
02:19:41,280 --> 02:19:43,110
the code is that we use a learning rate
which looks something like this. The

7016
02:19:43,110 --> 02:19:43,120
which looks something like this. The
 

7017
02:19:43,120 --> 02:19:45,030
which looks something like this. The
learning rate warms up in a linear

7018
02:19:45,030 --> 02:19:45,040
learning rate warms up in a linear
 

7019
02:19:45,040 --> 02:19:47,110
learning rate warms up in a linear
manner and then it decays in sort of

7020
02:19:47,110 --> 02:19:47,120
manner and then it decays in sort of
 

7021
02:19:47,120 --> 02:19:49,590
manner and then it decays in sort of
like a cosine manner. So the learning

7022
02:19:49,590 --> 02:19:49,600
like a cosine manner. So the learning
 

7023
02:19:49,600 --> 02:19:51,510
like a cosine manner. So the learning
rate is a combination of a warm up plus

7024
02:19:51,510 --> 02:19:51,520
rate is a combination of a warm up plus
 

7025
02:19:51,520 --> 02:19:53,670
rate is a combination of a warm up plus
a decay regime that is used again for

7026
02:19:53,670 --> 02:19:53,680
a decay regime that is used again for
 

7027
02:19:53,680 --> 02:19:55,830
a decay regime that is used again for
stabilizing the training. you'll see

7028
02:19:55,830 --> 02:19:55,840
stabilizing the training. you'll see
 

7029
02:19:55,840 --> 02:19:58,630
stabilizing the training. you'll see
that we have a learning a linear a

7030
02:19:58,630 --> 02:19:58,640
that we have a learning a linear a
 

7031
02:19:58,640 --> 02:20:00,950
that we have a learning a linear a
linear function for the warm-up and then

7032
02:20:00,950 --> 02:20:00,960
linear function for the warm-up and then
 

7033
02:20:00,960 --> 02:20:04,389
linear function for the warm-up and then
a cosine decay. So that's my learning

7034
02:20:04,389 --> 02:20:04,399
a cosine decay. So that's my learning
 

7035
02:20:04,399 --> 02:20:07,469
a cosine decay. So that's my learning
rate which I'm going to use with my uh

7036
02:20:07,469 --> 02:20:07,479
rate which I'm going to use with my uh
 

7037
02:20:07,479 --> 02:20:09,750
rate which I'm going to use with my uh
optimizer. Generally this is seen to

7038
02:20:09,750 --> 02:20:09,760
optimizer. Generally this is seen to
 

7039
02:20:09,760 --> 02:20:13,190
optimizer. Generally this is seen to
have the best uh performance when

7040
02:20:13,190 --> 02:20:13,200
have the best uh performance when
 

7041
02:20:13,200 --> 02:20:15,190
have the best uh performance when
training language models such as this.

7042
02:20:15,190 --> 02:20:15,200
training language models such as this.
 

7043
02:20:15,200 --> 02:20:18,429
training language models such as this.
So we are using this optimization

7044
02:20:18,429 --> 02:20:18,439
So we are using this optimization
 

7045
02:20:18,439 --> 02:20:20,870
So we are using this optimization
routine. And then finally what we are

7046
02:20:20,870 --> 02:20:20,880
routine. And then finally what we are
 

7047
02:20:20,880 --> 02:20:22,230
routine. And then finally what we are
going to do is that we are going to use

7048
02:20:22,230 --> 02:20:22,240
going to do is that we are going to use
 

7049
02:20:22,240 --> 02:20:24,630
going to do is that we are going to use
the Adam MW optimizer which I mentioned.

7050
02:20:24,630 --> 02:20:24,640
the Adam MW optimizer which I mentioned.
 

7051
02:20:24,640 --> 02:20:26,790
the Adam MW optimizer which I mentioned.
Now all of these steps are mentioned in

7052
02:20:26,790 --> 02:20:26,800
Now all of these steps are mentioned in
 

7053
02:20:26,800 --> 02:20:29,270
Now all of these steps are mentioned in
step number six and step number seven. I

7054
02:20:29,270 --> 02:20:29,280
step number six and step number seven. I
 

7055
02:20:29,280 --> 02:20:31,030
step number six and step number seven. I
already mentioned the autocast to you.

7056
02:20:31,030 --> 02:20:31,040
already mentioned the autocast to you.
 

7057
02:20:31,040 --> 02:20:32,230
already mentioned the autocast to you.
So that is done the gradient

7058
02:20:32,230 --> 02:20:32,240
So that is done the gradient
 

7059
02:20:32,240 --> 02:20:34,790
So that is done the gradient
accumulation step. Uh and this is the

7060
02:20:34,790 --> 02:20:34,800
accumulation step. Uh and this is the
 

7061
02:20:34,800 --> 02:20:35,630
accumulation step. Uh and this is the
learning

7062
02:20:35,630 --> 02:20:35,640
learning
 

7063
02:20:35,640 --> 02:20:38,550
learning
rate. Learning rate a linear regime plus

7064
02:20:38,550 --> 02:20:38,560
rate. Learning rate a linear regime plus
 

7065
02:20:38,560 --> 02:20:40,750
rate. Learning rate a linear regime plus
cosine regime followed with adm

7066
02:20:40,750 --> 02:20:40,760
cosine regime followed with adm
 

7067
02:20:40,760 --> 02:20:43,030
cosine regime followed with adm
optimizer. One more crucial thing to

7068
02:20:43,030 --> 02:20:43,040
optimizer. One more crucial thing to
 

7069
02:20:43,040 --> 02:20:44,309
optimizer. One more crucial thing to
note here is that these are the

7070
02:20:44,309 --> 02:20:44,319
note here is that these are the
 

7071
02:20:44,319 --> 02:20:46,309
note here is that these are the
hyperparameters which actually I would

7072
02:20:46,309 --> 02:20:46,319
hyperparameters which actually I would
 

7073
02:20:46,319 --> 02:20:48,230
hyperparameters which actually I would
encourage you to explore around a bit

7074
02:20:48,230 --> 02:20:48,240
encourage you to explore around a bit
 

7075
02:20:48,240 --> 02:20:50,469
encourage you to explore around a bit
after I share this code file with you

7076
02:20:50,469 --> 02:20:50,479
after I share this code file with you
 

7077
02:20:50,479 --> 02:20:52,309
after I share this code file with you
and note some of the things which which

7078
02:20:52,309 --> 02:20:52,319
and note some of the things which which
 

7079
02:20:52,319 --> 02:20:53,990
and note some of the things which which
are there. We are using a batch size

7080
02:20:53,990 --> 02:20:54,000
are there. We are using a batch size
 

7081
02:20:54,000 --> 02:20:56,030
are there. We are using a batch size
essentially of 1

7082
02:20:56,030 --> 02:20:56,040
essentially of 1
 

7083
02:20:56,040 --> 02:21:00,309
essentially of 1
32. Um and uh so batch size which we the

7084
02:21:00,309 --> 02:21:00,319
32. Um and uh so batch size which we the
 

7085
02:21:00,319 --> 02:21:04,710
32. Um and uh so batch size which we the
batch size which we are using is 32 and

7086
02:21:04,710 --> 02:21:04,720
batch size which we are using is 32 and
 

7087
02:21:04,720 --> 02:21:06,790
batch size which we are using is 32 and
the gradient accumulation steps is equal

7088
02:21:06,790 --> 02:21:06,800
the gradient accumulation steps is equal
 

7089
02:21:06,800 --> 02:21:08,389
the gradient accumulation steps is equal
to the batch size. So in this case my

7090
02:21:08,389 --> 02:21:08,399
to the batch size. So in this case my
 

7091
02:21:08,399 --> 02:21:10,070
to the batch size. So in this case my
update will be done after every batch

7092
02:21:10,070 --> 02:21:10,080
update will be done after every batch
 

7093
02:21:10,080 --> 02:21:13,030
update will be done after every batch
size. But if you choose to have a larger

7094
02:21:13,030 --> 02:21:13,040
size. But if you choose to have a larger
 

7095
02:21:13,040 --> 02:21:14,710
size. But if you choose to have a larger
batch size you can stick with gradient

7096
02:21:14,710 --> 02:21:14,720
batch size you can stick with gradient
 

7097
02:21:14,720 --> 02:21:17,349
batch size you can stick with gradient
accumulation steps of 32. My block size

7098
02:21:17,349 --> 02:21:17,359
accumulation steps of 32. My block size
 

7099
02:21:17,359 --> 02:21:19,590
accumulation steps of 32. My block size
is equal to 128. Remember that's my

7100
02:21:19,590 --> 02:21:19,600
is equal to 128. Remember that's my
 

7101
02:21:19,600 --> 02:21:23,110
is equal to 128. Remember that's my
context size. If you go over to the

7102
02:21:23,110 --> 02:21:23,120
context size. If you go over to the
 

7103
02:21:23,120 --> 02:21:25,670
context size. If you go over to the
earlier

7104
02:21:25,670 --> 02:21:25,680
earlier
 

7105
02:21:25,680 --> 02:21:27,750
earlier
uh over here when you create the input

7106
02:21:27,750 --> 02:21:27,760
uh over here when you create the input
 

7107
02:21:27,760 --> 02:21:29,750
uh over here when you create the input
output pairs the block size is equal to

7108
02:21:29,750 --> 02:21:29,760
output pairs the block size is equal to
 

7109
02:21:29,760 --> 02:21:31,349
output pairs the block size is equal to
my context size. We have seen that in

7110
02:21:31,349 --> 02:21:31,359
my context size. We have seen that in
 

7111
02:21:31,359 --> 02:21:35,190
my context size. We have seen that in
the earlier part of this video. Um then

7112
02:21:35,190 --> 02:21:35,200
the earlier part of this video. Um then
 

7113
02:21:35,200 --> 02:21:37,349
the earlier part of this video. Um then
the batch size and block size are done

7114
02:21:37,349 --> 02:21:37,359
the batch size and block size are done
 

7115
02:21:37,359 --> 02:21:39,110
the batch size and block size are done
the evaluation iterations which means

7116
02:21:39,110 --> 02:21:39,120
the evaluation iterations which means
 

7117
02:21:39,120 --> 02:21:40,950
the evaluation iterations which means
I'm going to print out my losses after

7118
02:21:40,950 --> 02:21:40,960
I'm going to print out my losses after
 

7119
02:21:40,960 --> 02:21:43,670
I'm going to print out my losses after
every 500 iterations. Remember here what

7120
02:21:43,670 --> 02:21:43,680
every 500 iterations. Remember here what
 

7121
02:21:43,680 --> 02:21:46,150
every 500 iterations. Remember here what
we did if the evaluation iterations are

7122
02:21:46,150 --> 02:21:46,160
we did if the evaluation iterations are
 

7123
02:21:46,160 --> 02:21:48,309
we did if the evaluation iterations are
500 in each iteration we are getting a

7124
02:21:48,309 --> 02:21:48,319
500 in each iteration we are getting a
 

7125
02:21:48,319 --> 02:21:50,230
500 in each iteration we are getting a
batch and we are getting its losses

7126
02:21:50,230 --> 02:21:50,240
batch and we are getting its losses
 

7127
02:21:50,240 --> 02:21:53,910
batch and we are getting its losses
right so this is essentially done for uh

7128
02:21:53,910 --> 02:21:53,920
right so this is essentially done for uh
 

7129
02:21:53,920 --> 02:21:57,030
right so this is essentially done for uh
uh this is done for 500 iterations and

7130
02:21:57,030 --> 02:21:57,040
uh this is done for 500 iterations and
 

7131
02:21:57,040 --> 02:21:58,389
uh this is done for 500 iterations and
then I take the mean of all those

7132
02:21:58,389 --> 02:21:58,399
then I take the mean of all those
 

7133
02:21:58,399 --> 02:21:59,990
then I take the mean of all those
iterations and I'm then then I'm going

7134
02:21:59,990 --> 02:22:00,000
iterations and I'm then then I'm going
 

7135
02:22:00,000 --> 02:22:02,030
iterations and I'm then then I'm going
to report the

7136
02:22:02,030 --> 02:22:02,040
to report the
 

7137
02:22:02,040 --> 02:22:04,630
to report the
loss then what else is the

7138
02:22:04,630 --> 02:22:04,640
loss then what else is the
 

7139
02:22:04,640 --> 02:22:06,710
loss then what else is the
hyperparameter we have the learning rate

7140
02:22:06,710 --> 02:22:06,720
hyperparameter we have the learning rate
 

7141
02:22:06,720 --> 02:22:09,349
hyperparameter we have the learning rate
minimum learning rate 5 into 10 minus 4

7142
02:22:09,349 --> 02:22:09,359
minimum learning rate 5 into 10 minus 4
 

7143
02:22:09,359 --> 02:22:10,870
minimum learning rate 5 into 10 minus 4
the warm-up steps these are the

7144
02:22:10,870 --> 02:22:10,880
the warm-up steps these are the
 

7145
02:22:10,880 --> 02:22:13,349
the warm-up steps these are the
hyperparameters in the in my learning

7146
02:22:13,349 --> 02:22:13,359
hyperparameters in the in my learning
 

7147
02:22:13,359 --> 02:22:16,870
hyperparameters in the in my learning
rate routine and my learning rate 1 into

7148
02:22:16,870 --> 02:22:16,880
rate routine and my learning rate 1 into
 

7149
02:22:16,880 --> 02:22:19,349
rate routine and my learning rate 1 into
so 10 to -4 essentially and the number

7150
02:22:19,349 --> 02:22:19,359
so 10 to -4 essentially and the number
 

7151
02:22:19,359 --> 02:22:21,030
so 10 to -4 essentially and the number
of iterations which I have that's going

7152
02:22:21,030 --> 02:22:21,040
of iterations which I have that's going
 

7153
02:22:21,040 --> 02:22:23,590
of iterations which I have that's going
to be equal to 20,000 all of these are

7154
02:22:23,590 --> 02:22:23,600
to be equal to 20,000 all of these are
 

7155
02:22:23,600 --> 02:22:25,510
to be equal to 20,000 all of these are
hyperparameters and you can feel free to

7156
02:22:25,510 --> 02:22:25,520
hyperparameters and you can feel free to
 

7157
02:22:25,520 --> 02:22:27,190
hyperparameters and you can feel free to
play around with this as you explore

7158
02:22:27,190 --> 02:22:27,200
play around with this as you explore
 

7159
02:22:27,200 --> 02:22:28,429
play around with this as you explore
more with this

7160
02:22:28,429 --> 02:22:28,439
more with this
 

7161
02:22:28,439 --> 02:22:31,030
more with this
code now once you define all of these

7162
02:22:31,030 --> 02:22:31,040
code now once you define all of these
 

7163
02:22:31,040 --> 02:22:32,550
code now once you define all of these
hyperparameters and once you have

7164
02:22:32,550 --> 02:22:32,560
hyperparameters and once you have
 

7165
02:22:32,560 --> 02:22:35,110
hyperparameters and once you have
understood the different things such as

7166
02:22:35,110 --> 02:22:35,120
understood the different things such as
 

7167
02:22:35,120 --> 02:22:37,510
understood the different things such as
uh automatic mixed precision gradient

7168
02:22:37,510 --> 02:22:37,520
uh automatic mixed precision gradient
 

7169
02:22:37,520 --> 02:22:40,950
uh automatic mixed precision gradient
accumulation and uh the learning rate

7170
02:22:40,950 --> 02:22:40,960
accumulation and uh the learning rate
 

7171
02:22:40,960 --> 02:22:43,990
accumulation and uh the learning rate
warm-up plus DK then we can go ahead to

7172
02:22:43,990 --> 02:22:44,000
warm-up plus DK then we can go ahead to
 

7173
02:22:44,000 --> 02:22:45,830
warm-up plus DK then we can go ahead to
pre-train the small language model. This

7174
02:22:45,830 --> 02:22:45,840
pre-train the small language model. This
 

7175
02:22:45,840 --> 02:22:47,510
pre-train the small language model. This
is the last step in the pre-training

7176
02:22:47,510 --> 02:22:47,520
is the last step in the pre-training
 

7177
02:22:47,520 --> 02:22:49,750
is the last step in the pre-training
part and you'll see that this is the

7178
02:22:49,750 --> 02:22:49,760
part and you'll see that this is the
 

7179
02:22:49,760 --> 02:22:51,990
part and you'll see that this is the
pre-training code. So what exactly

7180
02:22:51,990 --> 02:22:52,000
pre-training code. So what exactly
 

7181
02:22:52,000 --> 02:22:54,790
pre-training code. So what exactly
happens in the pre-training is that u

7182
02:22:54,790 --> 02:22:54,800
happens in the pre-training is that u
 

7183
02:22:54,800 --> 02:23:00,870
happens in the pre-training is that u
there are 1 2 3 4 5 6 7 and eight steps

7184
02:23:00,870 --> 02:23:00,880
there are 1 2 3 4 5 6 7 and eight steps
 

7185
02:23:00,880 --> 02:23:02,550
there are 1 2 3 4 5 6 7 and eight steps
which happen and I think now you are

7186
02:23:02,550 --> 02:23:02,560
which happen and I think now you are
 

7187
02:23:02,560 --> 02:23:04,230
which happen and I think now you are
fully equipped to understand all these

7188
02:23:04,230 --> 02:23:04,240
fully equipped to understand all these
 

7189
02:23:04,240 --> 02:23:06,790
fully equipped to understand all these
eight steps. For each iteration we have

7190
02:23:06,790 --> 02:23:06,800
eight steps. For each iteration we have
 

7191
02:23:06,800 --> 02:23:09,590
eight steps. For each iteration we have
to choose an X and a Y. What this means

7192
02:23:09,590 --> 02:23:09,600
to choose an X and a Y. What this means
 

7193
02:23:09,600 --> 02:23:11,230
to choose an X and a Y. What this means
is that for each

7194
02:23:11,230 --> 02:23:11,240
is that for each
 

7195
02:23:11,240 --> 02:23:14,309
is that for each
iteration what we have to do is that we

7196
02:23:14,309 --> 02:23:14,319
iteration what we have to do is that we
 

7197
02:23:14,319 --> 02:23:16,790
iteration what we have to do is that we
have to essentially go through we have

7198
02:23:16,790 --> 02:23:16,800
have to essentially go through we have
 

7199
02:23:16,800 --> 02:23:19,750
have to essentially go through we have
to go through my data set and remember

7200
02:23:19,750 --> 02:23:19,760
to go through my data set and remember
 

7201
02:23:19,760 --> 02:23:22,389
to go through my data set and remember
there is this torch rand. What this does

7202
02:23:22,389 --> 02:23:22,399
there is this torch rand. What this does
 

7203
02:23:22,399 --> 02:23:26,030
there is this torch rand. What this does
is that this will essentially

7204
02:23:26,030 --> 02:23:26,040
is that this will essentially
 

7205
02:23:26,040 --> 02:23:29,110
is that this will essentially
choose random indexes from my data set

7206
02:23:29,110 --> 02:23:29,120
choose random indexes from my data set
 

7207
02:23:29,120 --> 02:23:31,030
choose random indexes from my data set
and that will give me my input pair. So

7208
02:23:31,030 --> 02:23:31,040
and that will give me my input pair. So
 

7209
02:23:31,040 --> 02:23:32,389
and that will give me my input pair. So
let's say this is the first index,

7210
02:23:32,389 --> 02:23:32,399
let's say this is the first index,
 

7211
02:23:32,399 --> 02:23:34,309
let's say this is the first index,
second, third and fourth. If the batch

7212
02:23:34,309 --> 02:23:34,319
second, third and fourth. If the batch
 

7213
02:23:34,319 --> 02:23:37,110
second, third and fourth. If the batch
size is four, this will be x1, x2, x3

7214
02:23:37,110 --> 02:23:37,120
size is four, this will be x1, x2, x3
 

7215
02:23:37,120 --> 02:23:43,030
size is four, this will be x1, x2, x3
and x4. And uh my y1, y2, y3 and y4 will

7216
02:23:43,030 --> 02:23:43,040
and x4. And uh my y1, y2, y3 and y4 will
 

7217
02:23:43,040 --> 02:23:45,590
and x4. And uh my y1, y2, y3 and y4 will
be these y1, y2, y3 and y4 just shifted

7218
02:23:45,590 --> 02:23:45,600
be these y1, y2, y3 and y4 just shifted
 

7219
02:23:45,600 --> 02:23:47,830
be these y1, y2, y3 and y4 just shifted
to the right hand side by one. So that's

7220
02:23:47,830 --> 02:23:47,840
to the right hand side by one. So that's
 

7221
02:23:47,840 --> 02:23:49,270
to the right hand side by one. So that's
the first step which is getting the

7222
02:23:49,270 --> 02:23:49,280
the first step which is getting the
 

7223
02:23:49,280 --> 02:23:53,190
the first step which is getting the
batch. So I get my batch. Then what I do

7224
02:23:53,190 --> 02:23:53,200
batch. So I get my batch. Then what I do
 

7225
02:23:53,200 --> 02:23:56,309
batch. So I get my batch. Then what I do
is I pass my entire batch through the

7226
02:23:56,309 --> 02:23:56,319
is I pass my entire batch through the
 

7227
02:23:56,319 --> 02:23:57,990
is I pass my entire batch through the
model to get the logits. This is the

7228
02:23:57,990 --> 02:23:58,000
model to get the logits. This is the
 

7229
02:23:58,000 --> 02:23:59,830
model to get the logits. This is the
forward pass which we we had seen. We

7230
02:23:59,830 --> 02:23:59,840
forward pass which we we had seen. We
 

7231
02:23:59,840 --> 02:24:01,349
forward pass which we we had seen. We
take a batch, we pass it through the

7232
02:24:01,349 --> 02:24:01,359
take a batch, we pass it through the
 

7233
02:24:01,359 --> 02:24:04,349
take a batch, we pass it through the
entire uh language model and we get this

7234
02:24:04,349 --> 02:24:04,359
entire uh language model and we get this
 

7235
02:24:04,359 --> 02:24:06,950
entire uh language model and we get this
logits. Once we have the logits matrix,

7236
02:24:06,950 --> 02:24:06,960
logits. Once we have the logits matrix,
 

7237
02:24:06,960 --> 02:24:08,710
logits. Once we have the logits matrix,
then we calculate the loss between the

7238
02:24:08,710 --> 02:24:08,720
then we calculate the loss between the
 

7239
02:24:08,720 --> 02:24:10,750
then we calculate the loss between the
logits and my

7240
02:24:10,750 --> 02:24:10,760
logits and my
 

7241
02:24:10,760 --> 02:24:13,030
logits and my
targets. That's my loss function. So

7242
02:24:13,030 --> 02:24:13,040
targets. That's my loss function. So
 

7243
02:24:13,040 --> 02:24:15,750
targets. That's my loss function. So
that I calculate in my step number step

7244
02:24:15,750 --> 02:24:15,760
that I calculate in my step number step
 

7245
02:24:15,760 --> 02:24:17,990
that I calculate in my step number step
number three. Then what I do is we do

7246
02:24:17,990 --> 02:24:18,000
number three. Then what I do is we do
 

7247
02:24:18,000 --> 02:24:20,389
number three. Then what I do is we do
the back propagation. Correct? We are

7248
02:24:20,389 --> 02:24:20,399
the back propagation. Correct? We are
 

7249
02:24:20,399 --> 02:24:22,830
the back propagation. Correct? We are
going to do the back

7250
02:24:22,830 --> 02:24:22,840
going to do the back
 

7251
02:24:22,840 --> 02:24:25,030
going to do the back
propagation which I showed through this

7252
02:24:25,030 --> 02:24:25,040
propagation which I showed through this
 

7253
02:24:25,040 --> 02:24:27,750
propagation which I showed through this
arrow. So that is the back propagation.

7254
02:24:27,750 --> 02:24:27,760
arrow. So that is the back propagation.
 

7255
02:24:27,760 --> 02:24:29,830
arrow. So that is the back propagation.
And here what we are going to do is that

7256
02:24:29,830 --> 02:24:29,840
And here what we are going to do is that
 

7257
02:24:29,840 --> 02:24:31,590
And here what we are going to do is that
we are going to accumulate the gradients

7258
02:24:31,590 --> 02:24:31,600
we are going to accumulate the gradients
 

7259
02:24:31,600 --> 02:24:33,349
we are going to accumulate the gradients
till we reach gradient accumulation

7260
02:24:33,349 --> 02:24:33,359
till we reach gradient accumulation
 

7261
02:24:33,359 --> 02:24:36,070
till we reach gradient accumulation
number of steps. So essentially we are

7262
02:24:36,070 --> 02:24:36,080
number of steps. So essentially we are
 

7263
02:24:36,080 --> 02:24:38,150
number of steps. So essentially we are
going to do this gradient accumulation

7264
02:24:38,150 --> 02:24:38,160
going to do this gradient accumulation
 

7265
02:24:38,160 --> 02:24:41,270
going to do this gradient accumulation
is equal to 32. We are going to

7266
02:24:41,270 --> 02:24:41,280
is equal to 32. We are going to
 

7267
02:24:41,280 --> 02:24:43,990
is equal to 32. We are going to
accumulate the gradients for uh we are

7268
02:24:43,990 --> 02:24:44,000
accumulate the gradients for uh we are
 

7269
02:24:44,000 --> 02:24:45,990
accumulate the gradients for uh we are
going to accumulate the gradients for 32

7270
02:24:45,990 --> 02:24:46,000
going to accumulate the gradients for 32
 

7271
02:24:46,000 --> 02:24:49,030
going to accumulate the gradients for 32
steps. So essentially for 32 iterations

7272
02:24:49,030 --> 02:24:49,040
steps. So essentially for 32 iterations
 

7273
02:24:49,040 --> 02:24:50,750
steps. So essentially for 32 iterations
we are going to

7274
02:24:50,750 --> 02:24:50,760
we are going to
 

7275
02:24:50,760 --> 02:24:54,950
we are going to
uh accumulate the gradient and uh then

7276
02:24:54,950 --> 02:24:54,960
uh accumulate the gradient and uh then
 

7277
02:24:54,960 --> 02:24:56,710
uh accumulate the gradient and uh then
what we are going to do is that once we

7278
02:24:56,710 --> 02:24:56,720
what we are going to do is that once we
 

7279
02:24:56,720 --> 02:24:59,510
what we are going to do is that once we
accumulate the gradient for 32 steps

7280
02:24:59,510 --> 02:24:59,520
accumulate the gradient for 32 steps
 

7281
02:24:59,520 --> 02:25:01,030
accumulate the gradient for 32 steps
then and only then we are going to

7282
02:25:01,030 --> 02:25:01,040
then and only then we are going to
 

7283
02:25:01,040 --> 02:25:03,030
then and only then we are going to
update the parameters. So let's say we

7284
02:25:03,030 --> 02:25:03,040
update the parameters. So let's say we
 

7285
02:25:03,040 --> 02:25:04,950
update the parameters. So let's say we
do one step we do back propagation. We

7286
02:25:04,950 --> 02:25:04,960
do one step we do back propagation. We
 

7287
02:25:04,960 --> 02:25:06,469
do one step we do back propagation. We
are not going to update the gradients.

7288
02:25:06,469 --> 02:25:06,479
are not going to update the gradients.
 

7289
02:25:06,479 --> 02:25:08,790
are not going to update the gradients.
Then we'll do one more uh iteration.

7290
02:25:08,790 --> 02:25:08,800
Then we'll do one more uh iteration.
 

7291
02:25:08,800 --> 02:25:10,630
Then we'll do one more uh iteration.
We'll again do the back propagation. One

7292
02:25:10,630 --> 02:25:10,640
We'll again do the back propagation. One
 

7293
02:25:10,640 --> 02:25:12,070
We'll again do the back propagation. One
more iteration. Again do the back

7294
02:25:12,070 --> 02:25:12,080
more iteration. Again do the back
 

7295
02:25:12,080 --> 02:25:14,630
more iteration. Again do the back
propagation. And then after we are done

7296
02:25:14,630 --> 02:25:14,640
propagation. And then after we are done
 

7297
02:25:14,640 --> 02:25:17,030
propagation. And then after we are done
through 32 steps, then we are going to

7298
02:25:17,030 --> 02:25:17,040
through 32 steps, then we are going to
 

7299
02:25:17,040 --> 02:25:19,670
through 32 steps, then we are going to
update the parameters which we have. And

7300
02:25:19,670 --> 02:25:19,680
update the parameters which we have. And
 

7301
02:25:19,680 --> 02:25:22,150
update the parameters which we have. And
once the how are the parameters updated?

7302
02:25:22,150 --> 02:25:22,160
once the how are the parameters updated?
 

7303
02:25:22,160 --> 02:25:23,910
once the how are the parameters updated?
The parameters are updated through the

7304
02:25:23,910 --> 02:25:23,920
The parameters are updated through the
 

7305
02:25:23,920 --> 02:25:27,349
The parameters are updated through the
AdamW optimizer which we have chosen. So

7306
02:25:27,349 --> 02:25:27,359
AdamW optimizer which we have chosen. So
 

7307
02:25:27,359 --> 02:25:29,590
AdamW optimizer which we have chosen. So
the simplest vanilla version of the

7308
02:25:29,590 --> 02:25:29,600
the simplest vanilla version of the
 

7309
02:25:29,600 --> 02:25:31,190
the simplest vanilla version of the
gradient descent looks something like

7310
02:25:31,190 --> 02:25:31,200
gradient descent looks something like
 

7311
02:25:31,200 --> 02:25:34,309
gradient descent looks something like
this. Minus alpha* this. Right? The adam

7312
02:25:34,309 --> 02:25:34,319
this. Minus alpha* this. Right? The adam
 

7313
02:25:34,319 --> 02:25:36,230
this. Minus alpha* this. Right? The adam
is a bit more complex. It takes into

7314
02:25:36,230 --> 02:25:36,240
is a bit more complex. It takes into
 

7315
02:25:36,240 --> 02:25:38,630
is a bit more complex. It takes into
account the history of the curvature

7316
02:25:38,630 --> 02:25:38,640
account the history of the curvature
 

7317
02:25:38,640 --> 02:25:41,590
account the history of the curvature
which we have in our loss landscape and

7318
02:25:41,590 --> 02:25:41,600
which we have in our loss landscape and
 

7319
02:25:41,600 --> 02:25:45,190
which we have in our loss landscape and
it also uses a um adaptive learning

7320
02:25:45,190 --> 02:25:45,200
it also uses a um adaptive learning
 

7321
02:25:45,200 --> 02:25:47,590
it also uses a um adaptive learning
rate. So once the parameters are updated

7322
02:25:47,590 --> 02:25:47,600
rate. So once the parameters are updated
 

7323
02:25:47,600 --> 02:25:50,309
rate. So once the parameters are updated
then we also update the learning rate

7324
02:25:50,309 --> 02:25:50,319
then we also update the learning rate
 

7325
02:25:50,319 --> 02:25:52,230
then we also update the learning rate
and then again we evaluate the model

7326
02:25:52,230 --> 02:25:52,240
and then again we evaluate the model
 

7327
02:25:52,240 --> 02:25:54,070
and then again we evaluate the model
after every eval iterations which is

7328
02:25:54,070 --> 02:25:54,080
after every eval iterations which is
 

7329
02:25:54,080 --> 02:25:56,230
after every eval iterations which is
after every 100 iterations and this loop

7330
02:25:56,230 --> 02:25:56,240
after every 100 iterations and this loop
 

7331
02:25:56,240 --> 02:25:58,870
after every 100 iterations and this loop
is repeated. So we do the forward pass.

7332
02:25:58,870 --> 02:25:58,880
is repeated. So we do the forward pass.
 

7333
02:25:58,880 --> 02:26:00,630
is repeated. So we do the forward pass.
Once we do the forward pass we back

7334
02:26:00,630 --> 02:26:00,640
Once we do the forward pass we back
 

7335
02:26:00,640 --> 02:26:03,710
Once we do the forward pass we back
propagate the loss. Remember that

7336
02:26:03,710 --> 02:26:03,720
propagate the loss. Remember that
 

7337
02:26:03,720 --> 02:26:06,590
propagate the loss. Remember that
uh remember that we are essentially

7338
02:26:06,590 --> 02:26:06,600
uh remember that we are essentially
 

7339
02:26:06,600 --> 02:26:09,270
uh remember that we are essentially
accumulating uh we are accumulating the

7340
02:26:09,270 --> 02:26:09,280
accumulating uh we are accumulating the
 

7341
02:26:09,280 --> 02:26:11,630
accumulating uh we are accumulating the
gradients till we reach the gradient

7342
02:26:11,630 --> 02:26:11,640
gradients till we reach the gradient
 

7343
02:26:11,640 --> 02:26:13,990
gradients till we reach the gradient
accumulation number of steps and only

7344
02:26:13,990 --> 02:26:14,000
accumulation number of steps and only
 

7345
02:26:14,000 --> 02:26:16,469
accumulation number of steps and only
after that point we are going to update

7346
02:26:16,469 --> 02:26:16,479
after that point we are going to update
 

7347
02:26:16,479 --> 02:26:18,469
after that point we are going to update
the parameters. The parameters are

7348
02:26:18,469 --> 02:26:18,479
the parameters. The parameters are
 

7349
02:26:18,479 --> 02:26:20,790
the parameters. The parameters are
updated through the AdamW optimizer and

7350
02:26:20,790 --> 02:26:20,800
updated through the AdamW optimizer and
 

7351
02:26:20,800 --> 02:26:22,710
updated through the AdamW optimizer and
then we update my learning rate. My

7352
02:26:22,710 --> 02:26:22,720
then we update my learning rate. My
 

7353
02:26:22,720 --> 02:26:24,710
then we update my learning rate. My
learning rate is updated every time the

7354
02:26:24,710 --> 02:26:24,720
learning rate is updated every time the
 

7355
02:26:24,720 --> 02:26:26,230
learning rate is updated every time the
parameters are updated because we have

7356
02:26:26,230 --> 02:26:26,240
parameters are updated because we have
 

7357
02:26:26,240 --> 02:26:28,630
parameters are updated because we have
an adaptive learning rate in this case.

7358
02:26:28,630 --> 02:26:28,640
an adaptive learning rate in this case.
 

7359
02:26:28,640 --> 02:26:30,630
an adaptive learning rate in this case.
And then what we do is that after my

7360
02:26:30,630 --> 02:26:30,640
And then what we do is that after my
 

7361
02:26:30,640 --> 02:26:33,030
And then what we do is that after my
learning rate is updated I when I go

7362
02:26:33,030 --> 02:26:33,040
learning rate is updated I when I go
 

7363
02:26:33,040 --> 02:26:35,030
learning rate is updated I when I go
through evaluation iterations which in

7364
02:26:35,030 --> 02:26:35,040
through evaluation iterations which in
 

7365
02:26:35,040 --> 02:26:36,349
through evaluation iterations which in
this case let's say if it's 100

7366
02:26:36,349 --> 02:26:36,359
this case let's say if it's 100
 

7367
02:26:36,359 --> 02:26:38,550
this case let's say if it's 100
iterations let's see how much it is the

7368
02:26:38,550 --> 02:26:38,560
iterations let's see how much it is the
 

7369
02:26:38,560 --> 02:26:40,870
iterations let's see how much it is the
evaluation iterations which we have is

7370
02:26:40,870 --> 02:26:40,880
evaluation iterations which we have is
 

7371
02:26:40,880 --> 02:26:43,349
evaluation iterations which we have is
500. So when I do 500 iterations I'm

7372
02:26:43,349 --> 02:26:43,359
500. So when I do 500 iterations I'm
 

7373
02:26:43,359 --> 02:26:45,510
500. So when I do 500 iterations I'm
going to evaluate my loss which is the

7374
02:26:45,510 --> 02:26:45,520
going to evaluate my loss which is the
 

7375
02:26:45,520 --> 02:26:47,830
going to evaluate my loss which is the
mean overall this 500 iterations and I'm

7376
02:26:47,830 --> 02:26:47,840
mean overall this 500 iterations and I'm
 

7377
02:26:47,840 --> 02:26:50,750
mean overall this 500 iterations and I'm
going to uh I'm going to print out my

7378
02:26:50,750 --> 02:26:50,760
going to uh I'm going to print out my
 

7379
02:26:50,760 --> 02:26:53,670
going to uh I'm going to print out my
loss. Okay. So that's the main thing

7380
02:26:53,670 --> 02:26:53,680
loss. Okay. So that's the main thing
 

7381
02:26:53,680 --> 02:26:55,990
loss. Okay. So that's the main thing
which is uh happening in these eight

7382
02:26:55,990 --> 02:26:56,000
which is uh happening in these eight
 

7383
02:26:56,000 --> 02:26:58,150
which is uh happening in these eight
steps. So if you see the pre-training

7384
02:26:58,150 --> 02:26:58,160
steps. So if you see the pre-training
 

7385
02:26:58,160 --> 02:27:00,630
steps. So if you see the pre-training
loop for the small language model, you

7386
02:27:00,630 --> 02:27:00,640
loop for the small language model, you
 

7387
02:27:00,640 --> 02:27:02,230
loop for the small language model, you
can start with this thing where we get

7388
02:27:02,230 --> 02:27:02,240
can start with this thing where we get
 

7389
02:27:02,240 --> 02:27:04,870
can start with this thing where we get
the batch, we get the X and Y, we pass

7390
02:27:04,870 --> 02:27:04,880
the batch, we get the X and Y, we pass
 

7391
02:27:04,880 --> 02:27:07,910
the batch, we get the X and Y, we pass
the X and Y to the GPU, then as the next

7392
02:27:07,910 --> 02:27:07,920
the X and Y to the GPU, then as the next
 

7393
02:27:07,920 --> 02:27:09,830
the X and Y to the GPU, then as the next
step, what we are doing is we pass the X

7394
02:27:09,830 --> 02:27:09,840
step, what we are doing is we pass the X
 

7395
02:27:09,840 --> 02:27:11,270
step, what we are doing is we pass the X
and Y through the model, we get the

7396
02:27:11,270 --> 02:27:11,280
and Y through the model, we get the
 

7397
02:27:11,280 --> 02:27:13,990
and Y through the model, we get the
logits and we get the loss. We divide

7398
02:27:13,990 --> 02:27:14,000
logits and we get the loss. We divide
 

7399
02:27:14,000 --> 02:27:15,990
logits and we get the loss. We divide
the loss with the gradient accumulation

7400
02:27:15,990 --> 02:27:16,000
the loss with the gradient accumulation
 

7401
02:27:16,000 --> 02:27:17,830
the loss with the gradient accumulation
steps because remember we are going to

7402
02:27:17,830 --> 02:27:17,840
steps because remember we are going to
 

7403
02:27:17,840 --> 02:27:20,389
steps because remember we are going to
update the parameters only after we

7404
02:27:20,389 --> 02:27:20,399
update the parameters only after we
 

7405
02:27:20,399 --> 02:27:22,950
update the parameters only after we
accumulate these many steps. So just so

7406
02:27:22,950 --> 02:27:22,960
accumulate these many steps. So just so
 

7407
02:27:22,960 --> 02:27:25,030
accumulate these many steps. So just so
that so the loss will also get

7408
02:27:25,030 --> 02:27:25,040
that so the loss will also get
 

7409
02:27:25,040 --> 02:27:26,630
that so the loss will also get
accumulated right over these many

7410
02:27:26,630 --> 02:27:26,640
accumulated right over these many
 

7411
02:27:26,640 --> 02:27:28,790
accumulated right over these many
gradient steps. So I'm going to just if

7412
02:27:28,790 --> 02:27:28,800
gradient steps. So I'm going to just if
 

7413
02:27:28,800 --> 02:27:30,630
gradient steps. So I'm going to just if
I have 32 steps I'm going to divide the

7414
02:27:30,630 --> 02:27:30,640
I have 32 steps I'm going to divide the
 

7415
02:27:30,640 --> 02:27:32,630
I have 32 steps I'm going to divide the
loss by 32 and then I'm going to do the

7416
02:27:32,630 --> 02:27:32,640
loss by 32 and then I'm going to do the
 

7417
02:27:32,640 --> 02:27:34,790
loss by 32 and then I'm going to do the
backward pass. Here the parameters will

7418
02:27:34,790 --> 02:27:34,800
backward pass. Here the parameters will
 

7419
02:27:34,800 --> 02:27:37,590
backward pass. Here the parameters will
be updated only after the gradient. Uh

7420
02:27:37,590 --> 02:27:37,600
be updated only after the gradient. Uh
 

7421
02:27:37,600 --> 02:27:39,349
be updated only after the gradient. Uh
so here I'm going to do the backward

7422
02:27:39,349 --> 02:27:39,359
so here I'm going to do the backward
 

7423
02:27:39,359 --> 02:27:41,150
so here I'm going to do the backward
pass. The parameters are not yet

7424
02:27:41,150 --> 02:27:41,160
pass. The parameters are not yet
 

7425
02:27:41,160 --> 02:27:43,590
pass. The parameters are not yet
updated. Here the parameters are

7426
02:27:43,590 --> 02:27:43,600
updated. Here the parameters are
 

7427
02:27:43,600 --> 02:27:45,990
updated. Here the parameters are
updated. So scalar.step optimizer

7428
02:27:45,990 --> 02:27:46,000
updated. So scalar.step optimizer
 

7429
02:27:46,000 --> 02:27:47,830
updated. So scalar.step optimizer
parameters are updated only after I

7430
02:27:47,830 --> 02:27:47,840
parameters are updated only after I
 

7431
02:27:47,840 --> 02:27:49,429
parameters are updated only after I
reach the gradient accumulation number

7432
02:27:49,429 --> 02:27:49,439
reach the gradient accumulation number
 

7433
02:27:49,439 --> 02:27:52,070
reach the gradient accumulation number
of steps. Uh then my learning rate is

7434
02:27:52,070 --> 02:27:52,080
of steps. Uh then my learning rate is
 

7435
02:27:52,080 --> 02:27:55,110
of steps. Uh then my learning rate is
also my learning rate is also updated.

7436
02:27:55,110 --> 02:27:55,120
also my learning rate is also updated.
 

7437
02:27:55,120 --> 02:27:57,429
also my learning rate is also updated.
Um and then this whole thing basically

7438
02:27:57,429 --> 02:27:57,439
Um and then this whole thing basically
 

7439
02:27:57,439 --> 02:28:00,630
Um and then this whole thing basically
uh repeats in a loop right and here what

7440
02:28:00,630 --> 02:28:00,640
uh repeats in a loop right and here what
 

7441
02:28:00,640 --> 02:28:02,150
uh repeats in a loop right and here what
is happening is that once we have

7442
02:28:02,150 --> 02:28:02,160
is happening is that once we have
 

7443
02:28:02,160 --> 02:28:03,910
is happening is that once we have
reached the eval iterations we are going

7444
02:28:03,910 --> 02:28:03,920
reached the eval iterations we are going
 

7445
02:28:03,920 --> 02:28:05,590
reached the eval iterations we are going
to print out the losses using the

7446
02:28:05,590 --> 02:28:05,600
to print out the losses using the
 

7447
02:28:05,600 --> 02:28:07,790
to print out the losses using the
estimate loss function which we saw over

7448
02:28:07,790 --> 02:28:07,800
estimate loss function which we saw over
 

7449
02:28:07,800 --> 02:28:11,030
estimate loss function which we saw over
here this estimate loss function. So as

7450
02:28:11,030 --> 02:28:11,040
here this estimate loss function. So as
 

7451
02:28:11,040 --> 02:28:12,630
here this estimate loss function. So as
you see now many things for us are

7452
02:28:12,630 --> 02:28:12,640
you see now many things for us are
 

7453
02:28:12,640 --> 02:28:14,309
you see now many things for us are
coming together when we are pre-training

7454
02:28:14,309 --> 02:28:14,319
coming together when we are pre-training
 

7455
02:28:14,319 --> 02:28:16,389
coming together when we are pre-training
the large language model right we take

7456
02:28:16,389 --> 02:28:16,399
the large language model right we take
 

7457
02:28:16,399 --> 02:28:17,750
the large language model right we take
the batch we pass it through the

7458
02:28:17,750 --> 02:28:17,760
the batch we pass it through the
 

7459
02:28:17,760 --> 02:28:20,070
the batch we pass it through the
language model we get the output that's

7460
02:28:20,070 --> 02:28:20,080
language model we get the output that's
 

7461
02:28:20,080 --> 02:28:22,950
language model we get the output that's
what's happening until this stage

7462
02:28:22,950 --> 02:28:22,960
what's happening until this stage
 

7463
02:28:22,960 --> 02:28:24,790
what's happening until this stage
uh then what we do is we do the batch

7464
02:28:24,790 --> 02:28:24,800
uh then what we do is we do the batch
 

7465
02:28:24,800 --> 02:28:26,469
uh then what we do is we do the batch
propagation and we start accumulating

7466
02:28:26,469 --> 02:28:26,479
propagation and we start accumulating
 

7467
02:28:26,479 --> 02:28:28,550
propagation and we start accumulating
the gradients but remember that we are

7468
02:28:28,550 --> 02:28:28,560
the gradients but remember that we are
 

7469
02:28:28,560 --> 02:28:29,389
the gradients but remember that we are
not

7470
02:28:29,389 --> 02:28:29,399
not
 

7471
02:28:29,399 --> 02:28:33,270
not
uh um we are not updating the parameters

7472
02:28:33,270 --> 02:28:33,280
uh um we are not updating the parameters
 

7473
02:28:33,280 --> 02:28:35,110
uh um we are not updating the parameters
yet only when we reach the gradient

7474
02:28:35,110 --> 02:28:35,120
yet only when we reach the gradient
 

7475
02:28:35,120 --> 02:28:36,710
yet only when we reach the gradient
accumulation steps we update the

7476
02:28:36,710 --> 02:28:36,720
accumulation steps we update the
 

7477
02:28:36,720 --> 02:28:39,030
accumulation steps we update the
parameters then we change the learning

7478
02:28:39,030 --> 02:28:39,040
parameters then we change the learning
 

7479
02:28:39,040 --> 02:28:41,429
parameters then we change the learning
rate And essentially this process

7480
02:28:41,429 --> 02:28:41,439
rate And essentially this process
 

7481
02:28:41,439 --> 02:28:43,349
rate And essentially this process
continues and once we have 100

7482
02:28:43,349 --> 02:28:43,359
continues and once we have 100
 

7483
02:28:43,359 --> 02:28:45,830
continues and once we have 100
evaluation 100 iterations which are 500

7484
02:28:45,830 --> 02:28:45,840
evaluation 100 iterations which are 500
 

7485
02:28:45,840 --> 02:28:48,550
evaluation 100 iterations which are 500
iterations rather which are done we call

7486
02:28:48,550 --> 02:28:48,560
iterations rather which are done we call
 

7487
02:28:48,560 --> 02:28:50,469
iterations rather which are done we call
the estimate loss function and we print

7488
02:28:50,469 --> 02:28:50,479
the estimate loss function and we print
 

7489
02:28:50,479 --> 02:28:52,950
the estimate loss function and we print
out the training loss. We print out the

7490
02:28:52,950 --> 02:28:52,960
out the training loss. We print out the
 

7491
02:28:52,960 --> 02:28:54,389
out the training loss. We print out the
validation loss and we store the

7492
02:28:54,389 --> 02:28:54,399
validation loss and we store the
 

7493
02:28:54,399 --> 02:28:56,550
validation loss and we store the
training and the validation loss. Here

7494
02:28:56,550 --> 02:28:56,560
training and the validation loss. Here
 

7495
02:28:56,560 --> 02:28:58,150
training and the validation loss. Here
what we are also doing is that we are

7496
02:28:58,150 --> 02:28:58,160
what we are also doing is that we are
 

7497
02:28:58,160 --> 02:29:00,150
what we are also doing is that we are
going to save our model which is

7498
02:29:00,150 --> 02:29:00,160
going to save our model which is
 

7499
02:29:00,160 --> 02:29:02,550
going to save our model which is
performing the best in this best models

7500
02:29:02,550 --> 02:29:02,560
performing the best in this best models
 

7501
02:29:02,560 --> 02:29:04,950
performing the best in this best models
parameters. PT so that we don't have to

7502
02:29:04,950 --> 02:29:04,960
parameters. PT so that we don't have to
 

7503
02:29:04,960 --> 02:29:07,190
parameters. PT so that we don't have to
retrain again. Remember this is also

7504
02:29:07,190 --> 02:29:07,200
retrain again. Remember this is also
 

7505
02:29:07,200 --> 02:29:09,270
retrain again. Remember this is also
very important. Sometimes we forget to

7506
02:29:09,270 --> 02:29:09,280
very important. Sometimes we forget to
 

7507
02:29:09,280 --> 02:29:11,910
very important. Sometimes we forget to
store the model parameters, right? And

7508
02:29:11,910 --> 02:29:11,920
store the model parameters, right? And
 

7509
02:29:11,920 --> 02:29:13,590
store the model parameters, right? And
that's not usually a good thing. The

7510
02:29:13,590 --> 02:29:13,600
that's not usually a good thing. The
 

7511
02:29:13,600 --> 02:29:15,030
that's not usually a good thing. The
good practice is to whenever you're

7512
02:29:15,030 --> 02:29:15,040
good practice is to whenever you're
 

7513
02:29:15,040 --> 02:29:16,790
good practice is to whenever you're
training a model, just save the

7514
02:29:16,790 --> 02:29:16,800
training a model, just save the
 

7515
02:29:16,800 --> 02:29:18,590
training a model, just save the
parameters because training is a very

7516
02:29:18,590 --> 02:29:18,600
parameters because training is a very
 

7517
02:29:18,600 --> 02:29:20,790
parameters because training is a very
intensive computationally intensive

7518
02:29:20,790 --> 02:29:20,800
intensive computationally intensive
 

7519
02:29:20,800 --> 02:29:22,309
intensive computationally intensive
procedure, right? And you don't want to

7520
02:29:22,309 --> 02:29:22,319
procedure, right? And you don't want to
 

7521
02:29:22,319 --> 02:29:24,790
procedure, right? And you don't want to
be running the loop again and again. So

7522
02:29:24,790 --> 02:29:24,800
be running the loop again and again. So
 

7523
02:29:24,800 --> 02:29:26,790
be running the loop again and again. So
we are going to save all the model

7524
02:29:26,790 --> 02:29:26,800
we are going to save all the model
 

7525
02:29:26,800 --> 02:29:29,750
we are going to save all the model
parameters which ultimately result at

7526
02:29:29,750 --> 02:29:29,760
parameters which ultimately result at
 

7527
02:29:29,760 --> 02:29:32,309
parameters which ultimately result at
the end of uh or which give the best

7528
02:29:32,309 --> 02:29:32,319
the end of uh or which give the best
 

7529
02:29:32,319 --> 02:29:35,670
the end of uh or which give the best
loss. So the best validation loss will

7530
02:29:35,670 --> 02:29:35,680
loss. So the best validation loss will
 

7531
02:29:35,680 --> 02:29:37,270
loss. So the best validation loss will
be associated with a certain set of

7532
02:29:37,270 --> 02:29:37,280
be associated with a certain set of
 

7533
02:29:37,280 --> 02:29:39,590
be associated with a certain set of
parameters. I'm going to store all of

7534
02:29:39,590 --> 02:29:39,600
parameters. I'm going to store all of
 

7535
02:29:39,600 --> 02:29:43,429
parameters. I'm going to store all of
those parameters essentially. Okay. And

7536
02:29:43,429 --> 02:29:43,439
those parameters essentially. Okay. And
 

7537
02:29:43,439 --> 02:29:45,349
those parameters essentially. Okay. And
uh so that I can reload the model

7538
02:29:45,349 --> 02:29:45,359
uh so that I can reload the model
 

7539
02:29:45,359 --> 02:29:48,309
uh so that I can reload the model
anytime in the future when it is needed.

7540
02:29:48,309 --> 02:29:48,319
anytime in the future when it is needed.
 

7541
02:29:48,319 --> 02:29:50,230
anytime in the future when it is needed.
So this is my training loop. You can run

7542
02:29:50,230 --> 02:29:50,240
So this is my training loop. You can run
 

7543
02:29:50,240 --> 02:29:52,469
So this is my training loop. You can run
this training loop now. So once you

7544
02:29:52,469 --> 02:29:52,479
this training loop now. So once you
 

7545
02:29:52,479 --> 02:29:54,070
this training loop now. So once you
start running this training loop, here

7546
02:29:54,070 --> 02:29:54,080
start running this training loop, here
 

7547
02:29:54,080 --> 02:29:55,910
start running this training loop, here
is where the main iterations will start.

7548
02:29:55,910 --> 02:29:55,920
is where the main iterations will start.
 

7549
02:29:55,920 --> 02:29:57,830
is where the main iterations will start.
And let me now explain to you how these

7550
02:29:57,830 --> 02:29:57,840
And let me now explain to you how these
 

7551
02:29:57,840 --> 02:30:00,790
And let me now explain to you how these
iterations usually proceed. All right.

7552
02:30:00,790 --> 02:30:00,800
iterations usually proceed. All right.
 

7553
02:30:00,800 --> 02:30:02,630
iterations usually proceed. All right.
So I hope all of you are excited to

7554
02:30:02,630 --> 02:30:02,640
So I hope all of you are excited to
 

7555
02:30:02,640 --> 02:30:04,710
So I hope all of you are excited to
reach this stage because here is where

7556
02:30:04,710 --> 02:30:04,720
reach this stage because here is where
 

7557
02:30:04,720 --> 02:30:07,110
reach this stage because here is where
the whole running of the loop actually

7558
02:30:07,110 --> 02:30:07,120
the whole running of the loop actually
 

7559
02:30:07,120 --> 02:30:09,510
the whole running of the loop actually
happens and this is the best part of

7560
02:30:09,510 --> 02:30:09,520
happens and this is the best part of
 

7561
02:30:09,520 --> 02:30:11,429
happens and this is the best part of
this tutorial. Until now you have spent

7562
02:30:11,429 --> 02:30:11,439
this tutorial. Until now you have spent
 

7563
02:30:11,439 --> 02:30:13,030
this tutorial. Until now you have spent
around two two and a half hours to go

7564
02:30:13,030 --> 02:30:13,040
around two two and a half hours to go
 

7565
02:30:13,040 --> 02:30:15,830
around two two and a half hours to go
through the entire content so far and

7566
02:30:15,830 --> 02:30:15,840
through the entire content so far and
 

7567
02:30:15,840 --> 02:30:17,750
through the entire content so far and
I've tried my best to explain every

7568
02:30:17,750 --> 02:30:17,760
I've tried my best to explain every
 

7569
02:30:17,760 --> 02:30:20,870
I've tried my best to explain every
single part of um this code as much as

7570
02:30:20,870 --> 02:30:20,880
single part of um this code as much as
 

7571
02:30:20,880 --> 02:30:23,070
single part of um this code as much as
possible. So first I started

7572
02:30:23,070 --> 02:30:23,080
possible. So first I started
 

7573
02:30:23,080 --> 02:30:26,230
possible. So first I started
with data set then data prep-processing

7574
02:30:26,230 --> 02:30:26,240
with data set then data prep-processing
 

7575
02:30:26,240 --> 02:30:27,830
with data set then data prep-processing
assembling the model architecture

7576
02:30:27,830 --> 02:30:27,840
assembling the model architecture
 

7577
02:30:27,840 --> 02:30:29,670
assembling the model architecture
setting up the training pipeline

7578
02:30:29,670 --> 02:30:29,680
setting up the training pipeline
 

7579
02:30:29,680 --> 02:30:32,230
setting up the training pipeline
pre-training the language model. So now

7580
02:30:32,230 --> 02:30:32,240
pre-training the language model. So now
 

7581
02:30:32,240 --> 02:30:34,630
pre-training the language model. So now
here what I've done is that I've chosen

7582
02:30:34,630 --> 02:30:34,640
here what I've done is that I've chosen
 

7583
02:30:34,640 --> 02:30:38,630
here what I've done is that I've chosen
the runtime to be 800. So on a run how

7584
02:30:38,630 --> 02:30:38,640
the runtime to be 800. So on a run how
 

7585
02:30:38,640 --> 02:30:40,309
the runtime to be 800. So on a run how
many iterations? I've run 20,000

7586
02:30:40,309 --> 02:30:40,319
many iterations? I've run 20,000
 

7587
02:30:40,319 --> 02:30:42,309
many iterations? I've run 20,000
iterations over here and it took me

7588
02:30:42,309 --> 02:30:42,319
iterations over here and it took me
 

7589
02:30:42,319 --> 02:30:44,389
iterations over here and it took me
around 30 to 35 minutes to run all of

7590
02:30:44,389 --> 02:30:44,399
around 30 to 35 minutes to run all of
 

7591
02:30:44,399 --> 02:30:46,550
around 30 to 35 minutes to run all of
these iterations. As I've mentioned to

7592
02:30:46,550 --> 02:30:46,560
these iterations. As I've mentioned to
 

7593
02:30:46,560 --> 02:30:48,389
these iterations. As I've mentioned to
you earlier you can even run these

7594
02:30:48,389 --> 02:30:48,399
you earlier you can even run these
 

7595
02:30:48,399 --> 02:30:51,070
you earlier you can even run these
iterations on a T4 GPU. That's the free

7596
02:30:51,070 --> 02:30:51,080
iterations on a T4 GPU. That's the free
 

7597
02:30:51,080 --> 02:30:52,830
iterations on a T4 GPU. That's the free
GPU.

7598
02:30:52,830 --> 02:30:52,840
GPU.
 

7599
02:30:52,840 --> 02:30:55,590
GPU.
Um, but that will take around 6 to 8

7600
02:30:55,590 --> 02:30:55,600
Um, but that will take around 6 to 8
 

7601
02:30:55,600 --> 02:30:57,429
Um, but that will take around 6 to 8
hours I think. And laptop crashes

7602
02:30:57,429 --> 02:30:57,439
hours I think. And laptop crashes
 

7603
02:30:57,439 --> 02:30:59,510
hours I think. And laptop crashes
sometimes. I've also tested with that.

7604
02:30:59,510 --> 02:30:59,520
sometimes. I've also tested with that.
 

7605
02:30:59,520 --> 02:31:01,750
sometimes. I've also tested with that.
So if you look at the training loss and

7606
02:31:01,750 --> 02:31:01,760
So if you look at the training loss and
 

7607
02:31:01,760 --> 02:31:03,349
So if you look at the training loss and
the validation loss, you see that the

7608
02:31:03,349 --> 02:31:03,359
the validation loss, you see that the
 

7609
02:31:03,359 --> 02:31:05,510
the validation loss, you see that the
training loss continuously goes down.

7610
02:31:05,510 --> 02:31:05,520
training loss continuously goes down.
 

7611
02:31:05,520 --> 02:31:07,510
training loss continuously goes down.
The validation loss is also continuously

7612
02:31:07,510 --> 02:31:07,520
The validation loss is also continuously
 

7613
02:31:07,520 --> 02:31:09,190
The validation loss is also continuously
going down. That's a great sign that our

7614
02:31:09,190 --> 02:31:09,200
going down. That's a great sign that our
 

7615
02:31:09,200 --> 02:31:11,349
going down. That's a great sign that our
training is proceeding very nicely. And

7616
02:31:11,349 --> 02:31:11,359
training is proceeding very nicely. And
 

7617
02:31:11,359 --> 02:31:12,950
training is proceeding very nicely. And
towards the end we have the training

7618
02:31:12,950 --> 02:31:12,960
towards the end we have the training
 

7619
02:31:12,960 --> 02:31:14,950
towards the end we have the training
loss to be very low. Validation loss to

7620
02:31:14,950 --> 02:31:14,960
loss to be very low. Validation loss to
 

7621
02:31:14,960 --> 02:31:16,950
loss to be very low. Validation loss to
be very low. And more importantly, the

7622
02:31:16,950 --> 02:31:16,960
be very low. And more importantly, the
 

7623
02:31:16,960 --> 02:31:18,469
be very low. And more importantly, the
training and the validation loss are

7624
02:31:18,469 --> 02:31:18,479
training and the validation loss are
 

7625
02:31:18,479 --> 02:31:20,790
training and the validation loss are
very close to each other. that indicates

7626
02:31:20,790 --> 02:31:20,800
very close to each other. that indicates
 

7627
02:31:20,800 --> 02:31:23,510
very close to each other. that indicates
that we are not overfitting over here. I

7628
02:31:23,510 --> 02:31:23,520
that we are not overfitting over here. I
 

7629
02:31:23,520 --> 02:31:25,190
that we are not overfitting over here. I
very strongly think that if you run this

7630
02:31:25,190 --> 02:31:25,200
very strongly think that if you run this
 

7631
02:31:25,200 --> 02:31:27,349
very strongly think that if you run this
for 40,000 iterations, you might even

7632
02:31:27,349 --> 02:31:27,359
for 40,000 iterations, you might even
 

7633
02:31:27,359 --> 02:31:29,349
for 40,000 iterations, you might even
get better results. But here I'm

7634
02:31:29,349 --> 02:31:29,359
get better results. But here I'm
 

7635
02:31:29,359 --> 02:31:31,110
get better results. But here I'm
sticking with just those number of

7636
02:31:31,110 --> 02:31:31,120
sticking with just those number of
 

7637
02:31:31,120 --> 02:31:32,630
sticking with just those number of
iterations which are computationally

7638
02:31:32,630 --> 02:31:32,640
iterations which are computationally
 

7639
02:31:32,640 --> 02:31:35,510
iterations which are computationally
feasible for everyone once you run these

7640
02:31:35,510 --> 02:31:35,520
feasible for everyone once you run these
 

7641
02:31:35,520 --> 02:31:37,190
feasible for everyone once you run these
iterations. So finally I've got a

7642
02:31:37,190 --> 02:31:37,200
iterations. So finally I've got a
 

7643
02:31:37,200 --> 02:31:40,070
iterations. So finally I've got a
training loss of 2.3919 and validation

7644
02:31:40,070 --> 02:31:40,080
training loss of 2.3919 and validation
 

7645
02:31:40,080 --> 02:31:43,110
training loss of 2.3919 and validation
loss of 2.3918. You can plot these and

7646
02:31:43,110 --> 02:31:43,120
loss of 2.3918. You can plot these and
 

7647
02:31:43,120 --> 02:31:44,550
loss of 2.3918. You can plot these and
here you can see that both of them

7648
02:31:44,550 --> 02:31:44,560
here you can see that both of them
 

7649
02:31:44,560 --> 02:31:47,349
here you can see that both of them
smoothly start going down. If we are

7650
02:31:47,349 --> 02:31:47,359
smoothly start going down. If we are
 

7651
02:31:47,359 --> 02:31:49,030
smoothly start going down. If we are
doing overfitting then what usually

7652
02:31:49,030 --> 02:31:49,040
doing overfitting then what usually
 

7653
02:31:49,040 --> 02:31:50,710
doing overfitting then what usually
happens is that the training loss

7654
02:31:50,710 --> 02:31:50,720
happens is that the training loss
 

7655
02:31:50,720 --> 02:31:51,990
happens is that the training loss
continues to goes down but the

7656
02:31:51,990 --> 02:31:52,000
continues to goes down but the
 

7657
02:31:52,000 --> 02:31:53,990
continues to goes down but the
validation loss shoots up which is

7658
02:31:53,990 --> 02:31:54,000
validation loss shoots up which is
 

7659
02:31:54,000 --> 02:31:55,990
validation loss shoots up which is
clearly not happening in our case. This

7660
02:31:55,990 --> 02:31:56,000
clearly not happening in our case. This
 

7661
02:31:56,000 --> 02:31:58,790
clearly not happening in our case. This
is a beautiful graph which shows that

7662
02:31:58,790 --> 02:31:58,800
is a beautiful graph which shows that
 

7663
02:31:58,800 --> 02:32:00,389
is a beautiful graph which shows that
the training and validation loss are

7664
02:32:00,389 --> 02:32:00,399
the training and validation loss are
 

7665
02:32:00,399 --> 02:32:03,030
the training and validation loss are
proceeding smoothly. So at this part we

7666
02:32:03,030 --> 02:32:03,040
proceeding smoothly. So at this part we
 

7667
02:32:03,040 --> 02:32:04,870
proceeding smoothly. So at this part we
have successfully run a small language

7668
02:32:04,870 --> 02:32:04,880
have successfully run a small language
 

7669
02:32:04,880 --> 02:32:06,550
have successfully run a small language
model on our machine and we all should

7670
02:32:06,550 --> 02:32:06,560
model on our machine and we all should
 

7671
02:32:06,560 --> 02:32:09,030
model on our machine and we all should
be incredibly proud of it. This is a

7672
02:32:09,030 --> 02:32:09,040
be incredibly proud of it. This is a
 

7673
02:32:09,040 --> 02:32:11,750
be incredibly proud of it. This is a
model which has around 15 to I think

7674
02:32:11,750 --> 02:32:11,760
model which has around 15 to I think
 

7675
02:32:11,760 --> 02:32:13,750
model which has around 15 to I think
needs to be estimated but 15 to around

7676
02:32:13,750 --> 02:32:13,760
needs to be estimated but 15 to around
 

7677
02:32:13,760 --> 02:32:15,830
needs to be estimated but 15 to around
30 million parameters. So still it's

7678
02:32:15,830 --> 02:32:15,840
30 million parameters. So still it's
 

7679
02:32:15,840 --> 02:32:18,950
30 million parameters. So still it's
definitely a small language model and

7680
02:32:18,950 --> 02:32:18,960
definitely a small language model and
 

7681
02:32:18,960 --> 02:32:21,030
definitely a small language model and
it's probably around thousand times or

7682
02:32:21,030 --> 02:32:21,040
it's probably around thousand times or
 

7683
02:32:21,040 --> 02:32:23,429
it's probably around thousand times or
10,000 times lesser number of parameters

7684
02:32:23,429 --> 02:32:23,439
10,000 times lesser number of parameters
 

7685
02:32:23,439 --> 02:32:27,429
10,000 times lesser number of parameters
than GPT3, GPT4 etc. But the real test

7686
02:32:27,429 --> 02:32:27,439
than GPT3, GPT4 etc. But the real test
 

7687
02:32:27,439 --> 02:32:30,070
than GPT3, GPT4 etc. But the real test
of this model is how well are we able to

7688
02:32:30,070 --> 02:32:30,080
of this model is how well are we able to
 

7689
02:32:30,080 --> 02:32:32,070
of this model is how well are we able to
do in terms of generating new content

7690
02:32:32,070 --> 02:32:32,080
do in terms of generating new content
 

7691
02:32:32,080 --> 02:32:34,469
do in terms of generating new content
rate. So remember we started this uh

7692
02:32:34,469 --> 02:32:34,479
rate. So remember we started this uh
 

7693
02:32:34,479 --> 02:32:37,110
rate. So remember we started this uh
lecture with saying that we want to uh

7694
02:32:37,110 --> 02:32:37,120
lecture with saying that we want to uh
 

7695
02:32:37,120 --> 02:32:39,110
lecture with saying that we want to uh
we want to make a model which can

7696
02:32:39,110 --> 02:32:39,120
we want to make a model which can
 

7697
02:32:39,120 --> 02:32:41,750
we want to make a model which can
essentially give

7698
02:32:41,750 --> 02:32:41,760
essentially give
 

7699
02:32:41,760 --> 02:32:44,150
essentially give
um produce coherent text. Our model

7700
02:32:44,150 --> 02:32:44,160
um produce coherent text. Our model
 

7701
02:32:44,160 --> 02:32:46,070
um produce coherent text. Our model
needs to produce coherent text. So it

7702
02:32:46,070 --> 02:32:46,080
needs to produce coherent text. So it
 

7703
02:32:46,080 --> 02:32:48,070
needs to produce coherent text. So it
essentially needs to produce English

7704
02:32:48,070 --> 02:32:48,080
essentially needs to produce English
 

7705
02:32:48,080 --> 02:32:51,270
essentially needs to produce English
stories uh which sound like a story

7706
02:32:51,270 --> 02:32:51,280
stories uh which sound like a story
 

7707
02:32:51,280 --> 02:32:53,670
stories uh which sound like a story
right uh so that will be truly awesome

7708
02:32:53,670 --> 02:32:53,680
right uh so that will be truly awesome
 

7709
02:32:53,680 --> 02:32:55,590
right uh so that will be truly awesome
because then it means that our model our

7710
02:32:55,590 --> 02:32:55,600
because then it means that our model our
 

7711
02:32:55,600 --> 02:32:57,670
because then it means that our model our
model has learned the English language

7712
02:32:57,670 --> 02:32:57,680
model has learned the English language
 

7713
02:32:57,680 --> 02:33:00,110
model has learned the English language
it has also learned how to form stories

7714
02:33:00,110 --> 02:33:00,120
it has also learned how to form stories
 

7715
02:33:00,120 --> 02:33:02,790
it has also learned how to form stories
etc. So what I've done after this point

7716
02:33:02,790 --> 02:33:02,800
etc. So what I've done after this point
 

7717
02:33:02,800 --> 02:33:04,469
etc. So what I've done after this point
which is also going to be the last step

7718
02:33:04,469 --> 02:33:04,479
which is also going to be the last step
 

7719
02:33:04,479 --> 02:33:07,590
which is also going to be the last step
in our uh video today that is

7720
02:33:07,590 --> 02:33:07,600
in our uh video today that is
 

7721
02:33:07,600 --> 02:33:10,469
in our uh video today that is
essentially running the inference. In

7722
02:33:10,469 --> 02:33:10,479
essentially running the inference. In
 

7723
02:33:10,479 --> 02:33:12,070
essentially running the inference. In
the inference, what we are going to

7724
02:33:12,070 --> 02:33:12,080
the inference, what we are going to
 

7725
02:33:12,080 --> 02:33:15,030
the inference, what we are going to
simply do is that we take the uh

7726
02:33:15,030 --> 02:33:15,040
simply do is that we take the uh
 

7727
02:33:15,040 --> 02:33:16,950
simply do is that we take the uh
whenever the input is passed, it goes

7728
02:33:16,950 --> 02:33:16,960
whenever the input is passed, it goes
 

7729
02:33:16,960 --> 02:33:18,550
whenever the input is passed, it goes
through the trained model. Now,

7730
02:33:18,550 --> 02:33:18,560
through the trained model. Now,
 

7731
02:33:18,560 --> 02:33:20,469
through the trained model. Now,
similarly, we have the logits matrix. We

7732
02:33:20,469 --> 02:33:20,479
similarly, we have the logits matrix. We
 

7733
02:33:20,479 --> 02:33:22,630
similarly, we have the logits matrix. We
get the index of the highest value. We

7734
02:33:22,630 --> 02:33:22,640
get the index of the highest value. We
 

7735
02:33:22,640 --> 02:33:24,710
get the index of the highest value. We
get the next token ID that's decoded

7736
02:33:24,710 --> 02:33:24,720
get the next token ID that's decoded
 

7737
02:33:24,720 --> 02:33:27,590
get the next token ID that's decoded
back to text. We produce the next token.

7738
02:33:27,590 --> 02:33:27,600
back to text. We produce the next token.
 

7739
02:33:27,600 --> 02:33:29,429
back to text. We produce the next token.
We append to the previous input. That

7740
02:33:29,429 --> 02:33:29,439
We append to the previous input. That
 

7741
02:33:29,439 --> 02:33:31,150
We append to the previous input. That
again goes into the

7742
02:33:31,150 --> 02:33:31,160
again goes into the
 

7743
02:33:31,160 --> 02:33:33,590
again goes into the
model. And then this loop essentially

7744
02:33:33,590 --> 02:33:33,600
model. And then this loop essentially
 

7745
02:33:33,600 --> 02:33:35,270
model. And then this loop essentially
continues. So let's say if you have

7746
02:33:35,270 --> 02:33:35,280
continues. So let's say if you have
 

7747
02:33:35,280 --> 02:33:38,389
continues. So let's say if you have
certain input like this, what happens is

7748
02:33:38,389 --> 02:33:38,399
certain input like this, what happens is
 

7749
02:33:38,399 --> 02:33:40,590
certain input like this, what happens is
that this input goes through my

7750
02:33:40,590 --> 02:33:40,600
that this input goes through my
 

7751
02:33:40,600 --> 02:33:43,110
that this input goes through my
model. Okay, the input goes through my

7752
02:33:43,110 --> 02:33:43,120
model. Okay, the input goes through my
 

7753
02:33:43,120 --> 02:33:45,429
model. Okay, the input goes through my
model and then I have uh my model is

7754
02:33:45,429 --> 02:33:45,439
model and then I have uh my model is
 

7755
02:33:45,439 --> 02:33:47,110
model and then I have uh my model is
fully trained now. So parameters don't

7756
02:33:47,110 --> 02:33:47,120
fully trained now. So parameters don't
 

7757
02:33:47,120 --> 02:33:49,349
fully trained now. So parameters don't
update. Then I have the decoded token

7758
02:33:49,349 --> 02:33:49,359
update. Then I have the decoded token
 

7759
02:33:49,359 --> 02:33:52,150
update. Then I have the decoded token
over here. My decoded token that decoded

7760
02:33:52,150 --> 02:33:52,160
over here. My decoded token that decoded
 

7761
02:33:52,160 --> 02:33:54,469
over here. My decoded token that decoded
token is appended to the input now. So

7762
02:33:54,469 --> 02:33:54,479
token is appended to the input now. So
 

7763
02:33:54,479 --> 02:33:56,389
token is appended to the input now. So
now my input at the next iteration

7764
02:33:56,389 --> 02:33:56,399
now my input at the next iteration
 

7765
02:33:56,399 --> 02:33:57,830
now my input at the next iteration
becomes something like this with the

7766
02:33:57,830 --> 02:33:57,840
becomes something like this with the
 

7767
02:33:57,840 --> 02:33:59,750
becomes something like this with the
appended decoded token. it goes to my

7768
02:33:59,750 --> 02:33:59,760
appended decoded token. it goes to my
 

7769
02:33:59,760 --> 02:34:02,230
appended decoded token. it goes to my
model and then again my model produces a

7770
02:34:02,230 --> 02:34:02,240
model and then again my model produces a
 

7771
02:34:02,240 --> 02:34:03,590
model and then again my model produces a
new decoded token which is again

7772
02:34:03,590 --> 02:34:03,600
new decoded token which is again
 

7773
02:34:03,600 --> 02:34:05,830
new decoded token which is again
appended to the input. So this process

7774
02:34:05,830 --> 02:34:05,840
appended to the input. So this process
 

7775
02:34:05,840 --> 02:34:07,750
appended to the input. So this process
continues until we have to prescribe how

7776
02:34:07,750 --> 02:34:07,760
continues until we have to prescribe how
 

7777
02:34:07,760 --> 02:34:09,910
continues until we have to prescribe how
many new words we have to generate. So

7778
02:34:09,910 --> 02:34:09,920
many new words we have to generate. So
 

7779
02:34:09,920 --> 02:34:12,070
many new words we have to generate. So
if you see in chat GPT this is exactly

7780
02:34:12,070 --> 02:34:12,080
if you see in chat GPT this is exactly
 

7781
02:34:12,080 --> 02:34:14,550
if you see in chat GPT this is exactly
what is happening right. Let's take the

7782
02:34:14,550 --> 02:34:14,560
what is happening right. Let's take the
 

7783
02:34:14,560 --> 02:34:17,030
what is happening right. Let's take the
same example which we took yesterday or

7784
02:34:17,030 --> 02:34:17,040
same example which we took yesterday or
 

7785
02:34:17,040 --> 02:34:19,270
same example which we took yesterday or
in the previous part of this lecture

7786
02:34:19,270 --> 02:34:19,280
in the previous part of this lecture
 

7787
02:34:19,280 --> 02:34:21,510
in the previous part of this lecture
make a travel plan for Italy. I believe

7788
02:34:21,510 --> 02:34:21,520
make a travel plan for Italy. I believe
 

7789
02:34:21,520 --> 02:34:23,750
make a travel plan for Italy. I believe
we had taken this example. So here you

7790
02:34:23,750 --> 02:34:23,760
we had taken this example. So here you
 

7791
02:34:23,760 --> 02:34:25,830
we had taken this example. So here you
see first the first token is generated

7792
02:34:25,830 --> 02:34:25,840
see first the first token is generated
 

7793
02:34:25,840 --> 02:34:27,510
see first the first token is generated
then that will be again appended to the

7794
02:34:27,510 --> 02:34:27,520
then that will be again appended to the
 

7795
02:34:27,520 --> 02:34:29,910
then that will be again appended to the
input then the second token is generated

7796
02:34:29,910 --> 02:34:29,920
input then the second token is generated
 

7797
02:34:29,920 --> 02:34:31,590
input then the second token is generated
that will be again appended to the input

7798
02:34:31,590 --> 02:34:31,600
that will be again appended to the input
 

7799
02:34:31,600 --> 02:34:34,309
that will be again appended to the input
third token is generated etc. That's how

7800
02:34:34,309 --> 02:34:34,319
third token is generated etc. That's how
 

7801
02:34:34,319 --> 02:34:35,990
third token is generated etc. That's how
the inference process actually happens

7802
02:34:35,990 --> 02:34:36,000
the inference process actually happens
 

7803
02:34:36,000 --> 02:34:38,950
the inference process actually happens
in language model tasks we append tokens

7804
02:34:38,950 --> 02:34:38,960
in language model tasks we append tokens
 

7805
02:34:38,960 --> 02:34:40,870
in language model tasks we append tokens
then a new token is decoded that's again

7806
02:34:40,870 --> 02:34:40,880
then a new token is decoded that's again
 

7807
02:34:40,880 --> 02:34:42,630
then a new token is decoded that's again
appended to the input that again passes

7808
02:34:42,630 --> 02:34:42,640
appended to the input that again passes
 

7809
02:34:42,640 --> 02:34:44,230
appended to the input that again passes
through the model new token is decoded

7810
02:34:44,230 --> 02:34:44,240
through the model new token is decoded
 

7811
02:34:44,240 --> 02:34:46,070
through the model new token is decoded
that's again appended to the input etc

7812
02:34:46,070 --> 02:34:46,080
that's again appended to the input etc
 

7813
02:34:46,080 --> 02:34:48,710
that's again appended to the input etc
and this process actually continues. So

7814
02:34:48,710 --> 02:34:48,720
and this process actually continues. So
 

7815
02:34:48,720 --> 02:34:50,670
and this process actually continues. So
this is the whole inference

7816
02:34:50,670 --> 02:34:50,680
this is the whole inference
 

7817
02:34:50,680 --> 02:34:54,389
this is the whole inference
pipeline. Uh this step-by-step process

7818
02:34:54,389 --> 02:34:54,399
pipeline. Uh this step-by-step process
 

7819
02:34:54,399 --> 02:34:56,150
pipeline. Uh this step-by-step process
enables the model to generate text

7820
02:34:56,150 --> 02:34:56,160
enables the model to generate text
 

7821
02:34:56,160 --> 02:34:58,150
enables the model to generate text
sequentially building coherent phrases

7822
02:34:58,150 --> 02:34:58,160
sequentially building coherent phrases
 

7823
02:34:58,160 --> 02:35:00,630
sequentially building coherent phrases
from the initial input context. So what

7824
02:35:00,630 --> 02:35:00,640
from the initial input context. So what
 

7825
02:35:00,640 --> 02:35:02,230
from the initial input context. So what
we are going to do now in this part of

7826
02:35:02,230 --> 02:35:02,240
we are going to do now in this part of
 

7827
02:35:02,240 --> 02:35:05,030
we are going to do now in this part of
the code is that I have uh first what I

7828
02:35:05,030 --> 02:35:05,040
the code is that I have uh first what I
 

7829
02:35:05,040 --> 02:35:06,429
the code is that I have uh first what I
have done is that I have stored the

7830
02:35:06,429 --> 02:35:06,439
have done is that I have stored the
 

7831
02:35:06,439 --> 02:35:10,670
have done is that I have stored the
model. Uh let me see where that

7832
02:35:10,670 --> 02:35:10,680
model. Uh let me see where that
 

7833
02:35:10,680 --> 02:35:13,190
model. Uh let me see where that
is. Yeah, actually my model is already

7834
02:35:13,190 --> 02:35:13,200
is. Yeah, actually my model is already
 

7835
02:35:13,200 --> 02:35:14,790
is. Yeah, actually my model is already
the parameters are already stored in

7836
02:35:14,790 --> 02:35:14,800
the parameters are already stored in
 

7837
02:35:14,800 --> 02:35:16,870
the parameters are already stored in
best model params.pt PT which is the

7838
02:35:16,870 --> 02:35:16,880
best model params.pt PT which is the
 

7839
02:35:16,880 --> 02:35:19,270
best model params.pt PT which is the
best which is stored in a variable

7840
02:35:19,270 --> 02:35:19,280
best which is stored in a variable
 

7841
02:35:19,280 --> 02:35:22,870
best which is stored in a variable
called best model params path. So what I

7842
02:35:22,870 --> 02:35:22,880
called best model params path. So what I
 

7843
02:35:22,880 --> 02:35:25,830
called best model params path. So what I
have done here is that I'm getting this

7844
02:35:25,830 --> 02:35:25,840
have done here is that I'm getting this
 

7845
02:35:25,840 --> 02:35:27,349
have done here is that I'm getting this
path over here which is best model

7846
02:35:27,349 --> 02:35:27,359
path over here which is best model
 

7847
02:35:27,359 --> 02:35:29,030
path over here which is best model
params path and I'm loading the model

7848
02:35:29,030 --> 02:35:29,040
params path and I'm loading the model
 

7849
02:35:29,040 --> 02:35:30,790
params path and I'm loading the model
here. So the reason I'm showing this

7850
02:35:30,790 --> 02:35:30,800
here. So the reason I'm showing this
 

7851
02:35:30,800 --> 02:35:32,389
here. So the reason I'm showing this
part of the code is that even if you

7852
02:35:32,389 --> 02:35:32,399
part of the code is that even if you
 

7853
02:35:32,399 --> 02:35:35,030
part of the code is that even if you
turn off your uh kernel or if you

7854
02:35:35,030 --> 02:35:35,040
turn off your uh kernel or if you
 

7855
02:35:35,040 --> 02:35:37,830
turn off your uh kernel or if you
restart your instance since this path

7856
02:35:37,830 --> 02:35:37,840
restart your instance since this path
 

7857
02:35:37,840 --> 02:35:39,750
restart your instance since this path
still contains the best parameters you

7858
02:35:39,750 --> 02:35:39,760
still contains the best parameters you
 

7859
02:35:39,760 --> 02:35:42,230
still contains the best parameters you
can load your entire model using this

7860
02:35:42,230 --> 02:35:42,240
can load your entire model using this
 

7861
02:35:42,240 --> 02:35:44,630
can load your entire model using this
function called load state dict. This is

7862
02:35:44,630 --> 02:35:44,640
function called load state dict. This is
 

7863
02:35:44,640 --> 02:35:47,030
function called load state dict. This is
so useful and you will thank me for

7864
02:35:47,030 --> 02:35:47,040
so useful and you will thank me for
 

7865
02:35:47,040 --> 02:35:48,790
so useful and you will thank me for
sharing this because sometimes or many

7866
02:35:48,790 --> 02:35:48,800
sharing this because sometimes or many
 

7867
02:35:48,800 --> 02:35:50,710
sharing this because sometimes or many
times it has happened with me also that

7868
02:35:50,710 --> 02:35:50,720
times it has happened with me also that
 

7869
02:35:50,720 --> 02:35:52,550
times it has happened with me also that
I've trained a model but forgot to save

7870
02:35:52,550 --> 02:35:52,560
I've trained a model but forgot to save
 

7871
02:35:52,560 --> 02:35:54,630
I've trained a model but forgot to save
the parameters. But it's a very simple

7872
02:35:54,630 --> 02:35:54,640
the parameters. But it's a very simple
 

7873
02:35:54,640 --> 02:35:56,389
the parameters. But it's a very simple
thing right you just store all the learn

7874
02:35:56,389 --> 02:35:56,399
thing right you just store all the learn
 

7875
02:35:56,399 --> 02:35:59,429
thing right you just store all the learn
parameters in this best model params and

7876
02:35:59,429 --> 02:35:59,439
parameters in this best model params and
 

7877
02:35:59,439 --> 02:36:01,429
parameters in this best model params and
then you just load it whenever it is

7878
02:36:01,429 --> 02:36:01,439
then you just load it whenever it is
 

7879
02:36:01,439 --> 02:36:03,750
then you just load it whenever it is
needed. So here I'm using load state

7880
02:36:03,750 --> 02:36:03,760
needed. So here I'm using load state
 

7881
02:36:03,760 --> 02:36:05,510
needed. So here I'm using load state
dick that loads the entire model

7882
02:36:05,510 --> 02:36:05,520
dick that loads the entire model
 

7883
02:36:05,520 --> 02:36:06,990
dick that loads the entire model
parameters into the

7884
02:36:06,990 --> 02:36:07,000
parameters into the
 

7885
02:36:07,000 --> 02:36:09,349
parameters into the
model and then what I'm doing is that

7886
02:36:09,349 --> 02:36:09,359
model and then what I'm doing is that
 

7887
02:36:09,359 --> 02:36:11,030
model and then what I'm doing is that
I'm starting with a sentence that's my

7888
02:36:11,030 --> 02:36:11,040
I'm starting with a sentence that's my
 

7889
02:36:11,040 --> 02:36:13,830
I'm starting with a sentence that's my
input sentence. Now uh I will encode the

7890
02:36:13,830 --> 02:36:13,840
input sentence. Now uh I will encode the
 

7891
02:36:13,840 --> 02:36:16,790
input sentence. Now uh I will encode the
input sentence and then I will call this

7892
02:36:16,790 --> 02:36:16,800
input sentence and then I will call this
 

7893
02:36:16,800 --> 02:36:19,270
input sentence and then I will call this
model dot generate. So if you see if you

7894
02:36:19,270 --> 02:36:19,280
model dot generate. So if you see if you
 

7895
02:36:19,280 --> 02:36:21,990
model dot generate. So if you see if you
scroll above my GPT class which has been

7896
02:36:21,990 --> 02:36:22,000
scroll above my GPT class which has been
 

7897
02:36:22,000 --> 02:36:24,870
scroll above my GPT class which has been
defined over here my GPT class also has

7898
02:36:24,870 --> 02:36:24,880
defined over here my GPT class also has
 

7899
02:36:24,880 --> 02:36:26,950
defined over here my GPT class also has
a function which is called as generate.

7900
02:36:26,950 --> 02:36:26,960
a function which is called as generate.
 

7901
02:36:26,960 --> 02:36:28,790
a function which is called as generate.
What this generate function does is that

7902
02:36:28,790 --> 02:36:28,800
What this generate function does is that
 

7903
02:36:28,800 --> 02:36:31,830
What this generate function does is that
it actually the same logic which I

7904
02:36:31,830 --> 02:36:31,840
it actually the same logic which I
 

7905
02:36:31,840 --> 02:36:33,590
it actually the same logic which I
mentioned over here. It applies this

7906
02:36:33,590 --> 02:36:33,600
mentioned over here. It applies this
 

7907
02:36:33,600 --> 02:36:35,670
mentioned over here. It applies this
same logic right. So first what it will

7908
02:36:35,670 --> 02:36:35,680
same logic right. So first what it will
 

7909
02:36:35,680 --> 02:36:38,510
same logic right. So first what it will
do is that it will generate this logit

7910
02:36:38,510 --> 02:36:38,520
do is that it will generate this logit
 

7911
02:36:38,520 --> 02:36:41,830
do is that it will generate this logit
tensor. Okay. then it will get the next

7912
02:36:41,830 --> 02:36:41,840
tensor. Okay. then it will get the next
 

7913
02:36:41,840 --> 02:36:45,910
tensor. Okay. then it will get the next
id based on the highest index of the uh

7914
02:36:45,910 --> 02:36:45,920
id based on the highest index of the uh
 

7915
02:36:45,920 --> 02:36:48,070
id based on the highest index of the uh
last row of the logit tensor that then

7916
02:36:48,070 --> 02:36:48,080
last row of the logit tensor that then
 

7917
02:36:48,080 --> 02:36:50,070
last row of the logit tensor that then
it will append the id to the current ids

7918
02:36:50,070 --> 02:36:50,080
it will append the id to the current ids
 

7919
02:36:50,080 --> 02:36:51,670
it will append the id to the current ids
and then this whole process will repeat

7920
02:36:51,670 --> 02:36:51,680
and then this whole process will repeat
 

7921
02:36:51,680 --> 02:36:53,750
and then this whole process will repeat
in the loop that's exactly what I

7922
02:36:53,750 --> 02:36:53,760
in the loop that's exactly what I
 

7923
02:36:53,760 --> 02:36:56,070
in the loop that's exactly what I
mentioned over here right uh we have the

7924
02:36:56,070 --> 02:36:56,080
mentioned over here right uh we have the
 

7925
02:36:56,080 --> 02:36:59,870
mentioned over here right uh we have the
token ID which is the input token ID

7926
02:36:59,870 --> 02:36:59,880
token ID which is the input token ID
 

7927
02:36:59,880 --> 02:37:03,110
token ID which is the input token ID
uh uh sorry we have the let me write

7928
02:37:03,110 --> 02:37:03,120
uh uh sorry we have the let me write
 

7929
02:37:03,120 --> 02:37:04,790
uh uh sorry we have the let me write
this whole loop over here actually we

7930
02:37:04,790 --> 02:37:04,800
this whole loop over here actually we
 

7931
02:37:04,800 --> 02:37:06,990
this whole loop over here actually we
have the current input sequence

7932
02:37:06,990 --> 02:37:07,000
have the current input sequence
 

7933
02:37:07,000 --> 02:37:10,150
have the current input sequence
right and in my case the input sequence

7934
02:37:10,150 --> 02:37:10,160
right and in my case the input sequence
 

7935
02:37:10,160 --> 02:37:12,110
right and in my case the input sequence
let's say something like

7936
02:37:12,110 --> 02:37:12,120
let's say something like
 

7937
02:37:12,120 --> 02:37:15,990
let's say something like
uh one day a little one day a little

7938
02:37:15,990 --> 02:37:16,000
uh one day a little one day a little
 

7939
02:37:16,000 --> 02:37:19,030
uh one day a little one day a little
that input sequence is passed through my

7940
02:37:19,030 --> 02:37:19,040
that input sequence is passed through my
 

7941
02:37:19,040 --> 02:37:21,030
that input sequence is passed through my
model that input sequence is passed

7942
02:37:21,030 --> 02:37:21,040
model that input sequence is passed
 

7943
02:37:21,040 --> 02:37:23,030
model that input sequence is passed
through my model and then I get my logit

7944
02:37:23,030 --> 02:37:23,040
through my model and then I get my logit
 

7945
02:37:23,040 --> 02:37:25,590
through my model and then I get my logit
tensor now remember the logit tensor is

7946
02:37:25,590 --> 02:37:25,600
tensor now remember the logit tensor is
 

7947
02:37:25,600 --> 02:37:28,950
tensor now remember the logit tensor is
one day a little and each of them is a

7948
02:37:28,950 --> 02:37:28,960
one day a little and each of them is a
 

7949
02:37:28,960 --> 02:37:32,710
one day a little and each of them is a
50 or vocabulary size vector right now

7950
02:37:32,710 --> 02:37:32,720
50 or vocabulary size vector right now
 

7951
02:37:32,720 --> 02:37:34,790
50 or vocabulary size vector right now
since during the inference stage we

7952
02:37:34,790 --> 02:37:34,800
since during the inference stage we
 

7953
02:37:34,800 --> 02:37:36,469
since during the inference stage we
actually don't care about these first

7954
02:37:36,469 --> 02:37:36,479
actually don't care about these first
 

7955
02:37:36,479 --> 02:37:38,630
actually don't care about these first
three at all because we just have to

7956
02:37:38,630 --> 02:37:38,640
three at all because we just have to
 

7957
02:37:38,640 --> 02:37:41,030
three at all because we just have to
know what comes after little. So we only

7958
02:37:41,030 --> 02:37:41,040
know what comes after little. So we only
 

7959
02:37:41,040 --> 02:37:43,270
know what comes after little. So we only
look at the last row over here and we

7960
02:37:43,270 --> 02:37:43,280
look at the last row over here and we
 

7961
02:37:43,280 --> 02:37:45,190
look at the last row over here and we
look at the token ID corresponding to

7962
02:37:45,190 --> 02:37:45,200
look at the token ID corresponding to
 

7963
02:37:45,200 --> 02:37:46,870
look at the token ID corresponding to
the maximum probability that's my

7964
02:37:46,870 --> 02:37:46,880
the maximum probability that's my
 

7965
02:37:46,880 --> 02:37:49,110
the maximum probability that's my
decoded next token if that is girl

7966
02:37:49,110 --> 02:37:49,120
decoded next token if that is girl
 

7967
02:37:49,120 --> 02:37:50,950
decoded next token if that is girl
that's appended again and then this

7968
02:37:50,950 --> 02:37:50,960
that's appended again and then this
 

7969
02:37:50,960 --> 02:37:52,710
that's appended again and then this
whole appended sequence now goes through

7970
02:37:52,710 --> 02:37:52,720
whole appended sequence now goes through
 

7971
02:37:52,720 --> 02:37:55,110
whole appended sequence now goes through
the model once more that's how this loop

7972
02:37:55,110 --> 02:37:55,120
the model once more that's how this loop
 

7973
02:37:55,120 --> 02:37:57,910
the model once more that's how this loop
actually proceeds. So if you see what we

7974
02:37:57,910 --> 02:37:57,920
actually proceeds. So if you see what we
 

7975
02:37:57,920 --> 02:38:01,230
actually proceeds. So if you see what we
do initially is that we pass this

7976
02:38:01,230 --> 02:38:01,240
do initially is that we pass this
 

7977
02:38:01,240 --> 02:38:05,349
do initially is that we pass this
uh yeah we get the we get the logits now

7978
02:38:05,349 --> 02:38:05,359
uh yeah we get the we get the logits now
 

7979
02:38:05,359 --> 02:38:08,670
uh yeah we get the we get the logits now
and here you see f dots

7980
02:38:08,670 --> 02:38:08,680
and here you see f dots
 

7981
02:38:08,680 --> 02:38:11,910
and here you see f dots
softmax logit. So then we get the vector

7982
02:38:11,910 --> 02:38:11,920
softmax logit. So then we get the vector
 

7983
02:38:11,920 --> 02:38:14,150
softmax logit. So then we get the vector
of probabilities and then we get the

7984
02:38:14,150 --> 02:38:14,160
of probabilities and then we get the
 

7985
02:38:14,160 --> 02:38:16,950
of probabilities and then we get the
next id based on the last row of this

7986
02:38:16,950 --> 02:38:16,960
next id based on the last row of this
 

7987
02:38:16,960 --> 02:38:19,590
next id based on the last row of this
probability and that is then appended to

7988
02:38:19,590 --> 02:38:19,600
probability and that is then appended to
 

7989
02:38:19,600 --> 02:38:23,910
probability and that is then appended to
my current uh to my current ids.

7990
02:38:23,910 --> 02:38:23,920
my current uh to my current ids.
 

7991
02:38:23,920 --> 02:38:26,550
my current uh to my current ids.
So this is exactly uh this is exactly

7992
02:38:26,550 --> 02:38:26,560
So this is exactly uh this is exactly
 

7993
02:38:26,560 --> 02:38:28,469
So this is exactly uh this is exactly
what happens in the generation part. And

7994
02:38:28,469 --> 02:38:28,479
what happens in the generation part. And
 

7995
02:38:28,479 --> 02:38:30,389
what happens in the generation part. And
you might be wondering what is this top

7996
02:38:30,389 --> 02:38:30,399
you might be wondering what is this top
 

7997
02:38:30,399 --> 02:38:33,349
you might be wondering what is this top
k over here. So top k essentially means

7998
02:38:33,349 --> 02:38:33,359
k over here. So top k essentially means
 

7999
02:38:33,359 --> 02:38:35,030
k over here. So top k essentially means
that right now we are looking at my

8000
02:38:35,030 --> 02:38:35,040
that right now we are looking at my
 

8001
02:38:35,040 --> 02:38:37,030
that right now we are looking at my
whole set of probabilities right and

8002
02:38:37,030 --> 02:38:37,040
whole set of probabilities right and
 

8003
02:38:37,040 --> 02:38:38,790
whole set of probabilities right and
then I'm looking at that token with the

8004
02:38:38,790 --> 02:38:38,800
then I'm looking at that token with the
 

8005
02:38:38,800 --> 02:38:41,270
then I'm looking at that token with the
hash probability. Top k just restricts

8006
02:38:41,270 --> 02:38:41,280
hash probability. Top k just restricts
 

8007
02:38:41,280 --> 02:38:43,270
hash probability. Top k just restricts
to let's say we look at the maximum five

8008
02:38:43,270 --> 02:38:43,280
to let's say we look at the maximum five
 

8009
02:38:43,280 --> 02:38:46,150
to let's say we look at the maximum five
tokens etc and then we choose from these

8010
02:38:46,150 --> 02:38:46,160
tokens etc and then we choose from these
 

8011
02:38:46,160 --> 02:38:48,389
tokens etc and then we choose from these
five tokens. So these are some inference

8012
02:38:48,389 --> 02:38:48,399
five tokens. So these are some inference
 

8013
02:38:48,399 --> 02:38:49,750
five tokens. So these are some inference
strategies which are used. There are

8014
02:38:49,750 --> 02:38:49,760
strategies which are used. There are
 

8015
02:38:49,760 --> 02:38:51,190
strategies which are used. There are
some other strategies also which is

8016
02:38:51,190 --> 02:38:51,200
some other strategies also which is
 

8017
02:38:51,200 --> 02:38:53,590
some other strategies also which is
called as temperature scaling. So you'll

8018
02:38:53,590 --> 02:38:53,600
called as temperature scaling. So you'll
 

8019
02:38:53,600 --> 02:38:55,110
called as temperature scaling. So you'll
see one more factor here which is

8020
02:38:55,110 --> 02:38:55,120
see one more factor here which is
 

8021
02:38:55,120 --> 02:38:56,630
see one more factor here which is
temperature. So that temperature

8022
02:38:56,630 --> 02:38:56,640
temperature. So that temperature
 

8023
02:38:56,640 --> 02:38:58,550
temperature. So that temperature
effectively controls the creativity of

8024
02:38:58,550 --> 02:38:58,560
effectively controls the creativity of
 

8025
02:38:58,560 --> 02:39:01,190
effectively controls the creativity of
my output. The higher the temperature in

8026
02:39:01,190 --> 02:39:01,200
my output. The higher the temperature in
 

8027
02:39:01,200 --> 02:39:03,670
my output. The higher the temperature in
the soft max every token will have

8028
02:39:03,670 --> 02:39:03,680
the soft max every token will have
 

8029
02:39:03,680 --> 02:39:06,230
the soft max every token will have
higher entropy. So it is more likely

8030
02:39:06,230 --> 02:39:06,240
higher entropy. So it is more likely
 

8031
02:39:06,240 --> 02:39:08,309
higher entropy. So it is more likely
that sometimes some token will come up

8032
02:39:08,309 --> 02:39:08,319
that sometimes some token will come up
 

8033
02:39:08,319 --> 02:39:10,270
that sometimes some token will come up
sometimes some other token will come up

8034
02:39:10,270 --> 02:39:10,280
sometimes some other token will come up
 

8035
02:39:10,280 --> 02:39:12,670
sometimes some other token will come up
etc.

8036
02:39:12,670 --> 02:39:12,680
etc.
 

8037
02:39:12,680 --> 02:39:15,590
etc.
Um and in the top K what is done here is

8038
02:39:15,590 --> 02:39:15,600
Um and in the top K what is done here is
 

8039
02:39:15,600 --> 02:39:17,349
Um and in the top K what is done here is
that instead of distributing the

8040
02:39:17,349 --> 02:39:17,359
that instead of distributing the
 

8041
02:39:17,359 --> 02:39:19,349
that instead of distributing the
probability among all the tokens we

8042
02:39:19,349 --> 02:39:19,359
probability among all the tokens we
 

8043
02:39:19,359 --> 02:39:21,910
probability among all the tokens we
essentially look at only the top tokens

8044
02:39:21,910 --> 02:39:21,920
essentially look at only the top tokens
 

8045
02:39:21,920 --> 02:39:24,790
essentially look at only the top tokens
and uh we are going to look at those top

8046
02:39:24,790 --> 02:39:24,800
and uh we are going to look at those top
 

8047
02:39:24,800 --> 02:39:27,349
and uh we are going to look at those top
tokens and select the next token from

8048
02:39:27,349 --> 02:39:27,359
tokens and select the next token from
 

8049
02:39:27,359 --> 02:39:30,110
tokens and select the next token from
among those top K

8050
02:39:30,110 --> 02:39:30,120
among those top K
 

8051
02:39:30,120 --> 02:39:33,790
among those top K
um from among those top K

8052
02:39:33,790 --> 02:39:33,800
um from among those top K
 

8053
02:39:33,800 --> 02:39:36,309
um from among those top K
tokens. Okay. And for that we use this

8054
02:39:36,309 --> 02:39:36,319
tokens. Okay. And for that we use this
 

8055
02:39:36,319 --> 02:39:38,550
tokens. Okay. And for that we use this
function which is called torch.topk. So

8056
02:39:38,550 --> 02:39:38,560
function which is called torch.topk. So
 

8057
02:39:38,560 --> 02:39:40,389
function which is called torch.topk. So
you can search about this torch.topk.

8058
02:39:40,389 --> 02:39:40,399
you can search about this torch.topk.
 

8059
02:39:40,399 --> 02:39:43,429
you can search about this torch.topk.
top k and it returns the k largest

8060
02:39:43,429 --> 02:39:43,439
top k and it returns the k largest
 

8061
02:39:43,439 --> 02:39:45,510
top k and it returns the k largest
elements of the given input tensor along

8062
02:39:45,510 --> 02:39:45,520
elements of the given input tensor along
 

8063
02:39:45,520 --> 02:39:49,309
elements of the given input tensor along
the given

8064
02:39:49,309 --> 02:39:49,319

 

8065
02:39:49,319 --> 02:39:53,990

dimension. Um okay so that's how this

8066
02:39:53,990 --> 02:39:54,000
dimension. Um okay so that's how this
 

8067
02:39:54,000 --> 02:39:55,670
dimension. Um okay so that's how this
generation is done the next token

8068
02:39:55,670 --> 02:39:55,680
generation is done the next token
 

8069
02:39:55,680 --> 02:39:57,349
generation is done the next token
generation. So you'll see that here

8070
02:39:57,349 --> 02:39:57,359
generation. So you'll see that here
 

8071
02:39:57,359 --> 02:40:00,790
generation. So you'll see that here
towards the end I pass my input sequence

8072
02:40:00,790 --> 02:40:00,800
towards the end I pass my input sequence
 

8073
02:40:00,800 --> 02:40:02,630
towards the end I pass my input sequence
through this model generate and here

8074
02:40:02,630 --> 02:40:02,640
through this model generate and here
 

8075
02:40:02,640 --> 02:40:05,030
through this model generate and here
it's 200. So what are these 200 over

8076
02:40:05,030 --> 02:40:05,040
it's 200. So what are these 200 over
 

8077
02:40:05,040 --> 02:40:07,670
it's 200. So what are these 200 over
here? So this 200 are how many new

8078
02:40:07,670 --> 02:40:07,680
here? So this 200 are how many new
 

8079
02:40:07,680 --> 02:40:09,590
here? So this 200 are how many new
tokens I want to generate. So if you see

8080
02:40:09,590 --> 02:40:09,600
tokens I want to generate. So if you see
 

8081
02:40:09,600 --> 02:40:11,590
tokens I want to generate. So if you see
over here

8082
02:40:11,590 --> 02:40:11,600
over here
 

8083
02:40:11,600 --> 02:40:13,349
over here
uh if you see over here there is this

8084
02:40:13,349 --> 02:40:13,359
uh if you see over here there is this
 

8085
02:40:13,359 --> 02:40:15,429
uh if you see over here there is this
max new tokens right. So essentially

8086
02:40:15,429 --> 02:40:15,439
max new tokens right. So essentially
 

8087
02:40:15,439 --> 02:40:17,429
max new tokens right. So essentially
this loop continues for how many new

8088
02:40:17,429 --> 02:40:17,439
this loop continues for how many new
 

8089
02:40:17,439 --> 02:40:19,910
this loop continues for how many new
tokens we are generating. So this entire

8090
02:40:19,910 --> 02:40:19,920
tokens we are generating. So this entire
 

8091
02:40:19,920 --> 02:40:22,309
tokens we are generating. So this entire
loop continues for how many new tokens

8092
02:40:22,309 --> 02:40:22,319
loop continues for how many new tokens
 

8093
02:40:22,319 --> 02:40:24,309
loop continues for how many new tokens
we want to generate. So I have said here

8094
02:40:24,309 --> 02:40:24,319
we want to generate. So I have said here
 

8095
02:40:24,319 --> 02:40:26,230
we want to generate. So I have said here
that the maximum number of new tokens

8096
02:40:26,230 --> 02:40:26,240
that the maximum number of new tokens
 

8097
02:40:26,240 --> 02:40:28,389
that the maximum number of new tokens
which I want to generate is 200. And

8098
02:40:28,389 --> 02:40:28,399
which I want to generate is 200. And
 

8099
02:40:28,399 --> 02:40:30,630
which I want to generate is 200. And
then I can just print out my output. So

8100
02:40:30,630 --> 02:40:30,640
then I can just print out my output. So
 

8101
02:40:30,640 --> 02:40:32,630
then I can just print out my output. So
the input was once upon a time there was

8102
02:40:32,630 --> 02:40:32,640
the input was once upon a time there was
 

8103
02:40:32,640 --> 02:40:34,389
the input was once upon a time there was
a pumpkin and let's see what I have got

8104
02:40:34,389 --> 02:40:34,399
a pumpkin and let's see what I have got
 

8105
02:40:34,399 --> 02:40:37,110
a pumpkin and let's see what I have got
as the output. It was very special. The

8106
02:40:37,110 --> 02:40:37,120
as the output. It was very special. The
 

8107
02:40:37,120 --> 02:40:39,670
as the output. It was very special. The
pumpkin was very happy. The pumpkin were

8108
02:40:39,670 --> 02:40:39,680
pumpkin was very happy. The pumpkin were
 

8109
02:40:39,680 --> 02:40:41,590
pumpkin was very happy. The pumpkin were
very happy. One day a kind lady came to

8110
02:40:41,590 --> 02:40:41,600
very happy. One day a kind lady came to
 

8111
02:40:41,600 --> 02:40:43,830
very happy. One day a kind lady came to
the store to you. The delicious corn

8112
02:40:43,830 --> 02:40:43,840
the store to you. The delicious corn
 

8113
02:40:43,840 --> 02:40:45,910
the store to you. The delicious corn
helped the world play games alone. She

8114
02:40:45,910 --> 02:40:45,920
helped the world play games alone. She
 

8115
02:40:45,920 --> 02:40:47,670
helped the world play games alone. She
had two laser soldiers who lived in the

8116
02:40:47,670 --> 02:40:47,680
had two laser soldiers who lived in the
 

8117
02:40:47,680 --> 02:40:49,670
had two laser soldiers who lived in the
forest. The lady felt ashamed. Her

8118
02:40:49,670 --> 02:40:49,680
forest. The lady felt ashamed. Her
 

8119
02:40:49,680 --> 02:40:51,030
forest. The lady felt ashamed. Her
friend said, "Don't worry, I need a

8120
02:40:51,030 --> 02:40:51,040
friend said, "Don't worry, I need a
 

8121
02:40:51,040 --> 02:40:55,190
friend said, "Don't worry, I need a
board." Now, first, what I'm amazed at

8122
02:40:55,190 --> 02:40:55,200
board." Now, first, what I'm amazed at
 

8123
02:40:55,200 --> 02:40:56,870
board." Now, first, what I'm amazed at
looking at this is that this story is

8124
02:40:56,870 --> 02:40:56,880
looking at this is that this story is
 

8125
02:40:56,880 --> 02:40:59,030
looking at this is that this story is
not there anywhere in the data set.

8126
02:40:59,030 --> 02:40:59,040
not there anywhere in the data set.
 

8127
02:40:59,040 --> 02:41:01,270
not there anywhere in the data set.
Okay, that's the first thing we know.

8128
02:41:01,270 --> 02:41:01,280
Okay, that's the first thing we know.
 

8129
02:41:01,280 --> 02:41:03,590
Okay, that's the first thing we know.
Second thing, the grammar of the

8130
02:41:03,590 --> 02:41:03,600
Second thing, the grammar of the
 

8131
02:41:03,600 --> 02:41:06,070
Second thing, the grammar of the
sentences looks mostly correct. The

8132
02:41:06,070 --> 02:41:06,080
sentences looks mostly correct. The
 

8133
02:41:06,080 --> 02:41:07,990
sentences looks mostly correct. The
meaning does not look quite correct and

8134
02:41:07,990 --> 02:41:08,000
meaning does not look quite correct and
 

8135
02:41:08,000 --> 02:41:09,590
meaning does not look quite correct and
that's probably fine. When we increase

8136
02:41:09,590 --> 02:41:09,600
that's probably fine. When we increase
 

8137
02:41:09,600 --> 02:41:11,590
that's probably fine. When we increase
the number of iterations, I believe

8138
02:41:11,590 --> 02:41:11,600
the number of iterations, I believe
 

8139
02:41:11,600 --> 02:41:13,270
the number of iterations, I believe
we'll get to a stage where the stories

8140
02:41:13,270 --> 02:41:13,280
we'll get to a stage where the stories
 

8141
02:41:13,280 --> 02:41:15,030
we'll get to a stage where the stories
will even be coherent. But this is still

8142
02:41:15,030 --> 02:41:15,040
will even be coherent. But this is still
 

8143
02:41:15,040 --> 02:41:16,870
will even be coherent. But this is still
awesome, right? It could have printed

8144
02:41:16,870 --> 02:41:16,880
awesome, right? It could have printed
 

8145
02:41:16,880 --> 02:41:18,630
awesome, right? It could have printed
literally anything out there. But now

8146
02:41:18,630 --> 02:41:18,640
literally anything out there. But now
 

8147
02:41:18,640 --> 02:41:21,270
literally anything out there. But now
our model has learned something about

8148
02:41:21,270 --> 02:41:21,280
our model has learned something about
 

8149
02:41:21,280 --> 02:41:23,750
our model has learned something about
the model has learned something about

8150
02:41:23,750 --> 02:41:23,760
the model has learned something about
 

8151
02:41:23,760 --> 02:41:25,510
the model has learned something about
the language. The model has learned

8152
02:41:25,510 --> 02:41:25,520
the language. The model has learned
 

8153
02:41:25,520 --> 02:41:27,510
the language. The model has learned
something about how to generate stories.

8154
02:41:27,510 --> 02:41:27,520
something about how to generate stories.
 

8155
02:41:27,520 --> 02:41:29,510
something about how to generate stories.
So you can see that the model is kind of

8156
02:41:29,510 --> 02:41:29,520
So you can see that the model is kind of
 

8157
02:41:29,520 --> 02:41:31,590
So you can see that the model is kind of
making sense, right? The witch said, "So

8158
02:41:31,590 --> 02:41:31,600
making sense, right? The witch said, "So
 

8159
02:41:31,600 --> 02:41:33,429
making sense, right? The witch said, "So
there is a witch in this story. I found

8160
02:41:33,429 --> 02:41:33,439
there is a witch in this story. I found
 

8161
02:41:33,439 --> 02:41:35,030
there is a witch in this story. I found
it under the garden. It's important to

8162
02:41:35,030 --> 02:41:35,040
it under the garden. It's important to
 

8163
02:41:35,040 --> 02:41:37,270
it under the garden. It's important to
help others. We need to be careful. The

8164
02:41:37,270 --> 02:41:37,280
help others. We need to be careful. The
 

8165
02:41:37,280 --> 02:41:39,590
help others. We need to be careful. The
lady smiled and felt happy on herself.

8166
02:41:39,590 --> 02:41:39,600
lady smiled and felt happy on herself.
 

8167
02:41:39,600 --> 02:41:41,590
lady smiled and felt happy on herself.
All right. So although you can see that

8168
02:41:41,590 --> 02:41:41,600
All right. So although you can see that
 

8169
02:41:41,600 --> 02:41:44,710
All right. So although you can see that
all the sentences exactly don't quite

8170
02:41:44,710 --> 02:41:44,720
all the sentences exactly don't quite
 

8171
02:41:44,720 --> 02:41:47,510
all the sentences exactly don't quite
merge together, but within just 15 to 20

8172
02:41:47,510 --> 02:41:47,520
merge together, but within just 15 to 20
 

8173
02:41:47,520 --> 02:41:49,110
merge together, but within just 15 to 20
million parameters, we have got the

8174
02:41:49,110 --> 02:41:49,120
million parameters, we have got the
 

8175
02:41:49,120 --> 02:41:51,030
million parameters, we have got the
model to a stage where it generating

8176
02:41:51,030 --> 02:41:51,040
model to a stage where it generating
 

8177
02:41:51,040 --> 02:41:53,429
model to a stage where it generating
something which is coherent. It's not

8178
02:41:53,429 --> 02:41:53,439
something which is coherent. It's not
 

8179
02:41:53,439 --> 02:41:55,750
something which is coherent. It's not
random tokens. It's not random words.

8180
02:41:55,750 --> 02:41:55,760
random tokens. It's not random words.
 

8181
02:41:55,760 --> 02:41:57,429
random tokens. It's not random words.
It's trying to generate a story that

8182
02:41:57,429 --> 02:41:57,439
It's trying to generate a story that
 

8183
02:41:57,439 --> 02:41:59,349
It's trying to generate a story that
once upon a time there was a pumpkin. It

8184
02:41:59,349 --> 02:41:59,359
once upon a time there was a pumpkin. It
 

8185
02:41:59,359 --> 02:42:00,870
once upon a time there was a pumpkin. It
was very special. The pumpkin were very

8186
02:42:00,870 --> 02:42:00,880
was very special. The pumpkin were very
 

8187
02:42:00,880 --> 02:42:03,429
was very special. The pumpkin were very
happy, etc. That's incredible in my

8188
02:42:03,429 --> 02:42:03,439
happy, etc. That's incredible in my
 

8189
02:42:03,439 --> 02:42:05,710
happy, etc. That's incredible in my
opinion because we assembled this huge

8190
02:42:05,710 --> 02:42:05,720
opinion because we assembled this huge
 

8191
02:42:05,720 --> 02:42:07,990
opinion because we assembled this huge
architecture. We assembled this huge

8192
02:42:07,990 --> 02:42:08,000
architecture. We assembled this huge
 

8193
02:42:08,000 --> 02:42:09,750
architecture. We assembled this huge
architecture and we never told anything

8194
02:42:09,750 --> 02:42:09,760
architecture and we never told anything
 

8195
02:42:09,760 --> 02:42:11,670
architecture and we never told anything
to the model about language or grammar

8196
02:42:11,670 --> 02:42:11,680
to the model about language or grammar
 

8197
02:42:11,680 --> 02:42:13,910
to the model about language or grammar
or anything. We just trained the model

8198
02:42:13,910 --> 02:42:13,920
or anything. We just trained the model
 

8199
02:42:13,920 --> 02:42:16,070
or anything. We just trained the model
to predict the next token. And due to

8200
02:42:16,070 --> 02:42:16,080
to predict the next token. And due to
 

8201
02:42:16,080 --> 02:42:17,590
to predict the next token. And due to
this training, the model has come up

8202
02:42:17,590 --> 02:42:17,600
this training, the model has come up
 

8203
02:42:17,600 --> 02:42:19,190
this training, the model has come up
with a story on its own which is not

8204
02:42:19,190 --> 02:42:19,200
with a story on its own which is not
 

8205
02:42:19,200 --> 02:42:21,750
with a story on its own which is not
present in the data set at all. And this

8206
02:42:21,750 --> 02:42:21,760
present in the data set at all. And this
 

8207
02:42:21,760 --> 02:42:23,349
present in the data set at all. And this
story definitely needs to improve

8208
02:42:23,349 --> 02:42:23,359
story definitely needs to improve
 

8209
02:42:23,359 --> 02:42:25,190
story definitely needs to improve
further. And I think it can be because

8210
02:42:25,190 --> 02:42:25,200
further. And I think it can be because
 

8211
02:42:25,200 --> 02:42:26,950
further. And I think it can be because
this loss can further go down. The

8212
02:42:26,950 --> 02:42:26,960
this loss can further go down. The
 

8213
02:42:26,960 --> 02:42:28,630
this loss can further go down. The
training and validation losses are both

8214
02:42:28,630 --> 02:42:28,640
training and validation losses are both
 

8215
02:42:28,640 --> 02:42:30,710
training and validation losses are both
going down. So I encourage all of you to

8216
02:42:30,710 --> 02:42:30,720
going down. So I encourage all of you to
 

8217
02:42:30,720 --> 02:42:33,030
going down. So I encourage all of you to
run for more iterations and you can tell

8218
02:42:33,030 --> 02:42:33,040
run for more iterations and you can tell
 

8219
02:42:33,040 --> 02:42:34,870
run for more iterations and you can tell
me what's the kind of stories which you

8220
02:42:34,870 --> 02:42:34,880
me what's the kind of stories which you
 

8221
02:42:34,880 --> 02:42:36,309
me what's the kind of stories which you
are obtaining. What's the output which

8222
02:42:36,309 --> 02:42:36,319
are obtaining. What's the output which
 

8223
02:42:36,319 --> 02:42:37,469
are obtaining. What's the output which
you are

8224
02:42:37,469 --> 02:42:37,479
you are
 

8225
02:42:37,479 --> 02:42:40,710
you are
obtaining. I did one more example. A

8226
02:42:40,710 --> 02:42:40,720
obtaining. I did one more example. A
 

8227
02:42:40,720 --> 02:42:42,790
obtaining. I did one more example. A
little girl went to the woods. So the

8228
02:42:42,790 --> 02:42:42,800
little girl went to the woods. So the
 

8229
02:42:42,800 --> 02:42:45,190
little girl went to the woods. So the
model completed it like a little girl

8230
02:42:45,190 --> 02:42:45,200
model completed it like a little girl
 

8231
02:42:45,200 --> 02:42:46,950
model completed it like a little girl
went to the woods and she found a basket

8232
02:42:46,950 --> 02:42:46,960
went to the woods and she found a basket
 

8233
02:42:46,960 --> 02:42:48,710
went to the woods and she found a basket
for her. In one he enjoyed the great

8234
02:42:48,710 --> 02:42:48,720
for her. In one he enjoyed the great
 

8235
02:42:48,720 --> 02:42:50,870
for her. In one he enjoyed the great
basket and the tasty treats doll. Then

8236
02:42:50,870 --> 02:42:50,880
basket and the tasty treats doll. Then
 

8237
02:42:50,880 --> 02:42:52,870
basket and the tasty treats doll. Then
she decided to go home and the picnic.

8238
02:42:52,870 --> 02:42:52,880
she decided to go home and the picnic.
 

8239
02:42:52,880 --> 02:42:54,309
she decided to go home and the picnic.
As they walked the little girl was

8240
02:42:54,309 --> 02:42:54,319
As they walked the little girl was
 

8241
02:42:54,319 --> 02:42:55,990
As they walked the little girl was
getting tired and didn't want to drink.

8242
02:42:55,990 --> 02:42:56,000
getting tired and didn't want to drink.
 

8243
02:42:56,000 --> 02:42:57,670
getting tired and didn't want to drink.
She opened her pocket to her room and

8244
02:42:57,670 --> 02:42:57,680
She opened her pocket to her room and
 

8245
02:42:57,680 --> 02:42:59,910
She opened her pocket to her room and
got out. This is good, right? So, it

8246
02:42:59,910 --> 02:42:59,920
got out. This is good, right? So, it
 

8247
02:42:59,920 --> 02:43:01,590
got out. This is good, right? So, it
kind of makes sense. Little girl went to

8248
02:43:01,590 --> 02:43:01,600
kind of makes sense. Little girl went to
 

8249
02:43:01,600 --> 02:43:03,510
kind of makes sense. Little girl went to
the woods, bought a basket, then she

8250
02:43:03,510 --> 02:43:03,520
the woods, bought a basket, then she
 

8251
02:43:03,520 --> 02:43:05,590
the woods, bought a basket, then she
walked back home. She got tired, didn't

8252
02:43:05,590 --> 02:43:05,600
walked back home. She got tired, didn't
 

8253
02:43:05,600 --> 02:43:06,429
walked back home. She got tired, didn't
want to

8254
02:43:06,429 --> 02:43:06,439
want to
 

8255
02:43:06,439 --> 02:43:09,190
want to
drink. Uh, so something is starting to

8256
02:43:09,190 --> 02:43:09,200
drink. Uh, so something is starting to
 

8257
02:43:09,200 --> 02:43:10,870
drink. Uh, so something is starting to
make sense. Although everything is not

8258
02:43:10,870 --> 02:43:10,880
make sense. Although everything is not
 

8259
02:43:10,880 --> 02:43:14,070
make sense. Although everything is not
making sense. So, we have literally

8260
02:43:14,070 --> 02:43:14,080
making sense. So, we have literally
 

8261
02:43:14,080 --> 02:43:15,750
making sense. So, we have literally
constructed a language model out of

8262
02:43:15,750 --> 02:43:15,760
constructed a language model out of
 

8263
02:43:15,760 --> 02:43:17,550
constructed a language model out of
nothing today and built it fully from

8264
02:43:17,550 --> 02:43:17,560
nothing today and built it fully from
 

8265
02:43:17,560 --> 02:43:19,830
nothing today and built it fully from
scratch. And this is a language model

8266
02:43:19,830 --> 02:43:19,840
scratch. And this is a language model
 

8267
02:43:19,840 --> 02:43:21,670
scratch. And this is a language model
which does not have billions and

8268
02:43:21,670 --> 02:43:21,680
which does not have billions and
 

8269
02:43:21,680 --> 02:43:23,030
which does not have billions and
billions of parameters. It has

8270
02:43:23,030 --> 02:43:23,040
billions of parameters. It has
 

8271
02:43:23,040 --> 02:43:25,270
billions of parameters. It has
parameters around 15, 20, 30 million

8272
02:43:25,270 --> 02:43:25,280
parameters around 15, 20, 30 million
 

8273
02:43:25,280 --> 02:43:27,910
parameters around 15, 20, 30 million
parameters I think. Now one more thing

8274
02:43:27,910 --> 02:43:27,920
parameters I think. Now one more thing
 

8275
02:43:27,920 --> 02:43:29,990
parameters I think. Now one more thing
which you can do once you have access to

8276
02:43:29,990 --> 02:43:30,000
which you can do once you have access to
 

8277
02:43:30,000 --> 02:43:31,910
which you can do once you have access to
this code is that you can apply it to

8278
02:43:31,910 --> 02:43:31,920
this code is that you can apply it to
 

8279
02:43:31,920 --> 02:43:34,150
this code is that you can apply it to
any application which you want right you

8280
02:43:34,150 --> 02:43:34,160
any application which you want right you
 

8281
02:43:34,160 --> 02:43:36,469
any application which you want right you
can apply it to construct an SLM for

8282
02:43:36,469 --> 02:43:36,479
can apply it to construct an SLM for
 

8283
02:43:36,479 --> 02:43:37,630
can apply it to construct an SLM for
healthcare

8284
02:43:37,630 --> 02:43:37,640
healthcare
 

8285
02:43:37,640 --> 02:43:39,750
healthcare
applications where you can let's say

8286
02:43:39,750 --> 02:43:39,760
applications where you can let's say
 

8287
02:43:39,760 --> 02:43:42,469
applications where you can let's say
develop a model which answers questions

8288
02:43:42,469 --> 02:43:42,479
develop a model which answers questions
 

8289
02:43:42,479 --> 02:43:45,830
develop a model which answers questions
about chat diabetes patients etc but

8290
02:43:45,830 --> 02:43:45,840
about chat diabetes patients etc but
 

8291
02:43:45,840 --> 02:43:47,590
about chat diabetes patients etc but
just make sure that the data set is

8292
02:43:47,590 --> 02:43:47,600
just make sure that the data set is
 

8293
02:43:47,600 --> 02:43:50,230
just make sure that the data set is
small the data set is constricted the

8294
02:43:50,230 --> 02:43:50,240
small the data set is constricted the
 

8295
02:43:50,240 --> 02:43:52,469
small the data set is constricted the
reason the small language model works so

8296
02:43:52,469 --> 02:43:52,479
reason the small language model works so
 

8297
02:43:52,479 --> 02:43:54,309
reason the small language model works so
well in our case is because the data set

8298
02:43:54,309 --> 02:43:54,319
well in our case is because the data set
 

8299
02:43:54,319 --> 02:43:56,870
well in our case is because the data set
is small the data set is around 2

8300
02:43:56,870 --> 02:43:56,880
is small the data set is around 2
 

8301
02:43:56,880 --> 02:43:58,950
is small the data set is around 2
million rows and it's restricted to

8302
02:43:58,950 --> 02:43:58,960
million rows and it's restricted to
 

8303
02:43:58,960 --> 02:44:00,950
million rows and it's restricted to
stories for 3 to four year olds. That's

8304
02:44:00,950 --> 02:44:00,960
stories for 3 to four year olds. That's
 

8305
02:44:00,960 --> 02:44:02,710
stories for 3 to four year olds. That's
what makes construction of the small

8306
02:44:02,710 --> 02:44:02,720
what makes construction of the small
 

8307
02:44:02,720 --> 02:44:05,269
what makes construction of the small
language model possible. So when you get

8308
02:44:05,269 --> 02:44:05,279
language model possible. So when you get
 

8309
02:44:05,279 --> 02:44:07,269
language model possible. So when you get
access to this code now you can apply to

8310
02:44:07,269 --> 02:44:07,279
access to this code now you can apply to
 

8311
02:44:07,279 --> 02:44:09,030
access to this code now you can apply to
so many sectors. You can apply to the

8312
02:44:09,030 --> 02:44:09,040
so many sectors. You can apply to the
 

8313
02:44:09,040 --> 02:44:11,429
so many sectors. You can apply to the
healthcare sector to create a chatbot.

8314
02:44:11,429 --> 02:44:11,439
healthcare sector to create a chatbot.
 

8315
02:44:11,439 --> 02:44:13,030
healthcare sector to create a chatbot.
You can create you can apply it in the

8316
02:44:13,030 --> 02:44:13,040
You can create you can apply it in the
 

8317
02:44:13,040 --> 02:44:16,710
You can create you can apply it in the
automotive sector where your uh language

8318
02:44:16,710 --> 02:44:16,720
automotive sector where your uh language
 

8319
02:44:16,720 --> 02:44:18,630
automotive sector where your uh language
model answers questions about cars.

8320
02:44:18,630 --> 02:44:18,640
model answers questions about cars.
 

8321
02:44:18,640 --> 02:44:21,030
model answers questions about cars.
Let's say you can build a language model

8322
02:44:21,030 --> 02:44:21,040
Let's say you can build a language model
 

8323
02:44:21,040 --> 02:44:22,710
Let's say you can build a language model
therapist, a small language model

8324
02:44:22,710 --> 02:44:22,720
therapist, a small language model
 

8325
02:44:22,720 --> 02:44:25,190
therapist, a small language model
therapist on your own which when you

8326
02:44:25,190 --> 02:44:25,200
therapist on your own which when you
 

8327
02:44:25,200 --> 02:44:26,950
therapist on your own which when you
when you interact with it, it replies

8328
02:44:26,950 --> 02:44:26,960
when you interact with it, it replies
 

8329
02:44:26,960 --> 02:44:30,230
when you interact with it, it replies
with uh answers like a therapist would.

8330
02:44:30,230 --> 02:44:30,240
with uh answers like a therapist would.
 

8331
02:44:30,240 --> 02:44:31,750
with uh answers like a therapist would.
But let's say if you're building this,

8332
02:44:31,750 --> 02:44:31,760
But let's say if you're building this,
 

8333
02:44:31,760 --> 02:44:33,590
But let's say if you're building this,
right, the data set which you have

8334
02:44:33,590 --> 02:44:33,600
right, the data set which you have
 

8335
02:44:33,600 --> 02:44:35,349
right, the data set which you have
should not be all over the internet

8336
02:44:35,349 --> 02:44:35,359
should not be all over the internet
 

8337
02:44:35,359 --> 02:44:36,870
should not be all over the internet
because then your model will need to be

8338
02:44:36,870 --> 02:44:36,880
because then your model will need to be
 

8339
02:44:36,880 --> 02:44:38,790
because then your model will need to be
huge. You need to intelligently

8340
02:44:38,790 --> 02:44:38,800
huge. You need to intelligently
 

8341
02:44:38,800 --> 02:44:41,269
huge. You need to intelligently
construct a data set which let's say

8342
02:44:41,269 --> 02:44:41,279
construct a data set which let's say
 

8343
02:44:41,279 --> 02:44:43,910
construct a data set which let's say
would be the most useful to a therapist.

8344
02:44:43,910 --> 02:44:43,920
would be the most useful to a therapist.
 

8345
02:44:43,920 --> 02:44:46,070
would be the most useful to a therapist.
Uh similar to how we did for the present

8346
02:44:46,070 --> 02:44:46,080
Uh similar to how we did for the present
 

8347
02:44:46,080 --> 02:44:48,630
Uh similar to how we did for the present
study. So now this kind of this video

8348
02:44:48,630 --> 02:44:48,640
study. So now this kind of this video
 

8349
02:44:48,640 --> 02:44:50,309
study. So now this kind of this video
and this notebook which I'll share with

8350
02:44:50,309 --> 02:44:50,319
and this notebook which I'll share with
 

8351
02:44:50,319 --> 02:44:52,230
and this notebook which I'll share with
you opens the door to you to do all

8352
02:44:52,230 --> 02:44:52,240
you opens the door to you to do all
 

8353
02:44:52,240 --> 02:44:54,710
you opens the door to you to do all
kinds of experiments. But first what you

8354
02:44:54,710 --> 02:44:54,720
kinds of experiments. But first what you
 

8355
02:44:54,720 --> 02:44:56,710
kinds of experiments. But first what you
should do is that if you have free tire

8356
02:44:56,710 --> 02:44:56,720
should do is that if you have free tire
 

8357
02:44:56,720 --> 02:44:58,630
should do is that if you have free tire
of Google Collab just use that free

8358
02:44:58,630 --> 02:44:58,640
of Google Collab just use that free
 

8359
02:44:58,640 --> 02:45:01,990
of Google Collab just use that free
tire. But if you have access to 800 GPU

8360
02:45:01,990 --> 02:45:02,000
tire. But if you have access to 800 GPU
 

8361
02:45:02,000 --> 02:45:03,750
tire. But if you have access to 800 GPU
that will significantly improve your

8362
02:45:03,750 --> 02:45:03,760
that will significantly improve your
 

8363
02:45:03,760 --> 02:45:05,910
that will significantly improve your
life because it's 10 to 20 times faster

8364
02:45:05,910 --> 02:45:05,920
life because it's 10 to 20 times faster
 

8365
02:45:05,920 --> 02:45:08,070
life because it's 10 to 20 times faster
than the T4 GPU. Right? So I would

8366
02:45:08,070 --> 02:45:08,080
than the T4 GPU. Right? So I would
 

8367
02:45:08,080 --> 02:45:10,309
than the T4 GPU. Right? So I would
highly recommend going for this. just go

8368
02:45:10,309 --> 02:45:10,319
highly recommend going for this. just go
 

8369
02:45:10,319 --> 02:45:13,269
highly recommend going for this. just go
for this and then run this full code and

8370
02:45:13,269 --> 02:45:13,279
for this and then run this full code and
 

8371
02:45:13,279 --> 02:45:16,790
for this and then run this full code and
second of all run it for 30,000 40,000

8372
02:45:16,790 --> 02:45:16,800
second of all run it for 30,000 40,000
 

8373
02:45:16,800 --> 02:45:18,790
second of all run it for 30,000 40,000
50,000 iterations. It might take two to

8374
02:45:18,790 --> 02:45:18,800
50,000 iterations. It might take two to
 

8375
02:45:18,800 --> 02:45:21,030
50,000 iterations. It might take two to
three hours on A100 but I think it will

8376
02:45:21,030 --> 02:45:21,040
three hours on A100 but I think it will
 

8377
02:45:21,040 --> 02:45:22,389
three hours on A100 but I think it will
be worth it because I think you will

8378
02:45:22,389 --> 02:45:22,399
be worth it because I think you will
 

8379
02:45:22,399 --> 02:45:24,070
be worth it because I think you will
start to get awesome stories once you

8380
02:45:24,070 --> 02:45:24,080
start to get awesome stories once you
 

8381
02:45:24,080 --> 02:45:26,389
start to get awesome stories once you
run for those many number of iterations.

8382
02:45:26,389 --> 02:45:26,399
run for those many number of iterations.
 

8383
02:45:26,399 --> 02:45:29,269
run for those many number of iterations.
I have run for 60,000 iterations and the

8384
02:45:29,269 --> 02:45:29,279
I have run for 60,000 iterations and the
 

8385
02:45:29,279 --> 02:45:30,950
I have run for 60,000 iterations and the
stories are much more better than what

8386
02:45:30,950 --> 02:45:30,960
stories are much more better than what
 

8387
02:45:30,960 --> 02:45:33,750
stories are much more better than what
I'm showing you right now. So this is a

8388
02:45:33,750 --> 02:45:33,760
I'm showing you right now. So this is a
 

8389
02:45:33,760 --> 02:45:35,429
I'm showing you right now. So this is a
fully functional small language model

8390
02:45:35,429 --> 02:45:35,439
fully functional small language model
 

8391
02:45:35,439 --> 02:45:37,230
fully functional small language model
which we have built from

8392
02:45:37,230 --> 02:45:37,240
which we have built from
 

8393
02:45:37,240 --> 02:45:40,389
which we have built from
scratch and we went through these seven

8394
02:45:40,389 --> 02:45:40,399
scratch and we went through these seven
 

8395
02:45:40,399 --> 02:45:42,469
scratch and we went through these seven
steps sequentially together. We started

8396
02:45:42,469 --> 02:45:42,479
steps sequentially together. We started
 

8397
02:45:42,479 --> 02:45:44,830
steps sequentially together. We started
with the data set. Then we did the

8398
02:45:44,830 --> 02:45:44,840
with the data set. Then we did the
 

8399
02:45:44,840 --> 02:45:47,190
with the data set. Then we did the
tokenization. We created input output

8400
02:45:47,190 --> 02:45:47,200
tokenization. We created input output
 

8401
02:45:47,200 --> 02:45:49,469
tokenization. We created input output
pairs. We assembled the entire model

8402
02:45:49,469 --> 02:45:49,479
pairs. We assembled the entire model
 

8403
02:45:49,479 --> 02:45:51,590
pairs. We assembled the entire model
architecture. Um, and the model

8404
02:45:51,590 --> 02:45:51,600
architecture. Um, and the model
 

8405
02:45:51,600 --> 02:45:53,910
architecture. Um, and the model
architecture looks essentially something

8406
02:45:53,910 --> 02:45:53,920
architecture looks essentially something
 

8407
02:45:53,920 --> 02:45:55,910
architecture looks essentially something
like this what I've shown over here. And

8408
02:45:55,910 --> 02:45:55,920
like this what I've shown over here. And
 

8409
02:45:55,920 --> 02:45:57,670
like this what I've shown over here. And
once we assemble the entire model

8410
02:45:57,670 --> 02:45:57,680
once we assemble the entire model
 

8411
02:45:57,680 --> 02:45:59,190
once we assemble the entire model
architecture, we set up the training

8412
02:45:59,190 --> 02:45:59,200
architecture, we set up the training
 

8413
02:45:59,200 --> 02:46:01,910
architecture, we set up the training
pipeline. Uh, we pre-train the LLM. We

8414
02:46:01,910 --> 02:46:01,920
pipeline. Uh, we pre-train the LLM. We
 

8415
02:46:01,920 --> 02:46:03,429
pipeline. Uh, we pre-train the LLM. We
set up things such as gradient

8416
02:46:03,429 --> 02:46:03,439
set up things such as gradient
 

8417
02:46:03,439 --> 02:46:06,070
set up things such as gradient
accumulation steps and adaptive learning

8418
02:46:06,070 --> 02:46:06,080
accumulation steps and adaptive learning
 

8419
02:46:06,080 --> 02:46:08,230
accumulation steps and adaptive learning
rate. Then earlier in the code we have

8420
02:46:08,230 --> 02:46:08,240
rate. Then earlier in the code we have
 

8421
02:46:08,240 --> 02:46:09,830
rate. Then earlier in the code we have
done some things to improve the

8422
02:46:09,830 --> 02:46:09,840
done some things to improve the
 

8423
02:46:09,840 --> 02:46:12,630
done some things to improve the
performance such as saving the data or

8424
02:46:12,630 --> 02:46:12,640
performance such as saving the data or
 

8425
02:46:12,640 --> 02:46:15,269
performance such as saving the data or
saving the token ids to a bin file for

8426
02:46:15,269 --> 02:46:15,279
saving the token ids to a bin file for
 

8427
02:46:15,279 --> 02:46:17,429
saving the token ids to a bin file for
so that and we store it in a disk for

8428
02:46:17,429 --> 02:46:17,439
so that and we store it in a disk for
 

8429
02:46:17,439 --> 02:46:21,790
so that and we store it in a disk for
easy retrieval. Then we have

8430
02:46:21,790 --> 02:46:21,800
easy retrieval. Then we have
 

8431
02:46:21,800 --> 02:46:24,070
easy retrieval. Then we have
essentially yeah done something like

8432
02:46:24,070 --> 02:46:24,080
essentially yeah done something like
 

8433
02:46:24,080 --> 02:46:27,110
essentially yeah done something like
x.pin memory y.pin memory etc. So that

8434
02:46:27,110 --> 02:46:27,120
x.pin memory y.pin memory etc. So that
 

8435
02:46:27,120 --> 02:46:29,269
x.pin memory y.pin memory etc. So that
we lock the memory of a tensor in RAM

8436
02:46:29,269 --> 02:46:29,279
we lock the memory of a tensor in RAM
 

8437
02:46:29,279 --> 02:46:32,230
we lock the memory of a tensor in RAM
for a faster transfer of tensor to GPU.

8438
02:46:32,230 --> 02:46:32,240
for a faster transfer of tensor to GPU.
 

8439
02:46:32,240 --> 02:46:34,870
for a faster transfer of tensor to GPU.
After this we have also done some things

8440
02:46:34,870 --> 02:46:34,880
After this we have also done some things
 

8441
02:46:34,880 --> 02:46:36,269
After this we have also done some things
such as

8442
02:46:36,269 --> 02:46:36,279
such as
 

8443
02:46:36,279 --> 02:46:38,389
such as
torch.cast which is automatic mixed

8444
02:46:38,389 --> 02:46:38,399
torch.cast which is automatic mixed
 

8445
02:46:38,399 --> 02:46:41,110
torch.cast which is automatic mixed
precision so that we can automatically

8446
02:46:41,110 --> 02:46:41,120
precision so that we can automatically
 

8447
02:46:41,120 --> 02:46:43,830
precision so that we can automatically
use float 16 wherever it's safe so that

8448
02:46:43,830 --> 02:46:43,840
use float 16 wherever it's safe so that
 

8449
02:46:43,840 --> 02:46:46,550
use float 16 wherever it's safe so that
the computational speed is increased and

8450
02:46:46,550 --> 02:46:46,560
the computational speed is increased and
 

8451
02:46:46,560 --> 02:46:48,790
the computational speed is increased and
we are much more efficient. So all of

8452
02:46:48,790 --> 02:46:48,800
we are much more efficient. So all of
 

8453
02:46:48,800 --> 02:46:51,269
we are much more efficient. So all of
these things we have done to improve the

8454
02:46:51,269 --> 02:46:51,279
these things we have done to improve the
 

8455
02:46:51,279 --> 02:46:53,030
these things we have done to improve the
production quality of this application.

8456
02:46:53,030 --> 02:46:53,040
production quality of this application.
 

8457
02:46:53,040 --> 02:46:55,269
production quality of this application.
This is not just a toy project. You can

8458
02:46:55,269 --> 02:46:55,279
This is not just a toy project. You can
 

8459
02:46:55,279 --> 02:46:57,750
This is not just a toy project. You can
take this code and you can augment it to

8460
02:46:57,750 --> 02:46:57,760
take this code and you can augment it to
 

8461
02:46:57,760 --> 02:46:59,990
take this code and you can augment it to
make it more industry ready. And the

8462
02:46:59,990 --> 02:47:00,000
make it more industry ready. And the
 

8463
02:47:00,000 --> 02:47:01,429
make it more industry ready. And the
last thing which we did in today's

8464
02:47:01,429 --> 02:47:01,439
last thing which we did in today's
 

8465
02:47:01,439 --> 02:47:03,349
last thing which we did in today's
tutorial is to run inference and show

8466
02:47:03,349 --> 02:47:03,359
tutorial is to run inference and show
 

8467
02:47:03,359 --> 02:47:05,830
tutorial is to run inference and show
you the quality of the stories which can

8468
02:47:05,830 --> 02:47:05,840
you the quality of the stories which can
 

8469
02:47:05,840 --> 02:47:08,070
you the quality of the stories which can
be generated. Again at the end of this

8470
02:47:08,070 --> 02:47:08,080
be generated. Again at the end of this
 

8471
02:47:08,080 --> 02:47:10,070
be generated. Again at the end of this
video I want to especially thank Andre

8472
02:47:10,070 --> 02:47:10,080
video I want to especially thank Andre
 

8473
02:47:10,080 --> 02:47:12,309
video I want to especially thank Andre
Karpati and the nano GPD repository

8474
02:47:12,309 --> 02:47:12,319
Karpati and the nano GPD repository
 

8475
02:47:12,319 --> 02:47:14,790
Karpati and the nano GPD repository
which he had which inspired a lot of

8476
02:47:14,790 --> 02:47:14,800
which he had which inspired a lot of
 

8477
02:47:14,800 --> 02:47:16,950
which he had which inspired a lot of
sections over here. many of the

8478
02:47:16,950 --> 02:47:16,960
sections over here. many of the
 

8479
02:47:16,960 --> 02:47:19,230
sections over here. many of the
architecture sections, many of

8480
02:47:19,230 --> 02:47:19,240
architecture sections, many of
 

8481
02:47:19,240 --> 02:47:22,750
architecture sections, many of
the these uh efficiency based

8482
02:47:22,750 --> 02:47:22,760
the these uh efficiency based
 

8483
02:47:22,760 --> 02:47:25,269
the these uh efficiency based
calculations Andre talks about a lot in

8484
02:47:25,269 --> 02:47:25,279
calculations Andre talks about a lot in
 

8485
02:47:25,279 --> 02:47:27,349
calculations Andre talks about a lot in
his video also and even these

8486
02:47:27,349 --> 02:47:27,359
his video also and even these
 

8487
02:47:27,359 --> 02:47:29,030
his video also and even these
calculations later which we have done

8488
02:47:29,030 --> 02:47:29,040
calculations later which we have done
 

8489
02:47:29,040 --> 02:47:30,710
calculations later which we have done
with respect to gradient accumulation

8490
02:47:30,710 --> 02:47:30,720
with respect to gradient accumulation
 

8491
02:47:30,720 --> 02:47:33,510
with respect to gradient accumulation
steps. Um I've been inspired a lot from

8492
02:47:33,510 --> 02:47:33,520
steps. Um I've been inspired a lot from
 

8493
02:47:33,520 --> 02:47:35,750
steps. Um I've been inspired a lot from
him uh as a whole. Thanks a lot

8494
02:47:35,750 --> 02:47:35,760
him uh as a whole. Thanks a lot
 

8495
02:47:35,760 --> 02:47:37,590
him uh as a whole. Thanks a lot
everyone. I hope you took a lot out of

8496
02:47:37,590 --> 02:47:37,600
everyone. I hope you took a lot out of
 

8497
02:47:37,600 --> 02:47:39,510
everyone. I hope you took a lot out of
this tutorial. I'll be posting many such

8498
02:47:39,510 --> 02:47:39,520
this tutorial. I'll be posting many such
 

8499
02:47:39,520 --> 02:47:41,510
this tutorial. I'll be posting many such
deep dive workshops which is 2 to three

8500
02:47:41,510 --> 02:47:41,520
deep dive workshops which is 2 to three
 

8501
02:47:41,520 --> 02:47:43,429
deep dive workshops which is 2 to three
hour workshops which are not toy

8502
02:47:43,429 --> 02:47:43,439
hour workshops which are not toy
 

8503
02:47:43,439 --> 02:47:44,950
hour workshops which are not toy
projects but which are real projects

8504
02:47:44,950 --> 02:47:44,960
projects but which are real projects
 

8505
02:47:44,960 --> 02:47:47,590
projects but which are real projects
which you can use and build on top of

8506
02:47:47,590 --> 02:47:47,600
which you can use and build on top of
 

8507
02:47:47,600 --> 02:47:49,269
which you can use and build on top of
this to convert these projects into

8508
02:47:49,269 --> 02:47:49,279
this to convert these projects into
 

8509
02:47:49,279 --> 02:47:51,590
this to convert these projects into
research publications. You can add those

8510
02:47:51,590 --> 02:47:51,600
research publications. You can add those
 

8511
02:47:51,600 --> 02:47:53,630
research publications. You can add those
to your profile and that will make you a

8512
02:47:53,630 --> 02:47:53,640
to your profile and that will make you a
 

8513
02:47:53,640 --> 02:47:57,030
to your profile and that will make you a
truly strong ML and large language model

8514
02:47:57,030 --> 02:47:57,040
truly strong ML and large language model
 

8515
02:47:57,040 --> 02:47:58,950
truly strong ML and large language model
engineer. Thanks so much and I look

8516
02:47:58,950 --> 02:47:58,960
engineer. Thanks so much and I look
 

8517
02:47:58,960 --> 02:48:00,389
engineer. Thanks so much and I look
forward to seeing you in one of these

8518
02:48:00,389 --> 02:48:00,399
forward to seeing you in one of these
 

8519
02:48:00,399 --> 02:48:03,279
forward to seeing you in one of these
next workshops.

