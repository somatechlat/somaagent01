#!/usr/bin/env python3
"""
Test script for Agent Zero Memory Consolidation System

This script tests the intelligent memory consolidation functionality
and provides validation of the self-healing memory management capabilities.

Run this script to test:
1. Basic memory consolidation functionality
2. LLM-powered memory analysis
3. Integration with existing memory system
4. Error handling and edge cases
5. All five consolidation actions

Usage:
    python test_memory_consolidation.py
"""

import asyncio
import sys
import os
from datetime import datetime

# Add the project root to the path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + "/../..")

try:
    from python.helpers.memory_consolidation import (
        MemoryConsolidator,
        ConsolidationConfig,
        ConsolidationAction,
        create_memory_consolidator
    )
    from python.helpers.memory import Memory
    from agent import Agent, AgentConfig, ModelConfig
    from models import ModelProvider
except ImportError as e:
    print(f"‚ùå Import Error: {e}")
    print("Make sure you're running this from the Agent Zero root directory")
    sys.exit(1)


class MemoryConsolidationTester:
    """Comprehensive tester for memory consolidation functionality."""

    def __init__(self):
        self.test_results = []
        self.agent: Agent | None = None

    def _extract_success(self, result):
        """Helper to extract success boolean from consolidation result for backward compatibility."""
        if isinstance(result, dict):
            return result.get("success", False)
        elif isinstance(result, list):
            # Handler methods return lists of memory IDs, empty list = failure
            return len(result) > 0
        return bool(result)

    def _extract_memory_ids(self, result):
        """Helper to extract memory IDs from consolidation result."""
        if isinstance(result, dict):
            return result.get("memory_ids", [])
        elif isinstance(result, list):
            # Handler methods return lists directly
            return result
        return []

    async def setup_test_environment(self):
        """Set up a test agent and memory environment."""
        print("üîß Setting up test environment...")

        try:
            # Create test agent configuration
            chat_model = ModelConfig(
                provider=ModelProvider.OPENAI,
                name="gpt-4o-mini",
                ctx_length=8192
            )

            utility_model = ModelConfig(
                provider=ModelProvider.OPENAI,
                name="gpt-4o-mini",
                ctx_length=4096
            )

            embeddings_model = ModelConfig(
                provider=ModelProvider.OPENAI,
                name="text-embedding-3-small"
            )

            browser_model = ModelConfig(
                provider=ModelProvider.OPENAI,
                name="gpt-4o-mini"
            )

            config = AgentConfig(
                chat_model=chat_model,
                utility_model=utility_model,
                embeddings_model=embeddings_model,
                browser_model=browser_model,
                mcp_servers="",
                memory_subdir="test_consolidation"
            )

            # Import agent context
            from agent import AgentContext
            context = AgentContext(config, id="test-consolidation")
            self.agent = context.agent0

            # Set loop_data on agent immediately (required by agent methods)
            from agent import LoopData
            setattr(self.agent, 'loop_data', LoopData(iteration=0))

            print("‚úÖ Test environment setup complete")
            return True

        except Exception as e:
            print(f"‚ùå Failed to setup test environment: {e}")
            return False

    async def test_basic_consolidation_config(self):
        """Test basic consolidation configuration and instantiation."""
        print("\nüìã Testing basic consolidation configuration...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            # Test default configuration
            default_config = ConsolidationConfig()
            assert default_config.similarity_threshold == 0.7
            assert default_config.max_similar_memories == 10
            assert default_config.processing_timeout_seconds == 30

            # Test custom configuration
            custom_config = ConsolidationConfig(
                similarity_threshold=0.8,
                max_similar_memories=5,
                processing_timeout_seconds=60
            )
            assert custom_config.similarity_threshold == 0.8
            assert custom_config.max_similar_memories == 5

            # Test consolidator creation
            consolidator = MemoryConsolidator(self.agent, custom_config)
            assert consolidator.config.similarity_threshold == 0.8

            # Test factory function
            factory_consolidator = create_memory_consolidator(
                self.agent,
                similarity_threshold=0.9,
                max_similar_memories=15
            )
            assert factory_consolidator.config.similarity_threshold == 0.9
            assert factory_consolidator.config.max_similar_memories == 15

            self.test_results.append("‚úÖ Basic consolidation configuration")
            print("‚úÖ Basic consolidation configuration tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Basic consolidation configuration: {e}")
            print(f"‚ùå Basic consolidation configuration tests failed: {e}")
            return False

    async def test_memory_discovery(self):
        """Test memory discovery functionality."""
        print("\nüîç Testing memory discovery...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(
                self.agent,
                similarity_threshold=0.6,
                max_similar_memories=5
            )

            # Insert some test memories first
            db = await Memory.get(self.agent)

            test_memories = [
                "Python async/await is used for asynchronous programming",
                "FastAPI framework supports async request handling",
                "JavaScript promises handle asynchronous operations",
                "Alpine.js uses x-data for reactive components"
            ]

            for memory in test_memories:
                await db.insert_text(
                    memory,
                    {"area": Memory.Area.MAIN.value, "test": True}
                )

            # Test similar memory discovery
            new_memory = "Python asyncio provides tools for async programming"
            similar_memories = await consolidator._find_similar_memories(
                new_memory,
                Memory.Area.MAIN.value
            )

            assert len(similar_memories) > 0, "Should find similar memories"

            # Check that we found the related Python async memory
            python_memory_found = any(
                "async" in doc.page_content.lower() and "python" in doc.page_content.lower()
                for doc in similar_memories
            )
            assert python_memory_found, "Should find related Python async memory"

            self.test_results.append("‚úÖ Memory discovery functionality")
            print("‚úÖ Memory discovery tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Memory discovery: {e}")
            print(f"‚ùå Memory discovery tests failed: {e}")
            return False

    async def test_keyword_extraction(self):
        """Test keyword extraction from memory content."""
        print("\nüî§ Testing keyword extraction...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)

            test_memory = """
            Successfully implemented OAuth authentication using JWT tokens for the user login system.
            The solution handles token refresh and validation properly with middleware.
            """

            keywords = await consolidator._extract_search_keywords(test_memory)

            assert isinstance(keywords, list), "Should return a list"
            assert len(keywords) > 0, "Should extract some keywords"

            # Check for expected keywords (case-insensitive)
            keywords_text = " ".join(keywords).lower()
            expected_terms = ["oauth", "jwt", "authentication", "token"]

            found_terms = [term for term in expected_terms if term in keywords_text]
            assert len(found_terms) >= 2, f"Should find at least 2 expected terms, found: {found_terms}"

            self.test_results.append("‚úÖ Keyword extraction functionality")
            print(f"‚úÖ Keyword extraction tests passed. Extracted: {keywords}")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Keyword extraction: {e}")
            print(f"‚ùå Keyword extraction tests failed: {e}")
            return False

    async def test_keyword_extraction_edge_cases(self):
        """Test keyword extraction edge cases and fallbacks."""
        print("\nüî§ Testing keyword extraction edge cases...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)

            # Test empty string
            empty_keywords = await consolidator._extract_search_keywords("")
            assert isinstance(empty_keywords, list), "Should return a list for empty input"

            # Test very long content
            long_memory = (
                "This is a comprehensive technical documentation about implementing OAuth 2.0 authentication "
                "with JWT tokens in a FastAPI application using PostgreSQL database and Redis for caching. "
                "The system handles user registration, login, password reset, and session management. "
                "It includes middleware for request validation, rate limiting, and CORS handling. "
                "The architecture follows RESTful API design principles with proper error handling, "
                "logging, and monitoring capabilities. Security features include input sanitization, "
                "SQL injection prevention, XSS protection, and CSRF token validation. "
                "Performance optimizations include database indexing, query caching, and connection pooling. "
                "The deployment strategy uses Docker containers with Kubernetes orchestration for scalability. "
                "This is a very long memory that should be handled properly by the keyword extraction system."
            )
            long_keywords = await consolidator._extract_search_keywords(long_memory)
            assert isinstance(long_keywords, list), "Should handle long content"
            assert len(long_keywords) > 0, "Should extract keywords from long content"

            # Test content without periods
            no_periods = "This content has no sentence endings and should still work properly"
            no_period_keywords = await consolidator._extract_search_keywords(no_periods)
            assert isinstance(no_period_keywords, list), "Should handle content without periods"

            self.test_results.append("‚úÖ Keyword extraction edge cases")
            print("‚úÖ Keyword extraction edge case tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Keyword extraction edge cases: {e}")
            print(f"‚ùå Keyword extraction edge cases tests failed: {e}")
            return False

    async def test_consolidation_analysis(self):
        """Test LLM-powered consolidation analysis."""
        print("\nüß† Testing consolidation analysis...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)

            # Create test context with similar memories
            from python.helpers.memory_consolidation import MemoryAnalysisContext
            from langchain_core.documents import Document

            new_memory = "Updated API endpoint is now /api/v2/users instead of /api/users"

            similar_memories = [
                Document(
                    page_content="User API endpoint is /api/users for getting user data",
                    metadata={
                        "id": "mem_001",
                        "timestamp": "2024-01-01 10:00:00",
                        "area": "main"
                    }
                )
            ]

            context = MemoryAnalysisContext(
                new_memory=new_memory,
                similar_memories=similar_memories,
                area=Memory.Area.MAIN.value,
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                existing_metadata={"area": Memory.Area.MAIN.value}
            )

            result = await consolidator._analyze_memory_consolidation(context)

            assert hasattr(result, 'action'), "Should have action field"
            assert isinstance(result.action, ConsolidationAction), "Action should be ConsolidationAction enum"
            assert hasattr(result, 'reasoning'), "Should have reasoning field"

            print(f"‚úÖ Consolidation analysis completed. Action: {result.action.value}")
            print(f"   Reasoning: {result.reasoning}")

            self.test_results.append("‚úÖ Consolidation analysis functionality")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Consolidation analysis: {e}")
            print(f"‚ùå Consolidation analysis tests failed: {e}")
            return False

    async def test_consolidation_actions(self):
        """Test all five consolidation actions."""
        print("\n‚ö° Testing consolidation actions...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            from python.helpers.memory_consolidation import ConsolidationResult

            consolidator = create_memory_consolidator(self.agent)
            db = await Memory.get(self.agent)

            # Test KEEP_SEPARATE action
            keep_result = ConsolidationResult(
                action=ConsolidationAction.KEEP_SEPARATE,
                new_memory_content="Test memory for keep separate",
                metadata={"test_action": "keep_separate"}
            )

            success = await consolidator._handle_keep_separate(db, keep_result, Memory.Area.MAIN.value, {})
            assert self._extract_success(success), "KEEP_SEPARATE should succeed"

            # Test MERGE action
            merge_result = ConsolidationResult(
                action=ConsolidationAction.MERGE,
                memories_to_remove=["test_id_1", "test_id_2"],
                new_memory_content="Merged memory content",
                metadata={"test_action": "merge"}
            )

            success = await consolidator._handle_merge(db, merge_result, Memory.Area.MAIN.value, {})
            assert self._extract_success(success), "MERGE should succeed"

            # Test REPLACE action
            replace_result = ConsolidationResult(
                action=ConsolidationAction.REPLACE,
                memories_to_remove=["test_id_3"],
                new_memory_content="Replacement memory content",
                metadata={"test_action": "replace"}
            )

            success = await consolidator._handle_replace(db, replace_result, Memory.Area.MAIN.value, {})
            assert self._extract_success(success), "REPLACE should succeed"

            # Test UPDATE action (empty updates should still succeed)
            update_result = ConsolidationResult(
                action=ConsolidationAction.UPDATE,
                memories_to_update=[],
                new_memory_content="Additional memory for update",
                metadata={"test_action": "update"}
            )

            success = await consolidator._handle_update(db, update_result, Memory.Area.MAIN.value, {})
            assert self._extract_success(success), "UPDATE should succeed"

            self.test_results.append("‚úÖ Consolidation actions")
            print("‚úÖ Consolidation actions tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Consolidation actions: {e}")
            print(f"‚ùå Consolidation actions tests failed: {e}")
            return False

    async def test_full_consolidation_pipeline(self):
        """Test the complete consolidation pipeline."""
        print("\nüîÑ Testing full consolidation pipeline...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(
                self.agent,
                similarity_threshold=0.6,
                max_similar_memories=3,
                processing_timeout_seconds=60
            )

            # Insert a test memory
            test_memory = "FastAPI provides excellent async support for web APIs"
            metadata = {"area": Memory.Area.MAIN.value, "test_pipeline": True}

            result = await consolidator.process_new_memory(
                new_memory=test_memory,
                area=Memory.Area.MAIN.value,
                metadata=metadata
            )

            assert self._extract_success(result), "Consolidation pipeline should complete successfully"

            # Validate memory IDs are returned
            memory_ids = self._extract_memory_ids(result)
            assert len(memory_ids) > 0, "Should return at least one memory ID"

            # Validate memory IDs are valid strings
            for memory_id in memory_ids:
                assert isinstance(memory_id, str), "Memory ID should be a string"
                assert len(memory_id) > 0, "Memory ID should not be empty"

            # Verify the memory was actually inserted and is retrievable
            db = await Memory.get(self.agent)
            inserted_memories = await db.aget_by_ids(memory_ids)
            assert len(inserted_memories) == len(memory_ids), "All returned memory IDs should exist in database"

            # Verify content is findable by search
            recent_memories = await db.search_similarity_threshold(
                query=test_memory,
                limit=5,
                threshold=0.5,
                filter=f"area == '{Memory.Area.MAIN.value}'"
            )

            assert len(recent_memories) > 0, "Should find the processed memory via search"

            self.test_results.append("‚úÖ Full consolidation pipeline")
            print("‚úÖ Full consolidation pipeline tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Full consolidation pipeline: {e}")
            print(f"‚ùå Full consolidation pipeline tests failed: {e}")
            return False

    async def test_timeout_handling(self):
        """Test timeout handling in consolidation."""
        print("\n‚è±Ô∏è Testing timeout handling...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            # Test with very short timeout
            consolidator = create_memory_consolidator(
                self.agent,
                processing_timeout_seconds=0.001  # Very short timeout
            )

            test_memory = "This should timeout quickly"
            metadata = {"area": Memory.Area.MAIN.value, "test_timeout": True}

            # This should timeout and return {"success": False, "memory_ids": []}
            result = await consolidator.process_new_memory(
                new_memory=test_memory,
                area=Memory.Area.MAIN.value,
                metadata=metadata
            )

            # With such a short timeout, it should fail
            assert result["success"] is False, "Should timeout with very short timeout"
            assert result["memory_ids"] == [], "Should return empty memory_ids on timeout"

            self.test_results.append("‚úÖ Timeout handling")
            print("‚úÖ Timeout handling tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Timeout handling: {e}")
            print(f"‚ùå Timeout handling tests failed: {e}")
            return False

    async def test_division_by_zero_fix(self):
        """Test that the division by zero fix works properly."""
        print("\nüî¢ Testing division by zero fix...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)

            # Mock the keyword extraction to return empty list
            original_method = consolidator._extract_search_keywords

            async def mock_empty_keywords(new_memory, log_item=None):
                return []  # Return empty list to trigger the division by zero scenario

            consolidator._extract_search_keywords = mock_empty_keywords

            # This should not crash due to division by zero
            similar_memories = await consolidator._find_similar_memories(
                "Test memory",
                Memory.Area.MAIN.value
            )

            # Restore original method
            consolidator._extract_search_keywords = original_method

            assert isinstance(similar_memories, list), "Should return a list even with empty keywords"

            self.test_results.append("‚úÖ Division by zero fix")
            print("‚úÖ Division by zero fix tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Division by zero fix: {e}")
            print(f"‚ùå Division by zero fix tests failed: {e}")
            return False

    async def test_extension_integration(self):
        """Test actual integration with existing memory extensions using real conversation data."""
        print("\nüîå Testing extension integration...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            # Import the extensions
            from python.extensions.monologue_end._50_memorize_fragments import MemorizeMemories
            from python.extensions.monologue_end._51_memorize_solutions import MemorizeSolutions

            # Create extension instances
            fragments_ext = MemorizeMemories(agent=self.agent)
            solutions_ext = MemorizeSolutions(agent=self.agent)

            # Verify extensions are properly instantiated
            assert fragments_ext.agent == self.agent
            assert solutions_ext.agent == self.agent
            assert hasattr(fragments_ext, 'memorize'), "Fragments extension should have memorize method"
            assert hasattr(solutions_ext, 'memorize'), "Solutions extension should have memorize method"

            # Clear any existing memories to ensure clean test
            db = await Memory.get(self.agent)

            # Create realistic conversation history for testing
            from agent import UserMessage

            # Ensure loop_data is available before using hist methods (required by hist_add_ai_response)
            self.agent.loop_data.iteration = 1

            # Add user message
            user_msg = UserMessage("I need help installing FastAPI and creating a simple API endpoint")
            self.agent.hist_add_user_message(user_msg)

            # Add AI response with solution
            ai_response = """I'll help you install FastAPI and create a simple API endpoint.

First, install FastAPI:
```bash
pip install fastapi uvicorn
```

Then create a simple API:
```python
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    return {"Hello": "World"}

@app.get("/users/{user_id}")
def read_user(user_id: int):
    return {"user_id": user_id}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

Run the server with:
```bash
uvicorn main:app --reload
```

This creates a basic FastAPI app with two endpoints. The server will be available at http://localhost:8000"""

            self.agent.hist_add_ai_response(ai_response)

            # Add user response
            user_response = UserMessage("Thank you! The API is working perfectly. My name is TestUser and I'm working on a project called TestProject.")
            self.agent.hist_add_user_message(user_response)

            # Test that we can execute the extensions without errors
            try:
                fragments_task = await fragments_ext.execute(loop_data=self.agent.loop_data)
                if fragments_task:
                    # Wait a bit for the background task
                    await asyncio.sleep(2)
                extension_execute_success = True
            except Exception as e:
                print(f"   Extension execute failed: {e}")
                extension_execute_success = False

            # Test basic functionality by directly calling the consolidation system
            from python.helpers.memory_consolidation import create_memory_consolidator
            consolidator = create_memory_consolidator(self.agent)

            # Test inserting a simple memory
            simple_success = await consolidator.process_new_memory(
                new_memory="Test memory for extension integration",
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_extension": True}
            )

            assert self._extract_success(simple_success), "Basic consolidation should work"

            # Test that consolidation system was accessible
            recent_memories = await db.search_similarity_threshold(
                query="Test memory extension",
                limit=5,
                threshold=0.3,
                filter="test_extension == True"
            )

            consolidation_works = len(recent_memories) > 0

            self.test_results.append("‚úÖ Extension integration with real data")
            print("‚úÖ Extension integration tests passed")
            print("   - Extensions instantiated: ‚úÖ")
            print(f"   - Extension execute method: {'‚úÖ' if extension_execute_success else '‚ùå'}")
            print(f"   - Consolidation system accessible: {'‚úÖ' if consolidation_works else '‚ùå'}")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Extension integration: {e}")
            print(f"‚ùå Extension integration tests failed: {e}")
            return False

    async def test_llm_response_edge_cases(self):
        """Test edge cases in LLM responses."""
        print("\nü§ñ Testing LLM response edge cases...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)

            # Test consolidation analysis with empty similar memories
            from python.helpers.memory_consolidation import MemoryAnalysisContext
            from langchain_core.documents import Document

            # Test with no similar memories
            empty_context = MemoryAnalysisContext(
                new_memory="Test memory with no similar memories",
                similar_memories=[],
                area=Memory.Area.MAIN.value,
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                existing_metadata={"area": Memory.Area.MAIN.value}
            )

            result = await consolidator._analyze_memory_consolidation(empty_context)
            assert isinstance(result.action, ConsolidationAction), "Should handle empty similar memories"

            # Test with very many similar memories
            many_similar = [
                Document(
                    page_content=f"Test memory content {i}",
                    metadata={"id": f"mem_{i:03d}", "timestamp": "2024-01-01 10:00:00", "area": "main"}
                )
                for i in range(20)  # More than max_llm_context_memories
            ]

            many_context = MemoryAnalysisContext(
                new_memory="Test memory with many similar memories",
                similar_memories=many_similar,
                area=Memory.Area.MAIN.value,
                timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                existing_metadata={"area": Memory.Area.MAIN.value}
            )

            result = await consolidator._analyze_memory_consolidation(many_context)
            assert isinstance(result.action, ConsolidationAction), "Should handle many similar memories"

            self.test_results.append("‚úÖ LLM response edge cases")
            print("‚úÖ LLM response edge cases tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå LLM response edge cases: {e}")
            print(f"‚ùå LLM response edge cases tests failed: {e}")
            return False

    async def test_memory_content_edge_cases(self):
        """Test edge cases with memory content."""
        print("\nüìù Testing memory content edge cases...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)

            # Test empty memory content
            empty_success = await consolidator.process_new_memory(
                new_memory="",
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_empty": True}
            )
            assert isinstance(empty_success, (bool, dict)), "Should handle empty memory content"

            # Test memory with only whitespace
            whitespace_success = await consolidator.process_new_memory(
                new_memory="   \n\t   \n   ",
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_whitespace": True}
            )
            assert isinstance(whitespace_success, (bool, dict)), "Should handle whitespace-only content"

            # Test memory with special characters and Unicode
            unicode_memory = "Test with unicode: üöÄ fran√ßais ‰∏≠Êñá ÿßŸÑÿπÿ±ÿ®Ÿäÿ© √±√°√©√≠√≥√∫"
            unicode_success = await consolidator.process_new_memory(
                new_memory=unicode_memory,
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_unicode": True}
            )
            assert isinstance(unicode_success, (bool, dict)), "Should handle Unicode content"

            # Test memory with JSON-like content (potential parsing issues)
            json_like_memory = '{"fake": "json", "numbers": [1,2,3], "nested": {"key": "value"}}'
            json_success = await consolidator.process_new_memory(
                new_memory=json_like_memory,
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_json": True}
            )
            assert isinstance(json_success, (bool, dict)), "Should handle JSON-like content"

            # Test extremely long memory (realistic length)
            very_long_memory = "This is a very detailed technical specification. " * 200  # ~10,000 chars
            long_success = await consolidator.process_new_memory(
                new_memory=very_long_memory,
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_very_long": True}
            )
            assert isinstance(long_success, (bool, dict)), "Should handle very long content"

            self.test_results.append("‚úÖ Memory content edge cases")
            print("‚úÖ Memory content edge cases tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Memory content edge cases: {e}")
            print(f"‚ùå Memory content edge cases tests failed: {e}")
            return False

    async def test_configuration_edge_cases(self):
        """Test edge cases in configuration values."""
        print("\n‚öôÔ∏è Testing configuration edge cases...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            # Test extreme configuration values
            extreme_configs = [
                {"similarity_threshold": 0.0},  # Minimum threshold
                {"similarity_threshold": 1.0},  # Maximum threshold
                {"max_similar_memories": 1},    # Minimum memories
                {"max_similar_memories": 100},  # Large number of memories
                {"max_llm_context_memories": 1},  # Minimum context
                {"processing_timeout_seconds": 1},  # Very short timeout
            ]

            for config_override in extreme_configs:
                consolidator = create_memory_consolidator(self.agent, **config_override)
                assert consolidator.config is not None, f"Should handle config: {config_override}"

                # Try to process a simple memory with extreme config
                success = await consolidator.process_new_memory(
                    new_memory="Test memory with extreme config",
                    area=Memory.Area.MAIN.value,
                    metadata={"area": Memory.Area.MAIN.value, "test_config": True}
                )
                # Success or failure is okay, just shouldn't crash
                assert isinstance(success, (bool, dict)), f"Should return bool with config: {config_override}"

            self.test_results.append("‚úÖ Configuration edge cases")
            print("‚úÖ Configuration edge cases tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Configuration edge cases: {e}")
            print(f"‚ùå Configuration edge cases tests failed: {e}")
            return False

    async def test_database_edge_cases(self):
        """Test edge cases with database operations."""
        print("\nüóÉÔ∏è Testing database edge cases...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)
            db = await Memory.get(self.agent)

            # Test operations with non-existent memory IDs
            from python.helpers.memory_consolidation import ConsolidationResult

            # Test MERGE with non-existent IDs
            merge_result = ConsolidationResult(
                action=ConsolidationAction.MERGE,
                memories_to_remove=["non_existent_id_1", "non_existent_id_2"],
                new_memory_content="Merged content",
                metadata={"test_db": "merge_nonexistent"}
            )

            merge_success = await consolidator._handle_merge(db, merge_result, Memory.Area.MAIN.value, {})
            assert isinstance(merge_success, (bool, dict)), "Should handle non-existent IDs in MERGE"

            # Test REPLACE with non-existent IDs
            replace_result = ConsolidationResult(
                action=ConsolidationAction.REPLACE,
                memories_to_remove=["non_existent_id_3"],
                new_memory_content="Replacement content",
                metadata={"test_db": "replace_nonexistent"}
            )

            replace_success = await consolidator._handle_replace(db, replace_result, Memory.Area.MAIN.value, {})
            assert isinstance(replace_success, (bool, dict)), "Should handle non-existent IDs in REPLACE"

            # Test UPDATE with mix of valid and invalid IDs
            update_result = ConsolidationResult(
                action=ConsolidationAction.UPDATE,
                memories_to_update=[
                    {"id": "non_existent_id_4", "new_content": "Updated content 1"},
                    {"id": "non_existent_id_5", "new_content": "Updated content 2"}
                ],
                new_memory_content="Additional content",
                metadata={"test_db": "update_nonexistent"}
            )

            update_success = await consolidator._handle_update(db, update_result, Memory.Area.MAIN.value, {})
            assert isinstance(update_success, (bool, dict)), "Should handle non-existent IDs in UPDATE"

            # Test with malformed memory IDs
            malformed_result = ConsolidationResult(
                action=ConsolidationAction.MERGE,
                memories_to_remove=["", "malformed@id", "id with spaces"],
                new_memory_content="Content with malformed IDs",
                metadata={"test_db": "malformed_ids"}
            )

            # This should not crash
            try:
                malformed_success = await consolidator._handle_merge(db, malformed_result, Memory.Area.MAIN.value, {})
                assert isinstance(malformed_success, (bool, dict)), "Should handle malformed IDs gracefully"
            except Exception as e:
                # If it throws an exception, that's also acceptable behavior
                print(f"   Expected exception with malformed IDs: {type(e).__name__}")

            self.test_results.append("‚úÖ Database edge cases")
            print("‚úÖ Database edge cases tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Database edge cases: {e}")
            print(f"‚ùå Database edge cases tests failed: {e}")
            return False

    async def test_action_specific_edge_cases(self):
        """Test edge cases specific to each consolidation action."""
        print("\nüéØ Testing action-specific edge cases...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)
            db = await Memory.get(self.agent)

            from python.helpers.memory_consolidation import ConsolidationResult

            # Test KEEP_SEPARATE with empty content
            keep_empty_result = ConsolidationResult(
                action=ConsolidationAction.KEEP_SEPARATE,
                new_memory_content="",  # Empty content
                metadata={}
            )

            keep_empty_success = await consolidator._handle_keep_separate(db, keep_empty_result, Memory.Area.MAIN.value, {})
            assert not self._extract_success(keep_empty_success), "KEEP_SEPARATE should fail with empty content"

            # Test MERGE with empty content
            merge_empty_result = ConsolidationResult(
                action=ConsolidationAction.MERGE,
                memories_to_remove=["id1", "id2"],
                new_memory_content="",  # Empty content
                metadata={}
            )

            merge_empty_success = await consolidator._handle_merge(db, merge_empty_result, Memory.Area.MAIN.value, {})
            assert not self._extract_success(merge_empty_success), "MERGE should fail with empty content"

            # Test REPLACE with empty content
            replace_empty_result = ConsolidationResult(
                action=ConsolidationAction.REPLACE,
                memories_to_remove=["id3"],
                new_memory_content="",  # Empty content
                metadata={}
            )

            replace_empty_success = await consolidator._handle_replace(db, replace_empty_result, Memory.Area.MAIN.value, {})
            assert not self._extract_success(replace_empty_success), "REPLACE should fail with empty content"

            # Test UPDATE with empty updates and empty new content
            update_empty_result = ConsolidationResult(
                action=ConsolidationAction.UPDATE,
                memories_to_update=[],  # No updates
                new_memory_content="",  # No new content
                metadata={}
            )

            update_empty_success = await consolidator._handle_update(db, update_empty_result, Memory.Area.MAIN.value, {})
            assert not self._extract_success(update_empty_success), "UPDATE should fail with no updates and no content"

            # Test UPDATE with invalid update structure
            update_invalid_result = ConsolidationResult(
                action=ConsolidationAction.UPDATE,
                memories_to_update=[
                    {"invalid": "structure"},  # Missing 'id' and 'new_content'
                    {"id": "valid_id"},        # Missing 'new_content'
                    {"new_content": "content"}  # Missing 'id'
                ],
                new_memory_content="Valid new content",
                metadata={}
            )

            update_invalid_success = await consolidator._handle_update(db, update_invalid_result, Memory.Area.MAIN.value, {})
            # Should succeed because of the new_memory_content, even though updates are invalid
            assert self._extract_success(update_invalid_success), "UPDATE should succeed with valid new_memory_content"

            self.test_results.append("‚úÖ Action-specific edge cases")
            print("‚úÖ Action-specific edge cases tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Action-specific edge cases: {e}")
            print(f"‚ùå Action-specific edge cases tests failed: {e}")
            return False

    async def test_metadata_edge_cases(self):
        """Test edge cases with metadata handling."""
        print("\nüìä Testing metadata edge cases...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)

            # Test with no metadata
            no_metadata_success = await consolidator.process_new_memory(
                new_memory="Memory with no metadata",
                area=Memory.Area.MAIN.value,
                metadata={}  # Empty metadata
            )
            assert isinstance(no_metadata_success, (bool, dict)), "Should handle empty metadata"

            # Test with very large metadata
            large_metadata = {
                f"key_{i}": f"value_{i}" * 100 for i in range(50)  # Large metadata
            }
            large_metadata["area"] = Memory.Area.MAIN.value
            large_metadata["test_large_meta"] = "true"  # Convert boolean to string

            large_meta_success = await consolidator.process_new_memory(
                new_memory="Memory with large metadata",
                area=Memory.Area.MAIN.value,
                metadata=large_metadata
            )
            assert isinstance(large_meta_success, (bool, dict)), "Should handle large metadata"

            # Test with nested metadata structures
            nested_metadata = {
                "area": Memory.Area.MAIN.value,
                "nested": {
                    "level1": {
                        "level2": ["array", "of", "values"],
                        "number": 42,
                        "boolean": True
                    }
                },
                "test_nested_meta": True
            }

            nested_meta_success = await consolidator.process_new_memory(
                new_memory="Memory with nested metadata",
                area=Memory.Area.MAIN.value,
                metadata=nested_metadata
            )
            assert isinstance(nested_meta_success, (bool, dict)), "Should handle nested metadata"

            # Test with metadata containing special characters
            special_metadata = {
                "area": Memory.Area.MAIN.value,
                "unicode_key": "value with üöÄ unicode",
                "special@chars": "value#with$special%chars",
                "test_special_meta": True
            }

            special_meta_success = await consolidator.process_new_memory(
                new_memory="Memory with special character metadata",
                area=Memory.Area.MAIN.value,
                metadata=special_metadata
            )
            assert isinstance(special_meta_success, (bool, dict)), "Should handle special character metadata"

            self.test_results.append("‚úÖ Metadata edge cases")
            print("‚úÖ Metadata edge cases tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Metadata edge cases: {e}")
            print(f"‚ùå Metadata edge cases tests failed: {e}")
            return False

    async def test_concurrent_operations(self):
        """Test concurrent consolidation operations."""
        print("\nüîÑ Testing concurrent operations...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            # Create multiple consolidators
            consolidators = [
                create_memory_consolidator(self.agent) for _ in range(3)
            ]

            # Run multiple consolidation operations concurrently
            concurrent_tasks = []
            for i, consolidator in enumerate(consolidators):
                task = consolidator.process_new_memory(
                    new_memory=f"Concurrent memory operation {i}",
                    area=Memory.Area.MAIN.value,
                    metadata={"area": Memory.Area.MAIN.value, "concurrent_test": True, "operation_id": i}
                )
                concurrent_tasks.append(task)

            # Wait for all operations to complete
            results = await asyncio.gather(*concurrent_tasks, return_exceptions=True)

            # Check that all operations completed (success or failure is both okay)
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"   Concurrent operation {i} raised exception: {type(result).__name__}")
                else:
                    assert isinstance(result, (bool, dict)), f"Operation {i} should return boolean or dict"

            self.test_results.append("‚úÖ Concurrent operations")
            print("‚úÖ Concurrent operations tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Concurrent operations: {e}")
            print(f"‚ùå Concurrent operations tests failed: {e}")
            return False

    async def test_memory_area_edge_cases(self):
        """Test edge cases with different memory areas."""
        print("\nüìö Testing memory area edge cases...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)

            # Test all memory areas
            test_areas = [
                Memory.Area.MAIN.value,
                Memory.Area.FRAGMENTS.value,
                Memory.Area.SOLUTIONS.value,
                Memory.Area.INSTRUMENTS.value
            ]

            for area in test_areas:
                success = await consolidator.process_new_memory(
                    new_memory=f"Test memory for area {area}",
                    area=area,
                    metadata={"area": area, "test_area": area}
                )
                assert isinstance(success, bool), f"Should handle area {area}"

            # Test with invalid area (should still work)
            invalid_area_success = await consolidator.process_new_memory(
                new_memory="Test memory for invalid area",
                area="invalid_area",
                metadata={"area": "invalid_area", "test_invalid_area": True}
            )
            assert isinstance(invalid_area_success, bool), "Should handle invalid area"

            self.test_results.append("‚úÖ Memory area edge cases")
            print("‚úÖ Memory area edge cases tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Memory area edge cases: {e}")
            print(f"‚ùå Memory area edge cases tests failed: {e}")
            return False

    async def test_knowledge_source_awareness(self):
        """Test that knowledge sources can be properly stored and retrieved."""
        print("\nüìö Testing knowledge source awareness...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            db = await Memory.get(self.agent)

            # Step 1: Insert a knowledge source with very distinctive content
            knowledge_metadata = {
                "area": Memory.Area.MAIN.value,
                "knowledge_source": True,
                "source_file": "test_api_standards.md",
                "source_path": "/docs/test_api_standards.md",
                "file_type": "md",
                "import_timestamp": "2024-01-01 10:00:00"
            }

            # Use very specific, searchable content
            knowledge_content = (
                "KNOWLEDGE_MARKER_12345: Official REST API Documentation v4.0 "
                "HTTP status codes: 200 OK, 201 Created, 400 Bad Request, 401 Unauthorized, "
                "403 Forbidden, 404 Not Found, 500 Internal Server Error. "
                "Rate limiting standards: 1000 requests per hour maximum. "
                "Authentication required via Bearer tokens with SHA-256 encryption."
            )

            knowledge_id = await db.insert_text(knowledge_content, knowledge_metadata)
            print(f"   ‚úÖ Inserted knowledge source: {knowledge_id}")

            # Step 2: Verify knowledge source can be retrieved by ID
            retrieved_knowledge = await db.aget_by_ids([knowledge_id])
            assert len(retrieved_knowledge) == 1, "Knowledge source should be retrievable by ID"
            retrieved_doc = retrieved_knowledge[0]
            assert retrieved_doc.metadata.get('knowledge_source') is True, "Should have knowledge_source flag"
            assert "KNOWLEDGE_MARKER_12345" in retrieved_doc.page_content, "Content should be preserved"
            print("   ‚úÖ Knowledge source retrieved by ID successfully")

            # Step 3: Test semantic search with exact content match
            exact_search = await db.search_similarity_threshold(
                query="KNOWLEDGE_MARKER_12345 Official REST API Documentation",
                limit=5,
                threshold=0.3,
                filter=""
            )

            knowledge_in_exact_search = [
                doc for doc in exact_search
                if doc.metadata.get('knowledge_source', False)
            ]

            print(f"   Exact search found {len(exact_search)} total, {len(knowledge_in_exact_search)} knowledge sources")

            # This MUST work - exact content match with low threshold
            assert len(knowledge_in_exact_search) > 0, (
                f"Knowledge source with exact content match should be found. "
                f"Query: 'KNOWLEDGE_MARKER_12345 Official REST API Documentation', "
                f"Found {len(exact_search)} total memories, {len(knowledge_in_exact_search)} knowledge sources."
            )

            # Step 4: Test semantic search with related terms
            semantic_search = await db.search_similarity_threshold(
                query="REST API status codes rate limiting authentication",
                limit=10,
                threshold=0.4,
                filter=""
            )

            knowledge_in_semantic_search = [
                doc for doc in semantic_search
                if doc.metadata.get('knowledge_source', False)
            ]

            print(f"   Semantic search found {len(semantic_search)} total, {len(knowledge_in_semantic_search)} knowledge sources")

            # This should also work - semantic similarity
            assert len(knowledge_in_semantic_search) > 0, (
                f"Knowledge source should be found via semantic search. "
                f"Query: 'REST API status codes rate limiting authentication', "
                f"Found {len(semantic_search)} total memories, {len(knowledge_in_semantic_search)} knowledge sources."
            )

            # Cleanup
            await db.delete_documents_by_ids([knowledge_id])
            print("   ‚úÖ Cleaned up knowledge source")

            self.test_results.append("‚úÖ Knowledge source awareness")
            print("‚úÖ Knowledge source awareness tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Knowledge source awareness: {e}")
            print(f"‚ùå Knowledge source awareness tests failed: {e}")
            return False

    async def test_knowledge_directory_creation(self):
        """Test that knowledge import system creates missing directories robustly."""
        print("\nüìÅ Testing knowledge directory creation...")

        try:
            import tempfile
            import shutil
            from python.helpers import knowledge_import

            # Create a temporary directory for testing
            temp_base_dir = tempfile.mkdtemp()
            test_knowledge_dir = os.path.join(temp_base_dir, "nonexistent", "knowledge", "test")

            try:
                # Test that load_knowledge creates missing directories
                index = {}
                result_index = knowledge_import.load_knowledge(
                    log_item=None,
                    knowledge_dir=test_knowledge_dir,
                    index=index,
                    metadata={"area": "test"},
                    filename_pattern="**/*"
                )

                # Verify directory was created
                assert os.path.exists(test_knowledge_dir), "Should create missing knowledge directory"
                assert os.access(test_knowledge_dir, os.R_OK), "Created directory should be readable"
                assert isinstance(result_index, dict), "Should return valid index even for empty directory"

                # Test with existing directory
                result_index2 = knowledge_import.load_knowledge(
                    log_item=None,
                    knowledge_dir=test_knowledge_dir,
                    index=index,
                    metadata={"area": "test"},
                    filename_pattern="**/*"
                )

                assert isinstance(result_index2, dict), "Should handle existing directory properly"

                # Test with empty knowledge_dir parameter
                result_index3 = knowledge_import.load_knowledge(
                    log_item=None,
                    knowledge_dir="",
                    index=index,
                    metadata={"area": "test"},
                    filename_pattern="**/*"
                )

                assert isinstance(result_index3, dict), "Should handle empty knowledge_dir gracefully"

            finally:
                # Cleanup: remove the temporary directory
                if os.path.exists(temp_base_dir):
                    shutil.rmtree(temp_base_dir)

            self.test_results.append("‚úÖ Knowledge directory creation")
            print("‚úÖ Knowledge directory creation tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Knowledge directory creation: {e}")
            print(f"‚ùå Knowledge directory creation tests failed: {e}")
            return False

    async def cleanup_test_data(self):
        """Clean up ALL test data from memory - comprehensive cleanup."""
        print("\nüßπ Cleaning up test data...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            db = await Memory.get(self.agent)

            # Comprehensive list of ALL test filters used across the test suite
            test_filters = [
                "test == True",
                "test_pipeline == True",
                "test_timeout == True",
                "test_action != ''",
                "test_duplicate_bug == True",
                "test_isolation == True",
                "test_transaction == True",
                "test_corruption == True",
                "test_metadata_integrity == True",
                "test_llm_failure == True",
                "test_scenario != ''",
                "test_replace_safety == True",
                "test_similarity_fix == True",
                "test_circular == True",
                "test_performance == True",
                "test_knowledge_source == True",
                "test_knowledge_creation == True"
            ]

            total_removed = 0
            for filter_condition in test_filters:
                try:
                    test_memories = await db.search_similarity_threshold(
                        query="test",
                        limit=100,  # Increased limit to catch more test data
                        threshold=0.1,  # Very low threshold to catch all
                        filter=filter_condition
                    )

                    if test_memories:
                        test_ids = [doc.metadata.get('id') for doc in test_memories if doc.metadata.get('id')]
                        valid_test_ids = [id for id in test_ids if id is not None]
                        if valid_test_ids:
                            await db.delete_documents_by_ids(valid_test_ids)
                            total_removed += len(valid_test_ids)
                except Exception:
                    # Some filter conditions might not work, continue with others
                    continue

            # Additional cleanup: Remove any memory containing test-related keywords
            test_keywords = [
                "test memory", "test content", "consolidation testing",
                "DEPRECATED", "CURRENT V2.0", "API endpoint users",
                "FastAPI installation", "React component", "Alpine.js"
            ]

            for keyword in test_keywords:
                try:
                    keyword_memories = await db.search_similarity_threshold(
                        query=keyword,
                        limit=50,
                        threshold=0.3
                    )

                    # Only remove if they have test-related metadata
                    test_keyword_ids = []
                    for doc in keyword_memories:
                        metadata = doc.metadata
                        has_test_metadata = any(
                            key.startswith('test_') or key == 'test'
                            for key in metadata.keys()
                        )
                        if has_test_metadata and metadata.get('id'):
                            test_keyword_ids.append(metadata['id'])

                    if test_keyword_ids:
                        await db.delete_documents_by_ids(test_keyword_ids)
                        total_removed += len(test_keyword_ids)

                except Exception:
                    continue

            if total_removed > 0:
                print(f"   Removed {total_removed} test memories")
            else:
                print("   No test memories found to remove")
            print("‚úÖ Test data cleanup complete")

        except Exception as e:
            print(f"‚ö†Ô∏è Test cleanup warning: {e}")

    async def setup_individual_test(self, test_name: str):
        """Setup isolation for individual test."""
        print(f"üîß Setting up isolated environment for {test_name}")

        # Clean up any existing test data before starting
        await self.cleanup_test_data()

        # Note: Agent state reset is complex and not needed for memory tests
        # Each test should use unique metadata to avoid conflicts

        print(f"   Environment ready for {test_name}")

    async def teardown_individual_test(self, test_name: str):
        """Teardown and cleanup after individual test."""
        print(f"üßπ Cleaning up after {test_name}")

        # Remove any test data created by this specific test
        try:
            assert self.agent is not None, "Agent must be initialized"
            db = await Memory.get(self.agent)

            # Search for memories that might have been created in this test
            recent_memories = await db.search_similarity_threshold(
                query="test",
                limit=50,
                threshold=0.1
            )

            # Remove memories with test-related metadata
            test_ids = []
            for doc in recent_memories:
                metadata = doc.metadata
                has_test_metadata = any(
                    key.startswith('test_') or key == 'test'
                    for key in metadata.keys()
                )
                if has_test_metadata and metadata.get('id'):
                    test_ids.append(metadata['id'])

            if test_ids:
                await db.delete_documents_by_ids(test_ids)
                print(f"   Removed {len(test_ids)} test memories from {test_name}")

        except Exception as e:
            print(f"‚ö†Ô∏è Teardown warning for {test_name}: {e}")

    def print_test_summary(self):
        """Print a summary of all test results."""
        print("\n" + "=" * 60)
        print("üß™ MEMORY CONSOLIDATION TEST SUMMARY")
        print("=" * 60)

        passed = sum(1 for result in self.test_results if result.startswith("‚úÖ"))
        failed = sum(1 for result in self.test_results if result.startswith("‚ùå"))

        print(f"\nTotal Tests: {len(self.test_results)}")
        print(f"Passed: {passed}")
        print(f"Failed: {failed}")
        print(f"Success Rate: {(passed / len(self.test_results) * 100):.1f}%")

        print("\nDetailed Results:")
        for result in self.test_results:
            print(f"  {result}")

        if failed == 0:
            print("\nüéâ ALL TESTS PASSED! Memory consolidation system is ready for use.")
            print("‚úÖ Exit code will be 0 (success)")
        else:
            print(f"\n‚ö†Ô∏è {failed} test(s) failed. Please review the implementation.")
            print("‚ùå Exit code will be 1 (test failures)")

        return failed == 0

    async def run_all_tests(self):
        """Run the complete test suite with proper test isolation."""
        print("üöÄ Starting Memory Consolidation Test Suite")
        print("=" * 60)

        # Setup
        if not await self.setup_test_environment():
            print("‚ùå Failed to setup test environment. Exiting.")
            return False

        # Run all tests with isolation
        tests = [
            self.test_basic_consolidation_config,
            self.test_memory_discovery,
            self.test_keyword_extraction,
            self.test_keyword_extraction_edge_cases,
            self.test_consolidation_analysis,
            self.test_consolidation_actions,
            self.test_full_consolidation_pipeline,
            self.test_timeout_handling,
            self.test_division_by_zero_fix,
            self.test_extension_integration,
            self.test_llm_response_edge_cases,
            self.test_memory_content_edge_cases,
            self.test_configuration_edge_cases,
            self.test_database_edge_cases,
            self.test_action_specific_edge_cases,
            self.test_metadata_edge_cases,
            self.test_concurrent_operations,
            self.test_memory_area_edge_cases,
            self.test_knowledge_source_awareness,
            self.test_knowledge_directory_creation,
            self.test_consolidation_behavior,
            self.test_replace_similarity_safety,
            self.test_similarity_score_fix,
            self.test_duplicate_memory_bug,
            self.test_consolidation_transaction_safety,
            self.test_cross_area_isolation,
            self.test_memory_corruption_prevention,
            self.test_performance_with_many_similarities,
            self.test_circular_consolidation_prevention,
            self.test_metadata_preservation_integrity,
            self.test_llm_failure_graceful_degradation
        ]

        for test in tests:
            test_name = test.__name__
            try:
                # Setup isolated environment for this test
                await self.setup_individual_test(test_name)

                # Run the test
                await test()

                # Cleanup after the test
                await self.teardown_individual_test(test_name)

            except Exception as e:
                self.test_results.append(f"‚ùå {test_name}: Unexpected error - {e}")
                print(f"‚ùå {test_name} failed with unexpected error: {e}")

                # Still cleanup even if test failed
                try:
                    await self.teardown_individual_test(test_name)
                except Exception as cleanup_error:
                    print(f"‚ö†Ô∏è Cleanup failed for {test_name}: {cleanup_error}")

        # Final comprehensive cleanup
        await self.cleanup_test_data()

        # Summary
        return self.print_test_summary()

    async def test_consolidation_behavior(self):
        """Test actual consolidation behavior and decision-making with realistic scenarios."""
        print("\nüß† Testing consolidation behavior...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(
                self.agent,
                similarity_threshold=0.6,  # Lower threshold to find similar memories
                max_similar_memories=5,
                max_llm_context_memories=3
            )

            db = await Memory.get(self.agent)

            # Clear any existing test memories to avoid contamination from previous tests
            existing_test_memories = await db.search_similarity_threshold(
                query="test_scenario API endpoint",
                limit=50,
                threshold=0.1,
                filter="test_scenario == 'replace'"
            )
            if existing_test_memories:
                test_ids = [doc.metadata.get('id') for doc in existing_test_memories if doc.metadata.get('id') is not None]
                valid_test_ids = [id for id in test_ids if id is not None]
                if valid_test_ids:
                    await db.delete_documents_by_ids(valid_test_ids)
                    print(f"   Cleared {len(valid_test_ids)} existing test memories from previous runs")

            # Test Scenario 1: REPLACE - Outdated information should be replaced
            print("   Testing REPLACE scenario...")

            # Insert clearly outdated memory with explicit deprecation notice
            old_memory_id = await db.insert_text(
                "DEPRECATED V1.0 - SUPERSEDED BY V2.0: The old API endpoint /api/v1/users is OBSOLETE "
                "and should NO LONGER BE USED. It only supports GET requests with a limit of 100 requests "
                "per minute. This endpoint was officially deprecated on January 1, 2023 and WILL BE REMOVED.",
                {"area": Memory.Area.MAIN.value, "test_scenario": "replace", "version": "v1", "status": "deprecated"}
            )

            # Process new memory that explicitly states it replaces the old one
            new_memory = (
                "CURRENT V2.0 - OFFICIAL REPLACEMENT: The NEW API endpoint /api/v2/users REPLACES "
                "the deprecated /api/v1/users endpoint completely. This is the ONLY supported endpoint "
                "as of 2024. It supports GET, POST, PUT, and DELETE requests with 1000 requests per minute. "
                "The old v1 endpoint is OBSOLETE and should not be used."
            )
            success = await consolidator.process_new_memory(
                new_memory=new_memory,
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_scenario": "replace", "version": "v2"}
            )

            assert success, "Consolidation should succeed for replace scenario"

            # Debug: Check what memories exist after consolidation
            all_api_memories = await db.search_similarity_threshold(
                query="API endpoint users",
                limit=10,
                threshold=0.3,
                filter="test_scenario == 'replace'"
            )

            print(f"   Found {len(all_api_memories)} API memories after consolidation:")
            for i, doc in enumerate(all_api_memories):
                contains_v1 = "v1" in doc.page_content.lower()
                contains_v2 = "v2" in doc.page_content.lower()
                is_deprecated = "DEPRECATED" in doc.page_content
                is_obsolete = "OBSOLETE" in doc.page_content
                is_current = "CURRENT" in doc.page_content
                print(f"   [{i}] ID:{doc.metadata.get('id', 'unknown')[:8]}... "
                      f"v1:{contains_v1} v2:{contains_v2} DEP:{is_deprecated} "
                      f"OBS:{is_obsolete} CUR:{is_current}")
                print(f"       Content: {doc.page_content[:100]}...")

            # Check if old memory was removed
            old_memory_check = await db.aget_by_ids([old_memory_id])
            old_memory_still_exists = len(old_memory_check) > 0

            # Check for current API info in any memory
            current_api_found = False
            for i, doc in enumerate(all_api_memories):
                has_new_or_current = ("CURRENT" in doc.page_content or "NEW" in doc.page_content or "v2.0" in doc.page_content)
                has_replaces_or_1000 = ("REPLACES" in doc.page_content or "1000 requests" in doc.page_content)
                if has_new_or_current and has_replaces_or_1000:
                    current_api_found = True
                    print(f"   ‚úì Found current API info in memory [{i}]")
                else:
                    print(f"   - Memory [{i}]: NEW/CURRENT:{has_new_or_current}, REPLACES/1000:{has_replaces_or_1000}")
                    print(f"     Checking content: {repr(doc.page_content[:200])}")

            if not current_api_found:
                print("   ‚ùå No memory found with both NEW/CURRENT and REPLACES/1000")

            # Check that deprecated content is properly handled
            deprecated_properly_handled = True
            if old_memory_still_exists:
                old_content = old_memory_check[0].page_content
                # If old memory still exists, it should either be updated or clearly marked as superseded
                if "DEPRECATED V1.0 - SUPERSEDED BY V2.0" in old_content and "WILL BE REMOVED" in old_content:
                    # Original deprecated content is unchanged - this means LLM chose not to consolidate
                    # This is actually OK if we have the new current content elsewhere
                    if not current_api_found:
                        deprecated_properly_handled = False
                        print("   ‚ùå Old deprecated memory unchanged AND no current API info found")
                    else:
                        print("   ‚úì Old deprecated memory kept separate, but current API info available")
                else:
                    print("   ‚úì Old memory was updated/consolidated")

            # Verify we have current API information available
            assert current_api_found, (
                f"Should have current API information available somewhere. "
                f"Found {len(all_api_memories)} memories, but no current v2.0 API info."
            )

            # For REPLACE scenario, we expect EITHER:
            # 1. Old memory removed and new memory created, OR
            # 2. Old memory updated with new information, OR
            # 3. Old memory kept separate but new current information is available
            replace_successful = (not old_memory_still_exists) or deprecated_properly_handled

            assert replace_successful, (
                f"REPLACE scenario should result in proper handling of deprecated information. "
                f"Old memory exists: {old_memory_still_exists}, "
                f"Current API found: {current_api_found}, "
                f"Deprecated handled properly: {deprecated_properly_handled}"
            )

            self.test_results.append("‚úÖ Consolidation behavior validation")
            print("‚úÖ Consolidation behavior tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Consolidation behavior: {e}")
            print(f"‚ùå Consolidation behavior tests failed: {e}")
            return False

    async def test_replace_similarity_safety(self):
        """Test that REPLACE actions are blocked when similarity is too low for safety."""
        print("\nüõ°Ô∏è Testing REPLACE similarity safety mechanism...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            # Create consolidator with explicit safety threshold
            consolidator = create_memory_consolidator(
                self.agent,
                similarity_threshold=0.6,  # Low discovery threshold
                replace_similarity_threshold=0.9,  # High safety threshold
                max_similar_memories=3
            )

            db = await Memory.get(self.agent)

            # Insert a memory about Python
            original_memory_id = await db.insert_text(
                "Python list comprehensions provide an elegant way to create lists",
                {"area": Memory.Area.MAIN.value, "test_safety": "replace", "topic": "python"}
            )

            # Try to "replace" with a moderately related but different memory about JavaScript
            # This should have moderate similarity (both are programming languages) but should not be replaced
            different_memory = (
                "JavaScript array methods like map() and filter() are powerful tools for data transformation"
            )

            # Process the different memory - LLM might suggest REPLACE but safety should block it
            result = await consolidator.process_new_memory(
                new_memory=different_memory,
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_safety": "replace", "topic": "javascript"}
            )

            assert result, "Processing should succeed even if REPLACE is blocked"

            # Check that original memory still exists (should not be replaced due to low similarity)
            original_check = await db.aget_by_ids([original_memory_id])
            original_still_exists = len(original_check) > 0

            # Check what memories exist now
            all_test_memories = await db.search_similarity_threshold(
                query="programming languages list comprehensions array methods",
                limit=10,
                threshold=0.3,
                filter="test_safety == 'replace'"
            )

            python_memory_exists = any("Python" in doc.page_content for doc in all_test_memories)
            javascript_memory_exists = any("JavaScript" in doc.page_content for doc in all_test_memories)

            print(f"   Original memory exists: {original_still_exists}")
            print(f"   Python memory found: {python_memory_exists}")
            print(f"   JavaScript memory found: {javascript_memory_exists}")
            print(f"   Total memories: {len(all_test_memories)}")

            # Both memories should exist (original preserved + new added separately)
            assert original_still_exists, "Original memory should be preserved when similarity is too low for safe replacement"
            assert python_memory_exists, "Python memory should still exist"
            assert javascript_memory_exists, "JavaScript memory should exist (added separately)"

            # Test with high similarity - this should allow replacement
            very_similar_memory = (
                "Python list comprehensions are an elegant, concise way to create new lists from existing ones"
            )

            # This should be similar enough to allow replacement
            similar_result = await consolidator.process_new_memory(
                new_memory=very_similar_memory,
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_safety": "similar", "topic": "python"}
            )

            assert similar_result, "Processing of similar memory should succeed"

            self.test_results.append("‚úÖ REPLACE similarity safety mechanism")
            print("‚úÖ REPLACE similarity safety mechanism tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå REPLACE similarity safety: {e}")
            print(f"‚ùå REPLACE similarity safety tests failed: {e}")
            return False

    async def test_similarity_score_fix(self):
        """Test that similarity scores are logically consistent with search threshold."""
        print("\nüßÆ Testing similarity score calculation fix...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(
                self.agent,
                similarity_threshold=0.7,  # Test threshold
                max_similar_memories=5
            )

            db = await Memory.get(self.agent)

            # Insert multiple test memories to get a ranking
            test_memories = [
                "Python async programming with asyncio library provides powerful concurrency",
                "FastAPI framework supports async request handling for web applications",
                "JavaScript promises and async/await enable asynchronous programming",
                "React hooks like useEffect handle asynchronous operations in components",
                "Node.js event loop manages asynchronous I/O operations efficiently"
            ]

            memory_ids = []
            for memory in test_memories:
                memory_id = await db.insert_text(
                    memory,
                    {"area": Memory.Area.MAIN.value, "test_similarity_fix": True}
                )
                memory_ids.append(memory_id)

            # Search for similar memories (should find all of them)
            similar_memories = await consolidator._find_similar_memories(
                "Asynchronous programming techniques for modern applications",
                Memory.Area.MAIN.value
            )

            # Verify we found memories
            assert len(similar_memories) > 0, "Should find similar memories"

            # Check similarity scores
            all_scores_valid = True
            below_threshold_count = 0
            search_threshold = consolidator.config.similarity_threshold

            for doc in similar_memories:
                similarity_score = doc.metadata.get('_consolidation_similarity', 0.0)

                if similarity_score < search_threshold:
                    below_threshold_count += 1
                    all_scores_valid = False
                    print(f"   ‚ùå Invalid score: {similarity_score:.3f} < {search_threshold}")

            # Verify all scores are >= search threshold
            assert all_scores_valid, f"Found {below_threshold_count} scores below search threshold {search_threshold}"

            # Verify scores are in descending order
            scores = [doc.metadata.get('_consolidation_similarity', 0.0) for doc in similar_memories]
            is_descending = all(scores[i] >= scores[i + 1] for i in range(len(scores) - 1))
            assert is_descending, "Similarity scores should be in descending order"

            # Clean up test memories
            await db.delete_documents_by_ids(memory_ids)

            print(f"   ‚úÖ All {len(similar_memories)} similarity scores >= {search_threshold}")
            print(f"   ‚úÖ Scores in proper descending order: {[f'{s:.3f}' for s in scores[:3]]}...")

            self.test_results.append("‚úÖ Similarity score calculation fix")
            print("‚úÖ Similarity score calculation fix tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Similarity score calculation fix: {e}")
            print(f"‚ùå Similarity score calculation fix tests failed: {e}")
            return False

    async def test_duplicate_memory_bug(self):
        """Test the specific duplicate memory bug that was causing accumulation."""
        print("\nüîÑ Testing duplicate memory bug prevention...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(
                self.agent,
                similarity_threshold=0.6,
                max_similar_memories=5
            )

            db = await Memory.get(self.agent)

            # Step 1: Insert identical duplicate memories (simulating the bug scenario)
            duplicate_content = "DEPRECATED API ENDPOINT: The old /api/v1/users endpoint is deprecated. It only supports GET requests with a limit of 100 requests per minute."

            memory_id_1 = await db.insert_text(
                duplicate_content,
                {"area": Memory.Area.MAIN.value, "test_duplicate_bug": True, "version": "v1"}
            )

            memory_id_2 = await db.insert_text(
                duplicate_content,
                {"area": Memory.Area.MAIN.value, "test_duplicate_bug": True, "version": "v1"}
            )

            # Step 2: Verify we have 2 identical memories
            before_memories = await db.search_similarity_threshold(
                query=duplicate_content,
                limit=10,
                threshold=0.3,
                filter="test_duplicate_bug == True"
            )

            assert len(before_memories) == 2, f"Should start with 2 identical memories, found {len(before_memories)}"

            # Step 3: Process a new related memory (this should consolidate the duplicates)
            new_memory = "The current /api/v2/users endpoint replaces the deprecated v1 endpoint. It supports GET, POST, PUT, and DELETE requests with 1000 requests per minute."

            success = await consolidator.process_new_memory(
                new_memory=new_memory,
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_duplicate_bug": True, "version": "v2"}
            )

            assert success, "Consolidation should succeed"

            # Step 4: Verify consolidation worked - should have fewer total memories
            after_memories = await db.search_similarity_threshold(
                query="API endpoint users",
                limit=10,
                threshold=0.3,
                filter="test_duplicate_bug == True"
            )

            # The bug would result in accumulation (3+ memories), proper consolidation should result in 1-2 memories
            assert len(after_memories) <= 2, (
                f"Should consolidate to 1-2 memories, found {len(after_memories)} (indicates bug recurrence)"
            )

            # Step 5: Verify we have current API information
            current_api_found = any(
                "v2" in doc.page_content or "current" in doc.page_content.lower()
                for doc in after_memories
            )
            assert current_api_found, "Should have current API information after consolidation"

            # Step 6: Check that original duplicates were properly handled
            remaining_ids = [doc.metadata.get('id') for doc in after_memories]
            original_duplicates_remaining = sum(1 for id in [memory_id_1, memory_id_2] if id in remaining_ids)

            print(f"   Original duplicates remaining: {original_duplicates_remaining}/2")
            print(f"   Total memories after consolidation: {len(after_memories)}")

            # Either duplicates were consolidated (removed) or they exist but we have proper consolidation
            if original_duplicates_remaining == 2:
                # Both originals still exist, but consolidation should have added value
                assert len(after_memories) <= 3, "If originals remain, total should not exceed 3 memories"

            self.test_results.append("‚úÖ Duplicate memory bug prevention")
            print("‚úÖ Duplicate memory bug prevention tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Duplicate memory bug prevention: {e}")
            print(f"‚ùå Duplicate memory bug prevention tests failed: {e}")
            return False

    async def test_consolidation_transaction_safety(self):
        """Test that consolidation operations are transactionally safe."""
        print("\nüîí Testing consolidation transaction safety...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)
            db = await Memory.get(self.agent)

            # Insert test memories
            memory_ids = []
            for i in range(3):
                memory_id = await db.insert_text(
                    f"Test memory {i} for transaction safety testing",
                    {"area": Memory.Area.MAIN.value, "test_transaction": True, "index": i}
                )
                memory_ids.append(memory_id)

            # Test that partial failure doesn't corrupt the database
            from python.helpers.memory_consolidation import ConsolidationResult

            # Create a result that attempts to remove valid and invalid IDs
            mixed_result = ConsolidationResult(
                action=ConsolidationAction.MERGE,
                memories_to_remove=memory_ids[:2] + ["non_existent_id"],  # Mix valid and invalid
                new_memory_content="Consolidated memory content",
                metadata={"test_transaction": True}
            )

            # This should handle the invalid ID gracefully
            await consolidator._handle_merge(db, mixed_result, Memory.Area.MAIN.value, {})

            # Verify database state is consistent
            remaining_memories = await db.search_similarity_threshold(
                query="transaction safety testing",
                limit=10,
                threshold=0.3,
                filter="test_transaction == True"
            )

            # Should have the consolidated memory plus any that weren't removed due to invalid IDs
            assert len(remaining_memories) >= 1, "Should have at least the consolidated memory"

            # Verify no orphaned data
            for memory in remaining_memories:
                assert memory.metadata.get('test_transaction') is True, "All memories should have proper metadata"

            self.test_results.append("‚úÖ Consolidation transaction safety")
            print("‚úÖ Consolidation transaction safety tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Consolidation transaction safety: {e}")
            print(f"‚ùå Consolidation transaction safety tests failed: {e}")
            return False

    async def test_cross_area_isolation(self):
        """Test that consolidation doesn't accidentally cross memory areas."""
        print("\nüöß Testing cross-area isolation...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)
            db = await Memory.get(self.agent)

            # Insert similar content in different areas
            test_content = "Similar content for isolation testing"

            areas_and_ids = []
            for area in [Memory.Area.MAIN, Memory.Area.FRAGMENTS, Memory.Area.SOLUTIONS]:
                memory_id = await db.insert_text(
                    test_content,
                    {"area": area.value, "test_isolation": True}
                )
                areas_and_ids.append((area.value, memory_id))

            # Process consolidation in MAIN area only
            success = await consolidator.process_new_memory(
                new_memory="Updated content for isolation testing",
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_isolation": True}
            )

            assert success, "Consolidation should succeed"

            # Verify other areas are untouched
            for area_name, original_id in areas_and_ids:
                if area_name != Memory.Area.MAIN.value:
                    # Check that memories in other areas still exist
                    area_memories = await db.search_similarity_threshold(
                        query=test_content,
                        limit=5,
                        threshold=0.3,
                        filter=f"area == '{area_name}' and test_isolation == True"
                    )

                    assert len(area_memories) >= 1, f"Area {area_name} should still have its memories"

                    # Verify original memory still exists
                    original_still_exists = any(doc.metadata.get('id') == original_id for doc in area_memories)
                    assert original_still_exists, f"Original memory in {area_name} should not be affected by MAIN consolidation"

            self.test_results.append("‚úÖ Cross-area isolation")
            print("‚úÖ Cross-area isolation tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Cross-area isolation: {e}")
            print(f"‚ùå Cross-area isolation tests failed: {e}")
            return False

    async def test_memory_corruption_prevention(self):
        """Test that consolidation doesn't corrupt memory metadata or content."""
        print("\nüõ°Ô∏è Testing memory corruption prevention...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)
            db = await Memory.get(self.agent)

            # Insert memory with complex metadata
            complex_metadata = {
                "area": Memory.Area.MAIN.value,
                "test_corruption": True,
                "nested": {
                    "level1": {"level2": "deep_value"},
                    "array": [1, 2, 3]
                },
                "special_chars": "√†√°√¢√£√§√•√¶√ß√®√©√™√´",
                "unicode": "üöÄüîßüß™",
                "important_flag": True,
                "version": "1.0.0"
            }

            original_content = "Critical memory content with special chars: √†√°√¢√£√§√•√¶√ß√®√©√™√´ and unicode: üöÄüîßüß™"

            memory_id = await db.insert_text(
                original_content,
                complex_metadata
            )

            # Process consolidation that should preserve this memory
            success = await consolidator.process_new_memory(
                new_memory="Different content that shouldn't corrupt the original",
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_corruption": True}
            )

            assert success, "Consolidation should succeed"

            # Verify original memory integrity (if it still exists)
            remaining_memories = await db.search_similarity_threshold(
                query="Critical memory content special chars unicode",
                limit=10,
                threshold=0.3,
                filter="test_corruption == True"
            )

            # Check for any memory with the original content
            original_preserved = False
            for memory in remaining_memories:
                if "Critical memory content" in memory.page_content:
                    original_preserved = True

                    # Verify content integrity
                    assert "√†√°√¢√£√§√•√¶√ß√®√©√™√´" in memory.page_content, "Special characters should be preserved"
                    assert "üöÄüîßüß™" in memory.page_content, "Unicode should be preserved"

                    # Verify metadata integrity
                    metadata = memory.metadata
                    assert metadata.get('test_corruption') is True, "Boolean metadata should be preserved"
                    assert metadata.get('special_chars') == "√†√°√¢√£√§√•√¶√ß√®√©√™√´", "Special char metadata should be preserved"
                    assert metadata.get('unicode') == "üöÄüîßüß™", "Unicode metadata should be preserved"

                    # Check nested metadata if present
                    if 'nested' in metadata:
                        nested = metadata['nested']
                        if isinstance(nested, dict) and 'level1' in nested:
                            assert nested['level1'].get('level2') == "deep_value", "Nested metadata should be preserved"

            # Test that new memories have proper structure too
            for memory in remaining_memories:
                assert 'area' in memory.metadata, "All memories should have area metadata"
                assert 'timestamp' in memory.metadata, "All memories should have timestamp metadata"
                assert memory.metadata['area'] == Memory.Area.MAIN.value, "Area should be preserved"

            self.test_results.append("‚úÖ Memory corruption prevention")
            print("‚úÖ Memory corruption prevention tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Memory corruption prevention: {e}")
            print(f"‚ùå Memory corruption prevention tests failed: {e}")
            return False

    async def test_performance_with_many_similarities(self):
        """Test consolidation performance with many similar memories."""
        print("\n‚ö° Testing performance with many similar memories...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            # Use shorter timeout for performance test
            consolidator = create_memory_consolidator(
                self.agent,
                similarity_threshold=0.6,
                max_similar_memories=20,  # Allow more for this test
                processing_timeout_seconds=45  # Slightly longer timeout
            )

            db = await Memory.get(self.agent)

            # Insert many similar memories
            base_content = "Python programming language feature"
            memory_ids = []

            for i in range(15):  # Create many similar memories
                content = f"{base_content} number {i}: async/await, list comprehensions, decorators"
                memory_id = await db.insert_text(
                    content,
                    {"area": Memory.Area.MAIN.value, "test_performance": True, "index": i}
                )
                memory_ids.append(memory_id)

            # Measure consolidation time
            import time
            start_time = time.time()

            # Process a new similar memory
            success = await consolidator.process_new_memory(
                new_memory="Python programming language advanced features: generators, context managers, metaclasses",
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_performance": True}
            )

            end_time = time.time()
            processing_time = end_time - start_time

            assert success, "Consolidation should succeed even with many similar memories"
            assert processing_time < 40, f"Processing should complete within 40 seconds, took {processing_time:.2f}s"

            # Verify the system handled the load appropriately
            remaining_memories = await db.search_similarity_threshold(
                query="Python programming language",
                limit=25,
                threshold=0.3,
                filter="test_performance == True"
            )

            # Should have either consolidated some memories or handled them appropriately
            print(f"   Original memories: 15, Final memories: {len(remaining_memories)}")
            print(f"   Processing time: {processing_time:.2f} seconds")

            # Verify system didn't crash or corrupt data
            for memory in remaining_memories:
                assert 'area' in memory.metadata, "All memories should have proper metadata"
                assert memory.metadata.get('test_performance') is True, "Test flag should be preserved"

            self.test_results.append("‚úÖ Performance with many similarities")
            print("‚úÖ Performance with many similarities tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Performance with many similarities: {e}")
            print(f"‚ùå Performance with many similarities tests failed: {e}")
            return False

    async def test_circular_consolidation_prevention(self):
        """Test that consolidation doesn't create circular references or infinite loops."""
        print("\nüîÑ Testing circular consolidation prevention...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)
            db = await Memory.get(self.agent)

            # Create a scenario that could potentially cause circular consolidation
            # Memory A references Memory B content, Memory B references Memory A content

            memory_a_content = "Reference to memory B: The solution in memory B is optimal for this problem"
            memory_b_content = "Reference to memory A: The problem described in memory A requires this solution"

            memory_a_id = await db.insert_text(
                memory_a_content,
                {"area": Memory.Area.MAIN.value, "test_circular": True, "type": "problem"}
            )

            memory_b_id = await db.insert_text(
                memory_b_content,
                {"area": Memory.Area.MAIN.value, "test_circular": True, "type": "solution"}
            )

            # Process multiple consolidations in sequence to test for circular behavior
            consolidation_count = 0
            max_consolidations = 3

            for i in range(max_consolidations):
                new_content = f"Iteration {i}: Combined problem-solution approach referencing both memory A and memory B concepts"

                success = await consolidator.process_new_memory(
                    new_memory=new_content,
                    area=Memory.Area.MAIN.value,
                    metadata={"area": Memory.Area.MAIN.value, "test_circular": True, "iteration": i}
                )

                if success:
                    consolidation_count += 1

                # Check that we don't have exponential growth of memories (sign of circular issue)
                current_memories = await db.search_similarity_threshold(
                    query="reference memory",
                    limit=20,
                    threshold=0.3,
                    filter="test_circular == True"
                )

                assert len(current_memories) <= 10, f"Memory count should not grow exponentially: {len(current_memories)} memories at iteration {i}"

            # Verify final state is stable
            final_memories = await db.search_similarity_threshold(
                query="reference memory problem solution",
                limit=15,
                threshold=0.3,
                filter="test_circular == True"
            )

            print(f"   Consolidations completed: {consolidation_count}/{max_consolidations}")
            print(f"   Final memory count: {len(final_memories)}")

            # Should have reasonable number of memories, not exponential growth
            assert len(final_memories) <= 8, f"Final memory count should be reasonable: {len(final_memories)}"

            # Verify no corrupted references or infinite consolidation metadata
            for memory in final_memories:
                content = memory.page_content
                metadata = memory.metadata

                # Check for signs of circular corruption
                assert content.count("memory A") <= 2, "Should not have excessive references to memory A"
                assert content.count("memory B") <= 2, "Should not have excessive references to memory B"
                assert len(content) <= 1000, "Memory content should not grow excessively"

                # Check metadata for circular consolidation signs
                if 'consolidated_from' in metadata:
                    consolidated_from = metadata['consolidated_from']
                    if isinstance(consolidated_from, list):
                        assert len(consolidated_from) <= 5, "Should not consolidate from excessive number of memories"

            self.test_results.append("‚úÖ Circular consolidation prevention")
            print("‚úÖ Circular consolidation prevention tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Circular consolidation prevention: {e}")
            print(f"‚ùå Circular consolidation prevention tests failed: {e}")
            return False

    async def test_metadata_preservation_integrity(self):
        """Test that important metadata is preserved correctly during all consolidation types."""
        print("\nüìä Testing metadata preservation integrity...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)
            db = await Memory.get(self.agent)

            # Test metadata preservation across different consolidation actions
            test_scenarios = [
                {
                    "action": "merge_test",
                    "original_metadata": {
                        "area": Memory.Area.MAIN.value,
                        "test_metadata_integrity": True,
                        "priority": "high",
                        "source": "user_input",
                        "tags": ["important", "consolidation"],
                        "created_by": "test_user",
                        "version": "1.0"
                    },
                    "content": "Original content for merge testing with important metadata"
                },
                {
                    "action": "update_test",
                    "original_metadata": {
                        "area": Memory.Area.MAIN.value,
                        "test_metadata_integrity": True,
                        "confidentiality": "private",
                        "retention_period": 365,
                        "source_system": "api_v2",
                        "validation_status": "verified"
                    },
                    "content": "Original content for update testing with sensitive metadata"
                }
            ]

            memory_ids = []
            for scenario in test_scenarios:
                memory_id = await db.insert_text(
                    scenario["content"],
                    scenario["original_metadata"]
                )
                memory_ids.append((memory_id, scenario))

            # Process consolidation that should trigger different actions
            new_memory = "Enhanced content that should consolidate with existing memories while preserving critical metadata"

            success = await consolidator.process_new_memory(
                new_memory=new_memory,
                area=Memory.Area.MAIN.value,
                metadata={
                    "area": Memory.Area.MAIN.value,
                    "test_metadata_integrity": True,
                    "enhancement": "true",
                    "processor": "consolidation_system"
                }
            )

            assert success, "Consolidation should succeed"

            # Verify metadata preservation
            final_memories = await db.search_similarity_threshold(
                query="content testing metadata",
                limit=10,
                threshold=0.3,
                filter="test_metadata_integrity == True"
            )

            # Check that critical metadata is preserved
            critical_fields_preserved = {
                "priority": False,
                "confidentiality": False,
                "source": False,
                "created_by": False,
                "source_system": False,
                "validation_status": False
            }

            for memory in final_memories:
                metadata = memory.metadata

                # Check preservation of critical fields
                for field in critical_fields_preserved:
                    if field in metadata:
                        critical_fields_preserved[field] = True

                # Verify required fields are always present
                assert 'area' in metadata, "Area should always be preserved"
                assert 'timestamp' in metadata, "Timestamp should always be preserved"
                assert metadata.get('test_metadata_integrity') is True, "Test flag should be preserved"

                # Check for metadata corruption signs
                for key, value in metadata.items():
                    assert key is not None, "Metadata keys should not be None"
                    assert isinstance(key, str), "Metadata keys should be strings"

                    # Verify common metadata types
                    if key in ['priority', 'confidentiality', 'source', 'created_by', 'source_system']:
                        assert isinstance(value, str), f"String metadata field {key} should remain string"
                    elif key in ['retention_period']:
                        assert isinstance(value, (int, str)), f"Numeric metadata field {key} should remain numeric"
                    elif key in ['tags']:
                        assert isinstance(value, (list, str)), f"List metadata field {key} should remain list or be converted appropriately"

            # Verify that at least some critical metadata was preserved
            preserved_count = sum(critical_fields_preserved.values())
            print(f"   Critical metadata fields preserved: {preserved_count}/6")

            # Some metadata should be preserved, but not necessarily all (depends on consolidation decisions)
            assert preserved_count >= 2, f"Should preserve at least 2 critical metadata fields, preserved {preserved_count}"

            self.test_results.append("‚úÖ Metadata preservation integrity")
            print("‚úÖ Metadata preservation integrity tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå Metadata preservation integrity: {e}")
            print(f"‚ùå Metadata preservation integrity tests failed: {e}")
            return False

    async def test_llm_failure_graceful_degradation(self):
        """Test that consolidation degrades gracefully when LLM calls fail."""
        print("\nüîß Testing LLM failure graceful degradation...")

        try:
            assert self.agent is not None, "Agent must be initialized"

            consolidator = create_memory_consolidator(self.agent)
            db = await Memory.get(self.agent)

            # Insert a test memory
            memory_id = await db.insert_text(
                "Test memory for LLM failure graceful degradation",
                {"area": Memory.Area.MAIN.value, "test_llm_failure": True}
            )

            # Test with memory that should trigger consolidation
            new_memory = "Enhanced test memory for LLM failure graceful degradation testing"

            # Mock LLM failure by temporarily breaking the utility model call
            original_call_utility = self.agent.call_utility_model

            async def failing_utility_model(*args, **kwargs):
                raise Exception("Simulated LLM failure for testing")

            # Temporarily replace the utility model call
            self.agent.call_utility_model = failing_utility_model

            try:
                # This should fail gracefully and not crash the system
                success = await consolidator.process_new_memory(
                    new_memory=new_memory,
                    area=Memory.Area.MAIN.value,
                    metadata={"area": Memory.Area.MAIN.value, "test_llm_failure": True}
                )

                # System should handle failure gracefully
                # Success depends on implementation - it might return False (failure) or True (fallback)
                assert isinstance(success, bool), "Should return a boolean even on LLM failure"

            finally:
                # Restore original utility model call
                self.agent.call_utility_model = original_call_utility

            # Verify database is still in consistent state
            memories_after_failure = await db.search_similarity_threshold(
                query="test memory LLM failure",
                limit=10,
                threshold=0.3,
                filter="test_llm_failure == True"
            )

            # Should have at least the original memory (system didn't corrupt data)
            assert len(memories_after_failure) >= 1, "Should maintain data integrity despite LLM failure"

            # Verify memories are not corrupted
            for memory in memories_after_failure:
                assert 'area' in memory.metadata, "Memory metadata should remain intact"
                assert memory.metadata.get('test_llm_failure') is True, "Test metadata should be preserved"
                assert len(memory.page_content) > 0, "Memory content should not be empty"

            # Test recovery - system should work normally after LLM is restored
            recovery_success = await consolidator.process_new_memory(
                new_memory="Recovery test memory after LLM failure",
                area=Memory.Area.MAIN.value,
                metadata={"area": Memory.Area.MAIN.value, "test_llm_failure": True, "recovery": True}
            )

            # Should work normally now
            assert isinstance(recovery_success, bool), "Should work normally after LLM recovery"

            self.test_results.append("‚úÖ LLM failure graceful degradation")
            print("‚úÖ LLM failure graceful degradation tests passed")
            return True

        except Exception as e:
            self.test_results.append(f"‚ùå LLM failure graceful degradation: {e}")
            print(f"‚ùå LLM failure graceful degradation tests failed: {e}")
            return False


async def main():
    """Main test runner."""
    tester = MemoryConsolidationTester()
    success = await tester.run_all_tests()

    if success:
        print("\nüìö Next Steps:")
        print("1. Test with real conversations to see consolidation in action")
        print("2. Monitor memory quality improvements over time")
        print("3. Adjust consolidation thresholds based on your use case")
        print("4. Review consolidation logs to understand decisions")
        print("\nüîß Key Features Validated:")
        print("- ‚úÖ LLM-powered memory analysis and consolidation")
        print("- ‚úÖ All five consolidation actions (MERGE, REPLACE, KEEP_SEPARATE, UPDATE, SKIP)")
        print("- ‚úÖ Robust error handling and edge case management")
        print("- ‚úÖ Division by zero and timeout protection")
        print("- ‚úÖ Integration with existing memory extensions")

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Tests interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• Test suite crashed: {e}")
        sys.exit(1)
