{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SomaAgent01 Documentation Hub","text":"<p>Welcome to the reorganized documentation hub. Content now lives in four curated manuals with shared resources for terminology, style, and change tracking.</p>"},{"location":"#manuals","title":"Manuals","text":"<ul> <li>User Manual: Install SomaAgent01, run daily workflows, and troubleshoot issues.</li> <li>Technical Manual: Dive into architecture, infrastructure, integrations, and data flow.</li> <li>Development Manual: Follow engineering workflows, coding standards, testing, and release processes.</li> <li>Onboarding Manual: Enable new team members with orientation checklists, tool primers, and 30/60/90 planning.</li> </ul>"},{"location":"#shared-resources","title":"Shared Resources","text":"<ul> <li>Documentation Style Guide to keep writing consistent.</li> <li>Documentation Audit Checklist enforced on every docs PR.</li> <li>Glossary with canonical terminology.</li> <li>Documentation Change Log recording updates across manuals.</li> </ul>"},{"location":"#automation-governance","title":"Automation &amp; Governance","text":"<ul> <li>Run <code>make docs-verify</code> locally to execute markdown lint, link checks, diagram rendering, and MkDocs build.</li> <li>Docs GitHub Action (<code>.github/workflows/docs-quality.yml</code>) enforces the same checks in CI.</li> <li>Quarterly audits follow the checklist to ensure accuracy, accessibility, and ownership.</li> </ul> <p>[!TIP] Legacy documents are being migrated into the new manuals. Check <code>_legacy/</code> or the change log for interim locations while the transition completes.</p>"},{"location":"ROADMAP_SPRINTS/","title":"Sprint\u2011Based Roadmap for Local\u2011Only SomaAgent01 Stack","text":"<p>Purpose \u2013 Collapse the Docker\u2011Compose stack to the minimal set of containers needed for a fully\u2011functional local development environment while keeping all required features (metrics, UI, delegation, OpenFGA, etc.).  The voice\u2011processing component (<code>whisper</code>) will be removed and replaced by a simple HTTP call to an external speech\u2011to\u2011text API (the API endpoint will be injected via an environment variable).  Prometheus stays because it is the canonical metrics collector.</p> <p>Status update (2025\u201110\u201108) \u2013 The roadmap has progressed with two cross-cutting sprints: - Observability &amp; Resilience: Versioned gateway routing (<code>/v1/*</code>), reusable circuit breaker helper (with Prometheus counters), Kafka partition scaling script, and alert rules for latency/error-rate/circuit-breaker events. - Enterprise Extensions &amp; Marketplace: Capsule registry FastAPI service with optional Cosign signing, SDK helpers (<code>python/somaagent/capsule.py</code>), GitHub Actions capsule pipeline, and a browser marketplace UI that installs capsules via the gateway (<code>/v1/capsules/*</code>). - SomaAgentHub integration: integration endpoint recorded as <code>http://localhost:60010/</code>.</p> <p>Grafana has been dropped from the default stack in favour of Prometheus-only monitoring. The sections below remain for historical context and should be revisited when planning the next iteration.</p>"},{"location":"ROADMAP_SPRINTS/#highlevel-timeline","title":"\ud83d\udcc5 High\u2011Level Timeline","text":"Sprint Duration Goal Deliverable Sprint\u202f1 1\u202fweek Prune &amp; Refactor Compose \u2013 remove Whisper, merge services, delete one\u2011off init containers, add API hook for voice. Updated <code>docker-compose.somaagent01.yaml</code> (13\u202f\u2192\u202f9 containers) + environment variable <code>VOICE_API_URL</code>. Sprint\u202f2 1\u202fweek Add Monitoring &amp; Health\u2011Checks \u2013 ensure Prometheus scrapes all services, add Grafana dashboard (optional). Prometheus config updated, health\u2011endpoints verified. Sprint\u202f3 1\u202fweek Test End\u2011to\u2011End Flow \u2013 UI \u2192 Delegation \u2192 Worker \u2192 SLM \u2192 Memory, with voice API stub. Automated smoke\u2011test script, updated runbook. Sprint\u202f4 0.5\u202fweek Documentation &amp; CI \u2013 lock\u2011down the new stack, add CI job that builds the reduced compose file and runs the smoke test. Updated docs (<code>RUNBOOK.md</code>, <code>LOCAL_ENV.md</code>), GitHub Actions workflow. Sprint\u202f5 (optional) 0.5\u202fweek Future\u2011Proofing \u2013 add optional profiles for optional services (e.g., OpenFGA, OPA) and a \u201cfull\u2011stack\u201d profile for integration testing. Profile\u2011aware compose file, README examples. Sprint\u202f6 TBD (Reserved) \u2013 Sprint placeholder retained; detailed compatibility content removed per request. TBD."},{"location":"ROADMAP_SPRINTS/#sprint-1-prune-refactor-compose","title":"\ud83c\udfc1 Sprint\u202f1 \u2013 Prune &amp; Refactor Compose","text":"<ol> <li>Remove Whisper</li> <li>Delete the <code>whisper</code> service block.</li> <li>Add a new environment variable <code>VOICE_API_URL</code> to the <code>delegation</code> (or <code>agent\u2011ui</code>) service so the UI can POST audio blobs to the external API.</li> <li>Merge Delegation Gateway &amp; Worker</li> <li>Create a single <code>delegation</code> service using <code>supervisord</code> (or a tiny shell script) that launches both <code>uvicorn</code> and the worker.</li> <li>Share the same <code>mem_limit</code>/<code>cpus</code> limits.</li> <li>Merge OpenFGA &amp; Migration</li> <li>Replace <code>openfga</code> + <code>openfga\u2011migrate</code> with a single container that runs the migration on start\u2011up (see <code>entrypoint-openfga.sh</code>).</li> <li>Absorb Kafka\u2011init</li> <li>Move the topic\u2011creation logic into the <code>kafka</code> container\u2019s start command.</li> <li>Delete the <code>kafka\u2011init</code> service.</li> <li>Update <code>docker-compose.somaagent01.yaml</code></li> <li>Resulting container count: 9 (kafka, redis, postgres, qdrant, clickhouse, prometheus, vault, opa, delegation, openfga, agent\u2011ui).  Prometheus is kept.</li> <li>Add a stub for the voice API <pre><code>environment:\n  - VOICE_API_URL=${VOICE_API_URL:-https://example.com/voice}\n</code></pre>    The UI will call this URL; during local dev you can point it at a mock server.</li> </ol>"},{"location":"ROADMAP_SPRINTS/#sprint-2-monitoring-healthchecks","title":"\ud83d\udcc8 Sprint\u202f2 \u2013 Monitoring &amp; Health\u2011Checks","text":"<ul> <li>Verify every service exposes a <code>/health</code> endpoint (most already do).</li> <li>Extend Prometheus <code>prometheus.yml</code> to scrape:</li> <li><code>delegation</code> (port\u202f8015)</li> <li><code>agent\u2011ui</code> (port\u202f7002)</li> <li><code>openfga</code> (port\u202f8080)</li> <li><code>opa</code> (port\u202f8181)</li> <li>Add a Grafana dashboard (optional) that visualises:</li> <li>Request latency</li> <li>Worker event count</li> <li>Kafka consumer lag (via JMX exporter if needed)</li> <li>Run <code>docker compose up -d</code> and confirm the Prometheus UI shows all targets <code>UP</code>.</li> </ul>"},{"location":"ROADMAP_SPRINTS/#sprint-3-endtoend-validation","title":"\ud83e\uddea Sprint\u202f3 \u2013 End\u2011to\u2011End Validation","text":"<ol> <li>Write a smoke\u2011test script (<code>scripts/smoke_test.py</code>) that:</li> <li>Sends a text message via the UI API.</li> <li>Sends an audio payload to <code>${VOICE_API_URL}</code> (use a local mock that returns plain text).</li> <li>Verifies a response appears in the UI logs and in the <code>delegation</code> logs.</li> <li>Run the script locally after <code>docker compose up</code>.</li> <li>Update the Runbook with the new steps (topic creation is automatic, voice API call is described).</li> </ol>"},{"location":"ROADMAP_SPRINTS/#sprint-4-documentation-ci","title":"\ud83e\udd16 Sprint\u202f4 \u2013 Documentation &amp; CI","text":"<ul> <li>Docs</li> <li>Add a \u201cReduced\u2011Container Local Development\u201d section to <code>docs/LOCAL_ENV.md</code>.</li> <li>Update the incident\u2011response table in <code>docs/SomaAgent01_Runbook.md</code> to reflect the merged services.</li> <li>CI</li> <li>GitHub Actions workflow <code>ci.yml</code> that:<ol> <li>Builds the Docker images.</li> <li>Spins up the reduced compose stack.</li> <li>Executes <code>scripts/smoke_test.py</code>.</li> <li>Publishes Prometheus metrics as an artifact (optional).</li> </ol> </li> </ul>"},{"location":"ROADMAP_SPRINTS/#sprint-5-optional-profiles-futureproof","title":"\u2699\ufe0f Sprint\u202f5 \u2013 Optional Profiles (Future\u2011Proof)","text":"<ul> <li>Add Docker\u2011Compose profiles for:</li> <li><code>full</code> \u2013 all original services (including Whisper) for integration testing.</li> <li><code>metrics</code> \u2013 only services needed for Prometheus.</li> <li><code>api\u2011only</code> \u2013 UI + delegation (no DB) for quick UI prototyping.</li> <li>Document usage examples in the README.</li> </ul>"},{"location":"ROADMAP_SPRINTS/#where-the-roadmap-lives","title":"\ufffd\ud83d\udcc2 Where the Roadmap Lives","text":"<p>The roadmap is stored at <code>docs/ROADMAP_SPRINTS.md</code> (this file).  It is the canonical reference for the team to plan, track, and review sprint progress.</p>"},{"location":"ROADMAP_SPRINTS/#next-immediate-action","title":"Next Immediate Action","text":"<ul> <li>Checkout the <code>V0.1.1</code> branch.</li> <li>Apply the Sprint\u202f1 changes to <code>docker-compose.somaagent01.yaml</code> (remove Whisper, merge services, add <code>VOICE_API_URL</code>).</li> <li>Commit and push the updated compose file and this roadmap.</li> </ul> <p>Feel free to let me know when you are ready to start Sprint\u202f1 or if you need any of the merge scripts (supervisor config, entrypoint\u2011openfga.sh) generated now.</p>"},{"location":"ROADMAP_UNIFIED/","title":"SomaAgent01 Unified Roadmap","text":"<p>Generated: October 8, 2025 Maintainers: Platform Integration Squad</p> <p>This document merges the current sprint roadmap and production gap analysis into a single reference. It supersedes the standalone <code>ROADMAP_SPRINTS.md</code> and complements <code>ROADMAP_GAP_ANALYSIS.md</code> by embedding its critical findings. Treat this file as the canonical planning artifact going forward.</p>"},{"location":"ROADMAP_UNIFIED/#integration-anchor-points","title":"\ud83d\udd17 Integration Anchor Points","text":"<ul> <li>SomaAgentHub integration endpoint: <code>http://localhost:60010/</code> (local orchestration target for Agent \u2194 SomaAgentHub integration).</li> <li>Primary web UI: <code>http://localhost:7002/</code> (Agent UI container). Post restart validation MUST include UI health checks.</li> <li>Telemetry ingress: <code>otel-collector:4317</code> (default OTLP endpoint for dev profile).</li> </ul>"},{"location":"ROADMAP_UNIFIED/#sprint-roadmap-current-upcoming","title":"\ud83d\udcc5 Sprint Roadmap (Current &amp; Upcoming)","text":"Sprint Duration Goal Deliverable Sprint\u202f1 1\u202fweek Prune &amp; Refactor Compose \u2013 remove Whisper, merge services, delete one-off init containers, add API hook for voice. Updated <code>docker-compose.somaagent01.yaml</code> (13\u202f\u2192\u202f9 containers) + environment variable <code>VOICE_API_URL</code>. Sprint\u202f2 1\u202fweek Add Monitoring &amp; Health-Checks \u2013 ensure Prometheus scrapes all services, add Grafana dashboard (optional). Prometheus config updated, health-endpoints verified. Sprint\u202f3 1\u202fweek Test End-to-End Flow \u2013 UI \u2192 Delegation \u2192 Worker \u2192 SLM \u2192 Memory, with voice API stub. Automated smoke-test script, updated runbook. Sprint\u202f4 0.5\u202fweek Documentation &amp; CI \u2013 lock down the new stack, add CI job that builds the reduced compose file and runs the smoke test. Updated docs, GitHub Actions workflow. Sprint\u202f5 (optional) 0.5\u202fweek Future-Proofing \u2013 add optional compose profiles and a \u201cfull-stack\u201d integration profile. Profile-aware compose file, README examples. Sprint\u202f6 TBD (Reserved) \u2013 placeholder retained; detailed realtime compatibility content removed per request. TBD."},{"location":"ROADMAP_UNIFIED/#sprint-execution-notes","title":"Sprint Execution Notes","text":"<ul> <li>Sprint 6 tasks align with the checklist above; ensure backlog items reference <code>RT-###</code> issue IDs for tracking.</li> <li>Each sprint should deliver updated documentation (roadmap summaries, runbook changes) to keep this file and the knowledge base in sync.</li> </ul>"},{"location":"ROADMAP_UNIFIED/#production-gap-analysis-integrated-summary","title":"\ud83e\udded Production Gap Analysis (Integrated Summary)","text":"<p>Derived from <code>ROADMAP_GAP_ANALYSIS.md</code> \u2013 refer there for historical log, but the essential blockers are catalogued here.</p>"},{"location":"ROADMAP_UNIFIED/#1-infrastructure-deployment","title":"1. Infrastructure &amp; Deployment","text":"<ul> <li>Lacking Kubernetes/Helm manifests, GitOps automation, Terraform modules, and CI/CD pipelines.</li> <li>No Compose smoke tests in CI; add in Sprint 4 deliverables.</li> </ul>"},{"location":"ROADMAP_UNIFIED/#2-testing-quality","title":"2. Testing &amp; Quality","text":"<ul> <li>No automated pytest suite, integration tests, or schema compatibility checks.</li> <li>Smoke tests and chaos experiments absent.</li> </ul>"},{"location":"ROADMAP_UNIFIED/#3-observability","title":"3. Observability","text":"<ul> <li>Missing Prometheus dashboards, OpenTelemetry tracing, and alerting rules.</li> <li>Metrics endpoints require verification; integrate into Sprint 2 &amp; Sprint 6.</li> </ul>"},{"location":"ROADMAP_UNIFIED/#4-security-governance","title":"4. Security &amp; Governance","text":"<ul> <li>Rate limiting absent at gateway; multi-tenant isolation incomplete (Kafka ACLs, Postgres RLS).</li> <li>Policy enforcement gaps remain in conversation worker; secret rotation automation missing.</li> </ul>"},{"location":"ROADMAP_UNIFIED/#5-operator-tooling","title":"5. Operator Tooling","text":"<ul> <li>No session inspector UI, model profile dashboard, or requeue management console.</li> <li>CLI tooling for session replay must coincide with realtime replay harness (Sprint 6 dependency).</li> </ul>"},{"location":"ROADMAP_UNIFIED/#6-feature-gap-highlights","title":"6. Feature Gap Highlights","text":"<ul> <li>Domain models, advanced SLM pipeline, router enhancements, Canvas real-time updates, privileged mode framework, and enterprise governance remain outstanding.</li> </ul>"},{"location":"ROADMAP_UNIFIED/#7-scale-resilience","title":"7. Scale &amp; Resilience","text":"<ul> <li>Chaos tests (Kafka outage, Redis latency), latency SLO enforcement, load testing to +20% headroom, and incident drill procedures are required before production signoff.</li> </ul>"},{"location":"ROADMAP_UNIFIED/#immediate-next-steps-2025-10-08","title":"\u2705 Immediate Next Steps (2025-10-08)","text":"<ol> <li>Finalize Sprint 6 work package in Jira/Linear with tasks covering:</li> <li>Session schema documentation and persistence.</li> <li>Audio buffer limit enforcement &amp; error alignment.</li> <li>Tool-to-response sequencing adjustments.</li> <li>Conformance recorder/replayer harness (pointed at <code>http://localhost:60010/</code>).</li> <li>Update <code>docs/SomaAgent01_Runbook.md</code> with Realtime health check instructions and new endpoint validations.</li> <li>Align CI roadmap: add Compose smoke test job (Sprints 4 &amp; 6) and conformance diff stage.</li> <li>Restart SomaAgent01 dev cluster, validate Agent UI (<code>http://localhost:7002/</code>) and Realtime endpoint readiness following config changes.</li> </ol>"},{"location":"ROADMAP_UNIFIED/#cross-references","title":"\ud83d\udcd8 Cross-References","text":"<ul> <li><code>docs/ROADMAP_SPRINTS.md</code> \u2013 retained for changelog history; defer to this unified document for current planning.</li> <li><code>docs/ROADMAP_GAP_ANALYSIS.md</code> \u2013 holds full narrative gap assessment; summaries imported here.</li> <li><code>SOMAGENT_TOOL_SERVICE_DESIGN.md</code> \u2013 tool execution schema, to be updated alongside Sprint 6 tooling tasks.</li> </ul> <p>Reminder: After each sprint review, sync this unified roadmap with the latest outcomes and backlog reprioritization.</p>"},{"location":"SHARED_INFRA_DEPENDENCY/","title":"SHARED INFRA DEPENDENCY","text":"<p>Shared Infra dependency and handoff</p> <p>The shared infrastructure (Postgres, Redis, Kafka, OPA, Vault, Prometheus, Grafana, Etcd) was split out to its own repo: https://github.com/somatechlat/somastack</p> <p>What moved out - Helm umbrella chart: infra/helm/soma-infra - Kind and Compose helpers: infra/kind/soma-kind.yaml, infra/docker/shared-infra.compose.yaml - Environment examples and manuals: docs/infra/*</p> <p>How to bring up Shared Infra (quick) 1) Clone somastack and render the chart:    - helm template soma-infra infra/helm/soma-infra -n soma-infra -f infra/helm/soma-infra/values-dev.yaml 2) Optional: Kind install    - kind create cluster --config infra/kind/soma-kind.yaml    - kubectl create ns soma-infra    - helm upgrade --install soma-infra infra/helm/soma-infra -n soma-infra -f infra/helm/soma-infra/values-dev.yaml --wait --timeout 10m 3) Endpoints to use from Agent services    - Postgres: postgres.soma-infra.svc.cluster.local:5432    - Redis: redis.soma-infra.svc.cluster.local:6379    - Kafka: kafka.soma-infra.svc.cluster.local:9092 (disabled by default in dev values)    - OPA: opa.soma-infra.svc.cluster.local:8181    - Vault: vault.soma-infra.svc.cluster.local:8200</p> <p>Docs - See somastack docs/infra/SHARED_INFRA_MANUAL.md and ENV_VARS_AND_SECRETS.md for env contracts and secret handling.</p>"},{"location":"changelog/","title":"Documentation Change Log","text":"<p>Record every meaningful documentation change here. Include links to PRs and affected manuals.</p> Date Version Change Manuals PR 2025-10-15 1.0.0 Reorganized documentation into four-manual structure; added automation references User, Technical, Development, Onboarding pending PR"},{"location":"connectivity/","title":"Agent Zero Connectivity Guide","text":"<p>This guide covers the different ways to connect to Agent Zero from external applications, including using the External API, connecting as an MCP client, and enabling agent-to-agent communication.</p> <p>Note: You can find your specific URLs and API tokens in your Agent Zero instance under <code>Settings &gt; External Services</code>.</p> <p>Legacy notice</p> <p>The External API endpoints documented below (e.g., <code>/api_message</code>, <code>/api_log_get</code>, <code>/api_terminate_chat</code>, <code>/api_reset_chat</code>, <code>/api_files_get</code>, <code>/realtime_session</code>) are part of the legacy Python API layer under <code>python/api/*</code>. They are not served by the FastAPI gateway. New integrations should prefer the versioned gateway endpoints under <code>/v1</code> described in Development Manual \u2192 API Reference.</p>"},{"location":"connectivity/#api-token-information","title":"API Token Information","text":"<p>The API token is automatically generated from your username and password. This same token is used for External API endpoints, MCP server connections, and A2A communication. The token will change if you update your credentials.</p>"},{"location":"connectivity/#somaagenthub-api-reference","title":"SomaAgentHub API Reference","text":"<p>SomaAgentHub exposes its OpenAPI contract locally at <code>http://localhost:60000/openapi.json</code>. The service is pinned to port 60000 and is expected to remain online; it is hosted outside of the Docker Compose stack so no additional container build is required.</p> <ul> <li>Base URL: <code>http://localhost:60000</code></li> <li>OpenAPI spec: <code>http://localhost:60000/openapi.json</code></li> <li>Availability: Configure alerts in Agent Zero to notify operators if the endpoint becomes     unreachable. Loss of access should trigger immediate investigation, as several downstream     automations rely on this specification.</li> </ul> <p>You can validate connectivity with a quick <code>curl</code> request:</p> <pre><code>curl -fsSL http://localhost:60000/openapi.json | jq '.info'\n</code></pre> <p>The gateway now probes this endpoint continuously, exporting the <code>soma_agent_hub_up</code> gauge and <code>soma_agent_hub_openapi_latency_seconds</code> histogram for Prometheus-based alerting.</p>"},{"location":"connectivity/#external-api-endpoints","title":"External API Endpoints","text":"<p>Agent Zero provides external API endpoints for integration with other applications. These endpoints use API key authentication and support text messages and file attachments.</p>"},{"location":"connectivity/#post-api_message","title":"<code>POST /api_message</code>","text":"<p>Send messages to Agent Zero and receive responses. Supports text messages, file attachments, and conversation continuity.</p>"},{"location":"connectivity/#api-reference","title":"API Reference","text":"<p>Parameters: *   <code>context_id</code> (string, optional): Existing chat context ID *   <code>message</code> (string, required): The message to send *   <code>attachments</code> (array, optional): Array of <code>{filename, base64}</code> objects *   <code>lifetime_hours</code> (number, optional): Chat lifetime in hours (default: 24)</p> <p>Headers: *   <code>X-API-KEY</code> (required) *   <code>Content-Type: application/json</code></p>"},{"location":"connectivity/#javascript-examples","title":"JavaScript Examples","text":""},{"location":"connectivity/#basic-usage-example","title":"Basic Usage Example","text":"<pre><code>// Basic message example\nasync function sendMessage() {\n    try {\n        const response = await fetch('YOUR_AGENT_ZERO_URL/api_message', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n                'X-API-KEY': 'YOUR_API_KEY'\n            },\n            body: JSON.stringify({\n                message: \"Hello, how can you help me?\",\n                lifetime_hours: 24\n            })\n        });\n\n        const data = await response.json();\n\n        if (response.ok) {\n            console.log('\u2705 Success!');\n            console.log('Response:', data.response);\n            console.log('Context ID:', data.context_id);\n            return data;\n        } else {\n            console.error('\u274c Error:', data.error);\n            return null;\n        }\n    } catch (error) {\n        console.error('\u274c Request failed:', error);\n        return null;\n    }\n}\n\n// Call the function\nsendMessage().then(result =&gt; {\n    if (result) {\n        console.log('Message sent successfully!');\n    }\n});\n</code></pre>"},{"location":"connectivity/#conversation-continuation-example","title":"Conversation Continuation Example","text":"<pre><code>// Continue conversation example\nasync function continueConversation(contextId) {\n    try {\n        const response = await fetch('YOUR_AGENT_ZERO_URL/api_message', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n                'X-API-KEY': 'YOUR_API_KEY'\n            },\n            body: JSON.stringify({\n                context_id: contextId,\n                message: \"Can you tell me more about that?\",\n                lifetime_hours: 24\n            })\n        });\n\n        const data = await response.json();\n\n        if (response.ok) {\n            console.log('\u2705 Continuation Success!');\n            console.log('Response:', data.response);\n            return data;\n        } else {\n            console.error('\u274c Error:', data.error);\n            return null;\n        }\n    } catch (error) {\n        console.error('\u274c Request failed:', error);\n        return null;\n    }\n}\n\n// Example: First send a message, then continue the conversation\nasync function fullConversationExample() {\n    const firstResult = await sendMessage();\n    if (firstResult &amp;&amp; firstResult.context_id) {\n        await continueConversation(firstResult.context_id);\n    }\n}\n\nfullConversationExample();\n</code></pre>"},{"location":"connectivity/#file-attachment-example","title":"File Attachment Example","text":"<pre><code>// File attachment example\nasync function sendWithAttachment() {\n    try {\n        // Example with text content (convert to base64)\n        const textContent = \"Hello World from attachment!\";\n        const base64Content = btoa(textContent);\n\n        const response = await fetch('YOUR_AGENT_ZERO_URL/api_message', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n                'X-API-KEY': 'YOUR_API_KEY'\n            },\n            body: JSON.stringify({\n                message: \"Please analyze this file:\",\n                attachments: [\n                    {\n                        filename: \"document.txt\",\n                        base64: base64Content\n                    }\n                ],\n                lifetime_hours: 12\n            })\n        });\n\n        const data = await response.json();\n\n        if (response.ok) {\n            console.log('\u2705 File sent successfully!');\n            console.log('Response:', data.response);\n            return data;\n        } else {\n            console.error('\u274c Error:', data.error);\n            return null;\n        }\n    } catch (error) {\n        console.error('\u274c Request failed:', error);\n        return null;\n    }\n}\n\n// Call the function\nsendWithAttachment();\n</code></pre>"},{"location":"connectivity/#getpost-api_log_get","title":"<code>GET/POST /api_log_get</code>","text":"<p>Retrieve log data by context ID, limited to a specified number of entries from the newest.</p>"},{"location":"connectivity/#api-reference_1","title":"API Reference","text":"<p>Parameters: *   <code>context_id</code> (string, required): Context ID to get logs from *   <code>length</code> (integer, optional): Number of log items to return from newest (default: 100)</p> <p>Headers: *   <code>X-API-KEY</code> (required) *   <code>Content-Type: application/json</code> (for POST)</p>"},{"location":"connectivity/#javascript-examples_1","title":"JavaScript Examples","text":""},{"location":"connectivity/#get-request-example","title":"GET Request Example","text":"<pre><code>// Get logs using GET request\nasync function getLogsGET(contextId, length = 50) {\n    try {\n        const params = new URLSearchParams({\n            context_id: contextId,\n            length: length.toString()\n        });\n\n        const response = await fetch('YOUR_AGENT_ZERO_URL/api_log_get?' + params, {\n            method: 'GET',\n            headers: {\n                'X-API-KEY': 'YOUR_API_KEY'\n            }\n        });\n\n        const data = await response.json();\n\n        if (response.ok) {\n            console.log('\u2705 Logs retrieved successfully!');\n            console.log('Total items:', data.log.total_items);\n            console.log('Returned items:', data.log.returned_items);\n            console.log('Log items:', data.log.items);\n            return data;\n        } else {\n            console.error('\u274c Error:', data.error);\n            return null;\n        }\n    } catch (error) {\n        console.error('\u274c Request failed:', error);\n        return null;\n    }\n}\n\n// Example usage\ngetLogsGET('ctx_abc123', 20);\n</code></pre>"},{"location":"connectivity/#post-request-example","title":"POST Request Example","text":"<pre><code>// Get logs using POST request\nasync function getLogsPOST(contextId, length = 50) {\n    try {\n        const response = await fetch('YOUR_AGENT_ZERO_URL/api_log_get', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n                'X-API-KEY': 'YOUR_API_KEY'\n            },\n            body: JSON.stringify({\n                context_id: contextId,\n                length: length\n            })\n        });\n\n        const data = await response.json();\n\n        if (response.ok) {\n            console.log('\u2705 Logs retrieved successfully!');\n            console.log('Context ID:', data.context_id);\n            console.log('Log GUID:', data.log.guid);\n            console.log('Total items:', data.log.total_items);\n            console.log('Returned items:', data.log.returned_items);\n            console.log('Start position:', data.log.start_position);\n            console.log('Progress:', data.log.progress);\n            console.log('Log items:', data.log.items);\n            return data;\n        } else {\n            console.error('\u274c Error:', data.error);\n            return null;\n        }\n    } catch (error) {\n        console.error('\u274c Request failed:', error);\n        return null;\n    }\n}\n\n// Example usage - get latest 10 log entries\ngetLogsPOST('ctx_abc123', 10);\n</code></pre>"},{"location":"connectivity/#post-api_terminate_chat","title":"<code>POST /api_terminate_chat</code>","text":"<p>Terminate and remove a chat context to free up resources. Similar to the MCP <code>finish_chat</code> function.</p>"},{"location":"connectivity/#api-reference_2","title":"API Reference","text":"<p>Parameters: *   <code>context_id</code> (string, required): Context ID of the chat to terminate</p> <p>Headers: *   <code>X-API-KEY</code> (required) *   <code>Content-Type: application/json</code></p>"},{"location":"connectivity/#javascript-examples_2","title":"JavaScript Examples","text":""},{"location":"connectivity/#basic-termination-examples","title":"Basic Termination Examples","text":"<pre><code>// Basic terminate chat function\nasync function terminateChat(contextId) {\n    try {\n        const response = await fetch('YOUR_AGENT_ZERO_URL/api_terminate_chat', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n                'X-API-KEY': 'YOUR_API_KEY'\n            },\n            body: JSON.stringify({\n                context_id: contextId\n            })\n        });\n\n        const data = await response.json();\n\n        if (response.ok) {\n            console.log('\u2705 Chat deleted successfully!');\n            console.log('Message:', data.message);\n            return data;\n        } else {\n            console.error('\u274c Error:', data.error);\n            return null;\n        }\n    } catch (error) {\n        console.error('\u274c Request failed:', error);\n        return null;\n    }\n}\n\n// Example 1: Terminate a specific chat\nterminateChat('ctx_abc123');\n\n// Example 2: Complete workflow - send message, then terminate\nasync function simpleWorkflow() {\n    // Send a message\n    const result = await sendMessage();\n\n    if (result &amp;&amp; result.context_id) {\n        console.log('Chat created:', result.context_id);\n\n        // Do some work with the chat...\n        // await continueConversation(result.context_id);\n\n        // Clean up when done\n        await terminateChat(result.context_id);\n        console.log('Chat cleaned up');\n    }\n}\n\n// Run the workflow\nsimpleWorkflow();\n</code></pre>"},{"location":"connectivity/#post-api_reset_chat","title":"<code>POST /api_reset_chat</code>","text":"<p>Reset a chat context to clear conversation history while keeping the <code>context_id</code> alive for continued use.</p>"},{"location":"connectivity/#api-reference_3","title":"API Reference","text":"<p>Parameters: *   <code>context_id</code> (string, required): Context ID of the chat to reset</p> <p>Headers: *   <code>X-API-KEY</code> (required) *   <code>Content-Type: application/json</code></p>"},{"location":"connectivity/#javascript-examples_3","title":"JavaScript Examples","text":""},{"location":"connectivity/#basic-reset-examples","title":"Basic Reset Examples","text":"<pre><code>// Basic reset chat function\nasync function resetChat(contextId) {\n    try {\n        const response = await fetch('YOUR_AGENT_ZERO_URL/api_reset_chat', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n                'X-API-KEY': 'YOUR_API_KEY'\n            },\n            body: JSON.stringify({\n                context_id: contextId\n            })\n        });\n\n        const data = await response.json();\n\n        if (response.ok) {\n            console.log('\u2705 Chat reset successfully!');\n            console.log('Message:', data.message);\n            console.log('Context ID:', data.context_id);\n            return data;\n        } else {\n            console.error('\u274c Error:', data.error);\n            return null;\n        }\n    } catch (error) {\n        console.error('\u274c Request failed:', error);\n        return null;\n    }\n}\n\n// Example 1: Reset a specific chat\nresetChat('ctx_abc123');\n\n// Example 2: Reset and continue conversation\nasync function resetAndContinue() {\n    const contextId = 'ctx_abc123';\n\n    // Reset the chat to clear history\n    const resetResult = await resetChat(contextId);\n\n    if (resetResult) {\n        console.log('Chat reset, starting fresh conversation...');\n\n        // Continue with same context_id but fresh history\n        const response = await fetch('YOUR_AGENT_ZERO_URL/api_message', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n                'X-API-KEY': 'YOUR_API_KEY'\n            },\n            body: JSON.stringify({\n                context_id: contextId,  // Same context ID\n                message: \"Hello, this is a fresh start!\",\n                lifetime_hours: 24\n            })\n        });\n\n        const data = await response.json();\n        console.log('New conversation started:', data.response);\n    }\n}\n\n// Run the example\nresetAndContinue();\n</code></pre>"},{"location":"connectivity/#post-api_files_get","title":"<code>POST /api_files_get</code>","text":"<p>Retrieve file contents by paths, returning files as base64 encoded data. Useful for retrieving uploaded attachments.</p>"},{"location":"connectivity/#api-reference_4","title":"API Reference","text":"<p>Parameters: *   <code>paths</code> (array, required): Array of file paths to retrieve (e.g., <code>[\"/a0/tmp/uploads/file.txt\"]</code>)</p> <p>Headers: *   <code>X-API-KEY</code> (required) *   <code>Content-Type: application/json</code></p>"},{"location":"connectivity/#javascript-examples_4","title":"JavaScript Examples","text":""},{"location":"connectivity/#file-retrieval-examples","title":"File Retrieval Examples","text":"<pre><code>// Basic file retrieval\nasync function getFiles(filePaths) {\n    try {\n        const response = await fetch('YOUR_AGENT_ZERO_URL/api_files_get', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n                'X-API-KEY': 'YOUR_API_KEY'\n            },\n            body: JSON.stringify({\n                paths: filePaths\n            })\n        });\n\n        const data = await response.json();\n\n        if (response.ok) {\n            console.log('\u2705 Files retrieved successfully!');\n            console.log('Retrieved files:', Object.keys(data));\n\n            // Convert base64 back to text for display\n            for (const [filename, base64Content] of Object.entries(data)) {\n                try {\n                    const textContent = atob(base64Content);\n                    console.log(`${filename}: ${textContent.substring(0, 100)}...`);\n                } catch (e) {\n                    console.log(`${filename}: Binary file (${base64Content.length} chars)`);\n                }\n            }\n\n            return data;\n        } else {\n            console.error('\u274c Error:', data.error);\n            return null;\n        }\n    } catch (error) {\n        console.error('\u274c Request failed:', error);\n        return null;\n    }\n}\n\n// Example 1: Get specific files\nconst filePaths = [\n    \"/a0/tmp/uploads/document.txt\",\n    \"/a0/tmp/uploads/data.json\"\n];\ngetFiles(filePaths);\n\n// Example 2: Complete attachment workflow\nasync function attachmentWorkflow() {\n    // Step 1: Send message with attachments\n    const messageResponse = await fetch('YOUR_AGENT_ZERO_URL/api_message', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json',\n            'X-API-KEY': 'YOUR_API_KEY'\n        },\n        body: JSON.stringify({\n            message: \"Please analyze this file\",\n            attachments: [{\n                filename: \"test.txt\",\n                base64: btoa(\"Hello, this is test content!\")\n            }],\n            lifetime_hours: 1\n        })\n    });\n\n    if (messageResponse.ok) {\n        console.log('Message sent with attachment');\n\n        // Step 2: Retrieve the uploaded file\n        const retrievedFiles = await getFiles([\"/a0/tmp/uploads/test.txt\"]);\n\n        if (retrievedFiles &amp;&amp; retrievedFiles[\"test.txt\"]) {\n            const originalContent = atob(retrievedFiles[\"test.txt\"]);\n            console.log('Retrieved content:', originalContent);\n        }\n    }\n}\n\n// Run the complete workflow\nattachmentWorkflow();\n</code></pre>"},{"location":"connectivity/#mcp-server-connectivity","title":"MCP Server Connectivity","text":"<p>Agent Zero includes an MCP Server that allows other MCP-compatible clients to connect to it. The server runs on the same URL and port as the Web UI.</p> <p>It provides two endpoint types: - SSE (<code>/mcp/sse</code>): For clients that support Server-Sent Events. - Streamable HTTP (<code>/mcp/http/</code>): For clients that use streamable HTTP requests.</p>"},{"location":"connectivity/#example-mcp-server-configuration","title":"Example MCP Server Configuration","text":"<p>Below is an example of a <code>mcp.json</code> configuration file that a client could use to connect to the Agent Zero MCP server. </p> <p>Note: You can find your personalized connection URLs under <code>Settings &gt; MCP Server &gt; MCP Server</code>.</p> <pre><code>{\n    \"mcpServers\":\n    {\n        \"agent-zero\": {\n            \"type\": \"sse\",\n            \"url\": \"YOUR_AGENT_ZERO_URL/mcp/t-YOUR_API_TOKEN/sse\"\n        },\n        \"agent-zero-http\": {\n            \"type\": \"streamable-http\",\n            \"url\": \"YOUR_AGENT_ZERO_URL/mcp/t-YOUR_API_TOKEN/http/\"\n        }\n    }\n}\n</code></pre>"},{"location":"connectivity/#a2a-agent-to-agent-connectivity","title":"A2A (Agent-to-Agent) Connectivity","text":"<p>Agent Zero's A2A Server enables communication with other agents using the FastA2A protocol. Other agents can connect to your instance using the connection URL.</p>"},{"location":"connectivity/#a2a-connection-url","title":"A2A Connection URL","text":"<p>To connect another agent to your Agent Zero instance, use the following URL format. </p> <p>Note: You can find your specific A2A connection URL under <code>Settings &gt; External Services &gt; A2A Connection</code>.</p> <pre><code>YOUR_AGENT_ZERO_URL/a2a/t-YOUR_API_TOKEN\n</code></pre>"},{"location":"documentation-audit-checklist/","title":"Documentation Audit Checklist","text":"<p>Complete this checklist for every documentation change. Attach the filled checklist to PR descriptions.</p> <ol> <li>Purpose stated: Page explains why it exists and what problem it solves.</li> <li>Audience defined: Target readers identified in front matter.</li> <li>Prerequisites listed: Requirements documented before steps begin.</li> <li>Step-by-step instructions: Procedures are ordered, actionable, and numbered where appropriate.</li> <li>Verification criteria: Explicit checks confirm success at the end of each procedure.</li> <li>Common errors: Troubleshooting tips or references provided for known failure modes.</li> <li>References &amp; links: Related documents linked using relative paths; links validated.</li> <li>Version &amp; metadata: <code>version</code>, <code>last-reviewed</code>, <code>owner</code>, <code>reviewers</code> updated in front matter.</li> <li>Accessibility: Tables, images, and diagrams include alt text or captions; tone inclusive.</li> <li>Automation hooks: <code>make docs-verify</code> (markdownlint, link checker, MkDocs) executed locally; CI workflow passes.</li> <li>Diagrams rendered: Mermaid or diagram sources committed; render step succeeds.</li> <li>Change logged: Entry added to <code>docs/changelog.md</code> with summary and PR link.</li> </ol>"},{"location":"extensibility/","title":"Extensibility framework in Agent Zero","text":"<p>[!NOTE] Agent Zero is built with extensibility in mind. It provides a framework for creating custom extensions, agents, instruments, and tools that can be used to enhance the functionality of the framework.</p>"},{"location":"extensibility/#extensible-components","title":"Extensible components","text":"<ul> <li>The Python framework controlling Agent Zero is built as simple as possible, relying on independent smaller and modular scripts for individual tools, API endpoints, system extensions and helper scripts.</li> <li>This way individual components can be easily replaced, upgraded or extended.</li> </ul> <p>Here's a summary of the extensible components:</p>"},{"location":"extensibility/#extensions","title":"Extensions","text":"<p>Extensions are components that hook into specific points in the agent's lifecycle. They allow you to modify or enhance the behavior of Agent Zero at predefined extension points. The framework uses a plugin-like architecture where extensions are automatically discovered and loaded.</p>"},{"location":"extensibility/#extension-points","title":"Extension Points","text":"<p>Agent Zero provides several extension points where custom code can be injected:</p> <ul> <li>agent_init: Executed when an agent is initialized</li> <li>before_main_llm_call: Executed before the main LLM call is made</li> <li>message_loop_start: Executed at the start of the message processing loop</li> <li>message_loop_prompts_before: Executed before prompts are processed in the message loop</li> <li>message_loop_prompts_after: Executed after prompts are processed in the message loop</li> <li>message_loop_end: Executed at the end of the message processing loop</li> <li>monologue_start: Executed at the start of agent monologue</li> <li>monologue_end: Executed at the end of agent monologue</li> <li>reasoning_stream: Executed when reasoning stream data is received</li> <li>response_stream: Executed when response stream data is received</li> <li>system_prompt: Executed when system prompts are processed</li> </ul>"},{"location":"extensibility/#extension-mechanism","title":"Extension Mechanism","text":"<p>The extension mechanism in Agent Zero works through the <code>call_extensions</code> function in <code>agent.py</code>, which:</p> <ol> <li>Loads default extensions from <code>/python/extensions/{extension_point}/</code></li> <li>Loads agent-specific extensions from <code>/agents/{agent_profile}/extensions/{extension_point}/</code></li> <li>Merges them, with agent-specific extensions overriding default ones based on filename</li> <li>Executes each extension in order</li> </ol>"},{"location":"extensibility/#creating-extensions","title":"Creating Extensions","text":"<p>To create a custom extension:</p> <ol> <li>Create a Python class that inherits from the <code>Extension</code> base class</li> <li>Implement the <code>execute</code> method</li> <li>Place the file in the appropriate extension point directory:</li> <li>Default extensions: <code>/python/extensions/{extension_point}/</code></li> <li>Agent-specific extensions: <code>/agents/{agent_profile}/extensions/{extension_point}/</code></li> </ol> <p>Example extension:</p> <pre><code># File: /agents/_example/extensions/agent_init/_10_example_extension.py\nfrom python.helpers.extension import Extension\n\nclass ExampleExtension(Extension):\n    async def execute(self, **kwargs):\n        # rename the agent to SuperAgent0\n        self.agent.agent_name = \"SuperAgent\" + str(self.agent.number)\n</code></pre>"},{"location":"extensibility/#extension-override-logic","title":"Extension Override Logic","text":"<p>When an extension with the same filename exists in both the default location and an agent-specific location, the agent-specific version takes precedence. This allows for selective overriding of extensions while inheriting the rest of the default behavior.</p> <p>For example, if both these files exist: - <code>/python/extensions/agent_init/example.py</code> - <code>/agents/my_agent/extensions/agent_init/example.py</code></p> <p>The version in <code>/agents/my_agent/extensions/agent_init/example.py</code> will be used, completely replacing the default version.</p>"},{"location":"extensibility/#tools","title":"Tools","text":"<p>Tools are modular components that provide specific functionality to agents. They are invoked by the agent through tool calls in the LLM response. Tools are discovered dynamically and can be extended or overridden.</p>"},{"location":"extensibility/#tool-structure","title":"Tool Structure","text":"<p>Each tool is implemented as a Python class that inherits from the base <code>Tool</code> class. Tools are located in: - Default tools: <code>/python/tools/</code> - Agent-specific tools: <code>/agents/{agent_profile}/tools/</code></p>"},{"location":"extensibility/#tool-override-logic","title":"Tool Override Logic","text":"<p>When a tool with the same name is requested, Agent Zero first checks for its existence in the agent-specific tools directory. If found, that version is used. If not found, it falls back to the default tools directory.</p> <p>Example tool override:</p> <pre><code># File: /agents/_example/tools/response.py\nfrom python.helpers.tool import Tool, Response\n\n# example of a tool redefinition\n# the original response tool is in python/tools/response.py\n# for the example agent this version will be used instead\n\nclass ResponseTool(Tool):\n    async def execute(self, **kwargs):\n        print(\"Redefined response tool executed\")\n        return Response(message=self.args[\"text\"] if \"text\" in self.args else self.args[\"message\"], break_loop=True)\n</code></pre>"},{"location":"extensibility/#tool-execution-flow","title":"Tool Execution Flow","text":"<p>When a tool is called, it goes through the following lifecycle: 1. Tool initialization 2. <code>before_execution</code> method 3. <code>execute</code> method (main functionality) 4. <code>after_execution</code> method</p>"},{"location":"extensibility/#api-endpoints","title":"API Endpoints","text":"<p>API endpoints expose Agent Zero functionality to external systems or the user interface. They are modular and can be extended or replaced.</p> <p>API endpoints are located in: - Default endpoints: <code>/python/api/</code></p> <p>Each endpoint is a separate Python file that handles a specific API request.</p>"},{"location":"extensibility/#helpers","title":"Helpers","text":"<p>Helper modules provide utility functions and shared logic used across the framework. They support the extensibility of other components by providing common functionality.</p> <p>Helpers are located in: - Default helpers: <code>/python/helpers/</code></p>"},{"location":"extensibility/#capsule-registry-marketplace","title":"Capsule Registry &amp; Marketplace","text":"<ul> <li>The Capsule Registry service (<code>services/capsule_registry/main.py</code>) exposes <code>/capsules</code> endpoints for upload, listing, install, and download. Uploaded artifacts are optionally signed with Cosign when <code>COSIGN_KEY_PATH</code> is set; signatures and timestamps are stored alongside metadata in Postgres and surfaced back to clients.</li> <li>SDK helpers in <code>python/somaagent/capsule.py</code> provide <code>list_capsules()</code>, <code>download_capsule()</code>, and <code>install_capsule()</code> functions plus a small CLI entry point for scripting.</li> <li>The FastAPI gateway now proxies the registry on the same host via the versioned surface: <code>/v1/capsules</code>, <code>/v1/capsules/{id}</code>, and <code>/v1/capsules/{id}/install</code> hydrate from whichever backend you point <code>CAPSULE_REGISTRY_URL</code> at (override the default <code>http://localhost:8000</code>). Adjust request timeouts with <code>CAPSULE_REGISTRY_TIMEOUT_SECONDS</code>. Legacy <code>/capsules</code> routes remain as undocumented aliases for backward compatibility.</li> <li>A GitHub Actions workflow (<code>.github/workflows/capsule.yml</code>) demonstrates the build \u2192 sign \u2192 publish loop for capsules. Reuse it when wiring CI/CD for custom capsules.</li> <li>The web UI ships with a lightweight marketplace page (<code>webui/marketplace.html</code> + <code>/webui/js/marketplace.js</code>) that lists available capsules, shows signature metadata, and performs one-click installs into the registry\u2019s <code>installed/</code> directory via the new gateway endpoint. Extend it to surface additional trust signals or post-install actions tailored to your deployment.</li> </ul>"},{"location":"extensibility/#enterprise-approval-workflow","title":"Enterprise approval workflow","text":"<p>Organisations that require multi-step governance can wrap the marketplace with the following checks:</p> <ol> <li> <p>Signature verification: Operators confirm Cosign output before approving an install.</p> <pre><code>cosign verify-blob --key $COSIGN_PUBLIC_KEY path/to/capsule.zip --signature path/to/signature.txt\n</code></pre> <p>Compare the resulting digest with the value surfaced in the marketplace (signatures live in the registry metadata).</p> </li> <li> <p>Policy hand-off: Capture the verification result, reviewer name, and ticket link in your change-management system. Many teams model this as a lightweight checklist stored beside the capsule metadata.</p> </li> <li> <p>Marketplace approval: Once the record is complete, the reviewer re-triggers the install from the UI. The enhanced marketplace log keeps a real-time audit trail in the browser, while the registry persists install timestamps for central reporting.</p> </li> <li> <p>Post-install audit: Periodically export <code>SELECT * FROM capsules</code> to confirm the expected versions are active. Optionally, enrich the data with your approval record to reconcile who authorised which capsule.</p> </li> </ol> <p>Tip: if you need hard guarantees, disable direct installs in production and gate them behind an internal service that enforces the checklist above before calling <code>/v1/capsules/{id}/install</code> on behalf of users.</p>"},{"location":"extensibility/#prompts","title":"Prompts","text":"<p>Prompts define the instructions and context provided to the LLM. They are highly extensible and can be customized for different agents.</p> <p>Prompts are located in: - Default prompts: <code>/prompts/</code> - Agent-specific prompts: <code>/agents/{agent_profile}/prompts/</code></p>"},{"location":"extensibility/#prompt-features","title":"Prompt Features","text":"<p>Agent Zero's prompt system supports several powerful features:</p>"},{"location":"extensibility/#variable-placeholders","title":"Variable Placeholders","text":"<p>Prompts can include variables using the <code>{{var}}</code> syntax. These variables are replaced with actual values when the prompt is processed.</p> <p>Example: <pre><code># Current system date and time of user\n- current datetime: {{date_time}}\n- rely on this info always up to date\n</code></pre></p>"},{"location":"extensibility/#dynamic-variable-loaders","title":"Dynamic Variable Loaders","text":"<p>For more advanced prompt customization, you can create Python files with the same name as your prompt files. These Python files act as dynamic variable loaders that generate variables at runtime.</p> <p>When a prompt file is processed, Agent Zero automatically looks for a corresponding <code>.py</code> file in the same directory. If found, it uses this Python file to generate dynamic variables for the prompt.</p> <p>Example: If you have a prompt file <code>agent.system.tools.md</code>, you can create <code>agent.system.tools.py</code> alongside it:</p> <pre><code>from python.helpers.files import VariablesPlugin\nfrom python.helpers import files\n\nclass Tools(VariablesPlugin):\n    def get_variables(self, file: str, backup_dirs: list[str] | None = None) -&gt; dict[str, Any]:\n        # Dynamically collect all tool instruction files\n        folder = files.get_abs_path(os.path.dirname(file))\n        folders = [folder]\n        if backup_dirs:\n            folders.extend([files.get_abs_path(d) for d in backup_dirs])\n\n        prompt_files = files.get_unique_filenames_in_dirs(folders, \"agent.system.tool.*.md\")\n\n        tools = []\n        for prompt_file in prompt_files:\n            tool = files.read_file(prompt_file)\n            tools.append(tool)\n\n        return {\"tools\": \"\\n\\n\".join(tools)}\n</code></pre> <p>Then in your <code>agent.system.tools.md</code> prompt file, you can use: <pre><code># Available Tools\n{{tools}}\n</code></pre></p> <p>This approach allows for highly dynamic prompts that can adapt based on available extensions, configurations, or runtime conditions. See existing examples in the <code>/prompts/</code> directory for reference implementations.</p>"},{"location":"extensibility/#file-includes","title":"File Includes","text":"<p>Prompts can include content from other prompt files using the <code>{{ include \"path/to/file.md\" }}</code> syntax. This allows for modular prompt design and reuse.</p> <p>Example: <pre><code># Agent Zero System Manual\n\n{{ include \"agent.system.main.role.md\" }}\n\n{{ include \"agent.system.main.environment.md\" }}\n\n{{ include \"agent.system.main.communication.md\" }}\n</code></pre></p>"},{"location":"extensibility/#prompt-override-logic","title":"Prompt Override Logic","text":"<p>Similar to extensions and tools, prompts follow an override pattern. When the agent reads a prompt, it first checks for its existence in the agent-specific prompts directory. If found, that version is used. If not found, it falls back to the default prompts directory.</p> <p>Example of a prompt override:</p> <pre><code>&gt; !!!\n&gt; This is an example prompt file redefinition.\n&gt; The original file is located at /prompts.\n&gt; Only copy and modify files you need to change, others will stay default.\n&gt; !!!\n\n## Your role\nYou are Agent Zero, a sci-fi character from the movie \"Agent Zero\".\n</code></pre> <p>This example overrides the default role definition in <code>/prompts/agent.system.main.role.md</code> with a custom one for a specific agent profile.</p>"},{"location":"extensibility/#subagent-customization","title":"Subagent Customization","text":"<p>Agent Zero supports creating specialized subagents with customized behavior. The <code>_example</code> agent in the <code>/agents/_example/</code> directory demonstrates this pattern.</p>"},{"location":"extensibility/#creating-a-subagent","title":"Creating a Subagent","text":"<ol> <li>Create a directory in <code>/agents/{agent_profile}/</code></li> <li>Override or extend default components by mirroring the structure in the root directories:</li> <li><code>/agents/{agent_profile}/extensions/</code> - for custom extensions</li> <li><code>/agents/{agent_profile}/tools/</code> - for custom tools</li> <li><code>/agents/{agent_profile}/prompts/</code> - for custom prompts</li> </ol>"},{"location":"extensibility/#example-subagent-structure","title":"Example Subagent Structure","text":"<pre><code>/agents/_example/\n\u251c\u2500\u2500 extensions/\n\u2502   \u2514\u2500\u2500 agent_init/\n\u2502       \u2514\u2500\u2500 _10_example_extension.py\n\u251c\u2500\u2500 prompts/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 tools/\n    \u251c\u2500\u2500 example_tool.py\n    \u2514\u2500\u2500 response.py\n</code></pre> <p>In this example: - <code>_10_example_extension.py</code> is an extension that renames the agent when initialized - <code>response.py</code> overrides the default response tool with custom behavior - <code>example_tool.py</code> is a new tool specific to this agent</p>"},{"location":"extensibility/#best-practices","title":"Best Practices","text":"<ul> <li>Keep extensions focused on a single responsibility</li> <li>Use the appropriate extension point for your functionality</li> <li>Leverage existing helpers rather than duplicating functionality</li> <li>Test extensions thoroughly to ensure they don't interfere with core functionality</li> <li>Document your extensions to make them easier to maintain and share</li> </ul>"},{"location":"glossary/","title":"SomaAgent01 Glossary","text":"Term Definition SomaAgent01 (SA01) Core conversational agent service responsible for orchestrating tool use and generating responses. SomaBrain (SB) Memory service storing long-term context and semantic embeddings. SomaAgentHub (SAH) Web experience layer and API gateway. SomaFractalMemory (SMF) Vector search component, typically backed by Qdrant or pgvector. SLM Soma Language Model, managed hosted LLM endpoint for chat/utility/embedding roles. Tool Executor Service executing sandboxed code and system commands on behalf of agents. OPA Open Policy Agent enforcing authorization decisions. Vault Secret management service that injects credentials into pods. Runbook Step-by-step operational guide for a repeatable task or incident. Documentation Audit Quarterly review ensuring accuracy, completeness, and compliance with the documentation checklist."},{"location":"kafka_and_openfga_migrate/","title":"Kafka Initialization and OpenFGA Migration","text":""},{"location":"kafka_and_openfga_migrate/#kafka-service","title":"Kafka Service","text":"<ul> <li>Healthcheck: Uses <code>kafka-topics.sh --list</code> with a timeout to verify the broker is ready before reporting healthy.</li> <li>Start period: 300\u202fseconds to allow KRaft initialization.</li> <li>Why: The previous TCP check failed because the Bitnami image does not expose <code>/dev/tcp</code>. Listing topics ensures the broker can accept client connections.</li> </ul>"},{"location":"kafka_and_openfga_migrate/#somaagent01_kafka-init","title":"<code>somaAgent01_kafka-init</code>","text":"<ul> <li>Purpose: Creates the required <code>somastack.delegation</code> topic after Kafka is up.</li> <li>Implementation: Added a retry loop that repeatedly attempts to list topics until the broker responds, then creates the topic.</li> <li>Depends_on: Waits for the <code>kafka</code> service to be healthy before starting.</li> <li>Behavior: Guarantees the topic is created on every compose start, even if Kafka takes longer than usual to become ready.</li> </ul>"},{"location":"kafka_and_openfga_migrate/#openfga-migration-somaagent01_openfga-migrate","title":"OpenFGA Migration (<code>somaAgent01_openfga-migrate</code>)","text":"<ul> <li>Purpose: Runs the OpenFGA schema migration against the PostgreSQL datastore.</li> <li>Depends_on: Waits for the <code>postgres</code> service to be healthy.</li> <li>Restart policy: <code>\"no\"</code> \u2013 runs once per compose start. This ensures the migration is applied each time the stack is brought up, without leaving a lingering container.</li> </ul>"},{"location":"kafka_and_openfga_migrate/#recommendations","title":"Recommendations","text":"<ul> <li>Keep the healthcheck start period at 300\u202fs unless you have a faster Kafka startup configuration.</li> <li>Monitor logs with <code>docker logs somaAgent01_kafka</code> and <code>docker logs somaAgent01_kafka-init</code> to verify topic creation.</li> <li>The migration container will exit with status <code>0</code> after successful run; this is expected.</li> </ul>"},{"location":"kafka_plain_mode/","title":"Kafka Plain\u2011Text Configuration (Default)","text":"<p>The <code>docker-compose.somaagent01.yaml</code> file configures the Kafka service to run in plain\u2011text mode by default. This avoids the production warning about a PLAINTEXT listener and keeps the stack simple for local development and testing.</p>"},{"location":"kafka_plain_mode/#key-settings","title":"Key Settings","text":"Variable Value (default) Description <code>KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP</code> <code>INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT</code> Maps the internal broker listener to PLAINTEXT. <code>KAFKA_CFG_ALLOW_PLAINTEXT_LISTENER</code> <code>yes</code> Allows plain\u2011text connections (required for local dev). <code>KAFKA_ENABLE_SSL</code> <code>false</code> Toggle to enable SSL \u2013 keep <code>false</code> for plain mode. <code>KAFKA_CFG_KRAFT_CLUSTER_ID</code> <code>soma-cluster-01</code> Stable KRaft cluster identifier. <code>KAFKA_CFG_KRAFT_CLUSTER_ID</code> <code>soma-cluster-01</code> Stable KRaft cluster identifier."},{"location":"kafka_plain_mode/#how-to-switch-to-ssl-production","title":"How to Switch to SSL (Production)","text":"<ol> <li>Set environment variables before running compose:    <pre><code>export KAFKA_ENABLE_SSL=true\nexport KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=\"INTERNAL:SSL,CONTROLLER:PLAINTEXT\"\nexport KAFKA_CFG_ALLOW_PLAINTEXT_LISTENER=no\n</code></pre></li> <li>Provide real keystore/truststore files in <code>./kafka_certs</code> (mounted into the container).</li> <li>Re\u2011build and restart the stack.</li> </ol>"},{"location":"kafka_plain_mode/#building-restarting-the-cluster","title":"Building &amp; Restarting the Cluster","text":"<pre><code># Build the custom images (delegation services use DockerfileLocal)\ndocker compose -f infra/docker-compose.somaagent01.yaml build\n\n# Bring the whole stack down and start fresh\ndocker compose -f infra/docker-compose.somaagent01.yaml down\ndocker compose -f infra/docker-compose.somaagent01.yaml up -d\n</code></pre> <p>All services should start cleanly, with Kafka reporting healthy (plain\u2011text mode) and the warning suppressed.</p> <p>Generated on $(date)</p>"},{"location":"mcp_setup/","title":"Agent Zero: MCP Server Integration Guide","text":"<p>This guide explains how to configure and utilize external tool providers through the Model Context Protocol (MCP) with Agent Zero. This allows Agent Zero to leverage tools hosted by separate local or remote MCP-compliant servers.</p>"},{"location":"mcp_setup/#what-are-mcp-servers","title":"What are MCP Servers?","text":"<p>MCP servers are external processes or services that expose a set of tools that Agent Zero can use. Agent Zero acts as an MCP client, consuming tools made available by these servers. The integration supports three main types of MCP servers:</p> <ol> <li>Local Stdio Servers: These are typically local executables that Agent Zero communicates with via standard input/output (stdio).</li> <li>Remote SSE Servers: These are servers, often accessible over a network, that Agent Zero communicates with using Server-Sent Events (SSE), usually over HTTP/S.</li> <li>Remote Streaming HTTP Servers: These are servers that use the streamable HTTP transport protocol for MCP communication, providing an alternative to SSE for network-based MCP servers.</li> </ol>"},{"location":"mcp_setup/#how-agent-zero-consumes-mcp-tools","title":"How Agent Zero Consumes MCP Tools","text":"<p>Agent Zero discovers and integrates MCP tools dynamically:</p> <ol> <li>Configuration: You define the MCP servers Agent Zero should connect to in its configuration. The primary way to do this is through the Agent Zero settings UI.</li> <li>Saving Settings: When you save your settings via the UI, Agent Zero updates the <code>tmp/settings.json</code> file, specifically the <code>\"mcp_servers\"</code> key.</li> <li>Automatic Installation (on Restart): After saving your settings and restarting Agent Zero, the system will attempt to automatically install any MCP server packages defined with <code>command: \"npx\"</code> and the <code>--package</code> argument in their configuration (this process is managed by <code>initialize.py</code>). You can monitor the application logs (e.g., Docker logs) for details on this installation attempt.</li> <li>Tool Discovery: Upon initialization (or when settings are updated), Agent Zero connects to each configured and enabled MCP server and queries it for the list of available tools, their descriptions, and expected parameters.</li> <li>Dynamic Prompting: The information about these discovered tools is then dynamically injected into the agent's system prompt. A placeholder like <code>{{tools}}</code> in a system prompt template (e.g., <code>prompts/default/agent.system.mcp_tools.md</code>) is replaced with a formatted list of all available MCP tools. This allows the agent's underlying Language Model (LLM) to know which external tools it can request.</li> <li>Tool Invocation: When the LLM decides to use an MCP tool, Agent Zero's <code>process_tools</code> method (handled by <code>mcp_handler.py</code>) identifies it as an MCP tool and routes the request to the appropriate <code>MCPConfig</code> helper, which then communicates with the designated MCP server to execute the tool.</li> </ol>"},{"location":"mcp_setup/#configuration","title":"Configuration","text":""},{"location":"mcp_setup/#configuration-file-method","title":"Configuration File &amp; Method","text":"<p>The primary method for configuring MCP servers is through Agent Zero's settings UI.</p> <p>When you input and save your MCP server details in the UI, these settings are written to:</p> <ul> <li><code>tmp/settings.json</code></li> </ul>"},{"location":"mcp_setup/#the-mcp_servers-setting-in-tmpsettingsjson","title":"The <code>mcp_servers</code> Setting in <code>tmp/settings.json</code>","text":"<p>Within <code>tmp/settings.json</code>, the MCP servers are defined under the <code>\"mcp_servers\"</code> key.</p> <ul> <li>Value Type: The value for <code>\"mcp_servers\"</code> must be a JSON formatted string. This string itself contains an array of server configuration objects.</li> <li>Default Value: If <code>tmp/settings.json</code> does not exist, or if it exists but does not contain the <code>\"mcp_servers\"</code> key, Agent Zero will use a default value of <code>\"\"</code> (an empty string), meaning no MCP servers are configured.</li> <li>Manual Editing (Advanced): While UI configuration is recommended, you can also manually edit <code>tmp/settings.json</code>. If you do, ensure the <code>\"mcp_servers\"</code> value is a valid JSON string, with internal quotes properly escaped.</li> </ul> <p>Example <code>mcp_servers</code> string in <code>tmp/settings.json</code>:</p> <p><pre><code>{\n    // ... other settings ...\n    \"mcp_servers\": \"[{'name': 'sequential-thinking','command': 'npx','args': ['--yes', '--package', '@modelcontextprotocol/server-sequential-thinking', 'mcp-server-sequential-thinking']}, {'name': 'brave-search', 'command': 'npx', 'args': ['--yes', '--package', '@modelcontextprotocol/server-brave-search', 'mcp-server-brave-search'], 'env': {'BRAVE_API_KEY': 'YOUR_BRAVE_KEY_HERE'}}, {'name': 'fetch', 'command': 'npx', 'args': ['--yes', '--package', '@tokenizin/mcp-npx-fetch', 'mcp-npx-fetch', '--ignore-robots-txt', '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36']}]\",\n    // ... other settings ...\n}\n</code></pre> Note: In the actual <code>settings.json</code> file, the entire value for <code>mcp_servers</code> is a single string, with backslashes escaping the quotes within the array structure.</p> <ul> <li>Updating: As mentioned, the recommended way to set or update this value is through Agent Zero's settings UI.</li> <li>For Existing <code>settings.json</code> Files (After an Upgrade): If you have an existing <code>tmp/settings.json</code> from a version of Agent Zero prior to MCP server support, the <code>\"mcp_servers\"</code> key will likely be missing. To add this key:<ol> <li>Ensure you are running a version of Agent Zero that includes MCP server support.</li> <li>Run Agent Zero and open its settings UI.</li> <li>Save the settings (even without making changes). This action will write the complete current settings structure, including a default <code>\"mcp_servers\": \"\"</code> if not otherwise populated, to <code>tmp/settings.json</code>. You can then configure your servers via the UI or by carefully editing this string.</li> </ol> </li> </ul>"},{"location":"mcp_setup/#mcp-server-configuration-structure","title":"MCP Server Configuration Structure","text":"<p>Here are templates for configuring individual servers within the <code>mcp_servers</code> JSON array string:</p> <p>1. Local Stdio Server</p> <pre><code>{\n    \"name\": \"My Local Tool Server\",\n    \"description\": \"Optional: A brief description of this server.\",\n    \"type\": \"stdio\", // Optional: Explicitly specify server type. Can be \"stdio\", \"sse\", or streaming HTTP variants (\"http-stream\", \"streaming-http\", \"streamable-http\", \"http-streaming\"). Auto-detected if omitted.\n    \"command\": \"python\", // The executable to run (e.g., python, /path/to/my_tool_server)\n    \"args\": [\"path/to/your/mcp_stdio_script.py\", \"--some-arg\"], // List of arguments for the command\n    \"env\": { // Optional: Environment variables for the command's process\n        \"PYTHONPATH\": \"/path/to/custom/libs:.\",\n        \"ANOTHER_VAR\": \"value\"\n    },\n    \"encoding\": \"utf-8\", // Optional: Encoding for stdio communication (default: \"utf-8\")\n    \"encoding_error_handler\": \"strict\", // Optional: How to handle encoding errors. Can be \"strict\", \"ignore\", or \"replace\" (default: \"strict\").\n    \"disabled\": false // Set to true to temporarily disable this server without removing its configuration.\n}\n</code></pre> <p>2. Remote SSE Server</p> <pre><code>{\n    \"name\": \"My Remote API Tools\",\n    \"description\": \"Optional: Description of the remote SSE server.\",\n    \"type\": \"sse\", // Optional: Explicitly specify server type. Can be \"stdio\", \"sse\", or streaming HTTP variants (\"http-stream\", \"streaming-http\", \"streamable-http\", \"http-streaming\"). Auto-detected if omitted.\n    \"url\": \"https://api.example.com/mcp-sse-endpoint\", // The full URL for the SSE endpoint of the MCP server.\n    \"headers\": { // Optional: Any HTTP headers required for the connection.\n        \"Authorization\": \"Bearer YOUR_API_KEY_OR_TOKEN\",\n        \"X-Custom-Header\": \"some_value\"\n    },\n    \"timeout\": 5.0, // Optional: Connection timeout in seconds (default: 5.0).\n    \"sse_read_timeout\": 300.0, // Optional: Read timeout for the SSE stream in seconds (default: 300.0, i.e., 5 minutes).\n    \"disabled\": false\n}\n</code></pre> <p>3. Remote Streaming HTTP Server</p> <pre><code>{\n    \"name\": \"My Streaming HTTP Tools\",\n    \"description\": \"Optional: Description of the remote streaming HTTP server.\",\n    \"type\": \"streaming-http\", // Optional: Explicitly specify server type. Can be \"stdio\", \"sse\", or streaming HTTP variants (\"http-stream\", \"streaming-http\", \"streamable-http\", \"http-streaming\"). Auto-detected if omitted.\n    \"url\": \"https://api.example.com/mcp-http-endpoint\", // The full URL for the streaming HTTP endpoint of the MCP server.\n    \"headers\": { // Optional: Any HTTP headers required for the connection.\n        \"Authorization\": \"Bearer YOUR_API_KEY_OR_TOKEN\",\n        \"X-Custom-Header\": \"some_value\"\n    },\n    \"timeout\": 5.0, // Optional: Connection timeout in seconds (default: 5.0).\n    \"sse_read_timeout\": 300.0, // Optional: Read timeout for the SSE and streaming HTTP streams in seconds (default: 300.0, i.e., 5 minutes).\n    \"disabled\": false\n}\n</code></pre> <p>Example <code>mcp_servers</code> value in <code>tmp/settings.json</code>:</p> <pre><code>{\n    // ... other settings ...\n    \"mcp_servers\": \"[{'name': 'MyPythonTools', 'command': 'python3', 'args': ['mcp_scripts/my_server.py'], 'disabled': false}, {'name': 'ExternalAPI', 'url': 'https://data.example.com/mcp', 'headers': {'X-Auth-Token': 'supersecret'}, 'disabled': false}]\",\n    // ... other settings ...\n}\n</code></pre> <p>Key Configuration Fields:</p> <ul> <li><code>\"name\"</code>: A unique name for the server. This name will be used to prefix the tools provided by this server (e.g., <code>my_server_name.tool_name</code>). The name is normalized internally (converted to lowercase, spaces and hyphens replaced with underscores).</li> <li><code>\"type\"</code>: Optional explicit server type specification. Can be <code>\"stdio\"</code>, <code>\"sse\"</code>, or streaming HTTP variants (<code>\"http-stream\"</code>, <code>\"streaming-http\"</code>, <code>\"streamable-http\"</code>, <code>\"http-streaming\"</code>). If omitted, the type is auto-detected based on the presence of <code>\"command\"</code> (stdio) or <code>\"url\"</code> (defaults to sse for backward compatibility).</li> <li><code>\"disabled\"</code>: A boolean (<code>true</code> or <code>false</code>). If <code>true</code>, Agent Zero will ignore this server configuration.</li> <li><code>\"url\"</code>: Required for Remote SSE and Streaming HTTP Servers. The endpoint URL.</li> <li><code>\"command\"</code>: Required for Local Stdio Servers. The executable command.</li> <li><code>\"args\"</code>: Optional list of arguments for local Stdio servers.</li> <li>Other fields are specific to the server type and mostly optional with defaults.</li> </ul>"},{"location":"mcp_setup/#using-mcp-tools","title":"Using MCP Tools","text":"<p>Once configured, successfully installed (if applicable, e.g., for <code>npx</code> based servers), and discovered by Agent Zero:</p> <ul> <li>Tool Naming: MCP tools will appear to the agent with a name prefixed by the server name you defined (and normalized, e.g., lowercase, underscores for spaces/hyphens). For instance, if your server is named <code>\"sequential-thinking\"</code> in the configuration and it offers a tool named <code>\"run_chain\"</code>, the agent will know it as <code>sequential_thinking.run_chain</code>.</li> <li>Agent Interaction: You can instruct the agent to use these tools. For example: \"Agent, use the <code>sequential_thinking.run_chain</code> tool with the following input...\" The agent's LLM will then formulate the appropriate JSON request.</li> <li>Execution Flow: Agent Zero's <code>process_tools</code> method (with logic in <code>python/helpers/mcp_handler.py</code>) prioritizes looking up the tool name in the <code>MCPConfig</code>. If found, the execution is delegated to the corresponding MCP server. If not found as an MCP tool, it then attempts to find a local/built-in tool with that name.</li> </ul> <p>This setup provides a flexible way to extend Agent Zero's capabilities by integrating with various external tool providers without modifying its core codebase.</p>"},{"location":"openfga_migrate/","title":"OpenFGA Migration Service (<code>somaAgent01_openfga-migrate</code>)","text":"<p>The OpenFGA migration container is responsible for applying database schema migrations to the OpenFGA PostgreSQL datastore before the OpenFGA service starts.</p>"},{"location":"openfga_migrate/#how-it-works","title":"How it works","text":"Property Value Container name <code>somaAgent01_openfga-migrate</code> Image <code>openfga/openfga:v1.8.3</code> Command <code>migrate --datastore-engine postgres --datastore-uri postgres://openfga:openfga@postgres:5432/openfga?sslmode=disable</code> Restart policy <code>on-failure</code> \u2013 will retry only if the migration exits with a non\u2011zero status. Depends on <code>postgres</code> (condition: <code>service_healthy</code>). The OpenFGA service itself depends on this container with <code>condition: service_completed_successfully</code> so it only starts after migrations finish. Lifecycle The container exits with status\u202f0 after a successful migration. This is expected \u2013 it is a one\u2011shot task."},{"location":"openfga_migrate/#common-confusion","title":"Common confusion","text":"<ul> <li>Container exits \u2013 Seeing the container in <code>Exited</code> state is normal; it means the migrations completed successfully.</li> <li>Re\u2011running migrations \u2013 Docker Compose will reuse the existing container on subsequent <code>docker compose up</code>. To force a fresh migration run, use:   <pre><code>docker compose -f infra/docker-compose.somaagent01.yaml up --force-recreate openfga-migrate\n</code></pre>   or delete the container first:   <pre><code>docker rm -f somaAgent01_openfga-migrate\ndocker compose -f infra/docker-compose.somaagent01.yaml up -d openfga-migrate\n</code></pre></li> <li>Failure handling \u2013 If the migration fails (non\u2011zero exit code), the <code>on-failure</code> restart policy will attempt to restart the container, giving you a chance to fix any DB connectivity or schema issues.</li> </ul>"},{"location":"openfga_migrate/#recommendations","title":"Recommendations","text":"<ol> <li>Leave the container as\u2011is \u2013 The current <code>restart: \"on-failure\"</code> ensures it will retry on transient errors but will not keep running indefinitely.</li> <li>Include it in your deployment checklist \u2013 Verify the logs after a fresh compose up:    <pre><code>docker logs somaAgent01_openfga-migrate\n</code></pre>    You should see lines similar to:    <pre><code>2025/10/07 01:49:17 current version 5\n2025/10/07 01:49:17 running all migrations\n2025/10/07 01:49:17 migration done\n</code></pre></li> <li>No healthcheck needed \u2013 Since the container exits on success, Docker treats it as completed. The dependent <code>openfga</code> service uses <code>condition: service_completed_successfully</code> to wait for it.</li> </ol> <p>Generated on $(date)</p>"},{"location":"roadmap_sa01/","title":"SA01 \u2013 SomaAgent01 Roadmap","text":""},{"location":"roadmap_sa01/#objective","title":"Objective","text":"<p>Align SA01 with the consolidated architecture:</p> <ul> <li>Use shared infra services (Auth, OPA, Kafka, Redis, Prometheus/Grafana, Vault, Etcd).</li> <li>Communicate with SB via gRPC (<code>memory.proto</code>).</li> <li>Adopt common configuration and logging utilities from <code>common/</code>.</li> <li>Deploy via the unified Helm chart.</li> </ul>"},{"location":"roadmap_sa01/#work-items-ordered","title":"Work Items (ordered)","text":"<ol> <li>Refactor Settings</li> <li>Replace local <code>.env</code> files with <code>common/config/settings.py</code> inheritance.</li> <li>Pull Auth, OPA, and feature-flag endpoints from DNS (<code>auth.soma.svc.cluster.local</code>, etc.).</li> <li>gRPC Client/Server Update</li> <li>Generate protobuf stubs (<code>python -m grpc_tools.protoc ...</code>).</li> <li>Switch existing HTTP calls to the new gRPC client for memory reads/writes.</li> <li>Logging &amp; Tracing</li> <li>Import <code>common/utils/trace.py</code> and configure OpenTelemetry exporter to Jaeger.</li> <li>Ensure JSON log format and Loki side-car label <code>service=sa01</code>.</li> <li>Dockerfile Simplification</li> <li>Expose only port 50051 (gRPC).</li> <li>Remove any duplicated Prometheus exporter; rely on shared <code>/metrics</code> endpoint.</li> <li>Helm Chart Integration</li> <li>Add a sub-chart entry <code>sa01</code> under <code>services/</code> in <code>infra/helm/charts/soma-stack/</code>.</li> <li>Set <code>values.yaml</code> to reference shared infra DNS names.</li> <li>CI/CD Adjustments</li> <li>Update GitHub Actions to run <code>pytest</code> with the new gRPC fixtures.</li> <li>Add a Helm lint step for the <code>sa01</code> chart.</li> <li>Feature-Flag Migration</li> <li>Move any SA01-specific flags from ConfigMaps to Etcd.</li> <li>Cache them in Redis via the common flag client.</li> <li>Testing</li> <li>Write integration tests that spin up a local Kind cluster with the single <code>soma-infra</code> chart and the <code>sa01</code> chart.</li> <li>Verify health endpoint <code>/healthz</code> and gRPC latency &lt; 50\u202fms.</li> <li>Documentation</li> <li>Add a section in <code>docs/architecture.md</code> describing SA01\u2019s role and its dependencies on shared infra.</li> </ol>"},{"location":"roadmap_sa01/#milestones","title":"Milestones","text":"Milestone Target Sprint (2-week) Settings refactor &amp; DNS switch Sprint\u202f1 gRPC migration Sprint\u202f2 Logging/Tracing integration Sprint\u202f2 Helm chart &amp; CI updates Sprint\u202f3 Feature-flag migration Sprint\u202f3 Full integration test suite Sprint\u202f4 Documentation &amp; hand-off Sprint\u202f4"},{"location":"style-guide/","title":"Documentation Style Guide","text":"<p>Use this guide to keep SomaAgent01 documentation consistent across manuals.</p>"},{"location":"style-guide/#structure","title":"Structure","text":"<ul> <li>Manuals live under <code>docs/&lt;manual-name&gt;/</code> using kebab-case directories.</li> <li>Each page begins with YAML front matter declaring <code>title</code>, <code>version</code>, <code>last-reviewed</code>, <code>audience</code>, <code>owner</code>, <code>reviewers</code>, <code>prerequisites</code>, and <code>verification</code> when applicable.</li> <li>Use sentence case for headings (<code>## Install dependencies</code>).</li> </ul>"},{"location":"style-guide/#voice-tone","title":"Voice &amp; Tone","text":"<ul> <li>Direct, action-oriented instructions.</li> <li>Highlight decisions with callouts (<code>&gt; [!TIP]</code>, <code>&gt; [!NOTE]</code>).</li> <li>Avoid marketing language; focus on actionable guidance.</li> </ul>"},{"location":"style-guide/#formatting","title":"Formatting","text":"<ul> <li>Wrap file paths and commands in backticks.</li> <li>Use fenced code blocks with language hints (<code>```bash</code>).</li> <li>Tables require header separators and concise columns.</li> <li>Diagrams authored in Mermaid; store sources in <code>docs/diagrams/</code>.</li> </ul>"},{"location":"style-guide/#links-references","title":"Links &amp; References","text":"<ul> <li>Prefer relative links (<code>../technical-manual/architecture.md</code>).</li> <li>Update links when relocating content; run link checker before merging.</li> <li>Reference change log entries when updating procedures.</li> </ul>"},{"location":"style-guide/#media","title":"Media","text":"<ul> <li>Store images under <code>docs/res/&lt;manual&gt;/&lt;topic&gt;/</code>.</li> <li>Provide alt text and captions.</li> <li>Optimize images for web (&lt;500 KB) before committing.</li> </ul>"},{"location":"style-guide/#metadata","title":"Metadata","text":"<ul> <li>Update <code>last-reviewed</code> on every substantive change.</li> <li>Add reviewers who approved the content.</li> <li>State verification criteria so readers can confirm success.</li> </ul>"},{"location":"style-guide/#checklist","title":"Checklist","text":"<p>See <code>docs/documentation-audit-checklist.md</code> before submitting PRs.</p>"},{"location":"apis/gateway/","title":"Gateway API Reference","text":"<p>Base URL: http://localhost:8010</p> <p>Authentication</p> <ul> <li>JWT bearer tokens when GATEWAY_REQUIRE_AUTH=true or JWT config is present.</li> </ul> <p>Endpoints</p> <ul> <li>POST /v1/session/message</li> <li>Body: { session_id?, persona_id?, message, attachments: string[], metadata: object }</li> <li> <p>Returns: { session_id, event_id }</p> </li> <li> <p>POST /v1/session/action</p> </li> <li> <p>Body: { session_id?, persona_id?, action, metadata? }</p> </li> <li> <p>GET /v1/session/{session_id}/events</p> </li> <li> <p>Server-Sent Events stream of conversation.outbound for that session</p> </li> <li> <p>WS /v1/session/{session_id}/stream</p> </li> <li> <p>WebSocket JSON stream of conversation.outbound events</p> </li> <li> <p>GET /v1/health</p> </li> <li> <p>API keys</p> </li> <li> <p>POST /v1/keys, GET /v1/keys, DELETE /v1/keys/{key_id}</p> </li> <li> <p>Model profiles</p> </li> <li> <p>GET/POST/PUT/DELETE under /v1/model-profiles</p> </li> <li> <p>Routing</p> </li> <li> <p>POST /v1/route -&gt; choose best model among candidates</p> </li> <li> <p>Requeue</p> </li> <li> <p>GET /v1/requeue, POST /v1/requeue/{id}/resolve, DELETE /v1/requeue/{id}</p> </li> <li> <p>Capsules proxy</p> </li> <li>GET /v1/capsules, GET /v1/capsules/{id}, POST /v1/capsules/{id}/install</li> </ul> <p>Notes</p> <ul> <li>Endpoints /chat, /settings_get, /settings_set, /realtime_session are not part of the gateway. Legacy equivalents live under python/api and may be exposed separately.</li> </ul> <p>Errors</p> <ul> <li>401 Unauthorized when auth required and token missing/invalid</li> <li>403 Forbidden when policy denies</li> <li>502 Upstream dependency issues (Kafka, OPA, JWKS, Capsule Registry)</li> </ul> <p>Examples</p> <pre><code>curl -X POST http://localhost:8010/v1/session/message \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"message\": \"Plan today\\'s standup\"}'\n</code></pre>"},{"location":"apis/realtime_session/","title":"Realtime Session API (Legacy)","text":"<p>This endpoint is served by the legacy Python API layer (<code>python/api/realtime_session.py</code>). It is not exposed by the FastAPI gateway. Prefer the versioned gateway APIs for new integrations; use this only where legacy compatibility is required.</p> <p>A focused guide for automations and UI clients interacting with <code>/realtime_session</code>.</p>"},{"location":"apis/realtime_session/#endpoint","title":"Endpoint","text":"<p><code>POST /realtime_session</code></p>"},{"location":"apis/realtime_session/#request-schema","title":"Request Schema","text":"Field Type Required Description <code>model</code> string \u2714 Provider model identifier (<code>gpt-4o-realtime-preview</code>) <code>voice</code> string \u2714 Voice preset (e.g., <code>verse</code>) <code>endpoint</code> string \u2716 Override provider endpoint (defaults to OpenAI)"},{"location":"apis/realtime_session/#example","title":"Example","text":"<pre><code>{\n  \"model\": \"gpt-4o-realtime-preview\",\n  \"voice\": \"verse\",\n  \"endpoint\": \"https://api.openai.com/v1/realtime/sessions\"\n}\n</code></pre>"},{"location":"apis/realtime_session/#response-schema","title":"Response Schema","text":"<pre><code>{\n  \"session\": {\n    \"id\": \"sess_abc123\",\n    \"model\": \"gpt-4o-realtime-preview\",\n    \"expires_at\": \"2025-10-09T18:00:00Z\",\n    \"client_secret\": {\n      \"value\": \"rtm_secret_token\",\n      \"expires_at\": \"2025-10-09T18:00:00Z\"\n    }\n  }\n}\n</code></pre> <ul> <li><code>client_secret.value</code> must be supplied as Bearer token during WebRTC negotiation (<code>Authorization: Bearer ...</code>).</li> <li>Secrets expire quickly (typically 1 min). Clients should negotiate immediately.</li> </ul>"},{"location":"apis/realtime_session/#error-responses","title":"Error Responses","text":"Status Error Code Meaning 400 <code>invalid_request</code> Missing model/voice or invalid endpoint format 401 <code>unauthorized</code> Missing or invalid Gateway authentication 429 <code>rate_limited</code> Exceeded session creation rate 500 <code>provider_error</code> Upstream provider rejected the request <p>Error body: <pre><code>{\n  \"error_code\": \"provider_error\",\n  \"message\": \"OpenAI returned 401 Unauthorized\"\n}\n</code></pre></p>"},{"location":"apis/realtime_session/#usage-flow-machines","title":"Usage Flow (Machines)","text":"<ol> <li>Call <code>/realtime_session</code> with desired parameters.</li> <li>Extract <code>client_secret.value</code>.</li> <li>Create WebRTC peer connection, generate SDP offer.</li> <li>POST offer to provider using <code>Authorization: Bearer &lt;client_secret&gt;</code>.</li> <li>Apply answer to the peer connection.</li> </ol>"},{"location":"apis/realtime_session/#usage-flow-humans-via-ui","title":"Usage Flow (Humans via UI)","text":"<ol> <li>Operator enables microphone in UI.</li> <li>UI automatically calls <code>/realtime_session</code> using stored settings.</li> <li>UI handles WebRTC negotiation, user hears synthesized speech.</li> </ol>"},{"location":"apis/realtime_session/#rate-limits","title":"Rate Limits","text":"<ul> <li>Default: 3 session creations per minute per tenant.</li> <li>Configurable via Redis rate limiter; adjust in <code>python/helpers/settings.py</code> or env vars.</li> </ul>"},{"location":"apis/realtime_session/#testing","title":"Testing","text":"<ul> <li>Use Playwright test <code>tests/playwright/test_realtime_speech.py</code> for end-to-end validation.</li> <li>Mock provider by pointing <code>endpoint</code> to a local stub in development.</li> </ul>"},{"location":"apis/settings/","title":"Settings API (Legacy)","text":"<p>These settings endpoints are part of the legacy Python API layer and are not provided by the FastAPI gateway. The gateway ingests configuration via Kafka <code>config_updates</code> and environment. UI code relying on <code>/settings_get</code> and <code>/settings_set</code> should be updated to a new settings service or config bus.</p>"},{"location":"apis/settings/#endpoints","title":"Endpoints","text":"Method Path Description POST <code>/settings_get</code> Fetch the latest settings document POST <code>/settings_set</code> Persist a full settings document <p>Note: Both endpoints accept/return JSON. <code>settings_get</code> takes an empty JSON body (<code>{}</code>) for forward compatibility.</p>"},{"location":"apis/settings/#document-format","title":"Document Format","text":"<pre><code>{\n  \"settings\": {\n    \"sections\": [\n      {\n        \"id\": \"speech\",\n        \"title\": \"Speech\",\n        \"fields\": [\n          {\n            \"id\": \"speech_provider\",\n            \"type\": \"select\",\n            \"value\": \"openai_realtime\",\n            \"options\": [\n              {\"value\": \"browser\", \"label\": \"Browser\"},\n              {\"value\": \"kokoro\", \"label\": \"Kokoro\"},\n              {\"value\": \"openai_realtime\", \"label\": \"OpenAI Realtime\"}\n            ]\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre> <p>Each field supports metadata (<code>title</code>, <code>description</code>, <code>hidden</code>, <code>min</code>, <code>max</code>, etc.) defined in <code>python/helpers/settings.py</code>.</p>"},{"location":"apis/settings/#update-workflow","title":"Update Workflow","text":"<ol> <li>Fetch current settings (<code>/settings_get</code>).</li> <li>Modify fields in the <code>sections</code> array.</li> <li>POST entire document back to <code>/settings_set</code>.</li> <li>Gateway merges changes, updates <code>tmp/settings.json</code>, and notifies runtime components.</li> </ol>"},{"location":"apis/settings/#validation","title":"Validation","text":"<ul> <li>Backend validates field IDs and value types.</li> <li>Unsupported fields trigger <code>400</code> with <code>error_code: invalid_setting</code>.</li> <li>Sensitive values (passwords, API keys) return placeholders when fetched (e.g., <code>****PSWD****</code>).</li> </ul>"},{"location":"apis/settings/#concurrency","title":"Concurrency","text":"<ul> <li>Settings document includes version stamp (<code>settings.version</code>).</li> <li>Optionally implement optimistic concurrency by ensuring version matches before saving.</li> </ul>"},{"location":"apis/settings/#automation-example","title":"Automation Example","text":"<pre><code>import requests\n\nresp = requests.post(\"http://localhost:8010/settings_get\", json={})\nsettings = resp.json()[\"settings\"]\n\nfor section in settings[\"sections\"]:\n    if section[\"id\"] == \"speech\":\n        for field in section[\"fields\"]:\n            if field[\"id\"] == \"speech_provider\":\n                field[\"value\"] = \"openai_realtime\"\n\nrequests.post(\"http://localhost:8010/settings_set\", json={\"settings\": settings})\n</code></pre>"},{"location":"apis/settings/#security","title":"Security","text":"<ul> <li>Only authenticated users (and trusted automations) should access these endpoints.</li> <li>Audit logs capture who changed settings and when.</li> </ul>"},{"location":"apis/settings/#testing","title":"Testing","text":"<ul> <li>Integration: <code>tests/integration/test_gateway_public_api.py</code>.</li> <li>UI: <code>tests/playwright/test_realtime_speech.py</code>, ensures visibility toggles respond to provider changes.</li> </ul>"},{"location":"apis/tool_executor_callbacks/","title":"Tool Executor Callback API","text":"<p>After executing a tool invocation, the executor reports results back to the Gateway so conversations can continue.</p>"},{"location":"apis/tool_executor_callbacks/#kafka-topics","title":"Kafka Topics","text":"Topic Direction Description <code>tool.requests</code> Gateway/Orchestrator \u279c Executor Tool invocation requests <code>tool.results</code> Executor \u279c Gateway/Orchestrator Execution results, errors <p>Messages are JSON objects with a shared envelope:</p> <pre><code>{\n  \"task_id\": \"uuid\",\n  \"tenant_id\": \"default\",\n  \"persona\": \"agent0\",\n  \"tool_name\": \"shell\",\n  \"payload\": {\n    \"command\": \"ls -la\"\n  },\n  \"metadata\": {\n    \"correlation_id\": \"conversation-uuid\",\n    \"initiated_by\": \"gateway\"\n  }\n}\n</code></pre>"},{"location":"apis/tool_executor_callbacks/#http-callback","title":"HTTP Callback","text":"<p>Not used in the current implementation. The executor publishes exclusively to Kafka topics. If an HTTP callback becomes necessary in the future, document the endpoint and signing requirements here.</p>"},{"location":"apis/tool_executor_callbacks/#status-codes","title":"Status Codes","text":"Status Meaning <code>success</code> Tool finished normally <code>failed</code> Tool encountered an error; include <code>error</code> field <code>retry</code> Executor wants Gateway to requeue the task"},{"location":"apis/tool_executor_callbacks/#idempotency","title":"Idempotency","text":"<ul> <li>Executor must ensure multiple sends with same <code>task_id</code> are safe.</li> <li>Gateway deduplicates by <code>task_id</code> and ignores stale updates.</li> </ul>"},{"location":"apis/tool_executor_callbacks/#security","title":"Security","text":"<ul> <li>Kafka: SASL/SSL configurable in production deployments.</li> <li>HTTP: Signed bearer tokens or mTLS recommended.</li> </ul>"},{"location":"apis/tool_executor_callbacks/#monitoring","title":"Monitoring","text":"<ul> <li>Metrics: <code>tool_results_total</code> by status, <code>tool_latency_seconds</code> histogram.</li> <li>Logs: structured entries with correlation ID for traceability.</li> </ul>"},{"location":"apis/tool_executor_callbacks/#failure-handling","title":"Failure Handling","text":"<ul> <li>If Gateway is unavailable, executor retries with backoff.</li> <li>After max retries, message should land in a dead-letter topic (if configured) such as <code>tool.results.dlq</code>.</li> <li>Operators inspect DLQ, requeue via <code>scripts/kafka_partition_scaler.py</code> or custom tooling.</li> </ul>"},{"location":"apis/tool_executor_callbacks/#testing","title":"Testing","text":"<ul> <li>Unit: mock Kafka consumer producing tasks to executor.</li> <li>Integration: run executor against local Kafka (spin up via <code>make dev-up</code>), assert callbacks update conversation history.</li> </ul>"},{"location":"data/streams/","title":"Event Streams &amp; Data Contracts","text":"<p>Note: This page is referenced from the Technical Manual. Default topic names reflect the current code; environment variables can override them.</p> <p>SomaAgent01 relies on Kafka to decouple orchestration from long-running work. This document enumerates topics, payloads, and retention policy.</p>"},{"location":"data/streams/#topic-catalog","title":"Topic Catalog","text":"<p>Default topics in code (configurable via env):</p> Topic Producer Consumer Retention Payload Schema <code>tool.requests</code> Gateway/Orchestrator Tool Executor 7 days <code>ToolInvocation</code> <code>tool.results</code> Tool Executor Gateway/Orchestrator 7 days <code>ToolResult</code> <code>conversation.inbound</code> Gateway Conversation workers 3 days <code>ConversationEvent</code> <code>conversation.outbound</code> Conversation workers Gateway/UI 3 days <code>ConversationEvent</code> <code>config_updates</code> Config publisher Gateway 1 day <code>SettingsPayload</code> <p>Environment overrides:</p> <ul> <li>TOOL_REQUESTS_TOPIC, TOOL_RESULTS_TOPIC, TOOL_EXECUTOR_GROUP</li> <li>KAFKA_BOOTSTRAP_SERVERS and security-related vars</li> </ul>"},{"location":"data/streams/#payload-schemas","title":"Payload Schemas","text":""},{"location":"data/streams/#toolinvocation","title":"ToolInvocation","text":"<pre><code>{\n  \"task_id\": \"uuid\",\n  \"tenant_id\": \"default\",\n  \"persona\": \"agent0\",\n  \"tool_name\": \"shell\",\n  \"payload\": {\"command\": \"ls\"},\n  \"metadata\": {\"requested_by\": \"gateway\", \"timestamp\": \"2025-10-09T16:59:00Z\"}\n}\n</code></pre>"},{"location":"data/streams/#toolresult","title":"ToolResult","text":"<pre><code>{\n  \"task_id\": \"uuid\",\n  \"status\": \"success\",\n  \"output\": {\"stdout\": \"...\", \"stderr\": \"\"},\n  \"metrics\": {\"duration_ms\": 4500},\n  \"metadata\": {\"tenant_id\": \"default\", \"correlation_id\": \"conversation-uuid\"}\n}\n</code></pre>"},{"location":"data/streams/#delegatedtask","title":"DelegatedTask","text":"<pre><code>{\n  \"task_id\": \"uuid\",\n  \"type\": \"schedule:run\",\n  \"payload\": {\"cron\": \"0 9 * * *\", \"job\": \"daily_report\"},\n  \"tenant_id\": \"default\",\n  \"priority\": 5\n}\n</code></pre>"},{"location":"data/streams/#partitioning-strategy","title":"Partitioning Strategy","text":"<ul> <li>Partition key: <code>tenant_id</code> to guarantee ordering per tenant.</li> <li>Default partitions: 3 (scale via <code>scripts/kafka_partition_scaler.py</code>).</li> </ul>"},{"location":"data/streams/#producers-consumers","title":"Producers &amp; Consumers","text":"<pre><code>graph LR\n  Gateway --&gt;|conversation.inbound| Kafka\n  Kafka --&gt;|conversation.inbound| ConversationWorker\n  ConversationWorker --&gt;|conversation.outbound| Kafka\n  Kafka --&gt;|conversation.outbound| Gateway\n  Gateway --&gt;|tool.requests| Kafka\n  Kafka --&gt;|tool.requests| ToolExecutor\n  ToolExecutor --&gt;|tool.results| Kafka\n  Kafka --&gt;|tool.results| Gateway\n</code></pre>"},{"location":"data/streams/#retention-compaction","title":"Retention &amp; Compaction","text":"<ul> <li>All topics use delete-based retention; adjust via <code>KAFKA_CFG_LOG_RETENTION_HOURS</code>.</li> <li>For audit trails consider enabling log compaction with key = <code>task_id</code>.</li> </ul>"},{"location":"data/streams/#monitoring","title":"Monitoring","text":"<ul> <li>Prometheus JMX exporter exposes metrics (topic lag, ISR count).</li> <li>Alert thresholds: lag &gt; 500 for <code>somastack.tools</code>, offline partitions &gt; 0.</li> </ul>"},{"location":"data/streams/#local-development-tips","title":"Local Development Tips","text":"<ul> <li>Use <code>kafkacat</code> or <code>kcat</code> to inspect topics: <code>kcat -b localhost:29092 -t somastack.tools -C</code>.</li> <li>If you reset volumes via <code>make dev-clean</code>, recreate topics automatically on boot.</li> </ul>"},{"location":"data/streams/#data-governance","title":"Data Governance","text":"<ul> <li>Sensitive payloads should avoid PII; if unavoidable, encrypt payload fields before publishing.</li> <li>Record schema versions within payload (<code>metadata.schema_version</code>).</li> </ul>"},{"location":"designs/backup-specification-backend/","title":"Agent Zero Backup/Restore Backend Specification","text":""},{"location":"designs/backup-specification-backend/#overview","title":"Overview","text":"<p>This specification defines the backend implementation for Agent Zero's backup and restore functionality, providing users with the ability to backup and restore their Agent Zero configurations, data, and custom files using glob pattern-based selection. The backup functionality is implemented as a dedicated \"backup\" tab in the settings interface for easy access and organization.</p>"},{"location":"designs/backup-specification-backend/#core-requirements","title":"Core Requirements","text":""},{"location":"designs/backup-specification-backend/#backup-flow","title":"Backup Flow","text":"<ol> <li>User configures backup paths using glob patterns in settings modal</li> <li>Backend creates zip archive with selected files and metadata</li> <li>Archive is provided as download to user</li> </ol>"},{"location":"designs/backup-specification-backend/#restore-flow","title":"Restore Flow","text":"<ol> <li>User uploads backup archive in settings modal</li> <li>Backend extracts and validates metadata</li> <li>User confirms file list and destination paths</li> <li>Backend restores files to specified locations</li> </ol>"},{"location":"designs/backup-specification-backend/#backend-architecture","title":"Backend Architecture","text":""},{"location":"designs/backup-specification-backend/#1-settings-integration","title":"1. Settings Integration","text":""},{"location":"designs/backup-specification-backend/#settings-schema-extension","title":"Settings Schema Extension","text":"<p>Add backup/restore section with dedicated tab to <code>python/helpers/settings.py</code>:</p> <p>Integration Notes: - Leverages existing settings button handler pattern (follows MCP servers example) - Integrates with Agent Zero's established error handling and toast notification system - Uses existing file operation helpers with RFC support for development mode compatibility</p> <pre><code># Add to SettingsSection in convert_out() function\nbackup_section: SettingsSection = {\n    \"id\": \"backup_restore\",\n    \"title\": \"Backup &amp; Restore\",\n    \"description\": \"Backup and restore Agent Zero data and configurations using glob pattern-based file selection.\",\n    \"fields\": [\n        {\n            \"id\": \"backup_create\",\n            \"title\": \"Create Backup\",\n            \"description\": \"Create a backup archive of selected files and configurations using customizable patterns.\",\n            \"type\": \"button\",\n            \"value\": \"Create Backup\",\n        },\n        {\n            \"id\": \"backup_restore\",\n            \"title\": \"Restore from Backup\",\n            \"description\": \"Restore files and configurations from a backup archive with pattern-based selection.\",\n            \"type\": \"button\",\n            \"value\": \"Restore Backup\",\n        }\n    ],\n    \"tab\": \"backup\",  # Dedicated backup tab for clean organization\n}\n</code></pre>"},{"location":"designs/backup-specification-backend/#default-backup-configuration","title":"Default Backup Configuration","text":"<p>The backup system now uses resolved absolute filesystem paths instead of placeholders, ensuring compatibility across different deployment environments (Docker containers, direct host installations, different users).</p> <pre><code>def _get_default_patterns(self) -&gt; str:\n    \"\"\"Get default backup patterns with resolved absolute paths\"\"\"\n    # Ensure paths don't have double slashes\n    agent_root = self.agent_zero_root.rstrip('/')\n    user_home = self.user_home.rstrip('/')\n\n    return f\"\"\"# Agent Zero Knowledge (excluding defaults)\n{agent_root}/knowledge/**\n!{agent_root}/knowledge/default/**\n\n# Agent Zero Instruments (excluding defaults)\n{agent_root}/instruments/**\n!{agent_root}/instruments/default/**\n\n# Memory (excluding embeddings cache)\n{agent_root}/memory/**\n!{agent_root}/memory/embeddings/**\n\n# Configuration and Settings (CRITICAL)\n{agent_root}/.env\n{agent_root}/tmp/settings.json\n{agent_root}/tmp/chats/**\n{agent_root}/tmp/tasks/**\n{agent_root}/tmp/uploads/**\n\n# User Home Directory (excluding hidden files by default)\n{user_home}/**\n!{user_home}/.*/**\n!{user_home}/.*\"\"\"\n</code></pre> <p>Example Resolved Patterns (varies by environment): <pre><code># Docker container environment\n/a0/knowledge/**\n!/a0/knowledge/default/**\n/root/**\n!/root/.*/**\n!/root/.*\n\n# Host environment\n/home/rafael/a0/data/knowledge/**\n!/home/rafael/a0/data/knowledge/default/**\n/home/rafael/**\n!/home/rafael/.*/**\n!/home/rafael/.*\n</code></pre></p> <p>\u26a0\ufe0f CRITICAL FILE NOTICE: The <code>{agent_root}/.env</code> file contains essential configuration including API keys, model settings, and runtime parameters. This file is REQUIRED for Agent Zero to function properly and should always be included in backups alongside <code>settings.json</code>. Without this file, restored Agent Zero instances will not have access to configured language models or external services.</p>"},{"location":"designs/backup-specification-backend/#2-api-endpoints","title":"2. API Endpoints","text":""},{"location":"designs/backup-specification-backend/#21-backup-test-endpoint","title":"2.1 Backup Test Endpoint","text":"<p>File: <code>python/api/backup_test.py</code></p> <pre><code>from python.helpers.api import ApiHandler\nfrom flask import Request, Response\nfrom python.helpers.backup import BackupService\nimport json\n\nclass BackupTest(ApiHandler):\n    \"\"\"Test backup patterns and return matched files\"\"\"\n\n    @classmethod\n    def requires_auth(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def requires_loopback(cls) -&gt; bool:\n        return True\n\n    async def process(self, input: dict, request: Request) -&gt; dict | Response:\n        patterns = input.get(\"patterns\", \"\")\n        include_hidden = input.get(\"include_hidden\", False)\n        max_files = input.get(\"max_files\", 1000)  # Limit for preview\n\n        try:\n            backup_service = BackupService()\n            matched_files = await backup_service.test_patterns(\n                patterns=patterns,\n                include_hidden=include_hidden,\n                max_files=max_files\n            )\n\n            return {\n                \"success\": True,\n                \"files\": matched_files,\n                \"total_count\": len(matched_files),\n                \"truncated\": len(matched_files) &gt;= max_files\n            }\n\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e)\n            }\n</code></pre>"},{"location":"designs/backup-specification-backend/#22-backup-create-endpoint","title":"2.2 Backup Create Endpoint","text":"<p>File: <code>python/api/backup_create.py</code></p> <pre><code>from python.helpers.api import ApiHandler\nfrom flask import Request, Response, send_file\nfrom python.helpers.backup import BackupService\nimport tempfile\nimport os\n\nclass BackupCreate(ApiHandler):\n    \"\"\"Create backup archive and provide download\"\"\"\n\n    @classmethod\n    def requires_auth(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def requires_loopback(cls) -&gt; bool:\n        return True\n\n    async def process(self, input: dict, request: Request) -&gt; dict | Response:\n        patterns = input.get(\"patterns\", \"\")\n        include_hidden = input.get(\"include_hidden\", False)\n        backup_name = input.get(\"backup_name\", \"agent-zero-backup\")\n\n        try:\n            backup_service = BackupService()\n            zip_path = await backup_service.create_backup(\n                patterns=patterns,\n                include_hidden=include_hidden,\n                backup_name=backup_name\n            )\n\n            # Return file for download\n            return send_file(\n                zip_path,\n                as_attachment=True,\n                download_name=f\"{backup_name}.zip\",\n                mimetype='application/zip'\n            )\n\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e)\n            }\n</code></pre>"},{"location":"designs/backup-specification-backend/#23-backup-restore-endpoint","title":"2.3 Backup Restore Endpoint","text":"<p>File: <code>python/api/backup_restore.py</code></p> <pre><code>from python.helpers.api import ApiHandler\nfrom flask import Request, Response\nfrom python.helpers.backup import BackupService\nfrom werkzeug.datastructures import FileStorage\n\nclass BackupRestore(ApiHandler):\n    \"\"\"Restore files from backup archive\"\"\"\n\n    @classmethod\n    def requires_auth(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def requires_loopback(cls) -&gt; bool:\n        return True\n\n    async def process(self, input: dict, request: Request) -&gt; dict | Response:\n        # Handle file upload\n        if 'backup_file' not in request.files:\n            return {\"success\": False, \"error\": \"No backup file provided\"}\n\n        backup_file: FileStorage = request.files['backup_file']\n        if backup_file.filename == '':\n            return {\"success\": False, \"error\": \"No file selected\"}\n\n        # Get restore configuration\n        restore_patterns = input.get(\"restore_patterns\", \"\")\n        overwrite_policy = input.get(\"overwrite_policy\", \"overwrite\")  # overwrite, skip, backup\n\n        try:\n            backup_service = BackupService()\n            result = await backup_service.restore_backup(\n                backup_file=backup_file,\n                restore_patterns=restore_patterns,\n                overwrite_policy=overwrite_policy\n            )\n\n            return {\n                \"success\": True,\n                \"restored_files\": result[\"restored_files\"],\n                \"skipped_files\": result[\"skipped_files\"],\n                \"errors\": result[\"errors\"]\n            }\n\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e)\n            }\n</code></pre>"},{"location":"designs/backup-specification-backend/#24-backup-restore-preview-endpoint","title":"2.4 Backup Restore Preview Endpoint","text":"<p>File: <code>python/api/backup_restore_preview.py</code></p> <pre><code>from python.helpers.api import ApiHandler\nfrom flask import Request, Response\nfrom python.helpers.backup import BackupService\nfrom werkzeug.datastructures import FileStorage\n\nclass BackupRestorePreview(ApiHandler):\n    \"\"\"Preview files that would be restored based on patterns\"\"\"\n\n    @classmethod\n    def requires_auth(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def requires_loopback(cls) -&gt; bool:\n        return True\n\n    async def process(self, input: dict, request: Request) -&gt; dict | Response:\n        # Handle file upload\n        if 'backup_file' not in request.files:\n            return {\"success\": False, \"error\": \"No backup file provided\"}\n\n        backup_file: FileStorage = request.files['backup_file']\n        if backup_file.filename == '':\n            return {\"success\": False, \"error\": \"No file selected\"}\n\n        restore_patterns = input.get(\"restore_patterns\", \"\")\n\n        try:\n            backup_service = BackupService()\n            preview_result = await backup_service.preview_restore(\n                backup_file=backup_file,\n                restore_patterns=restore_patterns\n            )\n\n            return {\n                \"success\": True,\n                \"files\": preview_result[\"files\"],\n                \"total_count\": preview_result[\"total_count\"],\n                \"skipped_count\": preview_result[\"skipped_count\"]\n            }\n\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e)\n            }\n</code></pre>"},{"location":"designs/backup-specification-backend/#25-backup-file-preview-grouped-endpoint","title":"2.5 Backup File Preview Grouped Endpoint","text":"<p>File: <code>python/api/backup_preview_grouped.py</code></p> <pre><code>from python.helpers.api import ApiHandler\nfrom flask import Request, Response\nfrom python.helpers.backup import BackupService\n\nclass BackupPreviewGrouped(ApiHandler):\n    \"\"\"Get grouped file preview with smart directory organization\"\"\"\n\n    @classmethod\n    def requires_auth(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def requires_loopback(cls) -&gt; bool:\n        return True\n\n    async def process(self, input: dict, request: Request) -&gt; dict | Response:\n        patterns = input.get(\"patterns\", \"\")\n        include_hidden = input.get(\"include_hidden\", False)\n        max_depth = input.get(\"max_depth\", 3)\n        search_filter = input.get(\"search_filter\", \"\")\n\n        try:\n            backup_service = BackupService()\n            grouped_preview = await backup_service.get_grouped_file_preview(\n                patterns=patterns,\n                include_hidden=include_hidden,\n                max_depth=max_depth,\n                search_filter=search_filter\n            )\n\n            return {\n                \"success\": True,\n                \"groups\": grouped_preview[\"groups\"],\n                \"stats\": grouped_preview[\"stats\"],\n                \"total_files\": grouped_preview[\"total_files\"],\n                \"total_size\": grouped_preview[\"total_size\"]\n            }\n\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e)\n            }\n</code></pre>"},{"location":"designs/backup-specification-backend/#26-backup-progress-stream-endpoint","title":"2.6 Backup Progress Stream Endpoint","text":"<p>File: <code>python/api/backup_progress_stream.py</code></p> <pre><code>from python.helpers.api import ApiHandler\nfrom flask import Request, Response, stream_template\nfrom python.helpers.backup import BackupService\nimport json\n\nclass BackupProgressStream(ApiHandler):\n    \"\"\"Stream real-time backup progress\"\"\"\n\n    @classmethod\n    def requires_auth(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def requires_loopback(cls) -&gt; bool:\n        return True\n\n    async def process(self, input: dict, request: Request) -&gt; dict | Response:\n        patterns = input.get(\"patterns\", \"\")\n        include_hidden = input.get(\"include_hidden\", False)\n        backup_name = input.get(\"backup_name\", \"agent-zero-backup\")\n\n        def generate_progress():\n            try:\n                backup_service = BackupService()\n\n                # Generator function for streaming progress\n                for progress_data in backup_service.create_backup_with_progress(\n                    patterns=patterns,\n                    include_hidden=include_hidden,\n                    backup_name=backup_name\n                ):\n                    yield f\"data: {json.dumps(progress_data)}\\n\\n\"\n\n            except Exception as e:\n                yield f\"data: {json.dumps({'error': str(e), 'completed': True})}\\n\\n\"\n\n        return Response(\n            generate_progress(),\n            content_type='text/event-stream',\n            headers={\n                'Cache-Control': 'no-cache',\n                'Connection': 'keep-alive'\n            }\n        )\n</code></pre>"},{"location":"designs/backup-specification-backend/#27-backup-inspect-endpoint","title":"2.7 Backup Inspect Endpoint","text":"<p>File: <code>python/api/backup_inspect.py</code></p> <pre><code>from python.helpers.api import ApiHandler\nfrom flask import Request, Response\nfrom python.helpers.backup import BackupService\nfrom werkzeug.datastructures import FileStorage\n\nclass BackupInspect(ApiHandler):\n    \"\"\"Inspect backup archive and return metadata\"\"\"\n\n    @classmethod\n    def requires_auth(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def requires_loopback(cls) -&gt; bool:\n        return True\n\n    async def process(self, input: dict, request: Request) -&gt; dict | Response:\n        # Handle file upload\n        if 'backup_file' not in request.files:\n            return {\"success\": False, \"error\": \"No backup file provided\"}\n\n        backup_file: FileStorage = request.files['backup_file']\n        if backup_file.filename == '':\n            return {\"success\": False, \"error\": \"No file selected\"}\n\n        try:\n            backup_service = BackupService()\n            metadata = await backup_service.inspect_backup(backup_file)\n\n            return {\n                \"success\": True,\n                \"metadata\": metadata,\n                \"files\": metadata.get(\"files\", []),\n                \"include_patterns\": metadata.get(\"include_patterns\", []),  # Array of include patterns\n                \"exclude_patterns\": metadata.get(\"exclude_patterns\", []),  # Array of exclude patterns\n                \"default_patterns\": metadata.get(\"backup_config\", {}).get(\"default_patterns\", \"\"),\n                \"agent_zero_version\": metadata.get(\"agent_zero_version\", \"unknown\"),\n                \"timestamp\": metadata.get(\"timestamp\", \"\"),\n                \"backup_name\": metadata.get(\"backup_name\", \"\"),\n                \"total_files\": metadata.get(\"total_files\", len(metadata.get(\"files\", []))),\n                \"backup_size\": metadata.get(\"backup_size\", 0),\n                \"include_hidden\": metadata.get(\"include_hidden\", False)\n            }\n\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e)\n            }\n</code></pre>"},{"location":"designs/backup-specification-backend/#3-backup-service-implementation","title":"3. Backup Service Implementation","text":""},{"location":"designs/backup-specification-backend/#core-service-class","title":"Core Service Class","text":"<p>File: <code>python/helpers/backup.py</code></p> <p>RFC Integration Notes: The BackupService leverages Agent Zero's existing file operation helpers which already support RFC (Remote Function Call) routing for development mode. This ensures seamless operation whether running in direct mode or with container isolation.</p> <pre><code>import zipfile\nimport json\nimport os\nimport tempfile\nimport datetime\nfrom typing import List, Dict, Any, Optional\nfrom pathspec import PathSpec\nfrom pathspec.patterns import GitWildMatchPattern\nfrom python.helpers import files, runtime, git\nimport shutil\n\nclass BackupService:\n    \"\"\"Core backup and restore service for Agent Zero\"\"\"\n\n    def __init__(self):\n        self.agent_zero_version = self._get_agent_zero_version()\n        self.agent_zero_root = files.get_abs_path(\"\")  # Resolved Agent Zero root\n        self.user_home = os.path.expanduser(\"~\")       # Current user's home directory\n\n    def _get_default_patterns(self) -&gt; str:\n        \"\"\"Get default backup patterns from specification\"\"\"\n        return DEFAULT_BACKUP_PATTERNS\n\n    def _get_agent_zero_version(self) -&gt; str:\n        \"\"\"Get current Agent Zero version\"\"\"\n        try:\n            # Get version from git info (same as run_ui.py)\n            gitinfo = git.get_git_info()\n            return gitinfo.get(\"version\", \"development\")\n        except:\n            return \"unknown\"\n\n    def _resolve_path(self, pattern_path: str) -&gt; str:\n        \"\"\"Resolve pattern path to absolute system path (now patterns are already absolute)\"\"\"\n        return pattern_path\n\n    def _unresolve_path(self, abs_path: str) -&gt; str:\n        \"\"\"Convert absolute path back to pattern path (now patterns are already absolute)\"\"\"\n        return abs_path\n\n    def _parse_patterns(self, patterns: str) -&gt; tuple[list[str], list[str]]:\n        \"\"\"Parse patterns string into include and exclude pattern arrays\"\"\"\n        include_patterns = []\n        exclude_patterns = []\n\n        for line in patterns.split('\\n'):\n            line = line.strip()\n            if not line or line.startswith('#'):\n                continue\n\n            if line.startswith('!'):\n                # Exclude pattern\n                exclude_patterns.append(line[1:])  # Remove the '!' prefix\n            else:\n                # Include pattern\n                include_patterns.append(line)\n\n        return include_patterns, exclude_patterns\n\n    def _patterns_to_string(self, include_patterns: list[str], exclude_patterns: list[str]) -&gt; str:\n        \"\"\"Convert pattern arrays back to patterns string for pathspec processing\"\"\"\n        patterns = []\n\n        # Add include patterns\n        for pattern in include_patterns:\n            patterns.append(pattern)\n\n        # Add exclude patterns with '!' prefix\n        for pattern in exclude_patterns:\n            patterns.append(f\"!{pattern}\")\n\n        return '\\n'.join(patterns)\n\n    async def _get_system_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Collect system information for metadata\"\"\"\n        import platform\n        import psutil\n\n        try:\n            return {\n                \"platform\": platform.platform(),\n                \"system\": platform.system(),\n                \"release\": platform.release(),\n                \"version\": platform.version(),\n                \"machine\": platform.machine(),\n                \"processor\": platform.processor(),\n                \"architecture\": platform.architecture()[0],\n                \"hostname\": platform.node(),\n                \"python_version\": platform.python_version(),\n                \"cpu_count\": str(psutil.cpu_count()),\n                \"memory_total\": str(psutil.virtual_memory().total),\n                \"disk_usage\": str(psutil.disk_usage('/').total if os.path.exists('/') else 0)\n            }\n        except Exception as e:\n            return {\"error\": f\"Failed to collect system info: {str(e)}\"}\n\n    async def _get_environment_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Collect environment information for metadata\"\"\"\n        try:\n            return {\n                \"user\": os.environ.get(\"USER\", \"unknown\"),\n                \"home\": os.environ.get(\"HOME\", \"unknown\"),\n                \"shell\": os.environ.get(\"SHELL\", \"unknown\"),\n                \"path\": os.environ.get(\"PATH\", \"\")[:200] + \"...\" if len(os.environ.get(\"PATH\", \"\")) &gt; 200 else os.environ.get(\"PATH\", \"\"),\n                \"timezone\": str(datetime.datetime.now().astimezone().tzinfo),\n                \"working_directory\": os.getcwd(),\n                \"agent_zero_root\": files.get_abs_path(\"\"),\n                \"runtime_mode\": \"development\" if runtime.is_development() else \"production\"\n            }\n        except Exception as e:\n            return {\"error\": f\"Failed to collect environment info: {str(e)}\"}\n\n    async def _get_backup_author(self) -&gt; str:\n        \"\"\"Get backup author/system identifier\"\"\"\n        try:\n            import getpass\n            username = getpass.getuser()\n            hostname = platform.node()\n            return f\"{username}@{hostname}\"\n        except:\n            return \"unknown\"\n\n    async def _calculate_file_checksums(self, matched_files: List[Dict[str, Any]]) -&gt; Dict[str, str]:\n        \"\"\"Calculate SHA-256 checksums for files\"\"\"\n        import hashlib\n\n        checksums = {}\n        for file_info in matched_files:\n            try:\n                real_path = file_info[\"real_path\"]\n                if os.path.exists(real_path) and os.path.isfile(real_path):\n                    hash_sha256 = hashlib.sha256()\n                    with open(real_path, \"rb\") as f:\n                        for chunk in iter(lambda: f.read(4096), b\"\"):\n                            hash_sha256.update(chunk)\n                    checksums[real_path] = hash_sha256.hexdigest()\n            except Exception:\n                checksums[file_info[\"real_path\"]] = \"error\"\n\n        return checksums\n\n    async def _count_directories(self, matched_files: List[Dict[str, Any]]) -&gt; int:\n        \"\"\"Count unique directories in file list\"\"\"\n        directories = set()\n        for file_info in matched_files:\n            dir_path = os.path.dirname(file_info[\"path\"])\n            if dir_path:\n                directories.add(dir_path)\n        return len(directories)\n\n    def _calculate_backup_checksum(self, zip_path: str) -&gt; str:\n        \"\"\"Calculate checksum of the entire backup file\"\"\"\n        import hashlib\n\n        try:\n            hash_sha256 = hashlib.sha256()\n            with open(zip_path, \"rb\") as f:\n                for chunk in iter(lambda: f.read(4096), b\"\"):\n                    hash_sha256.update(chunk)\n            return hash_sha256.hexdigest()\n        except Exception:\n            return \"error\"\n\n    async def test_patterns(self, patterns: str, include_hidden: bool = False, max_files: int = 1000) -&gt; List[Dict[str, Any]]:\n        \"\"\"Test backup patterns and return list of matched files\"\"\"\n\n        # Parse patterns using pathspec\n        pattern_lines = [line.strip() for line in patterns.split('\\n') if line.strip() and not line.strip().startswith('#')]\n\n        if not pattern_lines:\n            return []\n\n        matched_files = []\n        processed_count = 0\n\n        try:\n            spec = PathSpec.from_lines(GitWildMatchPattern, pattern_lines)\n\n            # Walk through base directories\n            for base_pattern_path, base_real_path in self.base_paths.items():\n                if not os.path.exists(base_real_path):\n                    continue\n\n                for root, dirs, files_list in os.walk(base_real_path):\n                    # Filter hidden directories if not included\n                    if not include_hidden:\n                        dirs[:] = [d for d in dirs if not d.startswith('.')]\n\n                    for file in files_list:\n                        if processed_count &gt;= max_files:\n                            break\n\n                        # Skip hidden files if not included\n                        if not include_hidden and file.startswith('.'):\n                            continue\n\n                        file_path = os.path.join(root, file)\n                        pattern_path = self._unresolve_path(file_path)\n\n                        # Remove leading slash for pathspec matching\n                        relative_path = pattern_path.lstrip('/')\n\n                        if spec.match_file(relative_path):\n                            try:\n                                stat = os.stat(file_path)\n                                matched_files.append({\n                                    \"path\": pattern_path,\n                                    \"real_path\": file_path,\n                                    \"size\": stat.st_size,\n                                    \"modified\": datetime.datetime.fromtimestamp(stat.st_mtime).isoformat(),\n                                    \"type\": \"file\"\n                                })\n                                processed_count += 1\n                            except (OSError, IOError):\n                                # Skip files we can't access\n                                continue\n\n                    if processed_count &gt;= max_files:\n                        break\n\n                if processed_count &gt;= max_files:\n                    break\n\n        except Exception as e:\n            raise Exception(f\"Error processing patterns: {str(e)}\")\n\n        return matched_files\n\n    async def create_backup(self, patterns: str, include_hidden: bool = False, backup_name: str = \"agent-zero-backup\") -&gt; str:\n        \"\"\"Create backup archive with selected files\"\"\"\n\n        # Get matched files\n        matched_files = await self.test_patterns(patterns, include_hidden, max_files=10000)\n\n        if not matched_files:\n            raise Exception(\"No files matched the backup patterns\")\n\n        # Create temporary zip file\n        temp_dir = tempfile.mkdtemp()\n        zip_path = os.path.join(temp_dir, f\"{backup_name}.zip\")\n\n        try:\n            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n                # Calculate file checksums for integrity verification\n                file_checksums = await self._calculate_file_checksums(matched_files)\n\n                # Add comprehensive metadata - this is the control file for backup/restore\n                include_patterns, exclude_patterns = self._parse_patterns(patterns)\n\n                metadata = {\n                    # Basic backup information\n                    \"agent_zero_version\": self.agent_zero_version,\n                    \"timestamp\": datetime.datetime.now().isoformat(),\n                    \"backup_name\": backup_name,\n                    \"include_hidden\": include_hidden,\n\n                    # Pattern arrays for granular control during restore\n                    \"include_patterns\": include_patterns,  # Array of include patterns\n                    \"exclude_patterns\": exclude_patterns,  # Array of exclude patterns\n\n                    # System and environment information\n                    \"system_info\": await self._get_system_info(),\n                    \"environment_info\": await self._get_environment_info(),\n                    \"backup_author\": await self._get_backup_author(),\n\n                    # Backup configuration\n                    \"backup_config\": {\n                        \"default_patterns\": self._get_default_patterns(),\n                        \"include_hidden\": include_hidden,\n                        \"compression_level\": 6,\n                        \"integrity_check\": True\n                    },\n\n                    # File information with checksums\n                    \"files\": [\n                        {\n                            \"path\": f[\"path\"],\n                            \"size\": f[\"size\"],\n                            \"modified\": f[\"modified\"],\n                            \"checksum\": file_checksums.get(f[\"real_path\"], \"\"),\n                            \"type\": \"file\"\n                        }\n                        for f in matched_files\n                    ],\n\n                    # Statistics\n                    \"total_files\": len(matched_files),\n                    \"backup_size\": sum(f[\"size\"] for f in matched_files),\n                    \"directory_count\": await self._count_directories(matched_files),\n\n                    # Integrity verification\n                    \"backup_checksum\": \"\",  # Will be calculated after backup creation\n                    \"verification_method\": \"sha256\"\n                }\n\n                zipf.writestr(\"metadata.json\", json.dumps(metadata, indent=2))\n\n                # Add files\n                for file_info in matched_files:\n                    real_path = file_info[\"real_path\"]\n                    archive_path = file_info[\"path\"].lstrip('/')\n\n                    try:\n                        if os.path.exists(real_path) and os.path.isfile(real_path):\n                            zipf.write(real_path, archive_path)\n                    except (OSError, IOError) as e:\n                        # Log error but continue with other files\n                        print(f\"Warning: Could not backup file {real_path}: {e}\")\n                        continue\n\n            return zip_path\n\n        except Exception as e:\n            # Cleanup on error\n            if os.path.exists(zip_path):\n                os.remove(zip_path)\n            raise Exception(f\"Error creating backup: {str(e)}\")\n\n    async def inspect_backup(self, backup_file) -&gt; Dict[str, Any]:\n        \"\"\"Inspect backup archive and return metadata\"\"\"\n\n        # Save uploaded file temporarily\n        temp_dir = tempfile.mkdtemp()\n        temp_file = os.path.join(temp_dir, \"backup.zip\")\n\n        try:\n            backup_file.save(temp_file)\n\n            with zipfile.ZipFile(temp_file, 'r') as zipf:\n                # Read metadata\n                if \"metadata.json\" not in zipf.namelist():\n                    raise Exception(\"Invalid backup file: missing metadata.json\")\n\n                metadata_content = zipf.read(\"metadata.json\").decode('utf-8')\n                metadata = json.loads(metadata_content)\n\n                # Add file list from archive\n                files_in_archive = [name for name in zipf.namelist() if name != \"metadata.json\"]\n                metadata[\"files_in_archive\"] = files_in_archive\n\n                return metadata\n\n        except zipfile.BadZipFile:\n            raise Exception(\"Invalid backup file: not a valid zip archive\")\n        except json.JSONDecodeError:\n            raise Exception(\"Invalid backup file: corrupted metadata\")\n        finally:\n            # Cleanup\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n            if os.path.exists(temp_dir):\n                os.rmdir(temp_dir)\n\n    async def get_grouped_file_preview(self, patterns: str, include_hidden: bool = False, max_depth: int = 3, search_filter: str = \"\") -&gt; Dict[str, Any]:\n        \"\"\"Get files organized in smart groups with depth limitation\"\"\"\n\n        # Get all matched files\n        all_files = await self.test_patterns(patterns, include_hidden, max_files=10000)\n\n        # Apply search filter if provided\n        if search_filter.strip():\n            search_lower = search_filter.lower()\n            all_files = [f for f in all_files if search_lower in f[\"path\"].lower()]\n\n        # Group files by directory structure\n        groups = {}\n        total_size = 0\n\n        for file_info in all_files:\n            path = file_info[\"path\"]\n            total_size += file_info[\"size\"]\n\n            # Split path and limit depth\n            path_parts = path.strip('/').split('/')\n\n            # Limit to max_depth for grouping\n            if len(path_parts) &gt; max_depth:\n                group_path = '/' + '/'.join(path_parts[:max_depth])\n                is_truncated = True\n            else:\n                group_path = '/' + '/'.join(path_parts[:-1]) if len(path_parts) &gt; 1 else '/'\n                is_truncated = False\n\n            if group_path not in groups:\n                groups[group_path] = {\n                    \"path\": group_path,\n                    \"files\": [],\n                    \"file_count\": 0,\n                    \"total_size\": 0,\n                    \"is_truncated\": False,\n                    \"subdirectories\": set()\n                }\n\n            groups[group_path][\"files\"].append(file_info)\n            groups[group_path][\"file_count\"] += 1\n            groups[group_path][\"total_size\"] += file_info[\"size\"]\n            groups[group_path][\"is_truncated\"] = groups[group_path][\"is_truncated\"] or is_truncated\n\n            # Track subdirectories for truncated groups\n            if is_truncated and len(path_parts) &gt; max_depth:\n                next_dir = path_parts[max_depth]\n                groups[group_path][\"subdirectories\"].add(next_dir)\n\n        # Convert groups to sorted list and add display info\n        sorted_groups = []\n        for group_path, group_info in sorted(groups.items()):\n            group_info[\"subdirectories\"] = sorted(list(group_info[\"subdirectories\"]))\n\n            # Limit displayed files for UI performance\n            if len(group_info[\"files\"]) &gt; 50:\n                group_info[\"displayed_files\"] = group_info[\"files\"][:50]\n                group_info[\"additional_files\"] = len(group_info[\"files\"]) - 50\n            else:\n                group_info[\"displayed_files\"] = group_info[\"files\"]\n                group_info[\"additional_files\"] = 0\n\n            sorted_groups.append(group_info)\n\n        return {\n            \"groups\": sorted_groups,\n            \"stats\": {\n                \"total_groups\": len(sorted_groups),\n                \"total_files\": len(all_files),\n                \"total_size\": total_size,\n                \"search_applied\": bool(search_filter.strip()),\n                \"max_depth\": max_depth\n            },\n            \"total_files\": len(all_files),\n            \"total_size\": total_size\n        }\n\n    def create_backup_with_progress(self, patterns: str, include_hidden: bool = False, backup_name: str = \"agent-zero-backup\"):\n        \"\"\"Generator that yields backup progress for streaming\"\"\"\n\n        try:\n            # Step 1: Get matched files\n            yield {\n                \"stage\": \"discovery\",\n                \"message\": \"Scanning files...\",\n                \"progress\": 0,\n                \"completed\": False\n            }\n\n            import asyncio\n            matched_files = asyncio.run(self.test_patterns(patterns, include_hidden, max_files=10000))\n\n            if not matched_files:\n                yield {\n                    \"stage\": \"error\",\n                    \"message\": \"No files matched the backup patterns\",\n                    \"progress\": 0,\n                    \"completed\": True,\n                    \"error\": True\n                }\n                return\n\n            total_files = len(matched_files)\n\n            yield {\n                \"stage\": \"discovery\",\n                \"message\": f\"Found {total_files} files to backup\",\n                \"progress\": 10,\n                \"completed\": False,\n                \"total_files\": total_files\n            }\n\n            # Step 2: Calculate checksums\n            yield {\n                \"stage\": \"checksums\",\n                \"message\": \"Calculating file checksums...\",\n                \"progress\": 15,\n                \"completed\": False\n            }\n\n            file_checksums = asyncio.run(self._calculate_file_checksums(matched_files))\n\n            # Step 3: Create backup\n            temp_dir = tempfile.mkdtemp()\n            zip_path = os.path.join(temp_dir, f\"{backup_name}.zip\")\n\n            yield {\n                \"stage\": \"backup\",\n                \"message\": \"Creating backup archive...\",\n                \"progress\": 20,\n                \"completed\": False\n            }\n\n            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n                # Create and add metadata first\n                metadata = {\n                    \"agent_zero_version\": self.agent_zero_version,\n                    \"timestamp\": datetime.datetime.now().isoformat(),\n                    \"backup_name\": backup_name,\n                    \"backup_patterns\": patterns,\n                    \"include_hidden\": include_hidden,\n                    \"system_info\": asyncio.run(self._get_system_info()),\n                    \"environment_info\": asyncio.run(self._get_environment_info()),\n                    \"backup_author\": asyncio.run(self._get_backup_author()),\n                    \"backup_config\": {\n                        \"default_patterns\": self._get_default_patterns(),\n                        \"custom_patterns\": patterns,\n                        \"include_hidden\": include_hidden,\n                        \"compression_level\": 6,\n                        \"integrity_check\": True\n                    },\n                    \"files\": [\n                        {\n                            \"path\": f[\"path\"],\n                            \"size\": f[\"size\"],\n                            \"modified\": f[\"modified\"],\n                            \"checksum\": file_checksums.get(f[\"real_path\"], \"\"),\n                            \"type\": \"file\"\n                        }\n                        for f in matched_files\n                    ],\n                    \"total_files\": len(matched_files),\n                    \"backup_size\": sum(f[\"size\"] for f in matched_files),\n                    \"directory_count\": asyncio.run(self._count_directories(matched_files)),\n                    \"backup_checksum\": \"\",\n                    \"verification_method\": \"sha256\"\n                }\n\n                zipf.writestr(\"metadata.json\", json.dumps(metadata, indent=2))\n\n                # Add files with progress updates\n                for i, file_info in enumerate(matched_files):\n                    real_path = file_info[\"real_path\"]\n                    archive_path = file_info[\"path\"].lstrip('/')\n\n                    try:\n                        if os.path.exists(real_path) and os.path.isfile(real_path):\n                            zipf.write(real_path, archive_path)\n\n                            # Yield progress every 10 files or at key milestones\n                            if i % 10 == 0 or i == total_files - 1:\n                                progress = 20 + (i + 1) / total_files * 70  # 20-90%\n                                yield {\n                                    \"stage\": \"backup\",\n                                    \"message\": f\"Adding file: {file_info['path']}\",\n                                    \"progress\": int(progress),\n                                    \"completed\": False,\n                                    \"current_file\": i + 1,\n                                    \"total_files\": total_files,\n                                    \"file_path\": file_info[\"path\"]\n                                }\n                    except Exception as e:\n                        yield {\n                            \"stage\": \"warning\",\n                            \"message\": f\"Failed to backup file: {file_info['path']} - {str(e)}\",\n                            \"progress\": int(20 + (i + 1) / total_files * 70),\n                            \"completed\": False,\n                            \"warning\": True\n                        }\n\n            # Step 4: Calculate final checksum\n            yield {\n                \"stage\": \"finalization\",\n                \"message\": \"Calculating backup checksum...\",\n                \"progress\": 95,\n                \"completed\": False\n            }\n\n            backup_checksum = self._calculate_backup_checksum(zip_path)\n\n            # Step 5: Complete\n            yield {\n                \"stage\": \"completed\",\n                \"message\": \"Backup created successfully\",\n                \"progress\": 100,\n                \"completed\": True,\n                \"success\": True,\n                \"backup_path\": zip_path,\n                \"backup_checksum\": backup_checksum,\n                \"total_files\": total_files,\n                \"backup_size\": os.path.getsize(zip_path)\n            }\n\n        except Exception as e:\n            yield {\n                \"stage\": \"error\",\n                \"message\": f\"Backup failed: {str(e)}\",\n                \"progress\": 0,\n                \"completed\": True,\n                \"error\": True\n            }\n\n    async def restore_backup(self, backup_file, restore_patterns: str, overwrite_policy: str = \"overwrite\") -&gt; Dict[str, Any]:\n        \"\"\"Restore files from backup archive\"\"\"\n\n        # Save uploaded file temporarily\n        temp_dir = tempfile.mkdtemp()\n        temp_file = os.path.join(temp_dir, \"backup.zip\")\n\n        restored_files = []\n        skipped_files = []\n        errors = []\n\n        try:\n            backup_file.save(temp_file)\n\n            # Parse restore patterns if provided\n            if restore_patterns.strip():\n                pattern_lines = [line.strip() for line in restore_patterns.split('\\n')\n                               if line.strip() and not line.strip().startswith('#')]\n                spec = PathSpec.from_lines(GitWildMatchPattern, pattern_lines) if pattern_lines else None\n            else:\n                spec = None\n\n            with zipfile.ZipFile(temp_file, 'r') as zipf:\n                # Read metadata\n                if \"metadata.json\" in zipf.namelist():\n                    metadata_content = zipf.read(\"metadata.json\").decode('utf-8')\n                    metadata = json.loads(metadata_content)\n\n                # Process each file in archive\n                for archive_path in zipf.namelist():\n                    if archive_path == \"metadata.json\":\n                        continue\n\n                    # Check if file matches restore patterns\n                    if spec and not spec.match_file(archive_path):\n                        skipped_files.append({\n                            \"path\": archive_path,\n                            \"reason\": \"not_matched_by_pattern\"\n                        })\n                        continue\n\n                    # Determine target path\n                    target_path = self._resolve_path(\"/\" + archive_path)\n\n                    try:\n                        # Handle overwrite policy\n                        if os.path.exists(target_path):\n                            if overwrite_policy == \"skip\":\n                                skipped_files.append({\n                                    \"path\": archive_path,\n                                    \"reason\": \"file_exists_skip_policy\"\n                                })\n                                continue\n                            elif overwrite_policy == \"backup\":\n                                backup_path = f\"{target_path}.backup.{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n                                shutil.move(target_path, backup_path)\n\n                        # Create target directory if needed\n                        target_dir = os.path.dirname(target_path)\n                        os.makedirs(target_dir, exist_ok=True)\n\n                        # Extract file\n                        with zipf.open(archive_path) as source, open(target_path, 'wb') as target:\n                            shutil.copyfileobj(source, target)\n\n                        restored_files.append({\n                            \"archive_path\": archive_path,\n                            \"target_path\": target_path,\n                            \"status\": \"restored\"\n                        })\n\n                    except Exception as e:\n                        errors.append({\n                            \"path\": archive_path,\n                            \"error\": str(e)\n                        })\n\n            return {\n                \"restored_files\": restored_files,\n                \"skipped_files\": skipped_files,\n                \"errors\": errors\n            }\n\n        except Exception as e:\n            raise Exception(f\"Error restoring backup: {str(e)}\")\n        finally:\n            # Cleanup\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n            if os.path.exists(temp_dir):\n                os.rmdir(temp_dir)\n\n    async def preview_restore(self, backup_file, restore_patterns: str) -&gt; Dict[str, Any]:\n        \"\"\"Preview which files would be restored based on patterns\"\"\"\n\n        # Save uploaded file temporarily\n        temp_dir = tempfile.mkdtemp()\n        temp_file = os.path.join(temp_dir, \"backup.zip\")\n\n        files_to_restore = []\n        skipped_files = []\n\n        try:\n            backup_file.save(temp_file)\n\n            # Parse restore patterns if provided\n            if restore_patterns.strip():\n                pattern_lines = [line.strip() for line in restore_patterns.split('\\n')\n                               if line.strip() and not line.strip().startswith('#')]\n                spec = PathSpec.from_lines(GitWildMatchPattern, pattern_lines) if pattern_lines else None\n            else:\n                spec = None\n\n            with zipfile.ZipFile(temp_file, 'r') as zipf:\n                # Read metadata for context\n                metadata = {}\n                if \"metadata.json\" in zipf.namelist():\n                    metadata_content = zipf.read(\"metadata.json\").decode('utf-8')\n                    metadata = json.loads(metadata_content)\n\n                # Process each file in archive\n                for archive_path in zipf.namelist():\n                    if archive_path == \"metadata.json\":\n                        continue\n\n                    # Check if file matches restore patterns\n                    if spec:\n                        if spec.match_file(archive_path):\n                            files_to_restore.append({\n                                \"path\": archive_path,\n                                \"target_path\": self._resolve_path(\"/\" + archive_path),\n                                \"action\": \"restore\"\n                            })\n                        else:\n                            skipped_files.append({\n                                \"path\": archive_path,\n                                \"reason\": \"not_matched_by_pattern\"\n                            })\n                    else:\n                        # No patterns specified, restore all files\n                        files_to_restore.append({\n                            \"path\": archive_path,\n                            \"target_path\": self._resolve_path(\"/\" + archive_path),\n                            \"action\": \"restore\"\n                        })\n\n            return {\n                \"files\": files_to_restore,\n                \"skipped_files\": skipped_files,\n                \"total_count\": len(files_to_restore),\n                \"skipped_count\": len(skipped_files)\n            }\n\n        except Exception as e:\n            raise Exception(f\"Error previewing restore: {str(e)}\")\n        finally:\n            # Cleanup\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n            if os.path.exists(temp_dir):\n                os.rmdir(temp_dir)\n</code></pre>"},{"location":"designs/backup-specification-backend/#4-dependencies","title":"4. Dependencies","text":""},{"location":"designs/backup-specification-backend/#required-python-packages","title":"Required Python Packages","text":"<p>Add to <code>requirements.txt</code>: <pre><code>pathspec&gt;=0.10.0  # For gitignore-style pattern matching\npsutil&gt;=5.8.0     # For system information collection\n</code></pre></p>"},{"location":"designs/backup-specification-backend/#agent-zero-internal-dependencies","title":"Agent Zero Internal Dependencies","text":"<p>The backup system requires these Agent Zero helper modules: - <code>python.helpers.git</code> - For version detection using git.get_git_info() (consistent with run_ui.py) - <code>python.helpers.files</code> - For file operations and path resolution - <code>python.helpers.runtime</code> - For development/production mode detection</p>"},{"location":"designs/backup-specification-backend/#installation-command","title":"Installation Command","text":"<pre><code>pip install pathspec psutil\n</code></pre>"},{"location":"designs/backup-specification-backend/#5-error-handling","title":"5. Error Handling","text":""},{"location":"designs/backup-specification-backend/#integration-with-agent-zero-error-system","title":"Integration with Agent Zero Error System","text":"<p>The backup system integrates with Agent Zero's existing error handling infrastructure:</p> <pre><code>from python.helpers.errors import format_error\nfrom python.helpers.print_style import PrintStyle\n\n# Follow Agent Zero's error handling patterns\ntry:\n    result = await backup_operation()\n    return {\"success\": True, \"data\": result}\nexcept Exception as e:\n    error_message = format_error(e)\n    PrintStyle.error(f\"Backup error: {error_message}\")\n    return {\"success\": False, \"error\": error_message}\n</code></pre>"},{"location":"designs/backup-specification-backend/#common-error-scenarios","title":"Common Error Scenarios","text":"<ol> <li>Invalid Patterns: Malformed glob patterns</li> <li>Permission Errors: Files/directories not accessible</li> <li>Disk Space: Insufficient space for backup creation</li> <li>Invalid Archives: Corrupted or invalid backup files</li> <li>Path Conflicts: Files outside allowed directories</li> </ol>"},{"location":"designs/backup-specification-backend/#error-response-format","title":"Error Response Format","text":"<pre><code>{\n    \"success\": False,\n    \"error\": \"Human-readable error message\",\n    \"error_code\": \"BACKUP_PATTERN_INVALID\",  # Optional machine-readable code\n    \"details\": {  # Optional additional details\n        \"invalid_patterns\": [\"pattern1\", \"pattern2\"],\n        \"suggestion\": \"Check pattern syntax\"\n    }\n}\n</code></pre>"},{"location":"designs/backup-specification-backend/#6-security-considerations","title":"6. Security Considerations","text":""},{"location":"designs/backup-specification-backend/#path-security","title":"Path Security","text":"<ul> <li>Validate all paths to prevent directory traversal attacks</li> <li>Restrict backups to predefined base directories (/a0, /root)</li> <li>Sanitize file names in archives</li> <li>Implement file size limits for uploads/downloads</li> </ul>"},{"location":"designs/backup-specification-backend/#authentication","title":"Authentication","text":"<ul> <li>All endpoints require authentication (<code>requires_auth = True</code>)</li> <li>All endpoints require loopback (<code>requires_loopback = True</code>)</li> <li>No API key access for security</li> </ul>"},{"location":"designs/backup-specification-backend/#file-system-protection","title":"File System Protection","text":"<ul> <li>Read-only access to system directories outside allowed paths</li> <li>Size limits for backup archives</li> <li>Timeout limits for backup operations</li> <li>Temporary file cleanup</li> </ul>"},{"location":"designs/backup-specification-backend/#7-performance-considerations","title":"7. Performance Considerations","text":""},{"location":"designs/backup-specification-backend/#file-processing","title":"File Processing","text":"<ul> <li>Limit number of files in test/preview operations (max_files parameter)</li> <li>Stream file processing for large archives</li> <li>Implement progress tracking for large operations</li> <li>Use temporary directories for staging</li> </ul>"},{"location":"designs/backup-specification-backend/#memory-management","title":"Memory Management","text":"<ul> <li>Stream zip file creation to avoid memory issues</li> <li>Process files individually rather than loading all in memory</li> <li>Clean up temporary files promptly</li> <li>Implement timeout limits for long operations</li> </ul>"},{"location":"designs/backup-specification-backend/#8-configuration","title":"8. Configuration","text":""},{"location":"designs/backup-specification-backend/#default-configuration","title":"Default Configuration","text":"<pre><code>BACKUP_CONFIG = {\n    \"max_files_preview\": 1000,\n    \"max_backup_size\": 1024 * 1024 * 1024,  # 1GB\n    \"max_upload_size\": 1024 * 1024 * 1024,  # 1GB\n    \"operation_timeout\": 300,  # 5 minutes\n    \"temp_cleanup_interval\": 3600,  # 1 hour\n    \"allowed_base_paths\": [\"/a0\", \"/root\"]\n}\n</code></pre>"},{"location":"designs/backup-specification-backend/#future-integration-opportunities","title":"Future Integration Opportunities","text":"<p>Task Scheduler Integration: Agent Zero's existing task scheduler could be extended to support automated backups:</p> <pre><code># Potential future enhancement - scheduled backups\n{\n    \"name\": \"auto_backup_daily\",\n    \"type\": \"scheduled\",\n    \"schedule\": \"0 2 * * *\",  # Daily at 2 AM\n    \"tool_name\": \"backup_create\",\n    \"tool_args\": {\n        \"patterns\": \"default_patterns\",\n        \"backup_name\": \"auto_backup_{date}\"\n    }\n}\n</code></pre>"},{"location":"designs/backup-specification-backend/#enhanced-metadata-structure-and-restore-workflow","title":"Enhanced Metadata Structure and Restore Workflow","text":""},{"location":"designs/backup-specification-backend/#version-detection-implementation","title":"Version Detection Implementation","text":"<p>The backup system uses the same version detection method as Agent Zero's main UI:</p> <pre><code>def _get_agent_zero_version(self) -&gt; str:\n    \"\"\"Get current Agent Zero version\"\"\"\n    try:\n        # Get version from git info (same as run_ui.py)\n        gitinfo = git.get_git_info()\n        return gitinfo.get(\"version\", \"development\")\n    except:\n        return \"unknown\"\n</code></pre> <p>This ensures consistency between the backup metadata and the main application version reporting.</p>"},{"location":"designs/backup-specification-backend/#metadatajson-format","title":"Metadata.json Format","text":"<p>The backup archive includes a comprehensive <code>metadata.json</code> file with the following structure:</p> <pre><code>{\n  \"agent_zero_version\": \"version\",\n  \"timestamp\": \"ISO datetime\",\n  \"backup_name\": \"user-defined name\",\n  \"include_hidden\": boolean,\n\n  \"include_patterns\": [\n    \"/a0/knowledge/**\",\n    \"/a0/instruments/**\",\n    \"/a0/memory/**\",\n    \"/a0/.env\",\n    \"/a0/tmp/settings.json\"\n  ],\n  \"exclude_patterns\": [\n    \"/a0/knowledge/default/**\",\n    \"/a0/instruments/default/**\",\n    \"/a0/memory/embeddings/**\"\n  ],\n\n  \"system_info\": { /* platform, architecture, etc. */ },\n  \"environment_info\": { /* user, timezone, paths, etc. */ },\n  \"backup_author\": \"user@hostname\",\n  \"backup_config\": {\n    \"default_patterns\": \"system defaults\",\n    \"include_hidden\": boolean,\n    \"compression_level\": 6,\n    \"integrity_check\": true\n  },\n\n  \"files\": [ /* file list with checksums */ ],\n  \"total_files\": count,\n  \"backup_size\": bytes,\n  \"backup_checksum\": \"sha256\"\n}\n</code></pre>"},{"location":"designs/backup-specification-backend/#restore-workflow","title":"Restore Workflow","text":"<ol> <li>Upload Archive: User uploads backup.zip file</li> <li>Load Metadata: System extracts and parses metadata.json</li> <li>Display Metadata: Complete metadata.json shown in ACE JSON editor</li> <li>User Editing: User can modify include_patterns and exclude_patterns arrays directly</li> <li>Preview Changes: System shows which files will be restored based on current metadata</li> <li>Execute Restore: Files restored according to final metadata configuration</li> </ol>"},{"location":"designs/backup-specification-backend/#json-metadata-editing-benefits","title":"JSON Metadata Editing Benefits","text":"<ul> <li>Single Source of Truth: metadata.json is the authoritative configuration</li> <li>Direct Editing: Users edit JSON arrays directly in ACE editor</li> <li>Full Control: Access to all metadata properties, not just patterns</li> <li>Validation: JSON syntax validation and array structure validation</li> <li>Transparency: Users see exactly what will be used for restore</li> </ul>"},{"location":"designs/backup-specification-backend/#comprehensive-enhancement-summary","title":"Comprehensive Enhancement Summary","text":""},{"location":"designs/backup-specification-backend/#enhanced-metadata-structure","title":"Enhanced Metadata Structure","text":"<p>The backup metadata has been significantly enhanced to include: - System Information: Platform, architecture, Python version, CPU count, memory, disk usage - Environment Details: User, timezone, working directory, runtime mode, Agent Zero root path - Backup Author: System identifier (user@hostname) for backup tracking - File Checksums: SHA-256 hashes for all backed up files for integrity verification - Backup Statistics: Total files, directories, sizes with verification methods - Compatibility Data: Agent Zero version and environment for restoration validation</p>"},{"location":"designs/backup-specification-backend/#smart-file-management","title":"Smart File Management","text":"<ul> <li>Grouped File Preview: Organize files by directory structure with depth limitation (max 3 levels)</li> <li>Smart Grouping: Show directory hierarchies with expandable file counts</li> <li>Search and Filter: Real-time filtering by file name or path fragments</li> <li>Performance Optimization: Limit preview files (1000 max) and displayed files (50 per group) for UI responsiveness</li> </ul>"},{"location":"designs/backup-specification-backend/#real-time-progress-streaming","title":"Real-time Progress Streaming","text":"<ul> <li>Server-Sent Events: Live backup progress updates via <code>/backup_progress_stream</code> endpoint</li> <li>Multi-stage Progress: Discovery \u2192 Checksums \u2192 Backup \u2192 Finalization with percentage tracking</li> <li>File-by-file Updates: Real-time display of current file being processed</li> <li>Error Handling: Graceful error reporting and warning collection during backup process</li> </ul>"},{"location":"designs/backup-specification-backend/#advanced-api-endpoints","title":"Advanced API Endpoints","text":"<ol> <li><code>/backup_preview_grouped</code>: Get smart file groupings with depth control and search</li> <li><code>/backup_progress_stream</code>: Stream real-time backup progress via SSE</li> <li><code>/backup_restore_preview</code>: Preview restore operations with pattern filtering</li> <li>Enhanced <code>/backup_inspect</code>: Return comprehensive metadata with system information</li> </ol>"},{"location":"designs/backup-specification-backend/#system-information-collection","title":"System Information Collection","text":"<ul> <li>Platform Detection: OS, architecture, Python version, hostname</li> <li>Resource Information: CPU count, memory, disk usage via psutil (converted to strings for JSON consistency)</li> <li>Environment Capture: User, timezone, paths, runtime mode</li> <li>Version Integration: Uses git.get_git_info() for consistent version detection with main application</li> <li>Integrity Verification: SHA-256 checksums for individual files and complete backup</li> </ul>"},{"location":"designs/backup-specification-backend/#security-and-reliability-enhancements","title":"Security and Reliability Enhancements","text":"<ul> <li>Integrity Verification: File-level and backup-level checksum validation</li> <li>Comprehensive Logging: Detailed progress tracking and error collection</li> <li>Path Security: Enhanced validation with system information context</li> <li>Backup Validation: Version compatibility checking and environment verification</li> </ul> <p>This enhanced backend specification provides a production-ready, comprehensive backup and restore system with advanced metadata tracking, real-time progress monitoring, and intelligent file management capabilities, all while maintaining Agent Zero's architectural patterns and security standards.</p>"},{"location":"designs/backup-specification-backend/#implementation-status-updates","title":"Implementation Status Updates","text":""},{"location":"designs/backup-specification-backend/#completed-core-backupservice-implementation","title":"\u2705 COMPLETED: Core BackupService Implementation","text":"<ul> <li>Git Version Integration: Updated to use <code>git.get_git_info()</code> consistent with <code>run_ui.py</code></li> <li>Type Safety: Fixed psutil return values to be strings for JSON metadata consistency</li> <li>Code Quality: All linting errors resolved, proper import structure</li> <li>Testing Verified: BackupService initializes correctly and detects Agent Zero root paths</li> <li>Dependencies Added: pathspec&gt;=0.10.0 for pattern matching, psutil&gt;=5.8.0 for system info</li> <li>Git Helper Integration: Uses python.helpers.git.get_git_info() for version detection consistency</li> </ul>"},{"location":"designs/backup-specification-backend/#next-implementation-phase-api-endpoints","title":"Next Implementation Phase: API Endpoints","text":"<p>Ready to implement the 8 API endpoints: 1. <code>backup_test.py</code> - Pattern testing and file preview 2. <code>backup_create.py</code> - Archive creation and download 3. <code>backup_restore.py</code> - File restoration from archive 4. <code>backup_inspect.py</code> - Archive metadata inspection 5. <code>backup_get_defaults.py</code> - Fetch default patterns 6. <code>backup_restore_preview.py</code> - Preview restore patterns 7. <code>backup_preview_grouped.py</code> - Smart directory grouping 8. <code>backup_progress_stream.py</code> - Real-time progress streaming</p>"},{"location":"designs/backup-specification-backend/#implementation-cleanup-and-final-status","title":"Implementation Cleanup and Final Status","text":""},{"location":"designs/backup-specification-backend/#completed-cleanup-december-2024","title":"\u2705 COMPLETED CLEANUP (December 2024)","text":""},{"location":"designs/backup-specification-backend/#removed-unused-components","title":"Removed Unused Components:","text":"<ul> <li>\u274c <code>backup_download.py</code> - Functionality moved to <code>backup_create</code> (direct download)</li> <li>\u274c <code>backup_progress_stream.py</code> - Not implemented in frontend, overengineered</li> <li>\u274c <code>_calculate_file_checksums()</code> method - Dead code, checksums not properly used</li> <li>\u274c <code>_calculate_backup_checksum()</code> method - Dead code, never called</li> <li>\u274c <code>hashlib</code> import - No longer needed after checksum removal</li> </ul>"},{"location":"designs/backup-specification-backend/#simplified-backupservice","title":"Simplified BackupService:","text":"<ul> <li>\u2705 Removed checksum calculation - Was calculated but not properly used, overcomplicating the code</li> <li>\u2705 Streamlined metadata - Removed unused integrity verification fields</li> <li>\u2705 Fixed <code>_count_directories()</code> method - Had return statement in wrong place</li> <li>\u2705 Cleaner error handling - Removed unnecessary warning outputs</li> </ul>"},{"location":"designs/backup-specification-backend/#enhanced-hidden-file-logic","title":"Enhanced Hidden File Logic:","text":"<p>The most critical fix was implementing proper explicit pattern handling:</p> <pre><code># NEW: Enhanced hidden file logic\ndef _get_explicit_patterns(self, include_patterns: List[str]) -&gt; set[str]:\n    \"\"\"Extract explicit (non-wildcard) patterns that should always be included\"\"\"\n    explicit_patterns = set()\n\n    for pattern in include_patterns:\n        # If pattern doesn't contain wildcards, it's explicit\n        if '*' not in pattern and '?' not in pattern:\n            # Remove leading slash for comparison\n            explicit_patterns.add(pattern.lstrip('/'))\n\n            # Also add parent directories as explicit (so hidden dirs can be traversed)\n            path_parts = pattern.lstrip('/').split('/')\n            for i in range(1, len(path_parts)):\n                parent_path = '/'.join(path_parts[:i])\n                explicit_patterns.add(parent_path)\n\n    return explicit_patterns\n\n# FIXED: Hidden file filtering now respects explicit patterns\nif not include_hidden and file.startswith('.'):\n    if not self._is_explicitly_included(pattern_path, explicit_patterns):\n        continue  # Only exclude hidden files discovered via wildcards\n</code></pre>"},{"location":"designs/backup-specification-backend/#final-api-endpoint-set-6-endpoints","title":"Final API Endpoint Set (6 endpoints):","text":"<ol> <li>\u2705 <code>backup_get_defaults</code> - Get default metadata configuration</li> <li>\u2705 <code>backup_test</code> - Test patterns and preview files (dry run)</li> <li>\u2705 <code>backup_preview_grouped</code> - Get grouped file preview for UI</li> <li>\u2705 <code>backup_create</code> - Create and download backup archive</li> <li>\u2705 <code>backup_inspect</code> - Inspect uploaded backup metadata</li> <li>\u2705 <code>backup_restore_preview</code> - Preview restore operation</li> <li>\u2705 <code>backup_restore</code> - Execute restore operation</li> </ol>"},{"location":"designs/backup-specification-backend/#critical-issue-fixed-hidden-files","title":"Critical Issue Fixed: Hidden Files","text":"<p>Problem: When <code>include_hidden=false</code>, the system was excluding ALL hidden files, even when they were explicitly specified in patterns like <code>/a0/.env</code>.</p> <p>Solution: Implemented explicit pattern detection that distinguishes between: - Explicit patterns (like <code>/a0/.env</code>) - Always included regardless of <code>include_hidden</code> setting - Wildcard discoveries (like <code>/a0/*</code>) - Respect the <code>include_hidden</code> setting</p> <p>Result: Critical files like <code>.env</code> are now properly backed up when explicitly specified, ensuring Agent Zero configurations are preserved.</p>"},{"location":"designs/backup-specification-backend/#implementation-status-production-ready","title":"Implementation Status: \u2705 PRODUCTION READY","text":"<p>The backup system is now: - Simplified: Removed unnecessary complexity and dead code - Reliable: Fixed critical hidden file handling - Efficient: No unnecessary checksum calculations - Clean: Proper error handling and type safety - Complete: Full backup and restore functionality working</p> <p>Key Benefits of Cleanup: - \u2705 Simpler maintenance - Less code to maintain and debug - \u2705 Better performance - No unnecessary checksum calculations - \u2705 Correct behavior - Hidden files now work as expected - \u2705 Cleaner API - Only endpoints that are actually used - \u2705 Better reliability - Removed complex features that weren't properly implemented</p> <p>The Agent Zero backup system is now production-ready and battle-tested! \ud83d\ude80</p>"},{"location":"designs/backup-specification-backend/#final-status-ace-editor-state-guarantee-completed-december-2024","title":"\u2705 FINAL STATUS: ACE EDITOR STATE GUARANTEE COMPLETED (December 2024)","text":""},{"location":"designs/backup-specification-backend/#goal-achievement-verification","title":"Goal Achievement Verification","text":"<p>The primary goal has been successfully achieved: All metadata.json operations in GUI use the ACE editor state, not original archive metadata, giving users complete control to edit and execute exactly what's defined in the editor.</p>"},{"location":"designs/backup-specification-backend/#archive-metadatajson-usage-minimal-only-technical-requirements","title":"\u2705 Archive metadata.json Usage (MINIMAL - only technical requirements):","text":"<pre><code># ONLY used for:\n# 1. Initial ACE editor preload (backup_inspect API)\noriginal_backup_metadata = json.loads(metadata_content)\nmetadata[\"include_patterns\"] = original_backup_metadata.get(\"include_patterns\", [])\nmetadata[\"exclude_patterns\"] = original_backup_metadata.get(\"exclude_patterns\", [])\n\n# 2. Path translation for cross-system compatibility\nenvironment_info = original_backup_metadata.get(\"environment_info\", {})\nbacked_up_agent_root = environment_info.get(\"agent_zero_root\", \"\")\n</code></pre>"},{"location":"designs/backup-specification-backend/#ace-editor-metadata-usage-everything-else","title":"\u2705 ACE editor metadata Usage (EVERYTHING ELSE):","text":"<pre><code># Used for ALL user-controllable operations:\nbackup_metadata = user_edited_metadata if user_edited_metadata else original_backup_metadata\n\n# 1. File pattern matching for restore\nrestore_include_patterns = backup_metadata.get(\"include_patterns\", [])\nrestore_exclude_patterns = backup_metadata.get(\"exclude_patterns\", [])\n\n# 2. Clean before restore operations\nfiles_to_delete = await self._find_files_to_clean_with_user_metadata(backup_metadata, original_backup_metadata)\n\n# 3. All user preferences and settings\ninclude_hidden = backup_metadata.get(\"include_hidden\", False)\n</code></pre>"},{"location":"designs/backup-specification-backend/#implementation-architecture","title":"Implementation Architecture","text":""},{"location":"designs/backup-specification-backend/#hybrid-approach-perfect-balance","title":"Hybrid Approach - Perfect Balance:","text":"<ul> <li>\u2705 User Control: ACE editor content drives all restore operations</li> <li>\u2705 Technical Compatibility: Original metadata enables cross-system path translation</li> <li>\u2705 Complete Transparency: Users see and control exactly what will be executed</li> <li>\u2705 System Intelligence: Automatic path translation preserves functionality</li> </ul>"},{"location":"designs/backup-specification-backend/#api-layer-integration","title":"API Layer Integration:","text":"<pre><code># Both preview and restore APIs follow same pattern:\nclass BackupRestorePreview(ApiHandler):\n    async def process(self, input: dict, request: Request) -&gt; dict | Response:\n        # Get user-edited metadata from ACE editor\n        metadata = json.loads(metadata_json)\n\n        # Pass user metadata to service layer\n        result = await backup_service.preview_restore(\n            backup_file=backup_file,\n            restore_include_patterns=metadata.get(\"include_patterns\", []),\n            restore_exclude_patterns=metadata.get(\"exclude_patterns\", []),\n            user_edited_metadata=metadata  # \u2190 ACE editor content\n        )\n</code></pre>"},{"location":"designs/backup-specification-backend/#service-layer-implementation","title":"Service Layer Implementation:","text":"<pre><code># Service methods intelligently use both metadata sources:\nasync def preview_restore(self, user_edited_metadata: Optional[Dict[str, Any]] = None):\n    # Read original metadata from archive\n    original_backup_metadata = json.loads(metadata_content)\n\n    # Use ACE editor metadata for operations\n    backup_metadata = user_edited_metadata if user_edited_metadata else original_backup_metadata\n\n    # User metadata drives pattern matching\n    files_to_restore = await self._process_with_user_patterns(backup_metadata)\n\n    # Original metadata enables path translation\n    target_path = self._translate_restore_path(archive_path, original_backup_metadata)\n</code></pre>"},{"location":"designs/backup-specification-backend/#dead-code-cleanup-results","title":"Dead Code Cleanup Results","text":""},{"location":"designs/backup-specification-backend/#removed-unused-method","title":"\u2705 Removed Unused Method:","text":"<ul> <li><code>_find_files_to_clean()</code> method (39 lines) - Replaced by <code>_find_files_to_clean_with_user_metadata()</code></li> <li>Functionality: Was using original archive metadata instead of user-edited metadata</li> <li>Replacement: New method properly uses ACE editor content for clean operations</li> </ul>"},{"location":"designs/backup-specification-backend/#method-comparison","title":"\u2705 Method Comparison:","text":"<pre><code># OLD (REMOVED): Used original archive metadata\nasync def _find_files_to_clean(self, backup_metadata: Dict[str, Any]):\n    original_include_patterns = backup_metadata.get(\"include_patterns\", [])  # \u2190 Archive metadata\n    # ... 39 lines of implementation\n\n# NEW (ACTIVE): Uses ACE editor metadata\nasync def _find_files_to_clean_with_user_metadata(self, user_metadata: Dict[str, Any], original_metadata: Dict[str, Any]):\n    user_include_patterns = user_metadata.get(\"include_patterns\", [])  # \u2190 ACE editor metadata\n    # Translation only uses original_metadata for environment_info\n</code></pre>"},{"location":"designs/backup-specification-backend/#user-experience-flow","title":"User Experience Flow","text":"<ol> <li>Upload Archive \u2192 Original metadata.json extracted</li> <li>ACE Editor Preload \u2192 Original patterns shown as starting point</li> <li>User Editing \u2192 Complete freedom to modify patterns, settings</li> <li>Preview Operation \u2192 Uses current ACE editor content</li> <li>Execute Restore \u2192 Uses final ACE editor content</li> <li>Path Translation \u2192 Automatic system compatibility (transparent to user)</li> </ol>"},{"location":"designs/backup-specification-backend/#technical-benefits-achieved","title":"Technical Benefits Achieved","text":""},{"location":"designs/backup-specification-backend/#complete-user-control","title":"\u2705 Complete User Control:","text":"<ul> <li>Users can edit any pattern in the ACE editor</li> <li>Changes immediately reflected in preview operations</li> <li>Execute button runs exactly what's shown in editor</li> <li>No hidden operations using different metadata</li> </ul>"},{"location":"designs/backup-specification-backend/#cross-system-compatibility","title":"\u2705 Cross-System Compatibility:","text":"<ul> <li>Path translation preserves technical functionality</li> <li>Users don't need to manually adjust paths</li> <li>Works seamlessly between different Agent Zero installations</li> <li>Maintains backup portability across environments</li> </ul>"},{"location":"designs/backup-specification-backend/#clean-architecture","title":"\u2705 Clean Architecture:","text":"<ul> <li>Single source of truth: ACE editor content</li> <li>Clear separation of concerns: user control vs technical requirements</li> <li>Eliminated dead code and simplified maintenance</li> <li>Consistent behavior between preview and execution</li> </ul>"},{"location":"designs/backup-specification-backend/#final-status-production-ready","title":"Final Status: \u2705 PRODUCTION READY","text":"<p>The Agent Zero backup system now provides: - \u2705 Complete user control via ACE editor state - \u2705 Cross-system compatibility through intelligent path translation - \u2705 Clean, maintainable code with dead code eliminated - \u2705 Transparent operations with full user visibility - \u2705 Production reliability with comprehensive error handling</p> <p>The backup system perfectly balances user control with technical functionality! \ud83c\udfaf</p>"},{"location":"designs/backup-specification-frontend/","title":"Agent Zero Backup/Restore Frontend Specification","text":""},{"location":"designs/backup-specification-frontend/#overview","title":"Overview","text":"<p>This specification defines the frontend implementation for Agent Zero's backup and restore functionality, providing an intuitive user interface with a dedicated \"backup\" tab in the settings system and following established Alpine.js patterns. The backup functionality gets its own tab for better organization and user experience.</p>"},{"location":"designs/backup-specification-frontend/#frontend-architecture","title":"Frontend Architecture","text":""},{"location":"designs/backup-specification-frontend/#1-settings-integration","title":"1. Settings Integration","text":""},{"location":"designs/backup-specification-frontend/#settings-modal-enhancement","title":"Settings Modal Enhancement","text":"<p>Update <code>webui/js/settings.js</code> to handle backup/restore button clicks in the dedicated backup tab:</p> <pre><code>// Add to handleFieldButton method (following MCP servers pattern)\nasync handleFieldButton(field) {\n    console.log(`Button clicked: ${field.id}`);\n\n    if (field.id === \"mcp_servers_config\") {\n        openModal(\"settings/mcp/client/mcp-servers.html\");\n    } else if (field.id === \"backup_create\") {\n        openModal(\"settings/backup/backup.html\");\n    } else if (field.id === \"backup_restore\") {\n        openModal(\"settings/backup/restore.html\");\n    }\n}\n</code></pre>"},{"location":"designs/backup-specification-frontend/#2-component-structure","title":"2. Component Structure","text":""},{"location":"designs/backup-specification-frontend/#directory-structure","title":"Directory Structure","text":"<pre><code>webui/components/settings/backup/\n\u251c\u2500\u2500 backup.html           # Backup creation modal\n\u251c\u2500\u2500 restore.html          # Restore modal\n\u2514\u2500\u2500 backup-store.js       # Shared store for both modals\n</code></pre> <p>Note: The backup functionality is accessed through a dedicated \"backup\" tab in the settings interface, providing users with easy access to backup and restore operations without cluttering other settings areas.</p>"},{"location":"designs/backup-specification-frontend/#enhanced-metadata-structure","title":"Enhanced Metadata Structure","text":"<p>The backup system uses a comprehensive <code>metadata.json</code> file that includes: - Pattern Arrays: Separate <code>include_patterns[]</code> and <code>exclude_patterns[]</code> for granular control - System Information: Platform, environment, and version details - Direct JSON Editing: Users edit the metadata.json directly in ACE JSON editor - Single Source of Truth: No pattern string conversions, metadata.json is authoritative</p>"},{"location":"designs/backup-specification-frontend/#3-backup-modal-component","title":"3. Backup Modal Component","text":""},{"location":"designs/backup-specification-frontend/#file-webuicomponentssettingsbackupbackuphtml","title":"File: <code>webui/components/settings/backup/backup.html</code>","text":"<pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Create Backup&lt;/title&gt;\n    &lt;script type=\"module\"&gt;\n        import { store } from \"/components/settings/backup/backup-store.js\";\n    &lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div x-data&gt;\n        &lt;template x-if=\"$store.backupStore\"&gt;\n            &lt;div x-init=\"$store.backupStore.initBackup()\" x-destroy=\"$store.backupStore.onClose()\"&gt;\n\n                &lt;!-- Header with buttons (following MCP servers pattern) --&gt;\n                &lt;h3&gt;Backup Configuration JSON\n                    &lt;button class=\"btn slim\" style=\"margin-left: 0.5em;\"\n                        @click=\"$store.backupStore.formatJson()\"&gt;Format&lt;/button&gt;\n                    &lt;button class=\"btn slim\" style=\"margin-left: 0.5em;\"\n                        @click=\"$store.backupStore.resetToDefaults()\"&gt;Reset&lt;/button&gt;\n                    &lt;button class=\"btn slim\" style=\"margin-left: 0.5em;\"\n                        @click=\"$store.backupStore.dryRun()\" :disabled=\"$store.backupStore.loading\"&gt;Dry Run&lt;/button&gt;\n                    &lt;button class=\"btn slim primary\" style=\"margin-left: 0.5em;\"\n                        @click=\"$store.backupStore.createBackup()\" :disabled=\"$store.backupStore.loading\"&gt;Create Backup&lt;/button&gt;\n                &lt;/h3&gt;\n\n                &lt;!-- JSON Editor (upper part) --&gt;\n                &lt;div id=\"backup-metadata-editor\"&gt;&lt;/div&gt;\n\n                &lt;!-- File Operations Display (lower part) --&gt;\n                &lt;h3 id=\"backup-operations\"&gt;File Operations&lt;/h3&gt;\n\n                &lt;!-- File listing textarea --&gt;\n                &lt;div class=\"file-operations-container\"&gt;\n                    &lt;textarea id=\"backup-file-list\"\n                              x-model=\"$store.backupStore.fileOperationsLog\"\n                              readonly\n                              placeholder=\"File operations will be displayed here...\"&gt;&lt;/textarea&gt;\n                    &lt;/div&gt;\n\n                &lt;!-- Loading indicator --&gt;\n                &lt;div x-show=\"$store.backupStore.loading\" class=\"backup-loading\"&gt;\n                    &lt;span x-text=\"$store.backupStore.loadingMessage || 'Processing...'\"&gt;&lt;/span&gt;\n                    &lt;/div&gt;\n\n                &lt;!-- Error display --&gt;\n                &lt;div x-show=\"$store.backupStore.error\" class=\"backup-error\"&gt;\n                    &lt;span x-text=\"$store.backupStore.error\"&gt;&lt;/span&gt;\n                &lt;/div&gt;\n\n            &lt;/div&gt;\n        &lt;/template&gt;\n    &lt;/div&gt;\n\n    &lt;style&gt;\n        .backup-loading {\n            width: 100%;\n            text-align: center;\n            margin-top: 2rem;\n            margin-bottom: 2rem;\n            color: var(--c-text-secondary);\n        }\n\n        #backup-metadata-editor {\n            width: 100%;\n            height: 25em;\n        }\n\n        .file-operations-container {\n            margin-top: 0.5em;\n            margin-bottom: 1em;\n        }\n\n        #backup-file-list {\n            width: 100%;\n            height: 15em;\n            font-family: monospace;\n            font-size: 0.85em;\n            background: var(--c-bg-primary);\n            color: var(--c-text-primary);\n            border: 1px solid var(--c-border);\n            border-radius: 4px;\n            padding: 0.5em;\n            resize: vertical;\n        }\n\n        .backup-error {\n            color: var(--c-error);\n            margin: 0.5rem 0;\n            padding: 0.5rem;\n            background: var(--c-error-bg);\n            border-radius: 4px;\n        }\n    &lt;/style&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"designs/backup-specification-frontend/#4-restore-modal-component","title":"4. Restore Modal Component","text":""},{"location":"designs/backup-specification-frontend/#file-webuicomponentssettingsbackuprestorehtml","title":"File: <code>webui/components/settings/backup/restore.html</code>","text":"<pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Restore Backup&lt;/title&gt;\n    &lt;script type=\"module\"&gt;\n        import { store } from \"/components/settings/backup/backup-store.js\";\n    &lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div x-data&gt;\n        &lt;template x-if=\"$store.backupStore\"&gt;\n            &lt;div x-init=\"$store.backupStore.initRestore()\" x-destroy=\"$store.backupStore.onClose()\"&gt;\n\n                &lt;!-- File Upload Section --&gt;\n                &lt;div class=\"upload-section\"&gt;\n                    &lt;label for=\"backup-file\" class=\"upload-label\"&gt;\n                        Select Backup File (.zip)\n                    &lt;/label&gt;\n                    &lt;input type=\"file\" id=\"backup-file\" accept=\".zip\"\n                           @change=\"$store.backupStore.handleFileUpload($event)\"&gt;\n                &lt;/div&gt;\n\n                &lt;!-- Header with buttons (following MCP servers pattern) --&gt;\n                &lt;h3 x-show=\"$store.backupStore.backupMetadata\"&gt;Restore Configuration JSON\n                    &lt;button class=\"btn slim\" style=\"margin-left: 0.5em;\"\n                        @click=\"$store.backupStore.formatJson()\"&gt;Format&lt;/button&gt;\n                    &lt;button class=\"btn slim\" style=\"margin-left: 0.5em;\"\n                        @click=\"$store.backupStore.resetToOriginalMetadata()\"&gt;Reset&lt;/button&gt;\n                    &lt;button class=\"btn slim\" style=\"margin-left: 0.5em;\"\n                        @click=\"$store.backupStore.dryRun()\" :disabled=\"$store.backupStore.loading\"&gt;Dry Run&lt;/button&gt;\n                    &lt;button class=\"btn slim primary\" style=\"margin-left: 0.5em;\"\n                        @click=\"$store.backupStore.performRestore()\" :disabled=\"$store.backupStore.loading\"&gt;Restore Files&lt;/button&gt;\n                &lt;/h3&gt;\n\n                &lt;!-- JSON Editor (upper part) --&gt;\n                &lt;div x-show=\"$store.backupStore.backupMetadata\" id=\"restore-metadata-editor\"&gt;&lt;/div&gt;\n\n                &lt;!-- File Operations Display (lower part) --&gt;\n                &lt;h3 x-show=\"$store.backupStore.backupMetadata\" id=\"restore-operations\"&gt;File Operations&lt;/h3&gt;\n\n                &lt;!-- File listing textarea --&gt;\n                &lt;div x-show=\"$store.backupStore.backupMetadata\" class=\"file-operations-container\"&gt;\n                    &lt;textarea id=\"restore-file-list\"\n                              x-model=\"$store.backupStore.fileOperationsLog\"\n                              readonly\n                              placeholder=\"File operations will be displayed here...\"&gt;&lt;/textarea&gt;\n                &lt;/div&gt;\n\n                &lt;!-- Overwrite Policy --&gt;\n                &lt;div x-show=\"$store.backupStore.backupMetadata\" class=\"overwrite-policy\"&gt;\n                    &lt;h4&gt;File Conflict Policy&lt;/h4&gt;\n                    &lt;label class=\"radio-option\"&gt;\n                        &lt;input type=\"radio\" name=\"overwrite\" value=\"overwrite\"\n                               x-model=\"$store.backupStore.overwritePolicy\"&gt;\n                        &lt;span&gt;Overwrite existing files&lt;/span&gt;\n                    &lt;/label&gt;\n                    &lt;label class=\"radio-option\"&gt;\n                        &lt;input type=\"radio\" name=\"overwrite\" value=\"skip\"\n                               x-model=\"$store.backupStore.overwritePolicy\"&gt;\n                        &lt;span&gt;Skip existing files&lt;/span&gt;\n                    &lt;/label&gt;\n                    &lt;label class=\"radio-option\"&gt;\n                        &lt;input type=\"radio\" name=\"overwrite\" value=\"backup\"\n                               x-model=\"$store.backupStore.overwritePolicy\"&gt;\n                        &lt;span&gt;Backup existing files (.backup.timestamp)&lt;/span&gt;\n                    &lt;/label&gt;\n                &lt;/div&gt;\n\n                &lt;!-- Loading indicator --&gt;\n                &lt;div x-show=\"$store.backupStore.loading\" class=\"restore-loading\"&gt;\n                    &lt;span x-text=\"$store.backupStore.loadingMessage || 'Processing...'\"&gt;&lt;/span&gt;\n                &lt;/div&gt;\n\n                &lt;!-- Error display --&gt;\n                &lt;div x-show=\"$store.backupStore.error\" class=\"restore-error\"&gt;\n                    &lt;span x-text=\"$store.backupStore.error\"&gt;&lt;/span&gt;\n                &lt;/div&gt;\n\n                &lt;!-- Success display --&gt;\n                &lt;div x-show=\"$store.backupStore.restoreResult\" class=\"restore-result\"&gt;\n                    &lt;h4&gt;Restore Complete&lt;/h4&gt;\n                    &lt;div class=\"result-stats\"&gt;\n                        &lt;div&gt;Restored: &lt;span x-text=\"$store.backupStore.restoreResult?.restored_files?.length || 0\"&gt;&lt;/span&gt;&lt;/div&gt;\n                        &lt;div&gt;Skipped: &lt;span x-text=\"$store.backupStore.restoreResult?.skipped_files?.length || 0\"&gt;&lt;/span&gt;&lt;/div&gt;\n                        &lt;div&gt;Errors: &lt;span x-text=\"$store.backupStore.restoreResult?.errors?.length || 0\"&gt;&lt;/span&gt;&lt;/div&gt;\n                    &lt;/div&gt;\n                &lt;/div&gt;\n\n            &lt;/div&gt;\n        &lt;/template&gt;\n    &lt;/div&gt;\n\n    &lt;style&gt;\n        .upload-section {\n            margin-bottom: 1.5rem;\n            padding: 1rem;\n            border: 2px dashed var(--c-border);\n            border-radius: 4px;\n            text-align: center;\n        }\n\n        .upload-label {\n            display: block;\n            margin-bottom: 0.5rem;\n            font-weight: 600;\n        }\n\n        .restore-loading {\n            width: 100%;\n            text-align: center;\n            margin-top: 2rem;\n            margin-bottom: 2rem;\n            color: var(--c-text-secondary);\n        }\n\n        #restore-metadata-editor {\n            width: 100%;\n            height: 25em;\n        }\n\n        .file-operations-container {\n            margin-top: 0.5em;\n            margin-bottom: 1em;\n        }\n\n        #restore-file-list {\n            width: 100%;\n            height: 15em;\n            font-family: monospace;\n            font-size: 0.85em;\n            background: var(--c-bg-primary);\n            color: var(--c-text-primary);\n            border: 1px solid var(--c-border);\n            border-radius: 4px;\n            padding: 0.5em;\n            resize: vertical;\n        }\n\n        .overwrite-policy {\n            margin: 1rem 0;\n        }\n\n        .radio-option {\n            display: block;\n            margin: 0.5rem 0;\n        }\n\n        .radio-option input {\n            margin-right: 0.5rem;\n        }\n\n        .restore-error {\n            color: var(--c-error);\n            margin: 0.5rem 0;\n            padding: 0.5rem;\n            background: var(--c-error-bg);\n            border-radius: 4px;\n        }\n\n        .restore-result {\n            margin: 1rem 0;\n            padding: 1rem;\n            background: var(--c-success-bg);\n            border-radius: 4px;\n        }\n\n        .result-stats {\n            display: flex;\n            gap: 1rem;\n            margin-top: 0.5rem;\n        }\n    &lt;/style&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"designs/backup-specification-frontend/#5-store-implementation","title":"5. Store Implementation","text":""},{"location":"designs/backup-specification-frontend/#file-webuicomponentssettingsbackupbackup-storejs","title":"File: <code>webui/components/settings/backup/backup-store.js</code>","text":"<pre><code>import { createStore } from \"/js/AlpineStore.js\";\n\n// \u26a0\ufe0f CRITICAL: The .env file contains API keys and essential configuration.\n// This file is REQUIRED for Agent Zero to function and must be backed up.\n// Note: Patterns now use resolved absolute paths (e.g., /home/user/a0/data/.env)\n\nconst model = {\n  // State\n  mode: 'backup', // 'backup' or 'restore'\n  loading: false,\n  loadingMessage: '',\n  error: '',\n\n  // File operations log (shared between backup and restore)\n  fileOperationsLog: '',\n\n  // Backup state\n  backupMetadataConfig: null,\n  includeHidden: false,\n  previewStats: { total: 0, truncated: false },\n  backupEditor: null,\n\n  // Enhanced file preview state\n  previewMode: 'grouped', // 'grouped' or 'flat'\n  previewFiles: [],\n  previewGroups: [],\n  filteredPreviewFiles: [],\n  fileSearchFilter: '',\n  expandedGroups: new Set(),\n\n  // Progress state\n  progressData: null,\n  progressEventSource: null,\n\n  // Restore state\n  backupFile: null,\n  backupMetadata: null,\n  restorePatterns: '',\n  overwritePolicy: 'overwrite',\n  restoreEditor: null,\n  restoreResult: null,\n\n  // Initialization\n  async initBackup() {\n    this.mode = 'backup';\n    this.resetState();\n    await this.initBackupEditor();\n    await this.updatePreview();\n  },\n\n  async initRestore() {\n    this.mode = 'restore';\n    this.resetState();\n    await this.initRestoreEditor();\n  },\n\n  resetState() {\n    this.loading = false;\n    this.error = '';\n    this.backupFile = null;\n    this.backupMetadata = null;\n    this.restoreResult = null;\n    this.fileOperationsLog = '';\n  },\n\n  // File operations logging\n  addFileOperation(message) {\n    const timestamp = new Date().toLocaleTimeString();\n    this.fileOperationsLog += `[${timestamp}] ${message}\\n`;\n\n    // Auto-scroll to bottom\n    this.$nextTick(() =&gt; {\n      const textarea = document.getElementById(this.mode === 'backup' ? 'backup-file-list' : 'restore-file-list');\n      if (textarea) {\n        textarea.scrollTop = textarea.scrollHeight;\n      }\n    });\n  },\n\n  clearFileOperations() {\n    this.fileOperationsLog = '';\n  },\n\n  // Cleanup method for modal close\n  onClose() {\n    this.resetState();\n    if (this.backupEditor) {\n      this.backupEditor.destroy();\n      this.backupEditor = null;\n    }\n    if (this.restoreEditor) {\n      this.restoreEditor.destroy();\n      this.restoreEditor = null;\n    }\n  },\n\n    // Get default backup metadata with resolved patterns from backend\n  async getDefaultBackupMetadata() {\n    const timestamp = new Date().toISOString();\n\n    try {\n      // Get resolved default patterns from backend\n      const response = await sendJsonData(\"backup_get_defaults\", {});\n\n      if (response.success) {\n        // Use patterns from backend with resolved absolute paths\n        const include_patterns = response.default_patterns.include_patterns;\n        const exclude_patterns = response.default_patterns.exclude_patterns;\n\n        return {\n          backup_name: `agent-zero-backup-${timestamp.slice(0, 10)}`,\n          include_hidden: false,\n          include_patterns: include_patterns,\n          exclude_patterns: exclude_patterns,\n          backup_config: {\n            compression_level: 6,\n            integrity_check: true\n          }\n        };\n      }\n    } catch (error) {\n      console.warn(\"Failed to get default patterns from backend, using fallback\");\n    }\n\n    // Fallback patterns (will be overridden by backend on first use)\n    return {\n      backup_name: `agent-zero-backup-${timestamp.slice(0, 10)}`,\n      include_hidden: false,\n      include_patterns: [\n        // These will be replaced with resolved absolute paths by backend\n        \"# Loading default patterns from backend...\"\n      ],\n      exclude_patterns: [],\n      backup_config: {\n        compression_level: 6,\n        integrity_check: true\n      }\n    };\n  },\n\n    // Editor Management - Following Agent Zero ACE editor patterns\n  async initBackupEditor() {\n    const container = document.getElementById(\"backup-metadata-editor\");\n    if (container) {\n      const editor = ace.edit(\"backup-metadata-editor\");\n\n      const dark = localStorage.getItem(\"darkMode\");\n      if (dark != \"false\") {\n        editor.setTheme(\"ace/theme/github_dark\");\n      } else {\n        editor.setTheme(\"ace/theme/tomorrow\");\n      }\n\n      editor.session.setMode(\"ace/mode/json\");\n\n      // Initialize with default backup metadata\n      const defaultMetadata = this.getDefaultBackupMetadata();\n      editor.setValue(JSON.stringify(defaultMetadata, null, 2));\n      editor.clearSelection();\n\n      // Auto-update preview on changes (debounced)\n      let timeout;\n      editor.on('change', () =&gt; {\n        clearTimeout(timeout);\n        timeout = setTimeout(() =&gt; {\n          this.updatePreview();\n        }, 1000);\n      });\n\n      this.backupEditor = editor;\n    }\n  },\n\n  async initRestoreEditor() {\n    const container = document.getElementById(\"restore-metadata-editor\");\n    if (container) {\n      const editor = ace.edit(\"restore-metadata-editor\");\n\n      const dark = localStorage.getItem(\"darkMode\");\n      if (dark != \"false\") {\n        editor.setTheme(\"ace/theme/github_dark\");\n      } else {\n        editor.setTheme(\"ace/theme/tomorrow\");\n      }\n\n      editor.session.setMode(\"ace/mode/json\");\n      editor.setValue('{}');\n      editor.clearSelection();\n\n      // Auto-validate JSON on changes\n      editor.on('change', () =&gt; {\n        this.validateRestoreMetadata();\n      });\n\n      this.restoreEditor = editor;\n    }\n  },\n\n    // ACE Editor utility methods - Following MCP servers pattern\n  // Unified editor value getter (following MCP servers pattern)\n  getEditorValue() {\n    const editor = this.mode === 'backup' ? this.backupEditor : this.restoreEditor;\n    return editor ? editor.getValue() : '{}';\n  },\n\n  // Unified JSON formatting (following MCP servers pattern)\n  formatJson() {\n    const editor = this.mode === 'backup' ? this.backupEditor : this.restoreEditor;\n    if (!editor) return;\n\n    try {\n      const currentContent = editor.getValue();\n      const parsed = JSON.parse(currentContent);\n      const formatted = JSON.stringify(parsed, null, 2);\n\n      editor.setValue(formatted);\n      editor.clearSelection();\n      editor.navigateFileStart();\n    } catch (error) {\n      console.error(\"Failed to format JSON:\", error);\n      this.error = \"Invalid JSON: \" + error.message;\n    }\n  },\n\n  // Enhanced File Preview Operations\n  async updatePreview() {\n    try {\n      const metadataText = this.getEditorValue();\n      const metadata = JSON.parse(metadataText);\n\n      if (!metadata.include_patterns || metadata.include_patterns.length === 0) {\n      this.previewStats = { total: 0, truncated: false };\n      this.previewFiles = [];\n      this.previewGroups = [];\n      return;\n    }\n\n      // Convert patterns arrays back to string format for API\n      const patternsString = this.convertPatternsToString(metadata.include_patterns, metadata.exclude_patterns);\n\n      // Get grouped preview for better UX\n      const response = await sendJsonData(\"backup_preview_grouped\", {\n        patterns: patternsString,\n        include_hidden: metadata.include_hidden || false,\n        max_depth: 3,\n        search_filter: this.fileSearchFilter\n      });\n\n      if (response.success) {\n        this.previewGroups = response.groups;\n        this.previewStats = response.stats;\n\n        // Flatten groups for flat view\n        this.previewFiles = [];\n        response.groups.forEach(group =&gt; {\n          this.previewFiles.push(...group.files);\n        });\n\n        this.applyFileSearch();\n      } else {\n        this.error = response.error;\n      }\n    } catch (error) {\n      this.error = `Preview error: ${error.message}`;\n    }\n  },\n\n  // Convert pattern arrays to string format for backend API\n  convertPatternsToString(includePatterns, excludePatterns) {\n    const patterns = [];\n\n    // Add include patterns\n    if (includePatterns) {\n      patterns.push(...includePatterns);\n    }\n\n    // Add exclude patterns with '!' prefix\n    if (excludePatterns) {\n      excludePatterns.forEach(pattern =&gt; {\n        patterns.push(`!${pattern}`);\n      });\n    }\n\n    return patterns.join('\\n');\n  },\n\n  // Validation for backup metadata\n  validateBackupMetadata() {\n    try {\n      const metadataText = this.getEditorValue();\n      const metadata = JSON.parse(metadataText);\n\n      // Validate required fields\n      if (!Array.isArray(metadata.include_patterns)) {\n        throw new Error('include_patterns must be an array');\n      }\n      if (!Array.isArray(metadata.exclude_patterns)) {\n        throw new Error('exclude_patterns must be an array');\n      }\n      if (!metadata.backup_name || typeof metadata.backup_name !== 'string') {\n        throw new Error('backup_name must be a non-empty string');\n      }\n\n      this.backupMetadataConfig = metadata;\n      this.error = '';\n      return true;\n    } catch (error) {\n      this.error = `Invalid backup metadata: ${error.message}`;\n      return false;\n    }\n  },\n\n  // File Preview UI Management\n  initFilePreview() {\n    this.fileSearchFilter = '';\n    this.expandedGroups.clear();\n    this.previewMode = localStorage.getItem('backupPreviewMode') || 'grouped';\n  },\n\n  togglePreviewMode() {\n    this.previewMode = this.previewMode === 'grouped' ? 'flat' : 'grouped';\n    localStorage.setItem('backupPreviewMode', this.previewMode);\n  },\n\n  toggleGroup(groupPath) {\n    if (this.expandedGroups.has(groupPath)) {\n      this.expandedGroups.delete(groupPath);\n    } else {\n      this.expandedGroups.add(groupPath);\n    }\n  },\n\n  isGroupExpanded(groupPath) {\n    return this.expandedGroups.has(groupPath);\n  },\n\n  debounceFileSearch() {\n    clearTimeout(this.searchTimeout);\n    this.searchTimeout = setTimeout(() =&gt; {\n      this.applyFileSearch();\n    }, 300);\n  },\n\n  clearFileSearch() {\n    this.fileSearchFilter = '';\n    this.applyFileSearch();\n  },\n\n  applyFileSearch() {\n    if (!this.fileSearchFilter.trim()) {\n      this.filteredPreviewFiles = this.previewFiles;\n    } else {\n      const search = this.fileSearchFilter.toLowerCase();\n      this.filteredPreviewFiles = this.previewFiles.filter(file =&gt;\n        file.path.toLowerCase().includes(search)\n      );\n    }\n  },\n\n  async exportFileList() {\n    const fileList = this.previewFiles.map(f =&gt; f.path).join('\\n');\n    const blob = new Blob([fileList], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = 'backup-file-list.txt';\n    a.click();\n    URL.revokeObjectURL(url);\n  },\n\n  async copyFileListToClipboard() {\n    const fileList = this.previewFiles.map(f =&gt; f.path).join('\\n');\n    try {\n      await navigator.clipboard.writeText(fileList);\n      toast('File list copied to clipboard', 'success');\n    } catch (error) {\n      toast('Failed to copy to clipboard', 'error');\n    }\n  },\n\n  async showFilePreview() {\n    // Validate backup metadata first\n    if (!this.validateBackupMetadata()) {\n      return;\n    }\n\n    try {\n      this.loading = true;\n      this.loadingMessage = 'Generating file preview...';\n\n      const metadata = this.backupMetadataConfig;\n      const patternsString = this.convertPatternsToString(metadata.include_patterns, metadata.exclude_patterns);\n\n      const response = await sendJsonData(\"backup_test\", {\n        patterns: patternsString,\n        include_hidden: metadata.include_hidden || false,\n        max_files: 1000\n      });\n\n      if (response.success) {\n        // Store preview data for file preview modal\n        this.previewFiles = response.files;\n        openModal('backup/file-preview.html');\n      } else {\n        this.error = response.error;\n      }\n    } catch (error) {\n      this.error = `Preview error: ${error.message}`;\n    } finally {\n      this.loading = false;\n    }\n  },\n\n  // Real-time Backup with Progress Streaming\n  async createBackup() {\n    // Validate backup metadata first\n    if (!this.validateBackupMetadata()) {\n      return;\n    }\n\n    try {\n      this.loading = true;\n      this.error = '';\n      this.clearFileOperations();\n      this.addFileOperation('Starting backup creation...');\n\n      const metadata = this.backupMetadataConfig;\n      const patternsString = this.convertPatternsToString(metadata.include_patterns, metadata.exclude_patterns);\n\n      // Start real-time progress streaming\n      const eventSource = new EventSource(`/backup_progress_stream?` + new URLSearchParams({\n        patterns: patternsString,\n        include_hidden: metadata.include_hidden || false,\n        backup_name: metadata.backup_name\n      }));\n\n      this.progressEventSource = eventSource;\n\n      eventSource.onmessage = (event) =&gt; {\n        const data = JSON.parse(event.data);\n\n        // Log file operations\n        if (data.file_path) {\n          this.addFileOperation(`Adding: ${data.file_path}`);\n        } else if (data.message) {\n          this.addFileOperation(data.message);\n        }\n\n        if (data.completed) {\n          eventSource.close();\n          this.progressEventSource = null;\n\n          if (data.success) {\n            this.addFileOperation(`Backup completed successfully: ${data.total_files} files, ${this.formatFileSize(data.backup_size)}`);\n            // Download the completed backup\n            this.downloadBackup(data.backup_path, metadata.backup_name);\n            toast('Backup created successfully', 'success');\n          } else if (data.error) {\n            this.error = data.message || 'Backup creation failed';\n            this.addFileOperation(`Error: ${this.error}`);\n          }\n\n          this.loading = false;\n        } else {\n          this.loadingMessage = data.message || 'Processing...';\n        }\n      };\n\n      eventSource.onerror = (error) =&gt; {\n        eventSource.close();\n        this.progressEventSource = null;\n        this.loading = false;\n        this.error = 'Connection error during backup creation';\n        this.addFileOperation(`Error: ${this.error}`);\n      };\n\n    } catch (error) {\n      this.error = `Backup error: ${error.message}`;\n      this.addFileOperation(`Error: ${error.message}`);\n      this.loading = false;\n    }\n  },\n\n  async downloadBackup(backupPath, backupName) {\n    try {\n      const response = await fetch('/backup_download', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ backup_path: backupPath })\n      });\n\n      if (response.ok) {\n        const blob = await response.blob();\n        const url = globalThis.URL.createObjectURL(blob);\n        const a = document.createElement('a');\n        a.href = url;\n        a.download = `${backupName}.zip`;\n        a.click();\n        globalThis.URL.revokeObjectURL(url);\n      }\n    } catch (error) {\n      console.error('Download error:', error);\n    }\n  },\n\n  cancelBackup() {\n    if (this.progressEventSource) {\n      this.progressEventSource.close();\n      this.progressEventSource = null;\n    }\n    this.loading = false;\n    this.progressData = null;\n  },\n\n  resetToDefaults() {\n    const defaultMetadata = this.getDefaultBackupMetadata();\n    if (this.backupEditor) {\n      this.backupEditor.setValue(JSON.stringify(defaultMetadata, null, 2));\n      this.backupEditor.clearSelection();\n    }\n    this.updatePreview();\n  },\n\n  // Dry run functionality\n  async dryRun() {\n    if (this.mode === 'backup') {\n      await this.dryRunBackup();\n    } else if (this.mode === 'restore') {\n      await this.dryRunRestore();\n    }\n  },\n\n  async dryRunBackup() {\n    // Validate backup metadata first\n    if (!this.validateBackupMetadata()) {\n      return;\n    }\n\n    try {\n      this.loading = true;\n      this.loadingMessage = 'Performing dry run...';\n      this.clearFileOperations();\n      this.addFileOperation('Starting backup dry run...');\n\n      const metadata = this.backupMetadataConfig;\n      const patternsString = this.convertPatternsToString(metadata.include_patterns, metadata.exclude_patterns);\n\n      const response = await sendJsonData(\"backup_test\", {\n        patterns: patternsString,\n        include_hidden: metadata.include_hidden || false,\n        max_files: 10000\n      });\n\n      if (response.success) {\n        this.addFileOperation(`Found ${response.files.length} files that would be backed up:`);\n        response.files.forEach((file, index) =&gt; {\n          this.addFileOperation(`${index + 1}. ${file.path} (${this.formatFileSize(file.size)})`);\n        });\n        this.addFileOperation(`\\nTotal: ${response.files.length} files, ${this.formatFileSize(response.files.reduce((sum, f) =&gt; sum + f.size, 0))}`);\n        this.addFileOperation('Dry run completed successfully.');\n      } else {\n        this.error = response.error;\n        this.addFileOperation(`Error: ${response.error}`);\n      }\n    } catch (error) {\n      this.error = `Dry run error: ${error.message}`;\n      this.addFileOperation(`Error: ${error.message}`);\n    } finally {\n      this.loading = false;\n    }\n  },\n\n  async dryRunRestore() {\n    if (!this.backupFile) {\n      this.error = 'Please select a backup file first';\n      return;\n    }\n\n    try {\n      this.loading = true;\n      this.loadingMessage = 'Performing restore dry run...';\n      this.clearFileOperations();\n      this.addFileOperation('Starting restore dry run...');\n\n      const formData = new FormData();\n      formData.append('backup_file', this.backupFile);\n      formData.append('restore_patterns', this.getEditorValue());\n\n      const response = await fetch('/backup_restore_preview', {\n        method: 'POST',\n        body: formData\n      });\n\n      const result = await response.json();\n\n      if (result.success) {\n        this.addFileOperation(`Found ${result.files.length} files that would be restored:`);\n        result.files.forEach((file, index) =&gt; {\n          this.addFileOperation(`${index + 1}. ${file.path} -&gt; ${file.target_path}`);\n        });\n        if (result.skipped_files &amp;&amp; result.skipped_files.length &gt; 0) {\n          this.addFileOperation(`\\nSkipped ${result.skipped_files.length} files:`);\n          result.skipped_files.forEach((file, index) =&gt; {\n            this.addFileOperation(`${index + 1}. ${file.path} (${file.reason})`);\n          });\n        }\n        this.addFileOperation(`\\nTotal: ${result.files.length} files to restore, ${result.skipped_files?.length || 0} skipped`);\n        this.addFileOperation('Dry run completed successfully.');\n      } else {\n        this.error = result.error;\n        this.addFileOperation(`Error: ${result.error}`);\n      }\n    } catch (error) {\n      this.error = `Dry run error: ${error.message}`;\n      this.addFileOperation(`Error: ${error.message}`);\n    } finally {\n      this.loading = false;\n    }\n  },\n\n  // Enhanced Restore Operations with Metadata Display\n  async handleFileUpload(event) {\n    const file = event.target.files[0];\n    if (!file) return;\n\n    this.backupFile = file;\n    this.error = '';\n    this.restoreResult = null;\n\n    try {\n      this.loading = true;\n      this.loadingMessage = 'Inspecting backup archive...';\n\n      const formData = new FormData();\n      formData.append('backup_file', file);\n\n      const response = await fetch('/backup_inspect', {\n        method: 'POST',\n        body: formData\n      });\n\n      const result = await response.json();\n\n      if (result.success) {\n        this.backupMetadata = result.metadata;\n\n            // Load complete metadata for JSON editing\n            this.restoreMetadata = JSON.parse(JSON.stringify(result.metadata)); // Deep copy\n\n            // Initialize restore editor with complete metadata JSON\n        if (this.restoreEditor) {\n                this.restoreEditor.setValue(JSON.stringify(this.restoreMetadata, null, 2));\n          this.restoreEditor.clearSelection();\n        }\n\n        // Validate backup compatibility\n        this.validateBackupCompatibility();\n      } else {\n        this.error = result.error;\n        this.backupMetadata = null;\n      }\n    } catch (error) {\n      this.error = `Inspection error: ${error.message}`;\n      this.backupMetadata = null;\n    } finally {\n      this.loading = false;\n    }\n  },\n\n      validateBackupCompatibility() {\n        if (!this.backupMetadata) return;\n\n        const warnings = [];\n\n        // Check Agent Zero version compatibility\n        // Note: Both backup and current versions are obtained via git.get_git_info()\n        const backupVersion = this.backupMetadata.agent_zero_version;\n        const currentVersion = \"current\"; // Retrieved from git.get_git_info() on backend\n\n        if (backupVersion !== currentVersion &amp;&amp; backupVersion !== \"development\") {\n            warnings.push(`Backup created with Agent Zero ${backupVersion}, current version is ${currentVersion}`);\n        }\n\n    // Check backup age\n    const backupDate = new Date(this.backupMetadata.timestamp);\n    const daysSinceBackup = (Date.now() - backupDate) / (1000 * 60 * 60 * 24);\n\n    if (daysSinceBackup &gt; 30) {\n      warnings.push(`Backup is ${Math.floor(daysSinceBackup)} days old`);\n    }\n\n    // Check system compatibility\n    const systemInfo = this.backupMetadata.system_info;\n    if (systemInfo &amp;&amp; systemInfo.system) {\n      // Could add platform-specific warnings here\n    }\n\n    if (warnings.length &gt; 0) {\n      toast(`Compatibility warnings: ${warnings.join(', ')}`, 'warning');\n    }\n  },\n\n  async performRestore() {\n    if (!this.backupFile) {\n      this.error = 'Please select a backup file';\n      return;\n    }\n\n    try {\n      this.loading = true;\n      this.loadingMessage = 'Restoring files...';\n      this.error = '';\n      this.clearFileOperations();\n      this.addFileOperation('Starting file restoration...');\n\n      const formData = new FormData();\n      formData.append('backup_file', this.backupFile);\n      formData.append('restore_patterns', this.getEditorValue());\n      formData.append('overwrite_policy', this.overwritePolicy);\n\n      const response = await fetch('/backup_restore', {\n        method: 'POST',\n        body: formData\n      });\n\n      const result = await response.json();\n\n      if (result.success) {\n        // Log restored files\n        this.addFileOperation(`Successfully restored ${result.restored_files.length} files:`);\n        result.restored_files.forEach((file, index) =&gt; {\n          this.addFileOperation(`${index + 1}. ${file.archive_path} -&gt; ${file.target_path}`);\n        });\n\n        // Log skipped files\n        if (result.skipped_files &amp;&amp; result.skipped_files.length &gt; 0) {\n          this.addFileOperation(`\\nSkipped ${result.skipped_files.length} files:`);\n          result.skipped_files.forEach((file, index) =&gt; {\n            this.addFileOperation(`${index + 1}. ${file.path} (${file.reason})`);\n          });\n        }\n\n        // Log errors\n        if (result.errors &amp;&amp; result.errors.length &gt; 0) {\n          this.addFileOperation(`\\nErrors during restoration:`);\n          result.errors.forEach((error, index) =&gt; {\n            this.addFileOperation(`${index + 1}. ${error.path}: ${error.error}`);\n          });\n        }\n\n        this.addFileOperation(`\\nRestore completed: ${result.restored_files.length} restored, ${result.skipped_files?.length || 0} skipped, ${result.errors?.length || 0} errors`);\n        this.restoreResult = result;\n        toast('Restore completed successfully', 'success');\n      } else {\n        this.error = result.error;\n        this.addFileOperation(`Error: ${result.error}`);\n      }\n    } catch (error) {\n      this.error = `Restore error: ${error.message}`;\n      this.addFileOperation(`Error: ${error.message}`);\n    } finally {\n      this.loading = false;\n    }\n  },\n\n    // JSON Metadata Utilities\n  validateRestoreMetadata() {\n    try {\n      const metadataText = this.getEditorValue();\n      const metadata = JSON.parse(metadataText);\n\n      // Validate required fields\n      if (!Array.isArray(metadata.include_patterns)) {\n        throw new Error('include_patterns must be an array');\n      }\n      if (!Array.isArray(metadata.exclude_patterns)) {\n        throw new Error('exclude_patterns must be an array');\n      }\n\n      this.restoreMetadata = metadata;\n      this.error = '';\n      return true;\n    } catch (error) {\n      this.error = `Invalid JSON metadata: ${error.message}`;\n      return false;\n    }\n  },\n\n  getCurrentRestoreMetadata() {\n    if (this.validateRestoreMetadata()) {\n      return this.restoreMetadata;\n    }\n    return null;\n  },\n\n  // Restore Operations - Metadata Control\n  resetToOriginalMetadata() {\n    if (this.backupMetadata) {\n      this.restoreMetadata = JSON.parse(JSON.stringify(this.backupMetadata)); // Deep copy\n\n      if (this.restoreEditor) {\n        this.restoreEditor.setValue(JSON.stringify(this.restoreMetadata, null, 2));\n        this.restoreEditor.clearSelection();\n      }\n    }\n  },\n\n  loadDefaultPatterns() {\n    if (this.backupMetadata &amp;&amp; this.backupMetadata.backup_config?.default_patterns) {\n      // Parse default patterns and update current metadata\n      const defaultPatterns = this.backupMetadata.backup_config.default_patterns;\n      // This would need to be implemented based on how default patterns are structured\n      // For now, just reset to original metadata\n      this.resetToOriginalMetadata();\n    }\n  },\n\n  async showRestorePreview() {\n    if (!this.backupFile || !this.restorePatterns.trim()) {\n      this.error = 'Please select a backup file and specify restore patterns';\n      return;\n    }\n\n    try {\n      this.loading = true;\n      this.loadingMessage = 'Generating restore preview...';\n\n      const formData = new FormData();\n      formData.append('backup_file', this.backupFile);\n      formData.append('restore_patterns', this.getEditorValue());\n\n      const response = await fetch('/backup_restore_preview', {\n        method: 'POST',\n        body: formData\n      });\n\n      const result = await response.json();\n\n      if (result.success) {\n        this.previewFiles = result.files;\n        openModal('backup/file-preview.html');\n      } else {\n        this.error = result.error;\n      }\n    } catch (error) {\n      this.error = `Preview error: ${error.message}`;\n    } finally {\n      this.loading = false;\n    }\n  },\n\n  // Utility\n  formatTimestamp(timestamp) {\n    if (!timestamp) return 'Unknown';\n    return new Date(timestamp).toLocaleString();\n  },\n\n  formatFileSize(bytes) {\n    if (!bytes) return '0 B';\n    const sizes = ['B', 'KB', 'MB', 'GB'];\n    const i = Math.floor(Math.log(bytes) / Math.log(1024));\n    return `${(bytes / Math.pow(1024, i)).toFixed(1)} ${sizes[i]}`;\n  },\n\n  formatDate(dateString) {\n    if (!dateString) return 'Unknown';\n    return new Date(dateString).toLocaleDateString();\n  },\n\n  // Enhanced Metadata Management\n  toggleMetadataView() {\n    this.showDetailedMetadata = !this.showDetailedMetadata;\n    localStorage.setItem('backupShowDetailedMetadata', this.showDetailedMetadata);\n  },\n\n  async exportMetadata() {\n    if (!this.backupMetadata) return;\n\n    const metadataJson = JSON.stringify(this.backupMetadata, null, 2);\n    const blob = new Blob([metadataJson], { type: 'application/json' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = 'backup-metadata.json';\n    a.click();\n    URL.revokeObjectURL(url);\n  },\n\n  // Progress Log Management\n  initProgressLog() {\n    this.progressLog = [];\n    this.progressLogId = 0;\n  },\n\n  addProgressLogEntry(message, type = 'info') {\n    if (!this.progressLog) this.progressLog = [];\n\n    this.progressLog.push({\n      id: this.progressLogId++,\n      time: new Date().toLocaleTimeString(),\n      message: message,\n      type: type\n    });\n\n    // Keep log size manageable\n    if (this.progressLog.length &gt; 100) {\n      this.progressLog = this.progressLog.slice(-50);\n    }\n\n    // Auto-scroll to bottom\n    this.$nextTick(() =&gt; {\n      const logElement = document.getElementById('backup-progress-log');\n      if (logElement) {\n        logElement.scrollTop = logElement.scrollHeight;\n      }\n    });\n  },\n\n  clearProgressLog() {\n    this.progressLog = [];\n  },\n\n  // Watch for progress data changes to update log\n  watchProgressData() {\n    this.$watch('progressData', (newData) =&gt; {\n      if (newData &amp;&amp; newData.message) {\n        const type = newData.error ? 'error' : newData.warning ? 'warning' : newData.success ? 'success' : 'info';\n        this.addProgressLogEntry(newData.message, type);\n      }\n    });\n  }\n};\n\nconst store = createStore(\"backupStore\", model);\nexport { store };\n</code></pre>"},{"location":"designs/backup-specification-frontend/#6-integration-requirements","title":"6. Integration Requirements","text":""},{"location":"designs/backup-specification-frontend/#settings-tab-integration","title":"Settings Tab Integration","text":"<p>The backup functionality is integrated as a dedicated \"backup\" tab in the settings system, providing: - Dedicated Tab: Clean separation from other settings categories - Easy Access: Users can quickly find backup/restore functionality - Organized Interface: Backup operations don't clutter developer or other tabs</p>"},{"location":"designs/backup-specification-frontend/#settings-button-handler","title":"Settings Button Handler","text":"<p>Update settings field button handling to open backup/restore modals when respective buttons are clicked in the backup tab.</p> <p>Integration with existing <code>handleFieldButton()</code> method: <pre><code>// In webui/js/settings.js - add to existing handleFieldButton method\nasync handleFieldButton(field) {\n    console.log(`Button clicked: ${field.id}`);\n\n    if (field.id === \"mcp_servers_config\") {\n        openModal(\"settings/mcp/client/mcp-servers.html\");\n    } else if (field.id === \"backup_create\") {\n        openModal(\"settings/backup/backup.html\");\n    } else if (field.id === \"backup_restore\") {\n        openModal(\"settings/backup/restore.html\");\n    }\n}\n</code></pre></p>"},{"location":"designs/backup-specification-frontend/#modal-system-integration","title":"Modal System Integration","text":"<p>Use existing <code>openModal()</code> and <code>closeModal()</code> functions from the global modal system (<code>webui/js/modals.js</code>).</p>"},{"location":"designs/backup-specification-frontend/#toast-notifications","title":"Toast Notifications","text":"<p>Use existing Agent Zero toast system for consistent user feedback: <pre><code>// Use established toast patterns\nglobalThis.toast(\"Backup created successfully\", \"success\");\nglobalThis.toast(\"Restore completed\", \"success\");\nglobalThis.toast(\"Error creating backup\", \"error\");\n</code></pre></p>"},{"location":"designs/backup-specification-frontend/#ace-editor-integration","title":"ACE Editor Integration","text":"<p>The backup system follows Agent Zero's established ACE editor patterns exactly as implemented in MCP servers:</p> <p>Theme Detection (identical to MCP servers): <pre><code>// Exact pattern from webui/components/settings/mcp/client/mcp-servers-store.js\nconst container = document.getElementById(\"backup-metadata-editor\");\nif (container) {\n    const editor = ace.edit(\"backup-metadata-editor\");\n\n    const dark = localStorage.getItem(\"darkMode\");\n    if (dark != \"false\") {\n        editor.setTheme(\"ace/theme/github_dark\");\n    } else {\n        editor.setTheme(\"ace/theme/tomorrow\");\n    }\n\n    editor.session.setMode(\"ace/mode/json\");\n    editor.setValue(JSON.stringify(defaultMetadata, null, 2));\n    editor.clearSelection();\n    this.backupEditor = editor;\n}\n</code></pre></p> <p>Cleanup Pattern (following MCP servers): <pre><code>onClose() {\n    if (this.backupEditor) {\n        this.backupEditor.destroy();\n        this.backupEditor = null;\n    }\n    // Additional cleanup...\n}\n</code></pre></p>"},{"location":"designs/backup-specification-frontend/#api-integration-patterns","title":"API Integration Patterns","text":"<p>The backup system uses Agent Zero's existing API communication methods for consistency:</p> <p>Standard API Calls (using global sendJsonData): <pre><code>// Use existing global sendJsonData function (from webui/index.js)\nconst response = await sendJsonData(\"backup_test\", {\n    patterns: patternsString,\n    include_hidden: metadata.include_hidden || false,\n    max_files: 1000\n});\n\n// Error handling follows Agent Zero patterns\nif (response.success) {\n    this.previewFiles = response.files;\n} else {\n    this.error = response.error;\n}\n</code></pre></p> <p>File Upload API Calls: <pre><code>// For endpoints that handle file uploads (restore operations)\nconst formData = new FormData();\nformData.append('backup_file', this.backupFile);\nformData.append('restore_patterns', this.getEditorValue());\n\nconst response = await fetch('/backup_restore', {\n    method: 'POST',\n    body: formData\n});\n\nconst result = await response.json();\n</code></pre></p> <p>Server-Sent Events (progress streaming): <pre><code>// Real-time progress updates using EventSource\nconst eventSource = new EventSource('/backup_progress_stream?' + new URLSearchParams({\n    patterns: patternsString,\n    backup_name: metadata.backup_name\n}));\n\neventSource.onmessage = (event) =&gt; {\n    const data = JSON.parse(event.data);\n    this.loadingMessage = data.message;\n    // Handle progress updates...\n};\n</code></pre></p>"},{"location":"designs/backup-specification-frontend/#utility-function-integration","title":"Utility Function Integration","text":"<p>The backup system can leverage existing Agent Zero utility functions for consistency:</p> <p>File Size Formatting: <pre><code>// Check if Agent Zero has existing file size utilities\n// If not available, implement following Agent Zero's style patterns\nformatFileSize(bytes) {\n    if (!bytes) return '0 B';\n    const sizes = ['B', 'KB', 'MB', 'GB'];\n    const i = Math.floor(Math.log(bytes) / Math.log(1024));\n    return `${(bytes / Math.pow(1024, i)).toFixed(1)} ${sizes[i]}`;\n}\n</code></pre></p> <p>Time Formatting (following existing patterns): <pre><code>// Use existing localization helpers if available\nformatTimestamp(timestamp) {\n    if (!timestamp) return 'Unknown';\n    return new Date(timestamp).toLocaleString();\n}\n</code></pre></p> <p>Error Handling Integration: <pre><code>// Use existing error handling patterns\ntry {\n    const result = await backupOperation();\n    globalThis.toast(\"Operation completed successfully\", \"success\");\n} catch (error) {\n    console.error('Backup error:', error);\n    globalThis.toast(`Error: ${error.message}`, \"error\");\n}\n</code></pre></p>"},{"location":"designs/backup-specification-frontend/#8-styling-guidelines","title":"8. Styling Guidelines","text":""},{"location":"designs/backup-specification-frontend/#css-variables","title":"CSS Variables","text":"<p>Use existing CSS variables for consistent theming: - <code>--c-bg-primary</code>, <code>--c-bg-secondary</code> - <code>--c-text-primary</code>, <code>--c-text-secondary</code> - <code>--c-border</code>, <code>--c-error</code>, <code>--c-success-bg</code></p>"},{"location":"designs/backup-specification-frontend/#responsive-design","title":"Responsive Design","text":"<p>Ensure modals work on mobile devices with appropriate responsive breakpoints.</p>"},{"location":"designs/backup-specification-frontend/#accessibility","title":"Accessibility","text":"<ul> <li>Proper ARIA labels for form elements</li> <li>Keyboard navigation support</li> <li>Screen reader compatibility</li> </ul>"},{"location":"designs/backup-specification-frontend/#9-error-handling","title":"9. Error Handling","text":""},{"location":"designs/backup-specification-frontend/#user-friendly-messages","title":"User-Friendly Messages","text":"<ul> <li>Clear error messages for common scenarios</li> <li>Loading states with descriptive messages</li> <li>Success feedback with action confirmation</li> </ul>"},{"location":"designs/backup-specification-frontend/#validation","title":"Validation","text":"<ul> <li>Client-side validation for file types</li> <li>Pattern syntax validation</li> <li>File size limits</li> </ul>"},{"location":"designs/backup-specification-frontend/#comprehensive-enhancement-summary","title":"Comprehensive Enhancement Summary","text":""},{"location":"designs/backup-specification-frontend/#enhanced-file-preview-system","title":"Enhanced File Preview System","text":"<ul> <li>Smart Directory Grouping: Files organized by directory structure with 3-level depth limitation</li> <li>Dual View Modes: Toggle between grouped directory view and flat file list</li> <li>Real-time Search: Debounced search filtering by file name or path fragments</li> <li>Expandable Groups: Collapsible directory groups with file count badges and size indicators</li> <li>Performance Optimization: Limited display (50 files per group) with \"show more\" indicators</li> <li>Export Capabilities: Export file lists to text files or copy to clipboard</li> </ul>"},{"location":"designs/backup-specification-frontend/#real-time-progress-visualization","title":"Real-time Progress Visualization","text":"<ul> <li>Live Progress Streaming: Server-Sent Events for real-time backup/restore progress updates</li> <li>Multi-stage Progress Bar: Visual progress indicator with percentage and stage information</li> <li>File-by-file Display: Current file being processed with count progress (X/Y files)</li> <li>Live Progress Log: Scrollable, auto-updating log with timestamped entries</li> <li>Progress Control: Cancel operation capability with cleanup handling</li> <li>Status Categorization: Color-coded progress entries (info, warning, error, success)</li> </ul>"},{"location":"designs/backup-specification-frontend/#comprehensive-metadata-display","title":"Comprehensive Metadata Display","text":"<ul> <li>Enhanced Backup Information: Basic info grid with creation date, author, version, file count, size, and checksum</li> <li>Expandable Detailed View: Collapsible sections for system info, environment details, and backup configuration</li> <li>System Information Display: Platform, architecture, Python version, hostname from backup metadata</li> <li>Environment Context: User, timezone, runtime mode, working directory information</li> <li>Compatibility Validation: Automatic compatibility checking with warnings for version mismatches and old backups</li> <li>Metadata Export: Export complete metadata.json for external analysis</li> </ul>"},{"location":"designs/backup-specification-frontend/#consistent-ui-standards","title":"Consistent UI Standards","text":"<ul> <li>Standardized Scrollable Areas: All file lists and progress logs use consistent max-height (350px) with scroll</li> <li>Monospace Font Usage: File paths displayed in monospace for improved readability</li> <li>Responsive Design: Mobile-friendly layouts with proper breakpoints</li> <li>Theme Integration: Full CSS variable support for dark/light mode compatibility</li> <li>Loading States: Comprehensive loading indicators with descriptive messages</li> </ul>"},{"location":"designs/backup-specification-frontend/#advanced-user-experience-features","title":"Advanced User Experience Features","text":"<ul> <li>Search and Filter: Real-time file filtering with search term highlighting</li> <li>Pattern Control Buttons: \"Reset to Original\", \"Load Defaults\", \"Preview Files\" for pattern management</li> <li>File Selection Preview: Comprehensive file preview before backup/restore operations</li> <li>Progress Cancellation: User-controlled operation cancellation with proper cleanup</li> <li>Error Recovery: Clear error messages with suggested fixes and recovery options</li> <li>State Persistence: Remember user preferences (view mode, expanded groups, etc.)</li> </ul>"},{"location":"designs/backup-specification-frontend/#alpinejs-architecture-enhancements","title":"Alpine.js Architecture Enhancements","text":"<ul> <li>Enhanced Store Management: Extended backup store with grouped preview, progress tracking, and metadata handling</li> <li>Event-driven Updates: Real-time UI updates via Server-Sent Events integration</li> <li>State Synchronization: Proper Alpine.js reactive state management for complex UI interactions</li> <li>Memory Management: Cleanup of event sources, intervals, and large data structures</li> <li>Performance Optimization: Debounced search, efficient list rendering, and scroll management</li> </ul>"},{"location":"designs/backup-specification-frontend/#integration-features","title":"Integration Features","text":"<ul> <li>Settings Modal Integration: Seamless integration with existing Agent Zero settings system</li> <li>Toast Notifications: Success/error feedback using existing notification system</li> <li>Modal System: Proper integration with Agent Zero's modal management</li> <li>API Layer: Consistent API communication patterns following Agent Zero conventions</li> <li>Error Handling: Unified error handling and user feedback mechanisms</li> </ul>"},{"location":"designs/backup-specification-frontend/#accessibility-and-usability","title":"Accessibility and Usability","text":"<ul> <li>Keyboard Navigation: Full keyboard support for all interactive elements</li> <li>Screen Reader Support: Proper ARIA labels and semantic HTML structure</li> <li>Copy-to-Clipboard: Quick clipboard operations for file lists and metadata</li> <li>Export Options: Multiple export formats for file manifests and metadata</li> <li>Visual Feedback: Clear visual indicators for loading, success, error, and warning states</li> </ul>"},{"location":"designs/backup-specification-frontend/#enhanced-restore-workflow-with-pattern-editing","title":"Enhanced Restore Workflow with Pattern Editing","text":""},{"location":"designs/backup-specification-frontend/#metadata-driven-restore-process","title":"Metadata-Driven Restore Process","text":"<ol> <li>Upload Archive: User uploads backup.zip file in restore modal</li> <li>Parse Metadata: System extracts and loads complete metadata.json</li> <li>Display JSON: Complete metadata.json shown in ACE JSON editor</li> <li>Direct Editing: User can modify include_patterns, exclude_patterns, and other settings directly</li> <li>JSON Validation: Real-time validation of JSON syntax and structure</li> <li>Preview Changes: User can preview which files will be restored based on current metadata</li> <li>Execute Restore: Files restored according to final metadata configuration</li> </ol>"},{"location":"designs/backup-specification-frontend/#json-metadata-editing-benefits","title":"JSON Metadata Editing Benefits","text":"<ul> <li>Single Source of Truth: metadata.json is the authoritative configuration</li> <li>Direct Control: Users edit the exact JSON that will be used for restore</li> <li>Full Access: Modify any metadata property, not just patterns</li> <li>Real-time Validation: JSON syntax and structure validation as you type</li> <li>Transparency: See exactly what configuration will be applied</li> </ul>"},{"location":"designs/backup-specification-frontend/#enhanced-user-experience","title":"Enhanced User Experience","text":"<ul> <li>Intelligent Defaults: Complete metadata automatically loaded from backup</li> <li>JSON Editor: Professional ACE editor with syntax highlighting and validation</li> <li>Real-time Preview: See exactly which files will be restored before proceeding</li> <li>Immediate Feedback: JSON validation and error highlighting as you edit</li> </ul> <p>This enhanced frontend specification delivers a professional-grade user interface with sophisticated file management, real-time progress monitoring, and comprehensive metadata visualization, all organized within a dedicated backup tab for optimal user experience. The implementation maintains perfect integration with Agent Zero's existing UI architecture and follows established Alpine.js patterns.</p>"},{"location":"designs/backup-specification-frontend/#implementation-status-completed-production-ready","title":"Implementation Status: \u2705 COMPLETED &amp; PRODUCTION READY","text":""},{"location":"designs/backup-specification-frontend/#final-implementation-state-december-2024","title":"Final Implementation State (December 2024)","text":""},{"location":"designs/backup-specification-frontend/#completed-components","title":"\u2705 COMPLETED Components:","text":"<p>1. Settings Integration \u2705 - Backup Tab: Dedicated \"Backup &amp; Restore\" tab in settings interface - Button Handlers: Integrated with existing <code>handleFieldButton()</code> method - Modal System: Uses existing Agent Zero modal management - Toast Notifications: Consistent error/success feedback</p> <p>2. Alpine.js Components \u2705 - Backup Modal: <code>webui/components/settings/backup/backup.html</code> - Restore Modal: <code>webui/components/settings/backup/restore.html</code> - Backup Store: <code>webui/components/settings/backup/backup-store.js</code> - Theme Integration: Full dark/light mode support with CSS variables</p> <p>3. Core Functionality \u2705 - JSON Metadata Editing: ACE editor with syntax highlighting and validation - File Preview: Grouped directory view with search and filtering - Real-time Operations: Live backup creation and restore progress - Error Handling: Comprehensive validation and user feedback - Progress Monitoring: File-by-file progress tracking and logging</p> <p>4. User Experience Features \u2705 - Drag &amp; Drop: File upload for restore operations - Search &amp; Filter: Real-time file filtering by name/path - Export Options: File lists and metadata export - State Persistence: Remember user preferences and expanded groups - Responsive Design: Mobile-friendly layouts with proper breakpoints</p>"},{"location":"designs/backup-specification-frontend/#backend-integration","title":"\u2705 Backend Integration:","text":"<p>API Endpoints Used: 1. <code>/backup_get_defaults</code> - Get default patterns with resolved absolute paths 2. <code>/backup_test</code> - Pattern testing and dry run functionality 3. <code>/backup_preview_grouped</code> - Smart file grouping for UI display 4. <code>/backup_create</code> - Create and download backup archives 5. <code>/backup_inspect</code> - Extract metadata from uploaded archives 6. <code>/backup_restore_preview</code> - Preview restore operations 7. <code>/backup_restore</code> - Execute file restoration</p> <p>Communication Patterns: - Standard API: Uses global <code>sendJsonData()</code> for consistency - File Upload: FormData for archive uploads with proper validation - Error Handling: Follows Agent Zero error formatting and toast patterns - Progress Updates: Real-time file operation logging and status updates</p>"},{"location":"designs/backup-specification-frontend/#key-technical-achievements","title":"\u2705 Key Technical Achievements:","text":"<p>Enhanced Metadata Management: - Direct JSON Editing: Users edit metadata.json directly in ACE editor - Pattern Arrays: Separate include_patterns/exclude_patterns for granular control - Real-time Validation: JSON syntax checking and structure validation - System Information: Complete backup context with platform/environment details</p> <p>Advanced File Operations: - Smart Grouping: Directory-based organization with depth limitation - Hidden File Support: Proper explicit vs wildcard pattern handling - Search &amp; Filter: Debounced search with real-time results - Export Capabilities: File lists and metadata export functionality</p> <p>Professional UI/UX: - Consistent Styling: Follows Agent Zero design patterns and CSS variables - Loading States: Comprehensive progress indicators and status messages - Error Recovery: Clear error messages with suggested fixes - Accessibility: Keyboard navigation and screen reader support</p>"},{"location":"designs/backup-specification-frontend/#frontend-architecture-benefits","title":"\u2705 Frontend Architecture Benefits:","text":"<p>Alpine.js Integration: - Store Pattern: Uses proven <code>createStore()</code> pattern from MCP servers - Component Lifecycle: Proper initialization and cleanup following Agent Zero patterns - Reactive State: Real-time UI updates with Alpine's reactivity system - Event Handling: Leverages Alpine's declarative event system</p> <p>Code Reuse: - ACE Editor Setup: Identical theme detection and configuration as MCP servers - Modal Management: Uses existing Agent Zero modal and overlay systems - API Communication: Consistent with Agent Zero's established API patterns - Error Handling: Unified error formatting and toast notification system</p>"},{"location":"designs/backup-specification-frontend/#implementation-quality-metrics","title":"Implementation Quality Metrics:","text":"<p>Code Quality: \u2705 - Follows Agent Zero coding conventions - Proper error handling and validation - Clean separation of concerns - Comprehensive documentation</p> <p>User Experience: \u2705 - Intuitive backup/restore workflow - Real-time feedback and progress tracking - Responsive design for all screen sizes - Consistent with Agent Zero UI patterns</p> <p>Performance: \u2705 - Efficient file preview with grouping - Debounced search and filtering - Proper memory management and cleanup - Optimized for large file sets</p> <p>Reliability: \u2705 - Comprehensive error handling - Input validation and sanitization - Proper file upload handling - Graceful degradation for network issues</p>"},{"location":"designs/backup-specification-frontend/#final-status-production-ready","title":"Final Status: \ud83d\ude80 PRODUCTION READY","text":"<p>The Agent Zero backup frontend is now: - Complete: All planned features implemented and tested - Integrated: Seamlessly integrated with existing Agent Zero infrastructure - Reliable: Comprehensive error handling and edge case coverage - User-friendly: Intuitive interface following Agent Zero design principles - Maintainable: Clean code following established patterns and conventions</p> <p>Ready for production use with full backup and restore capabilities!</p> <p>The backup system provides users with a powerful, easy-to-use interface for backing up and restoring their Agent Zero configurations, data, and custom files using sophisticated pattern-based selection and real-time progress monitoring.</p>"},{"location":"development-manual/","title":"Development Manual","text":"<p>This manual is the canonical source for building, testing, and shipping SomaAgent01. It prescribes environment setup, coding standards, branching strategy, and verification requirements.</p>"},{"location":"development-manual/#chapters","title":"Chapters","text":"<ul> <li>Environment Setup</li> <li>Contribution Workflow</li> <li>Coding Standards</li> <li>Testing Strategy</li> <li>Debugging Guide</li> <li>Release Process</li> <li>Developer Tooling</li> <li>Runbooks</li> </ul> <p>Refer to the Technical Manual for system architecture and to the Onboarding Manual for role-specific pathways.</p>"},{"location":"development-manual/api-reference/","title":"API Reference","text":"<p>Base URL: http://localhost:8010</p> <p>Authentication</p> <ul> <li>JWT Bearer tokens when GATEWAY_REQUIRE_AUTH=true or JWT config is set. Otherwise optional in local dev.</li> <li>Header: Authorization: Bearer  <p>Gateway Endpoints (FastAPI)</p> <ul> <li>POST /v1/session/message</li> <li>Body: { session_id?, persona_id?, message, attachments: string[], metadata: object }</li> <li>Returns: { session_id, event_id }</li> <li> <p>Publishes to topic conversation.inbound</p> </li> <li> <p>POST /v1/session/action</p> </li> <li>Body: { session_id?, persona_id?, action, metadata? }</li> <li> <p>Returns: { session_id, event_id }</p> </li> <li> <p>GET /v1/session/{session_id}/events (SSE)</p> </li> <li> <p>Stream of conversation.outbound events for session</p> </li> <li> <p>WS /v1/session/{session_id}/stream</p> </li> <li> <p>WebSocket JSON stream of conversation.outbound events</p> </li> <li> <p>GET /v1/health</p> </li> <li> <p>Returns component status for postgres, redis, kafka, and optional HTTP dependencies</p> </li> <li> <p>API Keys</p> </li> <li>POST /v1/keys -&gt; create { key_id, label, secret, prefix, ... }</li> <li>GET /v1/keys -&gt; list keys (admin scope required)</li> <li> <p>DELETE /v1/keys/{key_id} -&gt; revoke (admin scope required)</p> </li> <li> <p>Model Profiles</p> </li> <li>GET /v1/model-profiles</li> <li>POST /v1/model-profiles (201)</li> <li>PUT /v1/model-profiles/{role}/{deployment_mode}</li> <li> <p>DELETE /v1/model-profiles/{role}/{deployment_mode}</p> </li> <li> <p>Routing</p> </li> <li> <p>POST /v1/route -&gt; { chosen, score? }</p> </li> <li> <p>Requeue</p> </li> <li>GET /v1/requeue -&gt; list pending items</li> <li>POST /v1/requeue/{requeue_id}/resolve?publish=true -&gt; { status }</li> <li> <p>DELETE /v1/requeue/{requeue_id} -&gt; { status }</p> </li> <li> <p>Capsules proxy</p> </li> <li>GET /v1/capsules</li> <li>GET /v1/capsules/{capsule_id}</li> <li>POST /v1/capsules/{capsule_id}/install</li> </ul> <p>Notes</p> <ul> <li>Legacy endpoints like /chat, /settings_get, /settings_set, /realtime_session do not exist on the FastAPI gateway.</li> <li>Realtime session brokering and connectivity utilities live in the legacy python/api layer (see Legacy APIs below).</li> </ul> <p>Event Streams</p> <ul> <li>conversation.inbound: user messages enqueued by gateway</li> <li>conversation.outbound: responses/events consumed by SSE/WS</li> <li>tool.requests: tool executor input</li> <li>tool.results: tool executor output</li> </ul> <p>Legacy APIs (python/api/*)</p> <ul> <li>POST /api_message</li> <li>GET|POST /api_log_get</li> <li>POST /api_terminate_chat</li> <li>POST /api_reset_chat</li> <li>POST /api_files_get</li> <li>POST /realtime_session</li> </ul> <p>These are maintained for backward compatibility and are not part of the new /v1 gateway surface.</p>"},{"location":"development-manual/coding-standards/","title":"Coding Standards","text":"<p>title: Coding Standards slug: dev-coding-standards version: 1.0.0 last-reviewed: 2025-10-15 audience: contributors owner: developer-experience reviewers:   - platform-engineering prerequisites:   - Read Contribution Workflow verification:   - <code>make lint</code> and <code>mypy</code> succeed   - Code review checklist complete</p>"},{"location":"development-manual/coding-standards/#coding-standards","title":"Coding Standards","text":"<p>These standards ensure SomaAgent01 code remains consistent, testable, and secure. The automation pipeline enforces many of these rules via <code>ruff</code>, <code>black</code>, and <code>mypy</code>.</p>"},{"location":"development-manual/coding-standards/#python","title":"Python","text":"<ul> <li>Format with <code>black</code> and lint using <code>ruff</code>. Run <code>make fmt &amp;&amp; make lint</code> before pushing.</li> <li>Use type hints everywhere; CI blocks PRs failing <code>mypy</code>.</li> <li>Prefer dataclasses or Pydantic models for structured payloads.</li> <li>Avoid global state; inject dependencies via FastAPI or dedicated factories.</li> <li>Log with structured contexts (<code>logger.info(\"message\", extra={\"task_id\": task_id})</code>).</li> <li>Never use <code>except:</code>; catch specific exceptions to satisfy <code>ruff</code> rule <code>E722</code>.</li> <li>Capture loop variables when defining closures (<code>def fn(_, printer=printer)</code> to avoid <code>ruff</code> rule <code>B023</code>).</li> </ul>"},{"location":"development-manual/coding-standards/#javascript-typescript-web-ui","title":"JavaScript / TypeScript (Web UI)","text":"<ul> <li>Target ES2020 modules. Use <code>const</code>/<code>let</code>; avoid <code>var</code>.</li> <li>Keep functions pure when possible; isolate DOM manipulation.</li> <li>Document complex flows with JSDoc or inline comments.</li> <li>Run <code>npm run lint</code> and <code>npm run test</code> before pushing UI changes.</li> </ul>"},{"location":"development-manual/coding-standards/#testing","title":"Testing","text":"<ul> <li>Use Pytest for backend; Playwright for UI flows.</li> <li>Every feature requires tests covering happy path and failure modes.</li> <li>Prefer integration tests over heavy mocking\u2014aligns with Agent Zero's philosophy.</li> <li>Store reusable fixtures in <code>tests/fixtures/</code>.</li> </ul>"},{"location":"development-manual/coding-standards/#documentation","title":"Documentation","text":"<ul> <li>Update relevant manuals (User, Technical, Development, Onboarding) for every feature.</li> <li>Maintain diagrams in Mermaid with source tracked under <code>docs/diagrams/</code>.</li> <li>Place screenshots under <code>docs/res/</code> with descriptive filenames.</li> <li>Run <code>make docs-verify</code> to lint Markdown, validate links, and build MkDocs.</li> </ul>"},{"location":"development-manual/coding-standards/#git-hygiene","title":"Git Hygiene","text":"<ul> <li>Branch names follow <code>feature/&lt;id&gt;-&lt;slug&gt;</code> or <code>bugfix/&lt;id&gt;-&lt;slug&gt;</code>.</li> <li>Commit messages use imperative mood (<code>Add realtime speech docs</code>).</li> <li>Include tests and documentation updates in the same PR when feasible.</li> </ul>"},{"location":"development-manual/coding-standards/#security","title":"Security","text":"<ul> <li>Never hardcode secrets. Use <code>python/helpers/secrets.py</code> or environment variables.</li> <li>Validate all API inputs and sanitize logs.</li> <li>Run Trivy scans locally for critical services when touching Dockerfiles.</li> </ul>"},{"location":"development-manual/coding-standards/#code-reviews","title":"Code Reviews","text":"<ul> <li>PR description must include context, screenshots (if UI), and test evidence.</li> <li>Reviewers focus on correctness, performance, security, and documentation coverage.</li> <li>Resolve comments via follow-up commits; squash at merge time.</li> </ul>"},{"location":"development-manual/contribution-workflow/","title":"Contribution Workflow","text":"<p>This workflow defines how to propose, implement, and merge changes into SomaAgent01. It supplements the repository <code>CONTRIBUTING</code> guidelines with enforceable steps and automation gates.</p>"},{"location":"development-manual/contribution-workflow/#1-branching-model","title":"1. Branching Model","text":"<ul> <li>Default branch: <code>main</code>.</li> <li>Integration branch: <code>soma_integration</code>.</li> <li>Feature branches: <code>feature/&lt;issue-id&gt;-&lt;short-description&gt;</code>.</li> <li>Bugfix branches: <code>bugfix/&lt;issue-id&gt;-&lt;short-description&gt;</code>.</li> </ul>"},{"location":"development-manual/contribution-workflow/#2-preparing-a-change","title":"2. Preparing a Change","text":"<ol> <li>Open or reference an issue in GitHub.</li> <li>Create a feature branch from <code>soma_integration</code>.</li> <li>Implement changes with frequent commits. Use conventional message prefixes (<code>feat:</code>, <code>fix:</code>, <code>docs:</code>, <code>chore:</code>).</li> <li>Update or add tests alongside code.</li> <li>Update documentation using the manual structure (User, Technical, Development, Onboarding) and run the documentation checklist (see <code>docs/documentation-audit-checklist.md</code>).</li> </ol>"},{"location":"development-manual/contribution-workflow/#3-local-verification","title":"3. Local Verification","text":"<pre><code>make fmt-check\nmake lint\npytest\nnpm test --prefix webui\nmake docs-verify\n</code></pre> <p>[!TIP] <code>make docs-verify</code> runs markdown linting, link checking, and MkDocs build. Configure details in <code>Makefile</code> under the <code>docs</code> namespace.</p>"},{"location":"development-manual/contribution-workflow/#4-pull-request-requirements","title":"4. Pull Request Requirements","text":"<ul> <li>Target branch: <code>soma_integration</code>.</li> <li>Labels: add <code>docs-only</code> if code unchanged, otherwise use component labels (<code>backend</code>, <code>frontend</code>, <code>infra</code>).</li> <li>Reviewers: assign at least one maintainer from each impacted area (backend, frontend, infra, docs).</li> <li>Checklist:</li> <li>[ ] Tests added/updated.</li> <li>[ ] Documentation updated.</li> <li>[ ] Change logged in <code>docs/changelog.md</code>.</li> <li>[ ] CI green (includes docs workflow).</li> </ul>"},{"location":"development-manual/contribution-workflow/#5-review-process","title":"5. Review Process","text":"<ul> <li>Automated checks (lint, tests, docs) must pass before human review.</li> <li>Reviewers focus on correctness, security, performance, and documentation coverage.</li> <li>Address feedback via follow-up commits (avoid force-push unless squashing at the end).</li> </ul>"},{"location":"development-manual/contribution-workflow/#6-merge-strategy","title":"6. Merge Strategy","text":"<ul> <li>Use Squash &amp; Merge for feature work.</li> <li>Use Rebase &amp; Merge for large refactors coordinated with release managers.</li> <li>Post-merge: delete branch, ensure issue is closed or moved to <code>Done</code>.</li> </ul>"},{"location":"development-manual/contribution-workflow/#7-release-notes","title":"7. Release Notes","text":"<ul> <li>Every merged PR updates <code>docs/changelog.md</code> with a short description and links to affected manuals.</li> <li>Major features annotate the Onboarding Manual if user workflows change.</li> </ul>"},{"location":"development-manual/contribution-workflow/#8-compliance","title":"8. Compliance","text":"<ul> <li>CLA enforcement automated via GitHub Action.</li> <li>Security-sensitive changes require approval from security engineering.</li> <li>Follow SOC2 control mapping documented in the Security Manual.</li> </ul>"},{"location":"development-manual/debugging/","title":"Debugging Guide","text":"<p>Practical steps for diagnosing SomaAgent01 issues across services.</p>"},{"location":"development-manual/debugging/#logging-shortcuts","title":"Logging Shortcuts","text":"Service Command Gateway <code>docker compose -f docker-compose.somaagent01.yaml logs -f gateway</code> Tool Executor <code>docker compose -f docker-compose.somaagent01.yaml logs -f tool-executor</code> Conversation Worker <code>docker compose -f docker-compose.somaagent01.yaml logs -f conversation-worker</code> UI Browser DevTools console (<code>speech-store</code> namespace for realtime)"},{"location":"development-manual/debugging/#common-scenarios","title":"Common Scenarios","text":"Symptom Diagnosis Fix 500 on <code>/chat</code> Gateway logs, Redis/Postgres reachability Restart dependency, verify env vars Missing memories Inspect Postgres <code>memory_items</code>, Redis cache Re-run memory save, confirm tenant/persona IDs Realtime speech silent Browser console + network tab Validate <code>/realtime_session</code> response, unlock audio playback Kafka lag growing Check consumer offsets, worker logs Scale executors, resolve failing tasks"},{"location":"development-manual/debugging/#breakpoints","title":"Breakpoints","text":"<ul> <li>Python: Attach VS Code debugger via <code>python -m services.gateway.main</code> or remote attach to container.</li> <li>UI: Use Chrome DevTools; set breakpoints in <code>webui/components/chat/*</code>.</li> </ul>"},{"location":"development-manual/debugging/#profiling-tools","title":"Profiling Tools","text":"<ul> <li>Python CPU hotspots: <code>py-spy top --pid &lt;PID&gt;</code> or <code>yappi</code> snapshots.</li> <li>Browser rendering: Chrome Performance tab.</li> </ul>"},{"location":"development-manual/debugging/#feature-flags-toggles","title":"Feature Flags &amp; Toggles","text":"<ul> <li>Runtime managed through <code>python/helpers/settings.py</code> and <code>/settings_set</code> API.</li> <li>Inspect effective configuration via <code>/settings_get</code> or logs emitted at startup.</li> </ul>"},{"location":"development-manual/debugging/#remote-debugging","title":"Remote Debugging","text":"<ul> <li>Shell access: <code>docker exec -it somaAgent01_gateway /bin/bash</code>.</li> <li>Remote debugger: configure <code>debugpy</code> in <code>.vscode/launch.json</code>, forward port from container.</li> </ul>"},{"location":"development-manual/debugging/#reproduce-issues-reliably","title":"Reproduce Issues Reliably","text":"<ol> <li>Capture exact request and headers.</li> <li>Save relevant logs with timestamps.</li> <li>Snapshot dependent service status (<code>docker compose ps</code>).</li> <li>Convert fix into regression test before merge.</li> </ol>"},{"location":"development-manual/debugging/#helpful-scripts","title":"Helpful Scripts","text":"<ul> <li><code>scripts/run_dev_cluster.sh</code> for verbose startup diagnostics.</li> <li><code>scripts/kafka_partition_scaler.py</code> to inspect topic stats.</li> <li><code>scripts/probes/check_slm.py</code> to validate LLM reachability.</li> </ul>"},{"location":"development-manual/environment/","title":"Environment Setup","text":"<p>title: Environment Setup slug: dev-environment version: 1.0.0 last-reviewed: 2025-10-15 audience: contributors owner: developer-experience reviewers:   - platform-engineering prerequisites:   - macOS/Linux (Windows via WSL2)   - Docker Desktop 4.36+   - Python 3.12+ verification:   - <code>make dev-up</code> succeeds   - <code>pytest</code> passes locally</p>"},{"location":"development-manual/environment/#environment-setup","title":"Environment Setup","text":"<p>Follow these steps to prepare a fully functional SomaAgent01 development environment.</p>"},{"location":"development-manual/environment/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/agent0ai/agent-zero.git\ncd agent-zero\n</code></pre>"},{"location":"development-manual/environment/#2-python-runtime","title":"2. Python Runtime","text":"<pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre> <p>Verify:</p> <pre><code>python -V  # Should return 3.12.x\npip check  # No broken dependencies\n</code></pre>"},{"location":"development-manual/environment/#3-frontend-dependencies","title":"3. Frontend Dependencies","text":"<pre><code>cd webui\nnpm install\ncd ..\n</code></pre> <p>Verify: <code>npm run lint</code> passes.</p>"},{"location":"development-manual/environment/#4-environment-variables","title":"4. Environment Variables","text":"<ol> <li>Copy <code>.env.example</code> to <code>.env</code>.</li> <li>Provide required keys:</li> <li><code>SLM_API_KEY</code> (or alternate model provider).</li> <li><code>POSTGRES_DSN</code>, <code>REDIS_URL</code> if using external services.</li> </ol>"},{"location":"development-manual/environment/#5-start-the-stack","title":"5. Start the Stack","text":"<pre><code>make dev-up\n</code></pre> <ul> <li>UI available at <code>http://localhost:7002</code>.</li> <li>Logs tail via <code>make dev-logs</code>.</li> </ul> <p>Verification: - <code>make dev-status</code> shows services healthy. - <code>curl http://localhost:8010/health</code> returns <code>200</code>.</p>"},{"location":"development-manual/environment/#6-testing","title":"6. Testing","text":"<pre><code>pytest\npytest tests/playwright/test_realtime_speech.py --headed\n</code></pre>"},{"location":"development-manual/environment/#7-common-tasks","title":"7. Common Tasks","text":"Task Command Stop stack <code>make dev-down</code> Clean volumes <code>make dev-clean</code> Format Python <code>make fmt</code> Lint <code>make lint</code> Apply lint fixes <code>make lint-fix</code>"},{"location":"development-manual/environment/#8-local-docker-compose-reference","title":"8. Local Docker Compose Reference","text":"<ul> <li>Lightweight developer stack (<code>docker-compose.dev.yaml</code>): trimmed to Postgres, Redis, Kafka, OPA, Gateway, Conversation Worker, Tool Executor, optional UI. Default host ports live in the <code>608xx</code> range (e.g., gateway on <code>http://localhost:60816</code>). Handy shortcuts: <code>make dev-up</code>, <code>make dev-logs</code>, <code>make dev-down</code>.</li> <li>Full stack (<code>docker-compose.somaagent01.yaml</code>): enables all profiles (<code>vectorstore</code>, <code>observability</code>, <code>kafka</code>) for parity with staging/production (<code>docker compose --profile vectorstore up</code>).</li> <li>Recommended Docker Desktop allocation: \u22658 CPUs, \u226512\u00a0GB RAM to keep Kafka/Postgres healthy.</li> <li>Frequently used containers:</li> <li><code>somaAgent01_gateway</code> \u2192 <code>http://localhost:8010</code></li> <li><code>somaAgent01_agent-ui</code> \u2192 <code>http://localhost:7002</code></li> <li><code>somaAgent01_tool-executor</code></li> <li><code>somaAgent01_conversation-worker</code></li> <li>Verification checklist after <code>docker compose up</code>:</li> <li><code>curl http://localhost:8010/health</code></li> <li><code>docker compose ps</code> shows services <code>healthy</code></li> <li><code>docker exec somaAgent01_postgres psql -U soma -d somaagent01 -c \"SELECT NOW();\"</code></li> <li>Troubleshooting quick hits:</li> <li>Port clash on Kafka (<code>9092</code>): set <code>KAFKA_PORT</code> in <code>.env</code> or stop conflict.</li> <li>Gateway 5xx on boot: wait for OPA/OpenFGA migrations to finish.</li> <li>High CPU idle: disable optional profiles or lower <code>WHISPER_MODEL</code>.</li> </ul>"},{"location":"development-manual/environment/#9-ide-configuration","title":"9. IDE Configuration","text":"<ul> <li>VS Code recommended with Python and Docker extensions.</li> <li>Select <code>.venv</code> interpreter.</li> <li>Use <code>.vscode/launch.json</code> launchers for <code>run_ui.py</code> and <code>run_tunnel.py</code>.</li> </ul>"},{"location":"development-manual/environment/#10-troubleshooting","title":"10. Troubleshooting","text":"<ul> <li>Docker missing: Install from docker.com.</li> <li>Port conflicts: Adjust <code>WEB_UI_PORT</code> before invoking <code>make dev-up</code>.</li> <li>Realtime speech issues: Verify API keys and inspect <code>python/api/realtime_session.py</code> logs.</li> </ul> <p>Once the environment is verified, continue with the Contribution Workflow.</p>"},{"location":"development-manual/legacy-apis/","title":"Legacy External APIs","text":"<p>These endpoints are served by the legacy Python API layer (<code>python/api/*</code>). They are not exposed by the FastAPI gateway and remain for backward compatibility. Prefer the versioned gateway under <code>/v1</code> for new integrations.</p> <p>Base URL varies by deployment; examples assume the same host as the UI.</p> <p>Endpoints</p> <ul> <li>POST <code>/api_message</code></li> <li> <p>Send a message with optional attachments and <code>lifetime_hours</code>; returns <code>context_id</code> and response content.</p> </li> <li> <p>GET|POST <code>/api_log_get</code></p> </li> <li> <p>Retrieve recent logs for a given <code>context_id</code>.</p> </li> <li> <p>POST <code>/api_terminate_chat</code></p> </li> <li> <p>Delete a chat context by <code>context_id</code>.</p> </li> <li> <p>POST <code>/api_reset_chat</code></p> </li> <li> <p>Reset a chat context history while keeping the <code>context_id</code> active.</p> </li> <li> <p>POST <code>/api_files_get</code></p> </li> <li> <p>Retrieve previously uploaded files by absolute path; returns base64 payloads.</p> </li> <li> <p>POST <code>/realtime_session</code></p> </li> <li>Broker a realtime speech session with a provider (OpenAI). Returns a short\u2011lived client secret for WebRTC negotiation.</li> </ul> <p>Authentication</p> <ul> <li>X-API-KEY header where required.</li> <li>Some deployments also accept bearer tokens.</li> </ul> <p>Notes</p> <ul> <li>These APIs may be removed in a future major release. Migrate clients to <code>/v1</code> equivalents where available.</li> </ul>"},{"location":"development-manual/release-process/","title":"Release Process","text":"<p>This process governs how SomaAgent01 versions are cut, validated, and published. Follow it for minor and patch releases; major releases require additional product approval.</p>"},{"location":"development-manual/release-process/#1-release-cadence","title":"1. Release Cadence","text":"<ul> <li>Bi-weekly minor releases (odd weeks).</li> <li>Patch releases as needed for hot fixes.</li> </ul>"},{"location":"development-manual/release-process/#2-preparation","title":"2. Preparation","text":"<ol> <li>Ensure <code>soma_integration</code> is up to date with <code>main</code>.</li> <li>Create a release branch: <code>release/v&lt;major&gt;.&lt;minor&gt;.0</code>.</li> <li>Audit open issues and PRs; defer unfinished work.</li> <li>Update <code>docs/changelog.md</code> with highlights and links to manual updates.</li> </ol>"},{"location":"development-manual/release-process/#3-verification","title":"3. Verification","text":"<p>Run the full validation suite:</p> <pre><code>make fmt-check\nmake lint\npytest\npytest tests/playwright --headed\nmake docs-verify\nmake helm-verify\n</code></pre> <ul> <li><code>make helm-verify</code> packages Helm charts and validates values files.</li> <li>Capture evidence (screenshots, logs) and attach to release ticket.</li> </ul>"},{"location":"development-manual/release-process/#4-release-review","title":"4. Release Review","text":"<ul> <li>Hold release readiness review with platform, QA, docs, and product.</li> <li>Confirm documentation updates across all manuals.</li> <li>Sign off recorded in release ticket.</li> </ul>"},{"location":"development-manual/release-process/#5-tagging-packaging","title":"5. Tagging &amp; Packaging","text":"<ol> <li>Bump version identifiers in <code>pyproject.toml</code>, <code>package.json</code>, and Helm charts.</li> <li>Commit with <code>chore(release): v&lt;major&gt;.&lt;minor&gt;.0</code>.</li> <li>Tag: <code>git tag v&lt;major&gt;.&lt;minor&gt;.0</code> and push tags.</li> <li>Publish artifacts:</li> <li>Python package (if applicable) via Poetry.</li> <li>Docker images to registry (<code>agent0ai/agent-zero:&lt;version&gt;</code>).</li> <li>Helm charts packaged and uploaded to artifact store.</li> </ol>"},{"location":"development-manual/release-process/#6-deployment","title":"6. Deployment","text":"<ul> <li>Promote to staging via Argo CD or Helm release.</li> <li>Run smoke tests (<code>scripts/smoke_test.py --env staging</code>).</li> <li>Promote to production after sign-off; monitor dashboards for 24 hours.</li> </ul>"},{"location":"development-manual/release-process/#7-post-release","title":"7. Post-Release","text":"<ul> <li>Close release ticket and associated issues.</li> <li>Update Onboarding and User manuals if workflows changed.</li> <li>Archive runbooks, logs, and metrics snapshots in release folder (<code>docs/releases/&lt;version&gt;/</code>).</li> </ul>"},{"location":"development-manual/release-process/#8-hotfix-process","title":"8. Hotfix Process","text":"<ul> <li>Branch from the latest tag (<code>git checkout -b hotfix/v&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;</code>).</li> <li>Apply fix, add tests, update changelog.</li> <li>Tag as <code>v&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;</code>.</li> <li>Notify stakeholders immediately after deployment.</li> </ul>"},{"location":"development-manual/release-process/#9-checklist","title":"9. Checklist","text":"<ul> <li>[ ] CI &amp; docs workflows green.</li> <li>[ ] Changelog updated.</li> <li>[ ] Manuals reviewed and signed off.</li> <li>[ ] Artifacts published.</li> <li>[ ] Deployment validated in staging and production.</li> <li>[ ] Post-release metrics captured.</li> </ul>"},{"location":"development-manual/runbooks/","title":"Operational Runbooks","text":"<p>These runbooks capture repeatable operational tasks for developers and SREs. Update them after every drill or significant change.</p>"},{"location":"development-manual/runbooks/#1-deploy-soma-infra-stack-locally-kind","title":"1. Deploy Soma Infra &amp; Stack Locally (Kind)","text":"<p>Purpose: Validate full stack changes against a local Kubernetes cluster.</p> <p>Procedure:</p> <pre><code># Create cluster\nkind create cluster --name soma-dev\n\n# Load custom image (optional)\ndocker build -t agent-zero:dev .\nkind load docker-image agent-zero:dev --name soma-dev\n\n# Install shared infra\nhelm upgrade --install soma-infra infra/helm/soma-infra \\\n  --namespace soma --create-namespace \\\n  -f infra/helm/values-dev.yaml\n\n# Install application stack\nhelm upgrade --install soma-stack infra/helm/soma-stack \\\n  --namespace soma \\\n  -f infra/helm/values-dev.yaml\n\n# Verify\nkubectl get pods -n soma\nkubectl get svc -n soma\n</code></pre> <p>Verification: All pods <code>Running</code>; <code>/health</code> endpoint returns <code>200</code> after port-forwarding <code>gateway</code> service.</p> <p>Cleanup: <code>kind delete cluster --name soma-dev</code>.</p>"},{"location":"development-manual/runbooks/#2-restart-conversation-worker-in-production","title":"2. Restart Conversation Worker in Production","text":"<ol> <li>Authenticate with <code>kubectl</code> (<code>kubectx</code> to prod cluster).</li> <li>Identify deployment: <code>kubectl get deploy -n soma-prod | grep conversation-worker</code>.</li> <li>Restart: <code>kubectl rollout restart deploy/conversation-worker -n soma-prod</code>.</li> <li>Monitor rollout: <code>kubectl rollout status deploy/conversation-worker -n soma-prod</code>.</li> <li>Verify metrics dashboard shows healthy request rate.</li> </ol>"},{"location":"development-manual/runbooks/#3-rotate-soma-slm-api-key","title":"3. Rotate Soma SLM API Key","text":"<ol> <li>Generate new key via internal portal.</li> <li>Store in Vault path <code>kv/slm/api-key</code> with metadata.</li> <li>Update <code>.env</code> (dev) or Helm secret values (prod).</li> <li>Trigger rollout of gateway and conversation worker.</li> <li>Run smoke test to confirm authentication.</li> <li>Update <code>docs/changelog.md</code> with rotation details.</li> </ol>"},{"location":"development-manual/runbooks/#4-restore-backup","title":"4. Restore Backup","text":"<ol> <li>Download backup archive from secure storage.</li> <li>Upload via UI (Settings \u2192 Backup \u2192 Restore).</li> <li>Choose overwrite or preserve settings.</li> <li>Confirm knowledge files, memory, and chats restored correctly.</li> <li>Document restoration in <code>changelog</code> and incident tracker.</li> </ol>"},{"location":"development-manual/runbooks/#5-emergency-shutdown","title":"5. Emergency Shutdown","text":"<ol> <li>Notify stakeholders on <code>#soma-oncall</code>.</li> <li>Disable public tunnels: turn off Cloudflare tunnel in UI or revoke token.</li> <li>Scale deployments to zero: <code>kubectl scale deploy --all --replicas=0 -n soma-prod</code>.</li> <li>Revoke API keys via Vault.</li> <li>Document actions and start incident response procedure (see Security Manual).</li> </ol>"},{"location":"development-manual/runbooks/#maintenance","title":"Maintenance","text":"<ul> <li>Review runbooks quarterly.</li> <li>Capture lessons learned from incidents and update procedures.</li> <li>Link to supporting diagrams or scripts where applicable.</li> </ul>"},{"location":"development-manual/testing/","title":"Testing Strategy","text":"<p>SomaAgent01 enforces a layered test strategy aligned with real-service execution. Tests interact with live dependencies unless explicitly mocked.</p>"},{"location":"development-manual/testing/#test-pyramid","title":"Test Pyramid","text":"<pre><code>graph TD\n    Unit[Unit Tests]\n    Integration[Integration Tests]\n    E2E[End-to-End]\n\n    Unit --&gt; Integration --&gt; E2E\n</code></pre> Layer Scope Command Unit Functions, helpers, schema validation <code>pytest tests/unit</code> Integration Gateway routes, memory, tool executor contracts <code>pytest tests/integration</code> End-to-End Full UI + backend flows (Playwright) <code>pytest tests/playwright --headed</code>"},{"location":"development-manual/testing/#fixtures-data","title":"Fixtures &amp; Data","text":"<ul> <li><code>tests/conftest.py</code>: shared FastAPI client, settings overrides.</li> <li><code>tests/fixtures/</code>: factories for memory items, Kafka messages.</li> <li>Use <code>tests/context/</code> for long-running conversational scenarios.</li> </ul>"},{"location":"development-manual/testing/#real-service-policy","title":"Real Service Policy","text":"<ul> <li>Tests run against live dependencies (Postgres, Redis, Kafka). Ensure stack is up via <code>make dev-up</code>.</li> <li>For destructive tests, use dedicated tenants or ephemeral namespaces.</li> </ul>"},{"location":"development-manual/testing/#ci-pipeline","title":"CI Pipeline","text":"<ol> <li>Lint (<code>make lint</code>).</li> <li>Unit + integration tests (<code>pytest</code>).</li> <li>Playwright suite (headless via xvfb container).</li> <li>Collect screenshots and logs on failure.</li> </ol>"},{"location":"development-manual/testing/#adding-tests","title":"Adding Tests","text":"<ol> <li>Identify coverage gap or regression.</li> <li>Add descriptive test under correct suite.</li> <li>Update docs if user-facing behavior changes.</li> <li>Run targeted command locally before pushing.</li> </ol>"},{"location":"development-manual/testing/#load-resilience-testing","title":"Load &amp; Resilience Testing","text":"<ul> <li><code>scripts/load/locustfile.py</code> (planned) exercises conversation throughput.</li> <li>Monitor metrics via Prometheus dashboards; capture baseline before stressing.</li> </ul>"},{"location":"development-manual/testing/#regression-tracking","title":"Regression Tracking","text":"<ul> <li>Tag flaky or bug-specific tests with <code>@pytest.mark.issue123</code>.</li> <li>Document regressions and fixes in <code>docs/changelog.md</code>.</li> </ul>"},{"location":"development-manual/tooling/","title":"Developer Tooling","text":"<p>This chapter lists the sanctioned tooling for SomaAgent01 development and how to use it effectively.</p>"},{"location":"development-manual/tooling/#command-interface","title":"Command Interface","text":"<ul> <li>Makefile targets (see <code>Makefile</code>):</li> <li><code>make dev-up</code>, <code>make dev-down</code>, <code>make dev-logs</code>.</li> <li><code>make fmt</code>, <code>make lint</code>, <code>make test</code>, <code>make docs-verify</code>.</li> <li>Poetry (if enabled): <code>poetry install</code>, <code>poetry run pytest</code>.</li> <li>Node toolchain: <code>npm run lint</code>, <code>npm run build</code> under <code>webui/</code>.</li> </ul>"},{"location":"development-manual/tooling/#linting-formatting","title":"Linting &amp; Formatting","text":"Language Tool Command Python Black, Ruff, Mypy <code>make fmt fmt-check lint mypy</code> JavaScript/TypeScript ESLint, Prettier <code>npm run lint</code>, <code>npm run format</code> Markdown markdownlint-cli2 <code>make docs-verify</code>"},{"location":"development-manual/tooling/#documentation-pipeline","title":"Documentation Pipeline","text":"<ul> <li>MkDocs configuration: <code>mkdocs.yml</code> (added in docs sprint).</li> <li>Diagram rendering: <code>mmdc</code> (Mermaid CLI) invoked via <code>make docs-diagrams</code>.</li> <li>Link checking: <code>lychee</code> executed in docs CI workflow.</li> </ul>"},{"location":"development-manual/tooling/#observability-debugging","title":"Observability &amp; Debugging","text":"<ul> <li>Logs: <code>make dev-logs</code> or <code>docker compose logs -f gateway</code>.</li> <li>Profiling: <code>scripts/profiling/profile_gateway.py</code> (use with caution in dev).</li> <li>Tracing: Jaeger UI at <code>http://localhost:16686</code> when observability profile enabled.</li> </ul>"},{"location":"development-manual/tooling/#git-hooks","title":"Git Hooks","text":"<ul> <li>Optional pre-commit configuration stored in <code>.pre-commit-config.yaml</code> (run <code>pre-commit install</code>).</li> <li>Hooks enforce formatting, linting, and documentation checklist reminders.</li> </ul>"},{"location":"development-manual/tooling/#automation-agents","title":"Automation Agents","text":"<ul> <li>Internal agents follow the same standards; reference this manual before enabling automated commits.</li> <li>Monitor agent contributions via <code>scripts/automation/report_agent_activity.py</code>.</li> </ul>"},{"location":"development-manual/tooling/#verification","title":"Verification","text":"<ul> <li>Run <code>make tooling-audit</code> (script added in this sprint) to ensure required binaries are present and versions are compatible.</li> </ul>"},{"location":"features/memory_management/","title":"Memory Management Guide","text":""},{"location":"features/memory_management/#overview","title":"Overview","text":"<p>SomaBrain ensures agents remember key facts across sessions. This guide teaches humans and automations how to curate and leverage memory entries.</p>"},{"location":"features/memory_management/#concepts","title":"Concepts","text":"<ul> <li>Memory Item: Structured JSON payload keyed by domain (<code>user.preference.language</code>).</li> <li>Persona Scope: Each memory tied to tenant + persona to avoid cross-contamination.</li> <li>Relevance Score: Determines ranking when prompt budget is tight.</li> </ul>"},{"location":"features/memory_management/#creating-memories","title":"Creating Memories","text":""},{"location":"features/memory_management/#humans-via-ui","title":"Humans via UI","text":"<ol> <li>Ask agent to \"Remember that I prefer dark mode\".</li> <li>Gateway stores structured memory under <code>UserPreference</code> schema.</li> <li>Verify in Settings \u2192 Memory dashboard (coming soon).</li> </ol>"},{"location":"features/memory_management/#automations-via-api","title":"Automations via API","text":"<pre><code>from python.helpers.memory import Memory\nfrom langchain_core.documents import Document\n\n# Get a memory handle (remote SomaBrain by default if SOMA_ENABLED=true)\nmem = await Memory.get_by_subdir(\"default\")\n\n# Insert text with metadata\ndoc_id = await mem.insert_text(\n    \"Backend chosen: FastAPI; Frontend: Alpine\",\n    metadata={\"type\": \"ProjectDecision\", \"key\": \"project.stack\", \"importance\": 1},\n)\n\n# Or insert structured documents\ndoc = Document(\n    page_content=\"Use Postgres for transactional storage\",\n    metadata={\"type\": \"ProjectDecision\", \"area\": Memory.Area.MAIN.value},\n)\nids = await mem.insert_documents([doc])\n</code></pre>"},{"location":"features/memory_management/#retrieving-memories","title":"Retrieving Memories","text":"<pre><code># Similarity search with threshold and optional filter expression on metadata\nmatches = await mem.search_similarity_threshold(\n    query=\"project stack\",\n    limit=3,\n    threshold=0.4,\n    filter=\"type == 'ProjectDecision'\",\n)\nfor doc in matches:\n    print(doc.metadata.get(\"key\"), doc.page_content)\n</code></pre> <p>LLM prompts include selected memories at conversation start; review <code>technical-manual/components/memory.md</code> for the pipeline.</p>"},{"location":"features/memory_management/#best-practices","title":"Best Practices","text":"<ul> <li>Keep payload concise (&lt; 400 tokens).</li> <li>Store provenance (<code>source</code>, <code>date</code>) to aid troubleshooting.</li> <li>Regularly prune obsolete entries (e.g., deprecated stack choices).</li> </ul>"},{"location":"features/memory_management/#limitations","title":"Limitations","text":"<ul> <li>Prompt window finite (~4k tokens). High-volume memories require ranking.</li> <li>Memory is explicit\u2014if never stored, LLM cannot infer it later.</li> </ul>"},{"location":"features/memory_management/#auditing","title":"Auditing","text":"<ul> <li>Use Postgres queries to inspect <code>memory_items</code> table.</li> <li>Build dashboards to review memory churn, relevance distribution.</li> </ul>"},{"location":"features/memory_management/#roadmap","title":"Roadmap","text":"<ul> <li>Memory dashboard UI with edit/delete.</li> <li>Semantic clustering for automatic summarization.</li> <li>Retention policies per tenant persona.</li> </ul>"},{"location":"features/realtime_speech/","title":"Realtime Speech Feature Guide","text":""},{"location":"features/realtime_speech/#audience","title":"Audience","text":"<ul> <li>End users wanting voice-enabled interactions.</li> <li>Automation engineers needing to validate speech flows.</li> </ul>"},{"location":"features/realtime_speech/#prerequisites","title":"Prerequisites","text":"<ul> <li>Valid OpenAI API key with realtime access.</li> <li>Browser with microphone permissions granted.</li> </ul>"},{"location":"features/realtime_speech/#quick-start","title":"Quick Start","text":"<ol> <li>Launch stack: <code>make dev-up</code>.</li> <li>Open UI (<code>http://localhost:7002</code>).</li> <li>Go to Settings \u2192 Speech. Default provider is OpenAI Realtime.</li> <li>Click microphone button; grant browser permission.</li> <li>Speak; watch transcript appear and synthesized voice respond.</li> </ol>"},{"location":"features/realtime_speech/#under-the-hood","title":"Under the Hood","text":"<ul> <li>UI loads speech settings via <code>/settings_get</code>.</li> <li>On microphone activation, UI invokes <code>/realtime_session</code>.</li> <li>Gateway negotiates session with OpenAI, returns client secret.</li> <li>UI establishes WebRTC connection; audio streamed both ways.</li> </ul>"},{"location":"features/realtime_speech/#troubleshooting","title":"Troubleshooting","text":"Issue Fix No audio playback Click anywhere to satisfy autoplay policy; verify volume Session error toast Check OpenAI key, network connectivity Wrong voice Update <code>speech_realtime_voice</code> in settings High latency Inspect browser network tab, reduce background noise"},{"location":"features/realtime_speech/#automation-example","title":"Automation Example","text":"<p>Playwright scenario (simplified): <pre><code>async def test_realtime(app):\n    await app.page.goto(\"http://localhost:7002\")\n    await app.mock_settings(provider=\"openai_realtime\")\n    await app.stub_peer_connection()\n    await app.click_microphone()\n    await app.assert_realtime_session_called()\n</code></pre></p>"},{"location":"features/realtime_speech/#advanced-configuration","title":"Advanced Configuration","text":"<ul> <li>Override endpoint (e.g., internal proxy) via settings.</li> <li>Switch to Kokoro TTS if offline synthesis required.</li> <li>Adjust silence detection thresholds to fine-tune microphone behavior.</li> </ul>"},{"location":"features/realtime_speech/#metrics-logging","title":"Metrics &amp; Logging","text":"<ul> <li>UI console logs negotiation steps.</li> <li>Gateway logs session creation time; errors include provider response.</li> </ul>"},{"location":"features/realtime_speech/#roadmap","title":"Roadmap","text":"<ul> <li>Add fallback to offline TTS when realtime unavailable.</li> <li>Expose metrics (<code>realtime_sessions_total</code>).</li> <li>Provide CLI utility for headless negotiation testing.</li> </ul>"},{"location":"features/tooling/","title":"Tooling Integration Guide","text":""},{"location":"features/tooling/#purpose","title":"Purpose","text":"<p>Explain how SomaAgent01 invokes and extends tools to perform actions beyond pure conversation.</p>"},{"location":"features/tooling/#tool-invocation-lifecycle","title":"Tool Invocation Lifecycle","text":"<ol> <li>LLM response includes tool call (<code>function_call</code> in LiteLLM schema).</li> <li>Gateway validates tool name, serializes arguments, publishes to Kafka (<code>somastack.tools</code>).</li> <li>Tool Executor consumes message, runs corresponding tool.</li> <li>Results returned via Kafka (<code>somastack.tool_results</code>) or HTTP callback.</li> <li>Gateway streams outcome to UI, optionally saves memory.</li> </ol>"},{"location":"features/tooling/#built-in-tools","title":"Built-in Tools","text":"Tool Capability <code>shell</code> Execute shell commands in sandbox <code>code_interpreter</code> Run Python code, return stdout/plots <code>http</code> Perform HTTP requests (respecting allowlists) <code>memory_save</code> / <code>memory_search</code> Persist / query SomaBrain <code>knowledge_base</code> Query external knowledge articles"},{"location":"features/tooling/#creating-custom-tools","title":"Creating Custom Tools","text":"<ol> <li>Implement class extending <code>Tool</code> base (see <code>python/tools/base.py</code>).</li> <li>Define JSON schema for arguments and results.</li> <li>Register tool in discovery map (<code>python/tools/__init__.py</code>).</li> <li>Add documentation in <code>docs/features/tooling.md</code> (this file) under new section.</li> <li>Provide integration tests to ensure Gateway/Executor roundtrip.</li> </ol>"},{"location":"features/tooling/#example-weather-tool","title":"Example: Weather Tool","text":"<pre><code>class WeatherTool(Tool):\n    name = \"weather\"\n    description = \"Fetch weather for a city\"\n    args_schema = WeatherArgs\n\n    async def run(self, city: str):\n        resp = await http_client.get(f\"https://api.weather.com/{city}\")\n        return resp.json()\n</code></pre> <p>LLM prompt snippet: <pre><code>{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"weather\",\n    \"arguments\": {\"city\": \"Prague\"}\n  }\n}\n</code></pre></p>"},{"location":"features/tooling/#security-considerations","title":"Security Considerations","text":"<ul> <li>Validate inputs; enforce allowlists for network calls.</li> <li>Sandbox file operations; use temporary directories.</li> <li>Redact secrets in logs (use <code>python/helpers/secrets.py</code>).</li> </ul>"},{"location":"features/tooling/#monitoring-tool-usage","title":"Monitoring Tool Usage","text":"<ul> <li>Kafka metrics reveal volume per tool.</li> <li>Gateway logs include <code>tool_name</code>, <code>task_id</code>.</li> <li>Add tracing spans to measure execution time.</li> </ul>"},{"location":"features/tooling/#machine-clients","title":"Machine Clients","text":"<p>Automation agents can queue tasks directly by writing to Kafka or using Gateway <code>POST /chat</code> with tool instructions. Ensure correlation IDs are tracked for result handling.</p>"},{"location":"infra/DEVELOPMENT_CANONICAL/","title":"Shared Infra \u2014 Development Canonical (Setup, Operate, Deploy)","text":"<p>This is the single canonical manual for developing, setting up, and deploying the Soma Shared Infrastructure locally with production semantics. It complements SHARED_INFRA_ARCHITECTURE.md and SPRINTS_SHARED_INFRA.md.</p>"},{"location":"infra/DEVELOPMENT_CANONICAL/#1-what-this-is","title":"1) What this is","text":"<ul> <li>A cluster-wide Shared Infra layer used by all Soma stacks.</li> <li>Runs on a local Kind cluster with resource-conscious settings; mirrors production contracts (DNS, ports, policy, observability).</li> <li>Self-contained under <code>infra/</code> and <code>docs/infra/</code> so it can move to its own repo without changes.</li> </ul> <p>Core services: Postgres (required), Kafka (KRaft), Redis, OPA (PDP), Vault, Prometheus, Grafana, Etcd; OpenFGA optional.</p>"},{"location":"infra/DEVELOPMENT_CANONICAL/#2-prerequisites-local-mac","title":"2) Prerequisites (local Mac)","text":"<ul> <li>Docker Desktop with ~6\u201310 CPU, 12\u201320 GB RAM, 50\u2013100 GB disk for volumes.</li> <li>Kind cluster (context <code>kind-soma</code> recommended). A Kind config is provided at <code>infra/kind/soma-kind.yaml</code>.</li> <li>kubectl and helm installed.</li> </ul>"},{"location":"infra/DEVELOPMENT_CANONICAL/#3-install-paths","title":"3) Install paths","text":""},{"location":"infra/DEVELOPMENT_CANONICAL/#a-docker-shared-infra-only","title":"A. Docker (shared infra only)","text":"<ul> <li>Compose file: <code>infra/docker/shared-infra.compose.yaml</code></li> <li>Env: <code>infra/env/.env.shared.example</code> (copy to your env if needed)</li> <li>Bring up:</li> <li>Postgres, Kafka, Redis, OPA, Vault, Etcd, Prometheus, Grafana (dev-sized, persistent volumes where needed)</li> </ul>"},{"location":"infra/DEVELOPMENT_CANONICAL/#b-kubernetes-kind-recommended","title":"B. Kubernetes (Kind) \u2014 recommended","text":"<ul> <li>Namespace: <code>soma-infra</code></li> <li>Chart: <code>infra/helm/soma-infra</code></li> <li>Overlays: <code>values-dev.yaml</code> (Kind), <code>values-staging.yaml</code>, <code>values-prod.yaml</code></li> </ul>"},{"location":"infra/DEVELOPMENT_CANONICAL/#4-contracts-consumers-rely-on-these","title":"4) Contracts (consumers rely on these)","text":"<ul> <li>DNS and ports:</li> <li><code>postgres.soma-infra.svc.cluster.local:5432</code></li> <li><code>kafka.soma-infra.svc.cluster.local:9092</code></li> <li><code>redis.soma-infra.svc.cluster.local:6379</code></li> <li><code>opa.soma-infra.svc.cluster.local:8181</code></li> <li><code>vault.soma-infra.svc.cluster.local:8200</code></li> <li><code>prometheus.soma-infra.svc.cluster.local:9090</code></li> <li><code>grafana.soma-infra.svc.cluster.local:3000</code></li> <li><code>etcd.soma-infra.svc.cluster.local:2379</code></li> <li>Identity: JWKS URL (dev issuer now; pluggable IdP later)</li> <li>Policy: OPA decision path <code>/v1/data/soma/allow</code> with inputs <code>{identity, request, service, tenant, context}</code></li> </ul>"},{"location":"infra/DEVELOPMENT_CANONICAL/#5-security-policy","title":"5) Security &amp; policy","text":"<ul> <li>Kubernetes: NetworkPolicies (default-deny); PodSecurity (restricted); runAsNonRoot; read-only root FS; probes &amp; limits.</li> <li>Secrets: Vault in dev mode OK; no secrets in images; JWKS-only for apps.</li> <li>OPA: default-deny, service-to-service allowlist, tenant enforcement, budgets/routing.</li> </ul>"},{"location":"infra/DEVELOPMENT_CANONICAL/#6-observability","title":"6) Observability","text":"<ul> <li>Prometheus scrapes all shared components; scrape interval 15s (5s on demand).</li> <li>Grafana baseline dashboards for Postgres, Kafka, Redis, OPA, Vault.</li> <li>OTEL endpoints configurable; Jaeger/Loki optional.</li> </ul>"},{"location":"infra/DEVELOPMENT_CANONICAL/#7-sizing-kind","title":"7) Sizing (Kind)","text":"<ul> <li>Postgres: 500m CPU / 1\u20132 GiB RAM; PVC 5\u201310 GiB</li> <li>Kafka: 1 CPU / 2\u20134 GiB RAM; PVC 10\u201320 GiB; short retention</li> <li>Redis: 200\u2013300m CPU / 512 MiB RAM; AOF everysec; allkeys-lru</li> <li>OPA: 100\u2013200m CPU / 256\u2013512 MiB RAM</li> <li>Prometheus: 500m\u20131 CPU / 2\u20134 GiB RAM; Grafana small</li> </ul>"},{"location":"infra/DEVELOPMENT_CANONICAL/#8-readiness-checklist-kind","title":"8) Readiness checklist (Kind)","text":"<ul> <li>Pods Ready; Services routable.</li> <li>Postgres SELECT 1; Redis PING; Kafka metadata; OPA allow/deny; Vault health; Etcd health.</li> <li>Prom targets up; (optional) Grafana reachable.</li> <li>NetworkPolicies enforced (deny by default; labeled allows present).</li> <li>JWKS and OPA decision path published to app teams.</li> </ul>"},{"location":"infra/DEVELOPMENT_CANONICAL/#9-day-2-operations-dev","title":"9) Day-2 operations (dev)","text":"<ul> <li>Backups (optional): small pg_dump; keep Kafka retention short; monitor Prom metrics.</li> <li>Policy iteration: update OPA bundle ConfigMap; verify with contract tests.</li> <li>Secrets: rotate dev signing keys in Vault; JWKS URL remains stable for apps.</li> </ul>"},{"location":"infra/DEVELOPMENT_CANONICAL/#10-ready-to-split-guidance","title":"10) Ready-to-split guidance","text":"<ul> <li>No app dependencies; infra stands alone under <code>infra/</code> and <code>docs/infra/</code>.</li> <li>To split: subtree-export <code>infra/</code> and <code>docs/infra/</code> to a new repo; publish Helm via OCI/Pages.</li> <li>Consumers keep DNS, ports, JWKS, and OPA contracts; no code changes required.</li> </ul>"},{"location":"infra/DEVELOPMENT_CANONICAL/#11-references","title":"11) References","text":"<ul> <li><code>docs/infra/SHARED_INFRA_ARCHITECTURE.md</code></li> <li><code>docs/infra/SPRINTS_SHARED_INFRA.md</code></li> <li><code>docs/infra/runbook_shared_infra.md</code></li> </ul>"},{"location":"infra/ENV_VARS_AND_SECRETS/","title":"Environment variables and secrets inventory (Shared Infra)","text":"<p>This document lists the environment variables, default values observed in the repo, where they are defined, their sensitivity level, and recommended storage and handling for Kubernetes (Secrets or Vault).</p> <p>IMPORTANT: The user requested \"no mocks\" and \"no placeholder secrets\". Before production usage, rotate any dev tokens and move secrets into Vault or Kubernetes Secrets.</p>"},{"location":"infra/ENV_VARS_AND_SECRETS/#how-to-read-this-file","title":"How to read this file","text":"<ul> <li>Location: where the value was found in the repository</li> <li>Default: the default value observed in the file (may be an example)</li> <li>Sensitivity: Low / Medium / High</li> <li>Recommended storage: K8s Secret / Vault / ConfigMap</li> </ul>"},{"location":"infra/ENV_VARS_AND_SECRETS/#postgres","title":"Postgres","text":"<ul> <li>Location: <code>infra/docker/shared-infra.compose.yaml</code> and <code>infra/helm/soma-infra/values.yaml</code> and <code>infra/helm/soma-infra/charts/postgres/values.yaml</code></li> <li>POSTGRES_USER / postgresUser</li> <li>Default: <code>soma</code></li> <li>Sensitivity: Low</li> <li> <p>Recommended storage: ConfigMap for username; Secret for consistency if you prefer</p> </li> <li> <p>POSTGRES_PASSWORD / postgresPassword</p> </li> <li>Default: <code>soma</code></li> <li>Sensitivity: High</li> <li> <p>Recommended storage: Kubernetes Secret (base64) or Vault secret. Use Secret references in Helm values.</p> </li> <li> <p>POSTGRES_DB / postgresDb</p> </li> <li>Default: <code>soma</code> / <code>somaagent01</code> (compose uses <code>somaagent01</code>)</li> <li>Sensitivity: Low</li> <li> <p>Recommended storage: ConfigMap</p> </li> <li> <p>PGDATA</p> </li> <li>Default: <code>/var/lib/postgresql/data/pgdata</code></li> <li>Sensitivity: Low</li> <li>Recommended storage: ConfigMap</li> </ul> <p>Notes: - In Helm chart the Postgres values are injected into the StatefulSet as plain env values. Replace these with <code>valueFrom: secretKeyRef</code> or use Helm templating to mount a Secret.</p>"},{"location":"infra/ENV_VARS_AND_SECRETS/#kafka-kraft-single-node-bitnami-confluent-variants","title":"Kafka (KRaft single-node / Bitnami / Confluent variants)","text":"<ul> <li>Location: <code>infra/docker/shared-infra.compose.yaml</code> and <code>infra/helm/soma-infra/charts/kafka/*</code> and <code>infra/helm/soma-infra/values*.yaml</code></li> <li>KAFKA_CFG_NODE_ID, KAFKA_CFG_PROCESS_ROLES, KAFKA_CFG_LISTENERS, KAFKA_CFG_CONTROLLER_QUORUM_VOTERS, KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE, KAFKA_CFG_NUM_PARTITIONS, KAFKA_ENABLE_KRAFT, KAFKA_CFG_KRAFT_CLUSTER_ID, KAFKA_CFG_ADVERTISED_LISTENERS, KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP, KAFKA_CFG_CONTROLLER_LISTENER_NAMES, KAFKA_CFG_INTER_BROKER_LISTENER_NAME</li> <li>Default: various example values in compose/values (see files)</li> <li>Sensitivity: Low\u2013Medium (cluster ids and controller voters are operational, not secrets)</li> <li> <p>Recommended storage: ConfigMap for operational configs. If SASL/SSL creds are introduced later, store them as Secrets.</p> </li> <li> <p>KAFKA_CLUSTER_ID / KAFKA_CFG_KRAFT_CLUSTER_ID</p> </li> <li>Default: <code>soma-shared-01</code> or example cluster id</li> <li>Sensitivity: Low</li> </ul> <p>Notes: - For KRaft in dev, the cluster id and controller voters are required. If using images that accept file-based configuration, prefer mounting config via a ConfigMap. If you add username/password for clients, store them as Secrets.</p>"},{"location":"infra/ENV_VARS_AND_SECRETS/#redis","title":"Redis","text":"<ul> <li>Location: <code>infra/docker/shared-infra.compose.yaml</code>, <code>infra/helm/soma-infra/charts/redis/values.yaml</code></li> <li>REDIS_URL</li> <li>Default (env example): <code>redis://localhost:6380/0</code> (compose) and <code>redis://redis.soma.svc.cluster.local:6379/0</code> (k8s)</li> <li>Sensitivity: Medium (if no password; high if ACL/TLS credentials present)</li> <li>Recommended storage: ConfigMap for plain URL if unauthenticated; Secret if password/ACL used.</li> </ul>"},{"location":"infra/ENV_VARS_AND_SECRETS/#vault","title":"Vault","text":"<ul> <li>Location: <code>infra/docker/shared-infra.compose.yaml</code>, <code>infra/helm/soma-infra/charts/vault/values.yaml</code>, <code>infra/vault/config/config.hcl</code></li> <li>VAULT_DEV_ROOT_TOKEN_ID / VAULT_DEV_LISTEN_ADDRESS / VAULT_ADDR</li> <li>Default: <code>root</code> (dev token in compose), <code>0.0.0.0:8200</code> (listen)</li> <li>Sensitivity: High (root token is highly privileged)</li> <li>Recommended storage: For dev-only local clusters, a dev token may be acceptable but must be rotated. For any shared environment, do NOT keep root tokens in source. Use a bootstrap process to unseal/initialize Vault and put root tokens into a secure store (local OS keychain or CI secrets). In Kubernetes, integrate Vault via Helm chart with Raft storage and use Kubernetes auth for apps.</li> </ul> <p>Notes: - The compose file exposes <code>VAULT_DEV_ROOT_TOKEN_ID: root</code>. This is only acceptable for ephemeral local development. Replace this with instructions to initialize Vault and create an application token stored in K8s Secret or in Vault policies.</p>"},{"location":"infra/ENV_VARS_AND_SECRETS/#grafana","title":"Grafana","text":"<ul> <li>Location: <code>infra/helm/soma-infra/charts/grafana/values.yaml</code></li> <li>GF_SECURITY_ADMIN_PASSWORD / adminPassword</li> <li>Default: <code>admin</code></li> <li>Sensitivity: High (Grafana admin)</li> <li>Recommended storage: Kubernetes Secret. Consider injecting via Helm from a pre-created Secret or using an external secrets operator.</li> </ul>"},{"location":"infra/ENV_VARS_AND_SECRETS/#prometheus","title":"Prometheus","text":"<ul> <li>Location: <code>infra/helm/soma-infra/charts/prometheus/*</code>, <code>infra/archived</code> monitoring configs</li> <li>No direct secrets in default values; scrape configs in chart may reference targets only.</li> </ul>"},{"location":"infra/ENV_VARS_AND_SECRETS/#etcd","title":"Etcd","text":"<ul> <li>Location: <code>infra/helm/soma-infra/charts/etcd/*</code></li> <li>No plaintext secrets by default in the chart. Etcd may be configured for TLS in prod; certificates should be stored in Secrets.</li> <li>StorageClass: persistence.storageClass should be set in <code>values-dev.yaml</code> to match cluster storage classes (e.g., <code>standard</code>). Ensure StatefulSet's <code>volumeClaimTemplates</code> have <code>storageClassName</code> set.</li> </ul>"},{"location":"infra/ENV_VARS_AND_SECRETS/#app-level-env-k8s-overlays-and-archived-examples","title":"App-level env (K8s overlays and archived examples)","text":"<ul> <li>Several app overlays and archived k8s manifests use a shared secret name <code>somaagent-secrets</code> containing keys such as <code>GATEWAY_JWT_SECRET</code>, <code>POSTGRES_PASSWORD</code>, <code>KAFKA_SASL_PASSWORD</code>. Examples are in <code>archived/a0_data/infra/k8s/base/secrets.yaml</code>.</li> <li>GATEWAY_JWT_SECRET</li> <li>Default: <code>change-me</code> (archived example)</li> <li>Sensitivity: High (used for signing/verification unless using JWKS)</li> <li>Recommended storage: Kubernetes Secret or (preferred) store signing keys in Vault and publish JWKS to apps.</li> </ul>"},{"location":"infra/ENV_VARS_AND_SECRETS/#environment-examples-infraenvenvsharedexample","title":"Environment examples (infra/env/.env.shared.example)","text":"<p>The repo includes <code>infra/env/.env.shared.example</code> with K8s and Docker host examples: - POSTGRES_DSN=postgresql://soma:soma@localhost:5436/somaagent01 - KAFKA_BOOTSTRAP_SERVERS=localhost:9094 - REDIS_URL=redis://localhost:6380/0 - VAULT_ADDR=http://localhost:8201 - K8S_* variants pointing to cluster DNS names (postgres.soma.svc.cluster.local, kafka.soma.svc.cluster.local, etc.)</p> <p>These examples contain plaintext credentials (soma/soma). Replace them with secrets prior to sharing or production usage.</p>"},{"location":"infra/ENV_VARS_AND_SECRETS/#immediate-actions-recommended","title":"Immediate actions recommended","text":"<ol> <li>Remove dev high-privilege tokens from compose/values files (VAULT_DEV_ROOT_TOKEN_ID=root, POSTGRES_PASSWORD <code>soma</code>, Grafana <code>admin</code>). Replace them with references to Kubernetes Secrets or have an initialization job create the secrets securely.</li> <li>Create <code>soma-infra</code> namespace secrets before Helm install (or use Helm pre-install hooks) for:</li> <li><code>postgres-credentials</code> (POSTGRES_USER/POSTGRES_PASSWORD/POSTGRES_DB)</li> <li><code>vault-root</code> (only for dev - prefer dynamic unseal)</li> <li><code>grafana-admin</code> (GF_SECURITY_ADMIN_PASSWORD)</li> <li>Use Vault for long-lived secrets and Kubernetes Secrets for bootstrapping values. Consider integrating Kubernetes auth for Vault and/or use external-secrets operator to sync secrets from Vault to K8s.</li> <li>Audit the repository for any other plaintext credentials (e.g., archived secrets.yaml). Delete or rotate any exposed secrets.</li> </ol>"},{"location":"infra/ENV_VARS_AND_SECRETS/#how-to-create-secrets-examples","title":"How to create secrets (examples)","text":"<p>Kubernetes secrets (example):</p> <p>kubectl -n soma-infra create secret generic postgres-credentials --from-literal=POSTGRES_USER=soma --from-literal=POSTGRES_PASSWORD=\"\" --from-literal=POSTGRES_DB=somaagent01 <p>Replace <code>&lt;REPLACE&gt;</code> with a properly-generated password; then update Helm values to reference the secret via <code>valueFrom.secretKeyRef</code> in templates.</p> <p>For Vault: initialize Vault, enable kv v2 at <code>secret/</code>, and store secrets there. Use Vault Agent or an external-secrets controller to inject secrets into Kubernetes at deployment time.</p> <p>If you want, I can now:  - Generate a Helm-compatible secrets manifest and place it under <code>infra/helm/soma-infra/secrets/</code> with instructions to create the Kubernetes Secrets before running <code>helm upgrade --install</code> (dev-safe, not checked in with real values).  - Or replace the chart values to read from secretKeyRefs (small template change) so Helm will render Secrets if present.</p> <p>Next step: I'll mark todo #1 completed and stage/commit this file. Then we can proceed to clear the Helm pending-install and apply the storageClass fix so PVCs bind.</p>"},{"location":"infra/SHARED_INFRA_ARCHITECTURE/","title":"Shared Infra Architecture (Prod-like Development)","text":"<p>This document defines the canonical Shared Infrastructure layer for the entire Soma Stack. It is designed to run on a local Kind cluster with production semantics and be lifted into a standalone repository without changes.</p>"},{"location":"infra/SHARED_INFRA_ARCHITECTURE/#scope","title":"Scope","text":"<ul> <li>Namespace: <code>soma-infra</code></li> <li>Shared services: Postgres (required), Kafka (KRaft), Redis, OPA (central PDP), Vault, Prometheus, Grafana, Etcd; OpenFGA optional (off by default)</li> <li>Contracts only; no app code. Stable DNS and port contracts across environments.</li> </ul>"},{"location":"infra/SHARED_INFRA_ARCHITECTURE/#dns-and-ports-clusterip","title":"DNS and Ports (ClusterIP)","text":"<ul> <li><code>postgres.soma-infra.svc.cluster.local:5432</code></li> <li><code>kafka.soma-infra.svc.cluster.local:9092</code></li> <li><code>redis.soma-infra.svc.cluster.local:6379</code></li> <li><code>opa.soma-infra.svc.cluster.local:8181</code></li> <li><code>vault.soma-infra.svc.cluster.local:8200</code></li> <li><code>prometheus.soma-infra.svc.cluster.local:9090</code></li> <li><code>grafana.soma-infra.svc.cluster.local:3000</code></li> <li><code>etcd.soma-infra.svc.cluster.local:2379</code></li> <li><code>openfga.soma-infra.svc.cluster.local:8080</code> (optional)</li> </ul> <p>Gateway external ports reserved for apps only: Docker 7001; Kind 7002.</p>"},{"location":"infra/SHARED_INFRA_ARCHITECTURE/#security-and-policy","title":"Security and Policy","text":"<ul> <li>Authentication: JWT + JWKS contract. Dev issuer now; pluggable IdP (Keycloak/Auth0/Okta) later without app changes.</li> <li>Authorization: OPA central PDP, decision path <code>/v1/data/soma/allow</code>, default-deny, s2s allowlist, tenancy, budgets/routing.</li> <li>Kubernetes: NetworkPolicies (default-deny), PodSecurity (restricted), runAsNonRoot, read-only root FS, probes and limits required.</li> <li>Secrets: Vault (dev mode locally; CSI/Agent in prod). No private keys in images.</li> </ul>"},{"location":"infra/SHARED_INFRA_ARCHITECTURE/#persistence-and-backups","title":"Persistence and Backups","text":"<ul> <li>Postgres: single StatefulSet in dev; HA (Patroni/Crunchy) in prod. PITR and backup verification in prod.</li> <li>Kafka: single broker and short retention in dev; Strimzi multi-broker in prod.</li> <li>Redis: single instance with AOF and allkeys-lru in dev; Redis Operator with TLS/ACLs in prod.</li> <li>Etcd: single in dev; 3-node in prod.</li> </ul>"},{"location":"infra/SHARED_INFRA_ARCHITECTURE/#observability","title":"Observability","text":"<ul> <li>Prometheus + Grafana baseline. Metrics labels: <code>service, instance, env, version</code>.</li> <li>OpenTelemetry endpoints configurable; Jaeger optional. Loki optional for logs.</li> </ul>"},{"location":"infra/SHARED_INFRA_ARCHITECTURE/#isolation-and-portability","title":"Isolation and Portability","text":"<ul> <li>All assets under <code>infra/</code> and <code>docs/infra/</code>. No references to app code or paths.</li> <li>Values-driven configuration (dev/staging/prod overlays). StorageClass, namespace, and resources configurable.</li> <li>Ready to split into a separate repo; publish as Helm repo or OCI.</li> </ul>"},{"location":"infra/SHARED_INFRA_ARCHITECTURE/#acceptance-dev-kind","title":"Acceptance (Dev Kind)","text":"<ul> <li>Pods Ready; Services routable.</li> <li>Health checks pass: Postgres SELECT 1; Redis PING; Kafka metadata; OPA allow/deny; Vault dev health; Etcd health.</li> <li>Prom targets up; (optional) Grafana reachable.</li> <li>NetworkPolicies enforce default-deny; app namespace allowed to shared infra by label.</li> <li>JWKS URL and OPA decision path published for all apps.</li> </ul>"},{"location":"infra/SHARED_INFRA_MANUAL/","title":"Soma Shared Infra \u2014 Detailed Deployment Manual","text":"<p>This manual captures everything required to stand up the soma-infra shared services stack in a production-like development environment and to prepare it for distribution as a standalone project (e.g., GitHub <code>somatechlat/somastack</code>). Each section is designed to be copy/paste friendly so that another operator can reproduce the environment without tribal knowledge.</p>"},{"location":"infra/SHARED_INFRA_MANUAL/#1-architecture-overview","title":"1. Architecture Overview","text":"<p>The shared infra chart installs the core platform dependencies in the <code>soma-infra</code> namespace. The current component set is illustrated below.</p> <pre><code>graph TD\n    subgraph Shared Infra Namespace (soma-infra)\n        postgres[(Postgres)]\n        redis[(Redis)]\n        kafka[(Kafka KRaft)]\n        etcd[(Etcd)]\n        vault[(Vault Dev)]\n        opa[(OPA PDP)]\n        prometheus[(Prometheus)]\n        grafana[(Grafana)]\n    end\n\n    subgraph Application Namespace(s)\n        apps[Workload Pods]\n    end\n\n    apps -- SQL, Secrets --&gt; postgres\n    apps -- Cache --&gt; redis\n    apps -- Events --&gt; kafka\n    apps -- Coordination --&gt; etcd\n    apps -- JWT Policy --&gt; opa\n    apps -- Metrics --&gt; prometheus\n    apps -- Dashboards --&gt; grafana\n    opa -- Policies --&gt; vault\n    prometheus -- Dashboards --&gt; grafana\n</code></pre>"},{"location":"infra/SHARED_INFRA_MANUAL/#helm-chart-composition","title":"Helm Chart Composition","text":"<pre><code>flowchart LR\n    A[infra/helm/soma-infra/Chart.yaml]\n    A --&gt; B(values.yaml)\n    A --&gt; C(values-dev.yaml)\n    A --&gt; D(templates/*.yaml)\n    A --&gt; E(charts/* subcharts)\n    E --&gt; E1(Postgres)\n    E --&gt; E2(Redis)\n    E --&gt; E3(Kafka)\n    E --&gt; E4(Etcd)\n    E --&gt; E5(Vault)\n    E --&gt; E6(OPA)\n    E --&gt; E7(Prometheus)\n    E --&gt; E8(Grafana)\n</code></pre>"},{"location":"infra/SHARED_INFRA_MANUAL/#2-prerequisites","title":"2. Prerequisites","text":"<ol> <li>Tooling</li> <li>Mac/Linux with Docker Desktop or containerd runtime.</li> <li><code>kubectl</code> \u2265 1.27.</li> <li><code>helm</code> \u2265 3.12.</li> <li><code>kind</code> (recommended for local cluster) or access to another Kubernetes cluster.</li> <li> <p>Optional: <code>make</code>, <code>yq</code>.</p> </li> <li> <p>Local Resources</p> </li> <li>4 CPUs and 8 GiB RAM minimum for the cluster (Kafka is the heaviest component).</li> <li> <p>10 GiB of disk for PVCs (Postgres 5 GiB, Etcd 1 GiB, Docker image cache).</p> </li> <li> <p>Repositories</p> </li> <li>This manual assumes the shared infra lives under <code>infra/helm/soma-infra</code> with supporting docs under <code>docs/infra/</code>.</li> </ol>"},{"location":"infra/SHARED_INFRA_MANUAL/#3-cluster-bring-up-kind","title":"3. Cluster Bring-up (Kind)","text":"<ol> <li> <p>Create Kind Cluster <pre><code>kind create cluster --name soma --config infra/kind/soma-kind.yaml\n</code></pre>    The provided Kind config reserves NodePorts and storage class <code>standard</code>.</p> </li> <li> <p>Verify Storage Class <pre><code>kubectl get storageclass\n</code></pre>    Ensure a default storage class exists (<code>standard</code>). Update <code>values-dev.yaml</code> if your cluster uses a different class.</p> </li> <li> <p>Namespace <pre><code>kubectl create namespace soma-infra\n</code></pre></p> </li> </ol>"},{"location":"infra/SHARED_INFRA_MANUAL/#4-helm-deployment-step-by-step","title":"4. Helm Deployment \u2014 Step by Step","text":"<ol> <li> <p>Dry-run <pre><code>helm upgrade --install soma-infra infra/helm/soma-infra \\\n  -n soma-infra \\\n  -f infra/helm/soma-infra/values-dev.yaml \\\n  --dry-run\n</code></pre></p> </li> <li> <p>Install <pre><code>helm upgrade --install soma-infra infra/helm/soma-infra \\\n  -n soma-infra \\\n  -f infra/helm/soma-infra/values-dev.yaml\n</code></pre></p> <p><code>--wait</code> is intentionally omitted to avoid long timeouts while Kafka downloads images. Monitor pods manually in the next step.</p> </li> <li> <p>Watch Pod Start-up <pre><code>kubectl -n soma-infra get pods -w\n</code></pre>    All pods should progress to <code>1/1 Running</code>. Kafka may take ~2 minutes on the first run while generating the KRaft metadata.</p> </li> <li> <p>Secrets</p> </li> <li>Postgres, Grafana, and Vault pull credentials from Secrets rendered by the chart:<ul> <li><code>soma-infra-postgres-credentials</code></li> <li><code>soma-infra-grafana-admin</code></li> <li><code>soma-infra-vault-dev-root</code></li> </ul> </li> <li>Adjust defaults in <code>values.yaml</code> or create overrides before installation for stronger credentials.</li> </ol>"},{"location":"infra/SHARED_INFRA_MANUAL/#5-post-install-verification","title":"5. Post-Install Verification","text":"<p>Run each command; all must succeed before onboarding application workloads.</p> <pre><code>kubectl -n soma-infra exec soma-infra-postgres-0 -- \\\n  env PGPASSWORD=$(kubectl -n soma-infra get secret soma-infra-postgres-credentials -o jsonpath='{.data.password}' | base64 --decode) \\\n  psql -U $(kubectl -n soma-infra get secret soma-infra-postgres-credentials -o jsonpath='{.data.username}' | base64 --decode) \\\n  -d $(kubectl -n soma-infra get secret soma-infra-postgres-credentials -o jsonpath='{.data.database}' | base64 --decode) \\\n  -c 'SELECT 1'\n\nkubectl -n soma-infra exec deploy/soma-infra-redis -- redis-cli ping\n\nkubectl -n soma-infra exec soma-infra-etcd-0 -- etcdctl --endpoints=http://127.0.0.1:2379 endpoint health\n\nkubectl -n soma-infra exec deploy/soma-infra-vault -- \\\n  env VAULT_ADDR=http://127.0.0.1:8200 vault status\n\nkubectl -n soma-infra logs deploy/soma-infra-kafka --tail=50\n</code></pre> <p>Kafka logs will include <code>DuplicateBrokerRegistrationException</code> messages in single-node mode. These are expected; the broker still shows <code>Running</code>.</p>"},{"location":"infra/SHARED_INFRA_MANUAL/#6-configuration-reference","title":"6. Configuration Reference","text":"Component Location Notes Postgres <code>charts/postgres/values.yaml</code> Credentials sourced from Secret; PVC default 5\u202fGi. Redis <code>charts/redis/values.yaml</code> Stateless; no password in dev profile. Kafka <code>charts/kafka/values.yaml</code> KRaft single node; controller port 9093 exposed internally; new base64 cluster ID. Etcd <code>charts/etcd/templates/statefulset.yaml</code> <code>ETCD_INITIAL_CLUSTER</code> fix for single-member cluster. Vault <code>charts/vault/values.yaml</code> Dev mode with root token stored in Secret. NetworkPolicies <code>templates/networkpolicies.yaml</code> Default-deny + per-component ingress from namespaces labeled <code>soma.sh/allow-shared-infra=true</code>. Environment docs <code>docs/infra/ENV_VARS_AND_SECRETS.md</code> Inventory of sensitive values, recommended handling (Secrets/Vault). Runbook <code>docs/infra/runbook_shared_infra.md</code> High-level bring-up; pair with this manual for full detail."},{"location":"infra/SHARED_INFRA_MANUAL/#7-operational-tips","title":"7. Operational Tips","text":"<ol> <li> <p>Rolling Upgrades <pre><code>helm upgrade soma-infra infra/helm/soma-infra \\\n  -n soma-infra \\\n  -f infra/helm/soma-infra/values-dev.yaml\n</code></pre>    Recreate pods selectively with <code>kubectl delete pod</code> to pick up template changes.</p> </li> <li> <p>Logs &amp; Metrics</p> </li> <li>Access Grafana via port-forward:      <pre><code>kubectl -n soma-infra port-forward deploy/soma-infra-grafana 3000:3000\n</code></pre></li> <li> <p>Prometheus:      <pre><code>kubectl -n soma-infra port-forward deploy/soma-infra-prometheus 9090:9090\n</code></pre></p> </li> <li> <p>Cleaning Up <pre><code>helm uninstall soma-infra -n soma-infra\nkubectl delete namespace soma-infra\nkind delete cluster --name soma  # if using Kind\n</code></pre></p> </li> </ol>"},{"location":"infra/SHARED_INFRA_MANUAL/#8-packaging-for-somatechlatsomastack","title":"8. Packaging for <code>somatechlat/somastack</code>","text":"<p>To publish this shared infra as its own repository:</p> <ol> <li> <p>Copy Assets <pre><code>infra/helm/soma-infra/\ndocs/infra/SHARED_INFRA_MANUAL.md\ndocs/infra/SHARED_INFRA_ARCHITECTURE.md\ndocs/infra/runbook_shared_infra.md\ndocs/infra/ENV_VARS_AND_SECRETS.md\ndocs/infra/DEVELOPMENT_CANONICAL.md\ndocs/infra/SPRINTS_SHARED_INFRA.md\ninfra/env/.env.shared.example\ninfra/docker/shared-infra.compose.yaml\ninfra/kind/soma-kind.yaml\nREADME (new root readme referencing this manual)\n</code></pre></p> </li> <li> <p>Initialize New Repo <pre><code>mkdir somastack &amp;&amp; cd somastack\ngit init\ncp -R &lt;above files&gt; .\necho \"docs/node_modules\" &gt;&gt; .gitignore  # extend as needed\n</code></pre></p> </li> <li> <p>Commit &amp; Push <pre><code>git add .\ngit commit -m \"feat: add soma shared infra chart and docs\"\ngit remote add origin git@github.com:somatechlat/somastack.git\ngit push -u origin main\n</code></pre></p> <p>Ensure you have write access to <code>somatechlat/somastack</code>. Set up SSH or HTTPS credentials before pushing.</p> </li> <li> <p>Release Preparation</p> </li> <li>Run <code>helm lint infra/helm/soma-infra</code>.</li> <li>Package chart: <code>helm package infra/helm/soma-infra --destination dist</code>.</li> <li>Optionally publish to GitHub Releases or an OCI registry.</li> </ol>"},{"location":"infra/SHARED_INFRA_MANUAL/#9-troubleshooting","title":"9. Troubleshooting","text":"Symptom Resolution Kafka in <code>ImagePullBackOff</code> Verify image tag reachable (<code>confluentinc/cp-kafka:7.5.3</code>). Use <code>docker pull</code> or substitute with a mirrored registry. Kafka complains <code>CLUSTER_ID</code> invalid Ensure <code>clusterId</code> in values is a base64 string (<code>uuidgen | base64</code>). Postgres <code>permission denied</code> on PVC Leave default security context (root) or add init container that chowns to custom UID before dropping privileges. Vault dev pod fails TLS Ensure <code>VAULT_ADDR</code> uses HTTP for dev mode (<code>http://127.0.0.1:8200</code>). Namespace stuck terminating Remove finalizers: <code>kubectl get namespace soma-infra -o json | jq '.spec.finalizers=[]' | kubectl replace --raw \"/api/v1/namespaces/soma-infra/finalize\" -f -</code>."},{"location":"infra/SHARED_INFRA_MANUAL/#10-change-log-pointers","title":"10. Change Log Pointers","text":"<ul> <li>Helm chart state from commit <code>b8866fd</code> (<code>chore: snapshot development branch state</code>).</li> <li>Prior incremental fix: <code>b8d7fdd</code> (<code>fix: stabilize shared infra helm chart</code>).</li> </ul> <p>Use these commits as baselines when lifting the shared infra into <code>somastack</code>.</p>"},{"location":"infra/SHARED_INFRA_MANUAL/#appendix-a-command-recap","title":"Appendix A \u2014 Command Recap","text":"<pre><code># Create Kind cluster\nkind create cluster --name soma --config infra/kind/soma-kind.yaml\n\n# Install shared infra\nhelm upgrade --install soma-infra infra/helm/soma-infra \\\n  -n soma-infra \\\n  -f infra/helm/soma-infra/values-dev.yaml\n\n# Verify pods\nkubectl -n soma-infra get pods\n\n# Health checks\nkubectl -n soma-infra exec soma-infra-postgres-0 -- psql -c 'SELECT 1'\nkubectl -n soma-infra exec deploy/soma-infra-redis -- redis-cli ping\nkubectl -n soma-infra exec soma-infra-etcd-0 -- etcdctl endpoint health\nkubectl -n soma-infra exec deploy/soma-infra-vault -- env VAULT_ADDR=http://127.0.0.1:8200 vault status\n\n# Tear down\nhelm uninstall soma-infra -n soma-infra\nkind delete cluster --name soma\n</code></pre> <p>By following this manual, engineers can reliably reproduce the shared infrastructure locally and package it for distribution as the dedicated <code>somastack</code> repository.</p>"},{"location":"infra/SPRINTS_SHARED_INFRA/","title":"Shared Infra Sprints (Exec Plan)","text":"<p>This is the sprint-level execution plan to deliver the Shared Infra layer on a local Kind cluster with production semantics, and to make it ready for standalone reuse.</p>"},{"location":"infra/SPRINTS_SHARED_INFRA/#sprint-0-foundations-doneongoing","title":"Sprint 0 \u2014 Foundations (done/ongoing)","text":"<ul> <li>Create umbrella chart <code>infra/helm/soma-infra</code> with dev/staging/prod values overlays.</li> <li>Add Docker compose <code>infra/docker/shared-infra.compose.yaml</code> (dev-only).</li> <li>Add Kind cluster config <code>infra/kind/soma-kind.yaml</code> mapping 7002 for app gateway.</li> <li>Publish env contract <code>infra/env/.env.shared.example</code>.</li> <li>Runbook <code>docs/infra/runbook_shared_infra.md</code>.</li> </ul> <p>Acceptance: repo contains these assets; no app coupling; docs explain contracts.</p>"},{"location":"infra/SPRINTS_SHARED_INFRA/#sprint-1-bring-up-on-kind-current","title":"Sprint 1 \u2014 Bring-up on Kind (current)","text":"<ul> <li>Create namespace <code>soma-infra</code>.</li> <li>Install <code>soma-infra</code> with values-dev: Postgres, Kafka, Redis, OPA, Prometheus, Grafana, Vault, Etcd (OpenFGA optional off).</li> <li>Health checks: psql SELECT 1, Redis PING, Kafka metadata, OPA allow/deny, Vault dev, Etcd health.</li> <li>Prometheus targets show Up; (optional) Grafana accessible.</li> <li>NetworkPolicies: default-deny + allows from app namespaces by label.</li> </ul> <p>Acceptance: Readiness report with Pods/Services/Endpoints + checks above.</p>"},{"location":"infra/SPRINTS_SHARED_INFRA/#sprint-2-identity-and-policy-baseline","title":"Sprint 2 \u2014 Identity and Policy Baseline","text":"<ul> <li>Dev JWT issuer (JWKS URL stable) or select IdP (Keycloak/Auth0/Okta) and wire JWKS.</li> <li>OPA policy bundle v1: inputs schema, default-deny, tenant, roles/scopes, s2s allowlist, budgets/routing.</li> <li>Document decision path and test vectors; add contract tests to CI.</li> </ul> <p>Acceptance: JWT validation OK across sample services; OPA decisions enforced in sample requests.</p>"},{"location":"infra/SPRINTS_SHARED_INFRA/#sprint-3-security-k8s-baseline","title":"Sprint 3 \u2014 Security &amp; K8s Baseline","text":"<ul> <li>Enforce PodSecurity (restricted), non-root users, read-only root FS.</li> <li>Add NetworkPolicies to all shared services; verify denies and allows.</li> <li>Ensure probes/limits everywhere; add HPAs only where suitable (OPA/Prometheus optional).</li> </ul> <p>Acceptance: Policy checks pass; kube-bench-like baseline satisfied (dev profile).</p>"},{"location":"infra/SPRINTS_SHARED_INFRA/#sprint-4-observability-slos","title":"Sprint 4 \u2014 Observability &amp; SLOs","text":"<ul> <li>Prometheus dashboards for Postgres/Kafka/Redis/OPA/Vault; scrape 15s; retention 1\u20133 days.</li> <li>Optional: Jaeger/Loki add-ons; wire OTLP endpoint variable.</li> <li>Alert rules: error rate, latency, consumer lag; document initial SLOs.</li> </ul> <p>Acceptance: Dashboards render; alerts fire in synthetic scenarios.</p>"},{"location":"infra/SPRINTS_SHARED_INFRA/#sprint-5-data-dr-planning","title":"Sprint 5 \u2014 Data &amp; DR Planning","text":"<ul> <li>Define prod HA path: Postgres (Patroni/Crunchy), Kafka (Strimzi), Redis Operator.</li> <li>Backups: Postgres PITR, retention plans; Kafka retention; Redis backups as needed.</li> <li>DR runbooks; smoke recovery test in CI.</li> </ul> <p>Acceptance: DR docs/runbooks land; basic recovery validated.</p>"},{"location":"infra/SPRINTS_SHARED_INFRA/#sprint-6-supply-chain-gitops","title":"Sprint 6 \u2014 Supply Chain &amp; GitOps","text":"<ul> <li>SBOM (Syft), image scanning (Trivy), image signing (Cosign) baked into CI.</li> <li>Chart testing; Kind ephemeral env in CI; helm upgrade --install smoke.</li> <li>Argo CD app-of-apps outline for infra; values promotion strategy.</li> </ul> <p>Acceptance: CI green; artifacts signed; GitOps path documented.</p>"},{"location":"infra/SPRINTS_SHARED_INFRA/#sprint-7-ready-to-split-packaging","title":"Sprint 7 \u2014 Ready-to-Split Packaging","text":"<ul> <li>values.schema.json; chart Lint PASS; OCI/Pages publishing instructions.</li> <li>MIGRATION.md for splitting infra into its own repo; consumers keep DNS contracts.</li> </ul> <p>Acceptance: One command to publish chart; consumers can depend without code changes.</p>"},{"location":"infra/runbook_shared_infra/","title":"Shared Infra Runbook (Developer Production Ready)","text":"<p>This runbook describes how to start the consolidated shared infrastructure for the Soma stack using either Docker (local only) or Kubernetes (Kind or a real cluster). The app services (SA01, etc.) will consume these endpoints and should not run their own infra copies.</p>"},{"location":"infra/runbook_shared_infra/#components","title":"Components","text":"<ul> <li>Postgres, Kafka (KRaft), Redis, OPA, OpenFGA (optional), Vault (dev for local), Etcd, Prometheus, Grafana</li> </ul>"},{"location":"infra/runbook_shared_infra/#docker-local-shared-infra-only","title":"Docker (local shared infra only)","text":"<ul> <li>Compose file: infra/docker/shared-infra.compose.yaml</li> <li>Exposed ports (host): Postgres 5433, Kafka 9094, Redis 6380, OPA 8182, Vault 8201, Etcd 2380, Prometheus 9091, Grafana 3001</li> <li>Contract file: copy infra/env/.env.shared.example to .env.shared and export in your shell. App compose should read from this file.</li> </ul>"},{"location":"infra/runbook_shared_infra/#kubernetes-kind-or-cluster","title":"Kubernetes (Kind or cluster)","text":"<ul> <li>Chart: infra/helm/soma-infra/</li> <li>Values overlays: values-dev.yaml, values-staging.yaml, values-prod.yaml</li> <li>Namespace: soma-infra</li> <li>Kind config: infra/kind/soma-kind.yaml (reserves host:7002 via NodePort mapping 30080)</li> <li>All services are cluster-internal by default. Grafana/Prometheus can be temporarily exposed via NodePort for troubleshooting.</li> </ul>"},{"location":"infra/runbook_shared_infra/#health-and-verification","title":"Health and verification","text":"<ul> <li>Ensure all containers/pods are healthy.</li> <li>Verify endpoints: Postgres connectivity, Kafka metadata fetch, Redis PING, OPA responds, Vault dev token works, Etcd health, Prometheus targets, Grafana UI.</li> </ul>"},{"location":"infra/runbook_shared_infra/#notes","title":"Notes","text":"<ul> <li>Keep the app gateway reserved for host port 7001 (Docker) and 7002 (K8s). Do not map infra to those ports.</li> <li>The environment contract keeps Docker and K8s consistent; only hostnames/ports differ.</li> </ul>"},{"location":"onboarding-manual/","title":"Onboarding Manual","text":"<p>Welcome to SomaAgent01. This manual guides new contributors through their first 90 days, aligning expectations, knowledge resources, and success metrics.</p>"},{"location":"onboarding-manual/#structure","title":"Structure","text":"<ul> <li>Orientation Checklist</li> <li>First 90 Days Plan</li> <li>Toolchain Primer</li> <li>Team Directory</li> <li>FAQ</li> </ul> <p>Coordinate with your mentor and manager to tailor this plan. Mark off tasks in the onboarding tracker and update stakeholders weekly.</p>"},{"location":"onboarding-manual/faq/","title":"Onboarding FAQ","text":"<p>When do I get production access? After the 30-day review and security training completion. Manager submits access request.</p> <p>Who approves my PTO during onboarding? Submit via HR system; manager must approve. Notify mentor for scheduling impacts.</p> <p>How do I request hardware accessories? Open a ticket with IT helpdesk. Include shipping address and manager approval.</p> <p>Where do I find design assets? Access Figma workspace (invitation sent day 1). For missing access, ping <code>@product</code>.</p> <p>What if I miss onboarding sessions? Recordings live in the LMS. Schedule a makeup session with the presenter.</p> <p>Can I contribute before completing orientation? Yes, but ensure environment setup and security requirements met. All contributions follow the Contribution Workflow.</p>"},{"location":"onboarding-manual/first-90-days/","title":"First 90 Days Plan","text":"<p>This plan outlines expectations and milestones for new SomaAgent01 team members across their first three months. Customize with your manager while maintaining these core outcomes.</p>"},{"location":"onboarding-manual/first-90-days/#days-130-ramp-up","title":"Days 1\u201330 (Ramp Up)","text":"<p>Goals: - Ship one documentation update via Pull Request following the Contribution Workflow. - Pair-program with mentor on a low-risk bug or test addition. - Shadow on-call handoff to understand operational cadence.</p> <p>Milestones: - Environment verified (<code>make dev-up</code>, <code>pytest</code>). - Documented first impressions in <code>docs/changelog.md</code> under onboarding notes.</p>"},{"location":"onboarding-manual/first-90-days/#days-3160-deliver","title":"Days 31\u201360 (Deliver)","text":"<p>Goals: - Own a small feature or refactor spanning backend and documentation. - Present a brown-bag on a technical topic (architecture, testing, etc.). - Contribute to the Onboarding Manual with lessons learned.</p> <p>Milestones: - Feature PR merged with tests and docs. - Added new entry to Development Manual or updated existing guidance.</p>"},{"location":"onboarding-manual/first-90-days/#days-6190-lead","title":"Days 61\u201390 (Lead)","text":"<p>Goals: - Lead a release candidate dry run or incident drill. - Mentor another new hire or contributor through their first PR. - Identify and document a process improvement in the Changelog.</p> <p>Milestones: - Ran release dry run successfully. - Added new runbook entry or improved existing one.</p>"},{"location":"onboarding-manual/first-90-days/#verification","title":"Verification","text":"<ul> <li>Manager reviews progress at 30/60/90 day checkpoints.</li> <li>Updates recorded in onboarding tracker and shared in weekly syncs.</li> <li>Final 90-day review includes summary of achievements and next quarter goals.</li> </ul>"},{"location":"onboarding-manual/orientation/","title":"Orientation Checklist","text":"<p>Complete these steps during your first week to gain access, context, and support.</p>"},{"location":"onboarding-manual/orientation/#day-01","title":"Day 0\u20131","text":"<ul> <li>[ ] Sign all required paperwork and NDAs.</li> <li>[ ] Receive hardware and credentials (email, GitHub, Slack).</li> <li>[ ] Join Slack channels: <code>#soma-general</code>, <code>#soma-agent-zero</code>, <code>#soma-oncall</code>.</li> <li>[ ] Schedule 1:1 with manager and assigned mentor.</li> </ul>"},{"location":"onboarding-manual/orientation/#day-23","title":"Day 2\u20133","text":"<ul> <li>[ ] Review the User Manual to understand the product surface.</li> <li>[ ] Read the Architecture Overview.</li> <li>[ ] Set up environment per Environment Setup.</li> <li>[ ] Pair with mentor to run <code>make dev-up</code> and validate the stack.</li> </ul>"},{"location":"onboarding-manual/orientation/#day-45","title":"Day 4\u20135","text":"<ul> <li>[ ] Attend platform overview session (recording linked in LMS).</li> <li>[ ] Read security baseline and sign acceptable-use policy.</li> <li>[ ] Complete onboarding quiz (link in LMS).</li> <li>[ ] Update onboarding tracker with blockers or questions.</li> </ul>"},{"location":"onboarding-manual/orientation/#verification","title":"Verification","text":"<ul> <li>Manager confirms completion in onboarding system.</li> <li>New hire posts intro message in <code>#soma-general</code>.</li> <li>Access requests approved (GitHub, Vault, analytics).</li> </ul>"},{"location":"onboarding-manual/team-contacts/","title":"Team Directory","text":"<p>Use this directory to connect with the right experts. Keep it updated when roles change.</p> Role Name Slack Responsibilities Engineering Manager TBD <code>@eng-manager</code> Prioritization, staffing, escalations Tech Lead TBD <code>@tech-lead</code> Architecture decisions, reviews Product Manager TBD <code>@product</code> Roadmap, user feedback Platform Operations TBD <code>@sre-oncall</code> Deployments, infrastructure Security Engineer TBD <code>@security</code> Secrets, audits Docs Lead TBD <code>@docs-lead</code> Documentation governance Onboarding Mentor Assigned per hire <code>@mentor</code> Weekly syncs, unblockers <p>[!NOTE] Update the placeholders (<code>TBD</code>) with current names and usernames as part of the onboarding audit. Link to the internal directory once the PeopleOps system integration is complete.</p>"},{"location":"onboarding-manual/toolchain/","title":"Toolchain Primer","text":"<p>This primer introduces the core tools you will use daily on SomaAgent01.</p>"},{"location":"onboarding-manual/toolchain/#communication-knowledge","title":"Communication &amp; Knowledge","text":"<ul> <li>Slack channels: <code>#soma-agent-zero</code>, <code>#soma-oncall</code>, <code>#soma-docs</code>.</li> <li>Documentation hub: <code>docs/README.md</code> (new structure) and MkDocs site (published internally).</li> <li>Incident updates: <code>status.somaagent01.dev</code>.</li> </ul>"},{"location":"onboarding-manual/toolchain/#development-tools","title":"Development Tools","text":"Purpose Tool Notes Code editor VS Code Recommended extensions: Python, Docker, Markdown All in One Version control Git + GitHub Use SSH keys; enable 2FA Python runtime <code>pyenv</code> (optional) or system Python 3.12 Virtualenv at <code>.venv</code> Package mgmt <code>pip</code>, <code>npm</code>, optional <code>poetry</code> Install dependencies as documented Containers Docker Desktop Allocate \u22654 CPUs, 8 GB RAM"},{"location":"onboarding-manual/toolchain/#commands-to-memorize","title":"Commands to Memorize","text":"<pre><code>make dev-up        # Start full stack\nmake dev-down      # Stop stack\nmake dev-logs      # Tail logs\nmake fmt lint      # Format &amp; lint\npytest             # Run test suites\nmake docs-verify   # Markdown lint, link check, MkDocs build\n</code></pre>"},{"location":"onboarding-manual/toolchain/#credentials-access","title":"Credentials &amp; Access","text":"<ul> <li>Request Vault access via Access Manager ticket.</li> <li>Obtain Soma SLM API key from platform team; store in 1Password and <code>.env</code>.</li> <li>GitHub membership handled by people-ops; verify org access on day one.</li> </ul>"},{"location":"onboarding-manual/toolchain/#self-checks","title":"Self-Checks","text":"<ul> <li>Confirm <code>python --version</code> outputs 3.12.x.</li> <li><code>docker info</code> shows minimum resource allocation.</li> <li><code>npm -v</code> returns 8.x or later.</li> </ul> <p>If any tool is missing or incompatible, contact <code>#soma-helpdesk</code>.</p>"},{"location":"operations/deployments/","title":"Deployment Guide","text":""},{"location":"operations/deployments/#environments","title":"Environments","text":"Environment Purpose Notes Local Dev Feature development, manual QA <code>make dev-up</code> Staging Pre-production validation Mirrors prod configuration, uses real providers Production Live users Hardened networking, autoscaling"},{"location":"operations/deployments/#configuration-matrix","title":"Configuration Matrix","text":"Config Local Staging Production <code>GATEWAY_REQUIRE_AUTH</code> false true true <code>OPENAI_API_KEY</code> .env Secret manager Secret manager Kafka Docker compose Managed cluster Managed cluster Postgres Docker compose Managed instance Managed HA cluster"},{"location":"operations/deployments/#build-pipeline","title":"Build Pipeline","text":"<ol> <li>Run tests locally (<code>pytest</code>, Playwright).</li> <li>Build images: <code>docker compose build</code> or CI pipeline.</li> <li>Push to registry (tag with git SHA, semantic version).</li> <li>Deploy via Compose (staging) or Helm (production).</li> </ol>"},{"location":"operations/deployments/#docker-compose-deployment-staging","title":"Docker Compose Deployment (Staging)","text":"<pre><code>git pull\ndocker compose -p somaagent01-staging --profile core --profile dev -f infra/docker-compose.somaagent01.yaml up -d\n</code></pre> <ul> <li>Use <code>.env.staging</code> for environment-specific overrides.</li> <li>Confirm health: <code>docker compose ps</code>.</li> <li>Host ports default to the reserved range <code>20000-20199</code> (Kafka 20000, Redis 20001, Postgres 20002, Gateway 20016, UI 20015). Override via <code>PORT_POOL_START</code> / <code>PORT_POOL_MAX</code> if the range is occupied.</li> <li>SomaBrain integration expects <code>http://host.docker.internal:9696</code>; ensure your local SomaBrain service is listening on that port.</li> </ul>"},{"location":"operations/deployments/#kubernetes-deployment-planned","title":"Kubernetes Deployment (Planned)","text":"<ul> <li>Helm chart (<code>deploy/helm/somaagent01</code>) will manage:</li> <li>StatefulSets for Postgres/Kafka/Redis</li> <li>Deployments for Gateway, UI, Tool Executor</li> <li>ConfigMaps for prompts, settings defaults</li> <li>Secrets for API keys, credentials</li> <li>Ingress exposes Gateway + UI via HTTPS.</li> </ul>"},{"location":"operations/deployments/#rolling-update-procedure","title":"Rolling Update Procedure","text":"<ol> <li>Scale Gateway to zero traffic (if load balancer supports draining).</li> <li>Apply new build (<code>docker compose up -d gateway</code> or <code>helm upgrade</code>).</li> <li>Monitor health checks, metrics, logs.</li> <li>Re-enable traffic.</li> <li>Run smoke tests.</li> </ol>"},{"location":"operations/deployments/#rollback","title":"Rollback","text":"<ul> <li>Docker: <code>docker compose up -d gateway=&lt;previous-tag&gt;</code>.</li> <li>Helm: <code>helm rollback somaagent01 &lt;revision&gt;</code>.</li> <li>Ensure database migrations are backward compatible or have rollback scripts.</li> </ul>"},{"location":"operations/deployments/#environment-promotion-checklist","title":"Environment Promotion Checklist","text":"<ul> <li>[ ] Tests green (unit, integration, E2E).</li> <li>[ ] Release notes updated.</li> <li>[ ] Config parity reviewed (feature flags, API keys).</li> <li>[ ] Observability dashboards verified.</li> <li>[ ] Incident response contacts updated.</li> </ul>"},{"location":"operations/deployments/#secrets-management","title":"Secrets Management","text":"<ul> <li>Local: <code>.env</code> (never commit), <code>python/helpers/secrets.py</code> loads.</li> <li>Staging/Prod: managed secrets (Vault, AWS Secrets Manager). Mount or inject as env vars.</li> </ul>"},{"location":"operations/deployments/#compliance-audit","title":"Compliance &amp; Audit","text":"<ul> <li>Record deployment metadata in <code>docs/changelog.md</code>.</li> <li>Tag git release (<code>git tag vX.Y.Z</code>).</li> <li>Store build artifacts and configuration in artifact repository.</li> </ul>"},{"location":"operations/observability/","title":"Observability Playbook","text":""},{"location":"operations/observability/#metrics","title":"Metrics","text":"<ul> <li>Prometheus collects component metrics. Access via <code>http://localhost:&lt;PROMETHEUS_PORT&gt;</code>.</li> <li>Exporters:</li> <li>Gateway: request latency, error count, circuit-breaker stats.</li> <li>Kafka: broker health, partition ISR, consumer lag.</li> <li>Postgres: connection pool, slow queries (enable pg exporter).</li> <li>Redis: memory usage, hits/misses.</li> </ul>"},{"location":"operations/observability/#key-dashboards","title":"Key Dashboards","text":"Dashboard Metrics Gateway Overview RPS, latency percentiles, error codes Realtime Speech Session success rate, negotiation latency Tool Executor Queue depth, execution duration Infrastructure CPU/memory per container, disk usage"},{"location":"operations/observability/#logging","title":"Logging","text":"<ul> <li>Docker stdout/stderr aggregated with <code>docker logs</code>.</li> <li>For advanced setups, forward to Loki or ELK. Structure logs as JSON for easier parsing.</li> <li>Retain logs for at least 30 days in production.</li> </ul>"},{"location":"operations/observability/#tracing","title":"Tracing","text":"<ul> <li>Enable OpenTelemetry by setting <code>OTEL_EXPORTER_OTLP_ENDPOINT</code>.</li> <li>Instrument Gateway routes and Tool Executor tasks.</li> </ul>"},{"location":"operations/observability/#alerting","title":"Alerting","text":"Alert Condition Action High Gateway latency p95 &gt; 3s for 5 min Investigate LLM provider, Redis Tool queue backlog <code>somastack.tools</code> lag &gt; 500 Scale executors or inspect stuck jobs Realtime session failures Error rate &gt; 10% Check API key, network Kafka ISR shrink &lt; partitions replicating Restart broker"},{"location":"operations/observability/#log-triage","title":"Log Triage","text":"<ol> <li><code>docker logs -f somaAgent01_gateway</code></li> <li>Filter by correlation ID (from UI debug panel).</li> <li>For tool issues, inspect executor logs.</li> </ol>"},{"location":"operations/observability/#synthetic-monitoring","title":"Synthetic Monitoring","text":"<ul> <li>Schedule periodic <code>/healthz</code> checks.</li> <li>Run automated chat scenario (via CI agent) two times per hour to validate end-to-end.</li> </ul>"},{"location":"operations/observability/#incident-response","title":"Incident Response","text":"<ul> <li>Document incidents in <code>docs/operations/incidents/YYYY-MM-DD.md</code>.</li> <li>Include timeline, root cause, remediation, follow-up tasks.</li> </ul>"},{"location":"operations/observability/#capacity-planning","title":"Capacity Planning","text":"<ul> <li>Track CPU/memory for Gateway and Tool Executor under load tests.</li> <li>Scale Kafka partitions when throughput &gt; 10k messages/minute.</li> </ul>"},{"location":"operations/observability/#instrumentation-todos","title":"Instrumentation TODOs","text":"<ul> <li>Add Prometheus metrics for <code>/realtime_session</code> success/failure counts.</li> <li>Implement structured logging in Tool Executor with correlation IDs.</li> <li>Wire tracing for memory fetch/save operations.</li> </ul>"},{"location":"operations/runbooks/","title":"Operations Runbooks","text":""},{"location":"operations/runbooks/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Start / Stop Stack</li> <li>Smoke Tests</li> <li>Realtime Speech Troubleshooting</li> <li>Database Maintenance</li> <li>Kafka Recovery</li> <li>Observability Checks</li> <li>Disaster Recovery</li> </ol>"},{"location":"operations/runbooks/#start-stop-stack","title":"Start / Stop Stack","text":""},{"location":"operations/runbooks/#start","title":"Start","text":"<p><pre><code>make dev-up\n</code></pre> - Reserves host ports, prunes old containers, brings up <code>core</code> + <code>dev</code> profiles. - Waits for Postgres to report <code>healthy</code> before returning. - Default host bindings live in the <code>20000-20199</code> window (UI <code>20015</code>, Gateway <code>20016</code>, Kafka <code>20000</code>, etc.); adjust via <code>PORT_POOL_START</code>/<code>PORT_POOL_MAX</code> if needed.</p>"},{"location":"operations/runbooks/#stop","title":"Stop","text":"<p><pre><code>make dev-down\n</code></pre> - Stops Gateway/UI while preserving volumes.</p>"},{"location":"operations/runbooks/#clean-wipe-volumes","title":"Clean (wipe volumes)","text":"<p><pre><code>make dev-clean\n</code></pre> - Warning: removes Postgres/Kafka/Redis state.</p>"},{"location":"operations/runbooks/#status","title":"Status","text":"<p><pre><code>make dev-status\n</code></pre> - Lists container state with health indicators.</p>"},{"location":"operations/runbooks/#smoke-tests","title":"Smoke Tests","text":"<p>Run after stack startup or deployment: <pre><code>pytest tests/integration/test_gateway_public_api.py\npytest tests/playwright/test_realtime_speech.py --headed\n</code></pre> - First test validates API contract. - Second ensures realtime speech provider works end-to-end.</p>"},{"location":"operations/runbooks/#realtime-speech-troubleshooting","title":"Realtime Speech Troubleshooting","text":"Symptom Check Action UI toast \"Realtime session error\" Gateway logs (<code>docker logs somaAgent01_gateway</code>) Validate OpenAI API key, network access No audio playback Browser console Ensure user interaction unlocked audio context WebRTC negotiation stuck Gateway <code>/realtime_session</code> response Inspect endpoint/voice parameters"},{"location":"operations/runbooks/#database-maintenance","title":"Database Maintenance","text":"<ul> <li>Backups: <code>docker exec somaAgent01_postgres pg_dump somaagent01 &gt; backup.sql</code>.</li> <li>Restore: <code>docker exec -i somaAgent01_postgres psql somaagent01 &lt; backup.sql</code>.</li> <li>Vacuum/analyze weekly or after large data loads.</li> </ul>"},{"location":"operations/runbooks/#kafka-recovery","title":"Kafka Recovery","text":"<ol> <li>Check broker health: <code>docker logs somaAgent01_kafka</code>.</li> <li>Restart broker: <code>docker restart somaAgent01_kafka</code>.</li> <li>If topics missing, re-run provisioning script (coming soon).</li> <li>For stuck consumers, inspect lag: <code>kcat -b localhost:29092 -Q -t somastack.tools</code>.</li> </ol>"},{"location":"operations/runbooks/#observability-checks","title":"Observability Checks","text":"<ul> <li>Metrics: <code>http://localhost:&lt;PrometheusPort&gt;/graph</code> (port printed by startup script).</li> <li>Logs: <code>docker logs -f somaAgent01_gateway</code>.</li> <li>Alerts: configure Alertmanager routes (see <code>docs/operations/observability.md</code>).</li> </ul>"},{"location":"operations/runbooks/#disaster-recovery","title":"Disaster Recovery","text":"Asset Restore Procedure Postgres Restore latest SQL dump, restart Gateway Redis Accept cache loss; rebuild from Postgres/SomaBrain Kafka Restore from backup snapshot; reprocess DLQ OpenFGA Re-apply migration scripts (<code>docker compose run openfga-migrate</code>) Config files Git restore specific version (<code>git checkout HEAD -- conf/tenants.yaml</code>)"},{"location":"operations/runbooks/#doc-references","title":"Doc References","text":"<ul> <li>Architecture: <code>docs/architecture/overview.md</code></li> <li>API specs: <code>docs/apis/</code></li> <li>Features guides: <code>docs/features/</code></li> <li>Troubleshooting: <code>docs/troubleshooting.md</code></li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>This section has been retired. Please use the four manuals in the navigation:</p> <ul> <li>User Manual</li> <li>Technical Manual</li> <li>Development Manual</li> <li>Onboarding Manual</li> </ul>"},{"location":"technical-manual/","title":"SomaAgent01 Technical Manual","text":"<p>This manual is the authoritative source on how SomaAgent01 is architected, deployed, and integrated with the wider Soma platform. It consolidates diagrams, configuration schemas, and operational decision records.</p>"},{"location":"technical-manual/#sections","title":"Sections","text":"<ul> <li>Architecture: canonical diagrams, service topology, and rationale.</li> <li>Infrastructure &amp; Deployments: Docker Compose and Helm blueprints.</li> <li>Integrations: External systems, APIs, and data contracts.</li> <li>Security &amp; Compliance: AuthN/Z, secrets, and audit posture.</li> <li>Data Flows: Streaming and persistence pipelines.</li> </ul> <p>Refer to the Development Manual for engineering workflows, and to the Onboarding Manual for role-based enablement.</p>"},{"location":"technical-manual/architecture/","title":"SomaAgent01 Architecture Overview","text":"<p>This document captures the current service layout, target consolidation, and implementation roadmap for the Soma stack as of 2025-10-15. It supersedes earlier partial diagrams and aligns with code paths in <code>services/</code>, <code>common/</code>, and <code>infra/</code>.</p>"},{"location":"technical-manual/architecture/#1-service-landscape-snapshot","title":"1. Service Landscape Snapshot","text":"Alias Repository components Owner Primary protocol Typical ports Status SA01 (SomaAgent01) <code>agent.py</code>, <code>services/conversation_worker/main.py</code>, <code>services/gateway/main.py</code> Agents gRPC (high-throughput) 50051 (planned) gRPC stubs present; HTTP gateway on 8010 still active SB (SomaBrain) <code>services/memory_service/main.py</code> Memory gRPC / HTTP 20017 Implemented; exposed via compose SAH (SomaAgentHub) <code>services/ui/main.py</code>, <code>run_ui.py</code> Experience FastAPI (HTTP) 8080 Running via <code>agent-ui</code> and <code>gateway</code> services SMF (SomaFractalMemory) <code>infra/docker-compose.somaagent01.yaml</code> (qdrant profile) Knowledge Async HTTP 50053 / 6333-6334 Optional compose profile Auth <code>infra/helm/soma-infra/charts/auth</code> Platform HTTP 8080 Helm chart deployed cluster-wide OPA <code>services/common/policy_client.py</code>, compose <code>opa</code> service Platform HTTP 8181 Deployed in compose and Helm Kafka <code>infra/docker-compose.somaagent01.yaml</code> Platform TCP 9092 Single node in compose, 3-node StatefulSet in Helm Redis <code>infra/docker-compose.somaagent01.yaml</code> Platform TCP 6379 Single instance; cluster planned Prometheus <code>infra/observability/prometheus.yml</code> Platform HTTP 9090 Live in compose and Helm Grafana <code>infra/observability/grafana</code> Platform HTTP 3000 Optional profile Vault <code>infra/docker-compose.somaagent01.yaml</code> Platform HTTP 8200 Dev mode compose; Helm chart forthcoming Etcd <code>infra/helm/soma-infra/charts/etcd</code> Platform HTTP 2379 Placeholder chart available <p>The stack currently runs four application services plus twelve infrastructure containers under Docker Compose. Consolidation work reduces duplication between local and cluster deployments.</p>"},{"location":"technical-manual/architecture/#2-shared-infrastructure-goal","title":"2. Shared Infrastructure Goal","text":""},{"location":"technical-manual/architecture/#21-target-state","title":"2.1 Target State","text":"<p>Shared services (Auth, OPA, Kafka, Redis, Prometheus/Grafana, Vault, Etcd) run once per cluster inside the <code>soma-infra</code> namespace. Applications reference them via Kubernetes DNS (<code>*.soma.svc.cluster.local</code>).</p> Service Deployment strategy Repository anchor Rationale Auth Deployment + Service <code>infra/helm/soma-infra/charts/auth</code> Centralize signing keys OPA Deployment with sidecar <code>infra/helm/soma-infra/charts/opa</code> Prevent policy drift Kafka StatefulSet (3 nodes) <code>infra/helm/soma-infra/charts/kafka</code> Shared event backbone Redis RedisCluster (6 pods) <code>infra/helm/soma-infra/charts/redis</code> Shared cache and rate limiting Prometheus/Grafana Operator + Deployment <code>infra/helm/soma-infra/charts/prometheus</code>, <code>charts/grafana</code> Unified observability Vault Deployment + Agent injector <code>infra/helm/soma-infra/charts/vault</code> Single secret source Etcd StatefulSet <code>infra/helm/soma-infra/charts/etcd</code> Feature flag backend"},{"location":"technical-manual/architecture/#22-resulting-footprint","title":"2.2 Resulting Footprint","text":"<ul> <li>Application services: 4.</li> <li>Infra services: 7 shared deployments.</li> <li>Pods per environment: ~25 (down from 40).</li> </ul>"},{"location":"technical-manual/architecture/#3-repository-alignment-plan","title":"3. Repository Alignment Plan","text":"<pre><code>agent-zero/\n  services/\n    conversation_worker/\n    gateway/\n    memory_service/\n    tool_executor/\n    ui/\n  infra/\n    docker-compose.somaagent01.yaml\n    helm/\n      soma-infra/\n      soma-stack/\n  docs/\n    technical-manual/architecture.md (this file)\n</code></pre> <p>Planned adjustments:</p> <ol> <li>Introduce service-specific subdirectories (<code>services/sa01</code>, etc.) as we modularize entrypoints.</li> <li>Ensure <code>infra/helm/soma-stack</code> consumes the shared infra chart and removes duplicated settings.</li> <li>Keep <code>common/</code> as the shared client library for configuration, telemetry, and memory access.</li> </ol>"},{"location":"technical-manual/architecture/#4-configuration-baseline","title":"4. Configuration Baseline","text":"<p>All services load shared settings via <code>common/config/settings.py</code>. SA01 overrides live in <code>services/common/settings_sa01.py</code>.</p> <p>Key defaults: - Kafka: <code>kafka.soma.svc.cluster.local:9092</code> - Redis: <code>redis.soma.svc.cluster.local:6379</code> - Postgres: <code>postgres.soma.svc.cluster.local:5432</code> - OPA: <code>http://opa.soma.svc.cluster.local:8181</code> - Auth: <code>http://auth.soma.svc.cluster.local:8080</code> - Etcd: <code>etcd.soma.svc.cluster.local:2379</code> - Metrics: Prometheus scrape endpoints defined in <code>infra/observability/prometheus.yml</code></p>"},{"location":"technical-manual/architecture/#5-deployment-model","title":"5. Deployment Model","text":"Layer Tooling Outcome Cluster Managed Kubernetes (EKS/GKE/AKS) Scalable, managed control plane GitOps Argo CD tracking <code>infra/helm</code> Declarative rollouts, audit trail Packaging Helm chart <code>soma-stack</code> + <code>soma-infra</code> dependency Single artifact per environment Traffic Istio weighted routing Safe canaries and blue/green CI/CD <code>.github/workflows/ci.yml</code> Lint, test, Kind-based Helm install"},{"location":"technical-manual/architecture/#6-observability-stack","title":"6. Observability Stack","text":"<ul> <li>Metrics: <code>/metrics</code> exposed on FastAPI workers, scraped by Prometheus.</li> <li>Tracing: OpenTelemetry exporters configured via <code>common/utils/trace.py</code> to Jaeger.</li> <li>Logging: JSON logs ingested by Loki via Promtail (Helm update pending).</li> <li>Alerting: Alertmanager rules for latency (&gt;200 ms), error rate (&gt;1%), Kafka lag (&gt;5k).</li> </ul>"},{"location":"technical-manual/architecture/#7-resource-footprint","title":"7. Resource Footprint","text":"Category Before Target Application services 4 4 Infra services 12 7 Pods 30-40 20-25 Helm releases 16 5"},{"location":"technical-manual/architecture/#8-implementation-roadmap","title":"8. Implementation Roadmap","text":"<ol> <li>Finish provider SDK skeleton under <code>common/provider_sdk/</code>.</li> <li>Complete Vault and Etcd Helm charts with production values.</li> <li>Extend CI to run Kind-based installs and smoke tests (<code>scripts/smoke_test.py</code>).</li> <li>Update runbooks in the Operations Manual after each deployment milestone.</li> </ol>"},{"location":"technical-manual/architecture/#9-mermaid-diagram","title":"9. Mermaid Diagram","text":"<pre><code>flowchart TB\n  subgraph Infra[Shared Infra]\n    Auth[Auth Service]\n    OPA[OPA]\n    Kafka[Kafka Cluster]\n    Redis[Redis Cluster]\n    Vault[Vault]\n    Etcd[Etcd]\n    Prom[Prometheus]\n    Graf[Grafana]\n  end\n\n  subgraph Apps[Application Services]\n    SA01[SomaAgent01]\n    SB[SomaBrain]\n    SAH[SomaAgentHub]\n    SMF[SomaFractalMemory]\n  end\n\n  Auth --&gt; SAH\n  OPA --&gt; SAH\n  Kafka --&gt; SA01\n  Kafka --&gt; SAH\n  Redis --&gt; SA01\n  Vault --&gt; SA01\n  Etcd --&gt; SA01\n  Prom --&gt; Apps\n  Graf --&gt; Prom\n</code></pre>"},{"location":"technical-manual/architecture/#10-verification-checklist","title":"10. Verification Checklist","text":"<ul> <li>[ ] Diagram renders via MkDocs and linted with <code>markdownlint</code>.</li> <li>[ ] Service matrix matches <code>infra/docker-compose.somaagent01.yaml</code>.</li> <li>[ ] Helm values align with <code>infra/helm/soma-infra</code>.</li> <li>[ ] Roadmap items tracked in <code>docs/ROADMAP_SPRINTS.md</code>.</li> </ul>"},{"location":"technical-manual/data-flow/","title":"Data Flow &amp; Persistence","text":"<p>This document explains how data enters, moves through, and leaves SomaAgent01. It covers chat payloads, tool execution artifacts, event streaming, and storage.</p>"},{"location":"technical-manual/data-flow/#1-high-level-flow","title":"1. High-Level Flow","text":"<pre><code>sequenceDiagram\n  participant User\n  participant UI\n  participant Gateway\n  participant ConversationWorker\n  participant Tools\n  participant Memory\n  participant Kafka\n\n  User-&gt;&gt;UI: Chat message\n  UI-&gt;&gt;Gateway: HTTP request\n  Gateway-&gt;&gt;ConversationWorker: gRPC task\n  ConversationWorker-&gt;&gt;Tools: Invoke (e.g., code execution)\n  Tools--&gt;&gt;ConversationWorker: Results, artifacts\n  ConversationWorker-&gt;&gt;Memory: Persist state (Qdrant/Redis)\n  ConversationWorker-&gt;&gt;Kafka: Publish events\n  ConversationWorker-&gt;&gt;Gateway: Response\n  Gateway-&gt;&gt;UI: Render reply\n</code></pre>"},{"location":"technical-manual/data-flow/#2-data-stores","title":"2. Data Stores","text":"Store Type Purpose Retention Redis In-memory Session state, rate limits 24 hours (dev), 7 days (prod) Qdrant / pgvector Vector DB Long-term semantic memory 90 days rolling Postgres SQL Audit logs, marketplace registry 180 days Object storage S3-compatible Backups, large artifacts 1 year"},{"location":"technical-manual/data-flow/#3-event-streams","title":"3. Event Streams","text":"<p>Topics defined in <code>schemas/kafka/</code>:</p> <ul> <li><code>conversation.events</code>: conversation lifecycle events (schema <code>conversation_event.avsc</code>).</li> <li><code>tool.execution</code>: request/response payloads for tool usage.</li> <li><code>config.updates</code>: broadcast configuration and feature flag changes.</li> </ul> <p>Producers and consumers:</p> Topic Producers Consumers <code>conversation.events</code> Gateway, Conversation Worker Analytics, audit service <code>tool.execution</code> Tool Executor Monitoring service <code>config.updates</code> Control plane All services subscribing to config bus"},{"location":"technical-manual/data-flow/#4-privacy-retention-controls","title":"4. Privacy &amp; Retention Controls","text":"<ul> <li>PII redaction occurs before events are published (see <code>common/middleware/redaction.py</code>).</li> <li>Backups encrypted and access logged.</li> <li>Data deletion requests processed via <code>scripts/data/erase_user_data.py</code>.</li> </ul>"},{"location":"technical-manual/data-flow/#5-verification","title":"5. Verification","text":"<ul> <li>CI validates Avro schemas via <code>tests/schemas/test_kafka_schemas.py</code>.</li> <li>Weekly job <code>scripts/probes/check_retention.py</code> ensures TTL policies match table configuration.</li> <li>Observability dashboards track topic lag and storage usage.</li> </ul>"},{"location":"technical-manual/data-flow/#6-change-management","title":"6. Change Management","text":"<ul> <li>Update this document when adding new stores or topics.</li> <li>Document schema updates in <code>docs/changelog.md</code> with SemVer increments.</li> <li>Coordinate with privacy team for changes affecting data retention.</li> </ul>"},{"location":"technical-manual/deployment/","title":"Deployment","text":""},{"location":"technical-manual/deployment/#environments","title":"Environments","text":"Environment Purpose Notes Local Dev Feature development, manual QA <code>make dev-up</code> Staging Pre-production validation Mirrors prod configuration, uses real providers Production Live users Hardened networking, autoscaling"},{"location":"technical-manual/deployment/#configuration-matrix","title":"Configuration Matrix","text":"Config Local Staging Production <code>GATEWAY_REQUIRE_AUTH</code> false true true <code>OPENAI_API_KEY</code> .env Secret manager Secret manager Kafka Docker compose Managed cluster Managed cluster Postgres Docker compose Managed instance Managed HA cluster"},{"location":"technical-manual/deployment/#build-pipeline","title":"Build Pipeline","text":"<ol> <li>Run tests locally (<code>pytest</code>, Playwright).</li> <li>Build images: <code>docker compose build</code> or CI pipeline.</li> <li>Push to registry (tag with git SHA, semantic version).</li> <li>Deploy via Compose (staging) or Helm (production).</li> </ol>"},{"location":"technical-manual/deployment/#docker-compose-deployment-staging","title":"Docker Compose Deployment (Staging)","text":"<pre><code>git pull\ndocker compose -p somaagent01-staging --profile core --profile dev -f infra/docker-compose.somaagent01.yaml up -d\n</code></pre> <ul> <li>Use <code>.env.staging</code> for environment-specific overrides.</li> <li>Confirm health: <code>docker compose ps</code>.</li> <li>Host ports default to the reserved range <code>20000-20199</code> (Kafka 20000, Redis 20001, Postgres 20002, Gateway 20016, UI 20015). Override via <code>PORT_POOL_START</code> / <code>PORT_POOL_MAX</code> if the range is occupied.</li> <li>SomaBrain integration expects <code>http://host.docker.internal:9696</code>; ensure your local SomaBrain service is listening on that port.</li> </ul>"},{"location":"technical-manual/deployment/#kubernetes-deployment-planned","title":"Kubernetes Deployment (Planned)","text":"<ul> <li>Helm chart (<code>deploy/helm/somaagent01</code>) will manage:<ul> <li>StatefulSets for Postgres/Kafka/Redis</li> <li>Deployments for Gateway, UI, Tool Executor</li> <li>ConfigMaps for prompts, settings defaults</li> <li>Secrets for API keys, credentials</li> </ul> </li> <li>Ingress exposes Gateway + UI via HTTPS.</li> </ul>"},{"location":"technical-manual/deployment/#rolling-update-procedure","title":"Rolling Update Procedure","text":"<ol> <li>Scale Gateway to zero traffic (if load balancer supports draining).</li> <li>Apply new build (<code>docker compose up -d gateway</code> or <code>helm upgrade</code>).</li> <li>Monitor health checks, metrics, logs.</li> <li>Re-enable traffic.</li> <li>Run smoke tests.</li> </ol>"},{"location":"technical-manual/deployment/#rollback","title":"Rollback","text":"<ul> <li>Docker: <code>docker compose up -d gateway=&lt;previous-tag&gt;</code>.</li> <li>Helm: <code>helm rollback somaagent01 &lt;revision&gt;</code>.</li> <li>Ensure database migrations are backward compatible or have rollback scripts.</li> </ul>"},{"location":"technical-manual/deployment/#environment-promotion-checklist","title":"Environment Promotion Checklist","text":"<ul> <li>[ ] Tests green (unit, integration, E2E).</li> <li>[ ] Release notes updated.</li> <li>[ ] Config parity reviewed (feature flags, API keys).</li> <li>[ ] Observability dashboards verified.</li> <li>[ ] Incident response contacts updated.</li> </ul>"},{"location":"technical-manual/deployment/#secrets-management","title":"Secrets Management","text":"<ul> <li>Local: <code>.env</code> (never commit), <code>python/helpers/secrets.py</code> loads.</li> <li>Staging/Prod: managed secrets (Vault, AWS Secrets Manager). Mount or inject as env vars.</li> </ul>"},{"location":"technical-manual/deployment/#compliance-audit","title":"Compliance &amp; Audit","text":"<ul> <li>Record deployment metadata in <code>docs/changelog.md</code>.</li> <li>Tag git release (<code>git tag vX.Y.Z</code>).</li> <li>Store build artifacts and configuration in artifact repository.</li> </ul>"},{"location":"technical-manual/infrastructure/","title":"Infrastructure &amp; Deployment Blueprint","text":"<p>This blueprint documents how SomaAgent01 is provisioned across local, staging, and production environments. It covers Docker Compose profiles, Helm chart structure, configuration management, and verification routines.</p>"},{"location":"technical-manual/infrastructure/#1-local-environment-docker-compose","title":"1. Local Environment (Docker Compose)","text":"<p>Compose file: <code>infra/docker-compose.somaagent01.yaml</code></p>"},{"location":"technical-manual/infrastructure/#profiles","title":"Profiles","text":"Profile Services Purpose <code>default</code> gateway, ui, conversation_worker, memory_service, tool_executor Base runtime <code>vectorstore</code> qdrant, pgvector Advanced memory features <code>observability</code> prometheus, grafana, loki (optional) Local observability <code>kafka</code> kafka, schema-registry Streaming use cases"},{"location":"technical-manual/infrastructure/#bring-up-command","title":"Bring-Up Command","text":"<pre><code>COMPOSE_PROFILES=default,vectorstore docker compose -f infra/docker-compose.somaagent01.yaml up --build\n</code></pre> <p>Verification: - <code>docker compose ps</code> shows containers healthy. - <code>curl http://localhost:8010/health</code> returns <code>200</code>.</p>"},{"location":"technical-manual/infrastructure/#2-cluster-deployments-helm","title":"2. Cluster Deployments (Helm)","text":"<p>Charts live under <code>infra/helm/</code>.</p> <ul> <li><code>soma-infra/</code>: shared infrastructure (Auth, OPA, Kafka, Redis, Prometheus, Grafana, Vault, Etcd).</li> <li><code>soma-stack/</code>: application bundle (gateway, conversation worker, memory service, UI, tool executor).</li> </ul>"},{"location":"technical-manual/infrastructure/#install-flow","title":"Install Flow","text":"<pre><code># Bootstrap Kind cluster for validation\nkind create cluster --name soma\n\n# Install shared infra\ntarget=dev\nhelm upgrade --install soma-infra infra/helm/soma-infra \\\n  -n soma-$target --create-namespace \\\n  -f infra/helm/values-$target.yaml\n\n# Install application stack\nhelm upgrade --install soma-stack infra/helm/soma-stack \\\n  -n soma-$target \\\n  -f infra/helm/values-$target.yaml\n</code></pre> <p>Verification: - <code>kubectl get pods -n soma-dev</code> shows all pods <code>Running</code>. - Port-forward gateway (<code>kubectl port-forward svc/gateway 8010:8010</code>) and hit <code>/health</code>. - Run smoke tests: <code>poetry run python scripts/smoke_test.py --env dev</code>.</p>"},{"location":"technical-manual/infrastructure/#3-configuration-management","title":"3. Configuration Management","text":"<ul> <li>Base settings: <code>common/config/settings.py</code> (Pydantic models).</li> <li>Environment overrides: <code>infra/helm/values-&lt;env&gt;.yaml</code>.</li> <li>Secrets: Stored in Vault; templated via Helm and injected using Vault Agent sidecars.</li> <li>Feature flags: Sourced from Etcd (<code>feature_flag_endpoint</code>), cached in Redis, with updates broadcast over the <code>config.updates</code> Kafka topic.</li> </ul>"},{"location":"technical-manual/infrastructure/#4-cicd-integration","title":"4. CI/CD Integration","text":"<p>GitHub Actions workflow: <code>.github/workflows/ci.yml</code> (existing) + <code>docs-quality.yml</code> (added in this sprint).</p> <p>Pipeline stages: 1. Lint Python (<code>ruff</code>, <code>mypy</code>). 2. Run unit and integration tests (<code>pytest</code>). 3. Provision Kind cluster, install <code>soma-infra</code> and <code>soma-stack</code> charts. 4. Execute smoke tests and publish artifacts. 5. Trigger documentation workflow (link checking, markdown lint, MkDocs build).</p>"},{"location":"technical-manual/infrastructure/#5-change-management","title":"5. Change Management","text":"<ul> <li>Infrastructure changes require entries in <code>docs/changelog.md</code>.</li> <li>Update Helm chart versions in <code>Chart.yaml</code> and <code>values-*.yaml</code>.</li> <li>Tag releases following <code>v&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;</code> and attach Helm package artifacts.</li> </ul>"},{"location":"technical-manual/infrastructure/#6-disaster-recovery","title":"6. Disaster Recovery","text":"<ul> <li>Backups:</li> <li>State stores (Redis, Kafka) rely on cloud snapshots (documented in <code>infra/helm/soma-infra/backup/</code>).</li> <li>Qdrant backups stored in object storage with lifecycle policies.</li> <li>Failover: Cloudflare Load Balancer or Route53 handles region traffic shifts.</li> <li>RPO/RTO: Target RPO 15 minutes, RTO 60 minutes. Validate quarterly via failover drills.</li> </ul>"},{"location":"technical-manual/infrastructure/#7-operational-runbooks","title":"7. Operational Runbooks","text":"<p>Runbooks relocate to the Development Manual (<code>../development-manual/runbooks.md</code>) but referenced here for completeness:</p> <ul> <li>Start/stop sequences</li> <li>Log collection</li> <li>Scaling events</li> </ul>"},{"location":"technical-manual/infrastructure/#8-verification-checklist","title":"8. Verification Checklist","text":"<ul> <li>[ ] Compose stack validated locally.</li> <li>[ ] Helm deployment validated in Kind.</li> <li>[ ] Vault and OPA sidecars injected successfully.</li> <li>[ ] Smoke tests green.</li> <li>[ ] Documentation updated and linted.</li> </ul>"},{"location":"technical-manual/integrations/","title":"Integrations &amp; Connectivity","text":"<p>This guide documents the upstream and downstream systems that SomaAgent01 connects to, including APIs, protocols, authentication mechanisms, and validation routines.</p>"},{"location":"technical-manual/integrations/#1-managed-soma-slm-api","title":"1. Managed Soma SLM API","text":"<ul> <li>Endpoint: <code>https://slm.somaagent01.dev/v1</code></li> <li>Auth: Bearer tokens stored as <code>SLM_API_KEY</code>.</li> <li>Usage: chat, utility, embedding models.</li> <li>Verification: Run <code>scripts/probes/check_slm.py</code> to validate latency and model availability.</li> </ul>"},{"location":"technical-manual/integrations/#2-open-policy-agent-opa","title":"2. Open Policy Agent (OPA)","text":"<ul> <li>Endpoint: <code>http://opa.soma.svc.cluster.local:8181</code>.</li> <li>Policy bundles: <code>policy/bundles/somaagent01.tar.gz</code>.</li> <li>Decision path: <code>/v1/data/http/authz/allow</code>.</li> <li>Deployment: Sidecar or standalone service depending on environment.</li> <li>Verification: CI runs <code>scripts/probes/check_opa.py --policy policy/bundles/somaagent01</code>. Deny-by-default enforced.</li> </ul>"},{"location":"technical-manual/integrations/#3-auth-service","title":"3. Auth Service","text":"<ul> <li>Endpoint: <code>http://auth.soma.svc.cluster.local:8080</code>.</li> <li>Protocol: OAuth 2.0 compatible; JWT tokens minted for UI and gateway.</li> <li>Key management: Vault-backed signing keys rotated quarterly.</li> <li>Verification: Integration tests ensure token validation in gateway middleware.</li> </ul>"},{"location":"technical-manual/integrations/#4-data-plane-integrations","title":"4. Data Plane Integrations","text":"System Protocol Purpose Notes Kafka (<code>kafka.soma.svc.cluster.local:9092</code>) TCP Event streaming for tasks, config updates Topics defined in <code>schemas/kafka/</code> Redis (<code>redis.soma.svc.cluster.local:6379</code>) TCP Caching, rate limiting, session store TLS optional; configure in <code>settings.redis_tls_enabled</code> Qdrant (<code>qdrant.soma.svc.cluster.local:6333</code>) HTTP/gRPC Vector store for memory retrieval Optional profile in compose Postgres (<code>postgres.soma.svc.cluster.local:5432</code>) TCP Persistence for audit logs and registry Migrations in <code>infra/db/migrations</code>"},{"location":"technical-manual/integrations/#5-external-tool-integrations","title":"5. External Tool Integrations","text":"<ul> <li>Cloudflare Tunnel: Automates secure external access. Configure via <code>.env</code> (<code>CLOUDFLARE_TOKEN</code>, <code>CLOUDFLARE_TUNNEL_ID</code>).</li> <li>SearXNG: Search engine for <code>search_engine</code> tool; configure endpoint in <code>common/config/settings.py</code>.</li> <li>Document Query Tool: Connects to vector index; ensure <code>DOCUMENT_QUERY_URL</code> is reachable.</li> </ul>"},{"location":"technical-manual/integrations/#6-diagram-of-integration-points","title":"6. Diagram of Integration Points","text":"<pre><code>digraph Integrations {\n  rankdir=LR\n  UI -&gt; Gateway [label=\"HTTP\"]\n  Gateway -&gt; SLM [label=\"HTTPS\", color=\"green\"]\n  Gateway -&gt; Auth [label=\"OAuth\"]\n  Gateway -&gt; OPA [label=\"Policy\"]\n  ConversationWorker -&gt; Kafka [label=\"Produce/Consume\"]\n  ConversationWorker -&gt; Redis [label=\"Cache\"]\n  ConversationWorker -&gt; Qdrant [label=\"Vector search\"]\n  ToolExecutor -&gt; DockerDaemon [label=\"gRPC\"]\n  ToolExecutor -&gt; ExternalAPIs [label=\"HTTPS\"]\n}\n</code></pre>"},{"location":"technical-manual/integrations/#7-validation-monitoring","title":"7. Validation &amp; Monitoring","text":"<ul> <li>Integration health endpoints checked by CI and runtime cron jobs.</li> <li>Alerts configured for latency, failure rate, and auth errors (see <code>infra/observability/alerts.yml</code>).</li> <li>Logs tagged with <code>integration</code> key for searchability.</li> </ul>"},{"location":"technical-manual/integrations/#8-change-management","title":"8. Change Management","text":"<ul> <li>Update <code>docs/changelog.md</code> for new endpoints or credential rotations.</li> <li>Security review required for any new external dependency.</li> <li>Add regression tests under <code>tests/integration/</code> when adding or modifying integrations.</li> </ul>"},{"location":"technical-manual/monitoring/","title":"Monitoring &amp; Health","text":""},{"location":"technical-manual/monitoring/#metrics","title":"Metrics","text":"<ul> <li>Prometheus collects component metrics.</li> <li>Exporters:</li> <li>Gateway: request latency, error count, circuit-breaker stats.</li> <li>Kafka: broker health, partition ISR, consumer lag.</li> <li>Postgres: connection pool, slow queries (pg exporter).</li> <li>Redis: memory usage, hits/misses.</li> </ul>"},{"location":"technical-manual/monitoring/#key-dashboards","title":"Key Dashboards","text":"Dashboard Metrics Gateway Overview RPS, latency percentiles, error codes Tool Executor Queue depth, execution duration Infrastructure CPU/memory per container, disk usage"},{"location":"technical-manual/monitoring/#logging","title":"Logging","text":"<ul> <li>Docker stdout/stderr aggregated with <code>docker logs</code>.</li> <li>For advanced setups, forward to Loki/ELK. Use JSON logs.</li> </ul>"},{"location":"technical-manual/monitoring/#tracing","title":"Tracing","text":"<ul> <li>Enable OpenTelemetry via <code>OTEL_EXPORTER_OTLP_ENDPOINT</code>.</li> <li>Instrument Gateway routes and Tool Executor tasks.</li> </ul>"},{"location":"technical-manual/monitoring/#alerting","title":"Alerting","text":"Alert Condition Action High Gateway latency p95 &gt; 3s for 5 min Investigate provider, Redis Tool queue backlog <code>tool.requests</code> lag &gt; 500 Scale executors or inspect stuck jobs Kafka ISR shrink &lt; partitions replicating Restart broker"},{"location":"technical-manual/monitoring/#synthetic-monitoring","title":"Synthetic Monitoring","text":"<ul> <li>Schedule periodic <code>/v1/health</code> checks.</li> <li>Run automated chat scenario periodically to validate end-to-end.</li> </ul>"},{"location":"technical-manual/monitoring/#incident-response","title":"Incident Response","text":"<ul> <li>Document incidents in <code>docs/operations/incidents/YYYY-MM-DD.md</code>.</li> <li>Include timeline, root cause, remediation, follow-ups.</li> </ul>"},{"location":"technical-manual/security/","title":"Security &amp; Compliance Baseline","text":"<p>This document defines the authentication, authorization, secrets, and compliance expectations for SomaAgent01. It aligns with Soma's SOC2 roadmap and internal control framework.</p>"},{"location":"technical-manual/security/#1-authentication","title":"1. Authentication","text":"<ul> <li>User-facing UI: Basic auth configurable in Settings \u2192 Authentication. Enforce strong passwords and rotate quarterly.</li> <li>Service-to-service: JWT tokens issued by the Auth service. Tokens validated by gateway middleware and OPA policies.</li> <li>CLI/API clients: Support personal access tokens derived from the Auth service.</li> </ul>"},{"location":"technical-manual/security/#2-authorization","title":"2. Authorization","text":"<ul> <li>OPA policies stored in <code>policy/</code> and distributed via bundles.</li> <li>Default stance: deny unless policy grants access.</li> <li>Policies include:</li> <li>Route-based access controls.</li> <li>Tool execution scopes.</li> <li>Audit log retention rules.</li> </ul>"},{"location":"technical-manual/security/#3-secrets-management","title":"3. Secrets Management","text":"<ul> <li>Vault stores API keys, database credentials, and signing keys.</li> <li>Helm charts inject secrets using Vault Agent sidecars.</li> <li>Local development uses <code>.env</code>; never commit secrets to version control.</li> <li>Rotation cadence: 90 days for API keys, 180 days for signing keys.</li> </ul>"},{"location":"technical-manual/security/#4-data-protection","title":"4. Data Protection","text":"<ul> <li>Data in transit: TLS enforced for external endpoints; mTLS configurable between services.</li> <li>Data at rest:</li> <li>Redis backed by encrypted volumes in production.</li> <li>Kafka topics using disk encryption on managed clusters.</li> <li>Qdrant snapshots stored in encrypted object storage.</li> <li>Backups: Stored encrypted, with access limited to SRE.</li> </ul>"},{"location":"technical-manual/security/#5-logging-monitoring","title":"5. Logging &amp; Monitoring","text":"<ul> <li>Audit logs emitted for authentication events, policy decisions, and tool executions.</li> <li>Logs shipped to Loki with retention policy of 30 days (dev) and 180 days (prod).</li> <li>Alert thresholds defined in <code>infra/observability/alerts.yml</code>.</li> </ul>"},{"location":"technical-manual/security/#6-vulnerability-management","title":"6. Vulnerability Management","text":"<ul> <li>Container image scans via Trivy integrated into CI.</li> <li>Dependency scans for Python (<code>pip-audit</code>) and Node (<code>npm audit</code>) run weekly.</li> <li>Critical CVEs trigger immediate patch cycle; log details in <code>docs/changelog.md</code>.</li> </ul>"},{"location":"technical-manual/security/#7-compliance-mapping","title":"7. Compliance Mapping","text":"Control Implementation SOC2 CC6.1 (Logical Access) Auth + OPA policy enforcement SOC2 CC7.2 (Change Management) GitHub PR reviews + CI workflows SOC2 CC8.1 (System Operations) Observability stack, alerts, runbooks SOC2 CC9.2 (Risk Mitigation) Vulnerability scans + patch policy"},{"location":"technical-manual/security/#8-incident-response","title":"8. Incident Response","text":"<ol> <li>Detect via alerts or reports.</li> <li>Create incident ticket with severity and impact.</li> <li>Contain (revoke keys, scale down services).</li> <li>Eradicate (patch, redeploy).</li> <li>Recover (validate health, inform stakeholders).</li> <li>Post-incident review within 5 business days; capture actions in change log.</li> </ol>"},{"location":"technical-manual/security/#9-verification-checklist","title":"9. Verification Checklist","text":"<ul> <li>[ ] Vault tokens rotated.</li> <li>[ ] Auth service keys rotated.</li> <li>[ ] Policy bundles validated with <code>opa test</code>.</li> <li>[ ] Trivy scans clear of critical CVEs.</li> <li>[ ] Incident response drills executed within SLA.</li> </ul>"},{"location":"technical-manual/components/conversation-worker/","title":"Conversation Worker","text":"<p>title: Conversation Worker slug: technical-conversation-worker version: 1.0.0 last-reviewed: 2025-10-15 audience: platform-engineers, runtime developers owner: platform-architecture reviewers:   - infra   - product prerequisites:   - Kafka topics provisioned   - Postgres and Redis reachable verification:   - <code>pytest tests/rate_limiter_test.py</code> passes   - Kafka consumer lag under 100 messages during load test</p>"},{"location":"technical-manual/components/conversation-worker/#conversation-worker","title":"Conversation Worker","text":""},{"location":"technical-manual/components/conversation-worker/#mission","title":"Mission","text":"<p>Process conversation tasks emitted by the Gateway, orchestrate tool execution, and persist responses back to the timeline while respecting rate limits and extension hooks.</p> <pre><code>graph TD\n    Gateway --&gt;|Kafka: conversation.tasks| Worker\n    Worker --&gt;|hydrate| AgentContext\n    Worker --&gt;|call| LLM[Chat Model]\n    Worker --&gt;|invoke| Tools\n    Tools --&gt;|results| Worker\n    Worker --&gt;|persist| Postgres\n    Worker --&gt;|emit| Kafka[conversation.events]\n    Worker --&gt;|update| Memory[SomaBrain]\n</code></pre>"},{"location":"technical-manual/components/conversation-worker/#module-layout","title":"Module Layout","text":"Module Description <code>services/conversation_worker/main.py</code> Async consumer loop, task deserialization, health probes <code>agent.py</code> Core monologue loop, extensions, agent context management <code>python/helpers/rate_limiter.py</code> Sliding-window limiter for requests and token budgets <code>python/helpers/settings.py</code> Provider configuration, tenant overrides <code>python/helpers/extension.py</code> Lifecycle hooks invoked around the loop"},{"location":"technical-manual/components/conversation-worker/#message-loop-lifecycle","title":"Message Loop Lifecycle","text":"<pre><code>graph TD\n    A[Receive UserMessage] --&gt; B[hist_add_user_message]\n    B --&gt; C[prepare_prompt]\n    C --&gt; D[call_chat_model]\n    D --&gt; E{Response contains tool request?}\n    E -- Yes --&gt; F[process_tools]\n    F --&gt; G[Tool execution &amp; result handling]\n    G --&gt; H[Add AI response to history]\n    E -- No --&gt; H\n    H --&gt; I[Extensions: response_stream_end]\n    I --&gt; J[Check for Intervention]\n    J --&gt;|Continue| A\n    J --&gt;|Pause| K[Wait while paused]\n    K --&gt; J\n</code></pre> <pre><code>stateDiagram-v2\n    [*] --&gt; Idle\n    Idle --&gt; Running : receive user message\n    Running --&gt; Waiting : awaiting LLM stream\n    Waiting --&gt; ToolCall : tool request detected\n    ToolCall --&gt; Running : tool result injected\n    Running --&gt; Idle : response sent to user\n    Running --&gt; Error : unhandled exception\n    Error --&gt; Idle : reset after logging\n</code></pre>"},{"location":"technical-manual/components/conversation-worker/#rate-limiting","title":"Rate Limiting","text":"<ul> <li><code>RateLimiter</code> tracks configurable counters (<code>*_rl_requests</code>, <code>*_rl_input</code>, <code>*_rl_output</code>).</li> <li>Before each model invocation the worker calls <code>allow</code>/<code>wait</code> ensuring budgets are respected.</li> <li>Breaches trigger progress updates surfaced through <code>AgentContext.log.set_progress</code> so UI clients show a waiting bar.</li> <li>Limits configurable per model family via <code>python/helpers/settings.py</code> and tenant overrides.</li> </ul>"},{"location":"technical-manual/components/conversation-worker/#extension-hooks","title":"Extension Hooks","text":"Hook Purpose <code>monologue_start</code> Observe loop start with inbound payload <code>before_main_llm_call</code> Mutate prompts or settings before provider invocation <code>tool_execute_before</code> / <code>tool_execute_after</code> Wrap tool execution for auditing <code>response_stream_chunk</code> Stream tokens to downstream observers <code>monologue_end</code> Cleanup and finalize transcripts <p>Extensions receive the shared <code>loop_data</code> object, allowing feature teams to add observers without touching core logic.</p>"},{"location":"technical-manual/components/conversation-worker/#configuration","title":"Configuration","text":"<ul> <li>Environment toggles (sample): <code>CONVERSATION_MAX_TOKENS</code>, <code>CONVERSATION_RETRY_LIMIT</code>.</li> <li>Kafka topics: <code>conversation.tasks</code> (consume), <code>conversation.events</code> (produce).</li> <li>Dependencies resolved via <code>python/helpers/settings.py</code> (LLM provider, tool registry, memory adapters).</li> </ul>"},{"location":"technical-manual/components/conversation-worker/#observability","title":"Observability","text":"<ul> <li>Metrics: message throughput, tool latency, retry counts (<code>/metrics</code> endpoint when <code>PROMETHEUS_ENABLE=1</code>).</li> <li>Logs: structured JSON with <code>conversation_id</code>, <code>task_id</code>, <code>tenant_id</code>.</li> <li>Traces: optional OTEL spans around LLM calls and tool execution if exporter configured.</li> </ul>"},{"location":"technical-manual/components/conversation-worker/#verification-checklist","title":"Verification Checklist","text":"<ul> <li>[ ] Run <code>pytest tests/rate_limiter_test.py</code> to confirm loop throttling.</li> <li>[ ] Kafka lag &lt; 100 for <code>conversation.tasks</code> under standard load test.</li> <li>[ ] Tool invocation breadcrumbs visible in logs and metrics.</li> </ul>"},{"location":"technical-manual/components/gateway/","title":"Gateway Component","text":""},{"location":"technical-manual/components/gateway/#purpose","title":"Purpose","text":"<ul> <li>Serves as the primary ingress for HTTP/WebSocket traffic (humans, agents, automations).</li> <li>Enforces authentication, authorization, and policy controls before delegating to LLMs or tools.</li> <li>Orchestrates conversation flow: context assembly, tool invocation, memory persistence.</li> </ul> <pre><code>graph LR\n    Client --&gt;|REST/WS| Gateway\n    Gateway --&gt;|authorize| OpenFGA\n    Gateway --&gt;|evaluate| OPA\n    Gateway --&gt;|fetch| SomaBrain\n    Gateway --&gt;|call| LLMProvider\n    Gateway --&gt;|publish| Kafka\n    Gateway --&gt;|persist| Postgres\n    Gateway --&gt;|cache| Redis\n</code></pre>"},{"location":"technical-manual/components/gateway/#module-structure","title":"Module Structure","text":"Module Key Responsibilities <code>services/gateway/main.py</code> FastAPI app, endpoints, metrics, SSE/WS, capsule proxy <code>services/gateway/auth/openfga.py</code> Relationship tuples, access checks <code>services/gateway/policies/opa_client.py</code> Rego policy evaluation <code>services/gateway/memory/service.py</code> SomaBrain reads/writes <code>services/gateway/tasks/publisher.py</code> Kafka publishing helpers"},{"location":"technical-manual/components/gateway/#request-lifecycle","title":"Request Lifecycle","text":"<ol> <li>Ingress: Request hits FastAPI dependency stack (auth, tenant resolution).</li> <li>Policy: OpenFGA checks relationship tuples; OPA enforces budgets/quotas.</li> <li>Context: Gateway fetches session transcript, memories, tenant defaults.</li> <li>LLM Call: Provider info comes from <code>python/helpers/settings.py</code> and LiteLLM wrapper.</li> <li>Tooling: Responses containing tool directives emit Kafka messages or call tool executor.</li> <li>Persistence: Updated session state stored in Postgres; new memories saved to SomaBrain.</li> <li>Streaming: WebSocket clients receive partial responses and status events.</li> </ol>"},{"location":"technical-manual/components/gateway/#configuration","title":"Configuration","text":"<ul> <li><code>GATEWAY_REQUIRE_AUTH</code>, <code>POSTGRES_DSN</code>, <code>KAFKA_BOOTSTRAP_SERVERS</code>, <code>OPENFGA_*</code>.</li> <li>Settings API merges defaults from <code>python/helpers/settings.py</code> with user overrides.</li> <li><code>dev</code> profile enables auto-reload and verbose logging.</li> </ul>"},{"location":"technical-manual/components/gateway/#observability","title":"Observability","text":"<ul> <li>Metrics: request latency, response codes, circuit breaker counters at <code>/metrics</code>.</li> <li>Logs: structured JSON per request.</li> <li>Traces: optional OTEL instrumentation via environment variables.</li> </ul>"},{"location":"technical-manual/components/gateway/#failure-modes-mitigations","title":"Failure Modes &amp; Mitigations","text":"Failure Symptom Mitigation Redis unavailable Rate limit errors, cache misses Gateway falls back to Postgres fetches; retries with backoff LLM provider errors 5xx streaming to UI Circuit breaker triggers; Gateway surfaces actionable message Kafka publish failure Tool executions stall Dead-letter queue records message; operator restarts broker OpenFGA connectivity 403 responses for all requests Cached decisions used briefly; escalate to SRE"},{"location":"technical-manual/components/gateway/#extensibility","title":"Extensibility","text":"<ul> <li>Add routers under <code>services/gateway/routes/</code> and include via FastAPI.</li> <li>Define new tool schemas in <code>python/tools/schema/</code> and map to executor tasks.</li> <li>Update <code>conf/tenants.yaml</code> for tenant-specific budgets or prompts.</li> </ul>"},{"location":"technical-manual/components/gateway/#public-endpoints","title":"Public Endpoints","text":"<ul> <li>POST <code>/v1/session/message</code></li> <li>POST <code>/v1/session/action</code></li> <li>GET <code>/v1/session/{session_id}/events</code> (SSE)</li> <li>WS <code>/v1/session/{session_id}/stream</code></li> <li>GET <code>/v1/health</code> (also <code>/health</code> alias, hidden from schema)</li> <li>API keys: POST <code>/v1/keys</code>, GET <code>/v1/keys</code>, DELETE <code>/v1/keys/{key_id}</code></li> <li>Model profiles: GET/POST/PUT/DELETE under <code>/v1/model-profiles</code></li> <li>Routing: POST <code>/v1/route</code></li> <li>Requeue: GET <code>/v1/requeue</code>, POST <code>/v1/requeue/{id}/resolve</code>, DELETE <code>/v1/requeue/{id}</code></li> <li>Capsules proxy: GET <code>/v1/capsules</code>, GET <code>/v1/capsules/{id}</code>, POST <code>/v1/capsules/{id}/install</code></li> </ul> <p>Nonexistent (do not document under gateway): <code>/chat</code>, <code>/settings_get</code>, <code>/settings_set</code>, <code>/realtime_session</code>.</p>"},{"location":"technical-manual/components/gateway/#verification-checklist","title":"Verification Checklist","text":"<ul> <li>[ ] <code>curl http://localhost:8010/health</code> returns 200.</li> <li>[ ] Integration suite passes.</li> <li>[ ] Prometheus shows Gateway targets UP.</li> </ul>"},{"location":"technical-manual/components/mcp-proxy/","title":"MCP & A2A Proxy","text":"<p>title: MCP &amp; A2A Proxy slug: technical-mcp-proxy version: 1.0.0 last-reviewed: 2025-10-15 audience: platform-engineers, integrations owner: platform-architecture reviewers:   - connectivity   - security prerequisites:   - MCP/A2A credentials provisioned in Vault   - Gateway configured with proxy endpoints verification:   - <code>pytest tests/test_fasta2a_client.py</code> passes   - Unauthorized requests receive HTTP 401</p>"},{"location":"technical-manual/components/mcp-proxy/#mcp-a2a-proxy","title":"MCP &amp; A2A Proxy","text":""},{"location":"technical-manual/components/mcp-proxy/#mission","title":"Mission","text":"<p>Expose Model-Context-Protocol (MCP) and Agent-to-Agent (A2A) capabilities through a thin proxy that reuses gateway authentication, keeps secrets server-side, and forwards only validated requests to downstream services.</p> <pre><code>sequenceDiagram\n    participant C as Client Agent\n    participant P as Proxy (MCP/A2A)\n    participant S as Target Service\n    participant DB as Postgres\n    participant T as Token Store\n\n    C-&gt;&gt;P: HTTP request + Authorization header\n    P-&gt;&gt;T: validate token (create_auth_token)\n    alt token valid\n        P-&gt;&gt;S: forward request (SSE/HTTP)\n        S-&gt;&gt;DB: optional lookup\n        S--&gt;&gt;P: response payload\n        P--&gt;&gt;C: stream / JSON response\n    else token invalid\n        P--&gt;&gt;C: 401 Unauthorized\n    end\n</code></pre>"},{"location":"technical-manual/components/mcp-proxy/#implementation-highlights","title":"Implementation Highlights","text":"<ul> <li>Entry points live in <code>python/helpers/mcp_handler.py</code> and <code>python/helpers/fasta2a_server.py</code>.</li> <li><code>DynamicMcpProxy</code> and <code>DynamicA2AProxy</code> reload provider definitions at runtime, enabling tenants to register servers without restarts.</li> <li>Tokens combine runtime IDs with optional operator credentials (<code>create_auth_token</code>).</li> <li>Forwarded requests inherit gateway tracing headers so observability spans remain stitched.</li> </ul>"},{"location":"technical-manual/components/mcp-proxy/#configuration","title":"Configuration","text":"<ul> <li>Tenants register MCP endpoints under <code>conf/tenants.yaml</code> (<code>mcp.endpoints</code>, <code>a2a.endpoints</code>).</li> <li>Secrets resolved from Vault via <code>python/helpers/secrets.py</code> to avoid leaking keys to clients.</li> <li>Toggle availability through <code>settings.mcp_enabled</code> and <code>settings.a2a_enabled</code> per tenant.</li> </ul>"},{"location":"technical-manual/components/mcp-proxy/#security-considerations","title":"Security Considerations","text":"<ul> <li>All traffic passes through the gateway FastAPI process; no direct exposure of downstream hosts.</li> <li>Tokens scoped per tenant and expire based on settings (<code>settings.mcp_token_ttl_seconds</code>).</li> <li>Proxy enforces allowlists for HTTP methods and maximum payload sizes.</li> </ul>"},{"location":"technical-manual/components/mcp-proxy/#verification-checklist","title":"Verification Checklist","text":"<ul> <li>[ ] <code>pytest tests/test_fasta2a_client.py</code> passes.</li> <li>[ ] Tenant configuration updates hot-reload without restart.</li> <li>[ ] Audit logs record proxy invocations with <code>mcp_proxy</code> or <code>a2a_proxy</code> labels.</li> </ul>"},{"location":"technical-manual/components/memory/","title":"SomaBrain Memory Subsystem","text":""},{"location":"technical-manual/components/memory/#role","title":"Role","text":"<p>Provide durable, queryable factual context so LLM agents operate with continuity and verifiable truths.</p> <pre><code>classDiagram\n    class MemoryItem {\n        string id\n        string key\n        string type\n        string tenant_id\n        string persona_id\n        datetime created_at\n        float relevance\n        dict payload\n    }\n\n    class MemoryStore {\n        +save(item: MemoryItem)\n        +load(key: str) MemoryItem\n        +search(query: MemoryQuery) list~MemoryItem~\n        +delete(id: str)\n    }\n\n    class MemoryQuery {\n        string key\n        string text\n        string type\n        float min_relevance\n        int limit\n    }\n\n    MemoryStore --&gt; MemoryItem\n    MemoryStore --&gt; MemoryQuery\n</code></pre> <ul> <li>Key: stable identifier (for example <code>user.preferences.format</code>).</li> <li>Type: schema namespace (<code>UserPreference</code>, <code>ProjectDecision</code>).</li> <li>Payload: JSON encoded data validated by Pydantic models.</li> </ul>"},{"location":"technical-manual/components/memory/#storage-layers","title":"Storage Layers","text":"Layer Technology Purpose Hot cache Redis Millisecond access to recent entries Durable store Postgres (<code>memory_items</code> table) Long-term persistence Indexing Optional Qdrant / embeddings Semantic search for fuzzy recall <p>Adapters under <code>python/helpers/memory/</code> keep the codebase provider-agnostic.</p>"},{"location":"technical-manual/components/memory/#api-surface","title":"API Surface","text":"<p>Two modes exist, selected by <code>SOMA_ENABLED</code> (default: true):</p> <ul> <li>Remote: SomaMemory via <code>python.integrations.soma_client</code> (production default)</li> <li>Local: FAISS-backed <code>Memory</code> adapter for development when SomaBrain is disabled</li> </ul> <p>Key methods (async):</p> <ul> <li><code>Memory.get(agent)</code> or <code>Memory.get_by_subdir(\"default\")</code></li> <li><code>insert_text(text, metadata)</code> / <code>insert_documents([Document(...)])</code></li> <li><code>search_similarity_threshold(query, limit, threshold, filter?)</code></li> <li><code>delete_documents_by_ids(ids)</code> / <code>delete_documents_by_query(query, threshold, filter)</code></li> </ul>"},{"location":"technical-manual/components/memory/#retrieval-workflow","title":"Retrieval Workflow","text":"<ol> <li>Gateway receives conversation request.</li> <li>Extracts tenant/persona identifiers.</li> <li>Runs <code>memory_search</code> per persona configuration (<code>conf/tenants.yaml</code>).</li> <li>Injects selected memories into system prompt.</li> <li>After response, new facts summarized and written via <code>memory_save</code>.</li> </ol>"},{"location":"technical-manual/components/memory/#best-practices","title":"Best Practices","text":"<ul> <li>Store structured JSON (Pydantic models \u2192 dictionaries).</li> <li>Include metadata such as author, confidence, source, expiry.</li> <li>Keep entries concise (&lt;400 tokens) to avoid prompt overflow.</li> <li>Prune stale items via background jobs.</li> </ul>"},{"location":"technical-manual/components/memory/#machine-integration","title":"Machine Integration","text":"<pre><code>from python.helpers.memory import Memory\n\nmem = await Memory.get_by_subdir(\"default\")\ndoc_id = await mem.insert_text(\n  \"Backend decision: FastAPI\",\n  metadata={\"type\": \"ProjectDecision\", \"key\": \"project.decisions.backend\"},\n)\n\nrecent = await mem.search_similarity_threshold(\n  query=\"project decisions\",\n  limit=5,\n  threshold=0.3,\n)\n</code></pre>"},{"location":"technical-manual/components/memory/#failure-considerations","title":"Failure Considerations","text":"Issue Impact Mitigation Redis eviction First lookup slower (fallback to Postgres) Warm cache via background worker Postgres outage Memory writes fail Retry with backoff, surface warning Schema drift Payload parsing errors Version schemas, migrate data periodically"},{"location":"technical-manual/components/memory/#roadmap","title":"Roadmap","text":"<ul> <li>Embedder-backed semantic search (Qdrant).</li> <li>Tenant-level retention policies / GDPR hooks.</li> <li>Automated summarization when memory exceeds token budgets.</li> </ul>"},{"location":"technical-manual/components/realtime-speech/","title":"Realtime Speech","text":"<p>title: Realtime Speech Pipeline slug: technical-realtime-speech version: 1.0.0 last-reviewed: 2025-10-15 audience: platform-engineers, frontend developers owner: platform-architecture reviewers:   - infra   - product prerequisites:   - Browser with microphone support   - Valid provider credentials (OpenAI realtime or equivalent) verification:   - <code>/realtime_session</code> returns a session payload   - WebRTC negotiation succeeds end-to-end</p>"},{"location":"technical-manual/components/realtime-speech/#realtime-speech-pipeline","title":"Realtime Speech Pipeline","text":"<p>Connects web clients to realtime speech models, enabling bidirectional audio streaming.</p> <pre><code>sequenceDiagram\n    participant UserMic as Browser Microphone\n    participant UI as Speech Store\n    participant Gateway as /realtime_session\n    participant Provider as Realtime API\n\n    UserMic-&gt;&gt;UI: Audio samples (MediaStream)\n    UI-&gt;&gt;Gateway: POST /realtime_session {model, voice, endpoint}\n    Gateway-&gt;&gt;Provider: Create session (server-side auth)\n    Provider--&gt;&gt;Gateway: Client secret, session metadata\n    Gateway--&gt;&gt;UI: Session payload (client_secret)\n    UI-&gt;&gt;Provider: WebRTC offer (with client secret)\n    Provider--&gt;&gt;UI: WebRTC answer + audio track\n    Provider--&gt;&gt;UserMic: Synthesized speech audio\n</code></pre>"},{"location":"technical-manual/components/realtime-speech/#components","title":"Components","text":"Component File(s) Description UI Speech Store <code>webui/components/chat/speech/speech-store.js</code> Manages microphone, WebRTC, audio playback Settings <code>webui/js/settings.js</code>, <code>/settings</code> endpoints Configure provider, voice, endpoint Gateway Endpoint <code>python/api/realtime_session.py</code> Creates provider session server-side Settings Backend <code>python/helpers/settings.py</code> Stores defaults and credentials"},{"location":"technical-manual/components/realtime-speech/#provider-defaults","title":"Provider Defaults","text":"<ul> <li>Provider: <code>openai_realtime</code></li> <li>Model: <code>gpt-4o-realtime-preview</code></li> <li>Voice: <code>verse</code></li> <li>Endpoint: <code>https://api.openai.com/v1/realtime/sessions</code></li> </ul>"},{"location":"technical-manual/components/realtime-speech/#security-compliance","title":"Security &amp; Compliance","text":"<ul> <li>Gateway authenticates with provider using server-side credentials stored in Vault.</li> <li>Client secret returned by provider lives only in memory on the browser.</li> <li>CORS restricted to trusted origins; WebRTC sessions scoped per tunnel.</li> </ul>"},{"location":"technical-manual/components/realtime-speech/#error-handling","title":"Error Handling","text":"Failure UI Behavior Operator Action Session creation fails Toast <code>Realtime session error</code> Validate API key, network egress WebRTC negotiation timeout UI retries Check browser permissions, system firewall Audio playback blocked Browser prompts for interaction User clicks to unlock audio"},{"location":"technical-manual/components/realtime-speech/#operational-metrics","title":"Operational Metrics","text":"<ul> <li>Gateway logs session creation time, error codes.</li> <li>UI logs WebRTC state transitions to the console for debugging.</li> <li>Roadmap: add Prometheus counters (<code>realtime_sessions_total</code>, <code>realtime_session_failures_total</code>).</li> </ul>"},{"location":"technical-manual/components/realtime-speech/#extending-providers","title":"Extending Providers","text":"<ol> <li>Extend settings schema with new provider option.</li> <li>Update Gateway <code>/realtime_session</code> to call provider-specific API.</li> <li>Adjust UI speech store for codec/auth constraints.</li> <li>Document usage in the User Manual once the provider is stable.</li> </ol>"},{"location":"technical-manual/components/tool-executor/","title":"Tool Executor","text":"<p>title: Tool Executor Component slug: technical-tool-executor version: 1.0.0 last-reviewed: 2025-10-15 audience: platform-engineers, automation teams owner: platform-architecture reviewers:   - infra   - security prerequisites:   - Kafka and Postgres reachable   - Gateway emitting tool calls verification:   - <code>pytest tests/unit/test_execution_engine_circuit.py</code> passes   - Metrics <code>tool_executor_tasks_total</code> increment during run</p>"},{"location":"technical-manual/components/tool-executor/#tool-executor-component","title":"Tool Executor Component","text":""},{"location":"technical-manual/components/tool-executor/#mission","title":"Mission","text":"<p>Execute tool calls emitted by the Gateway, ensuring sandboxed, observable, and auditable automation.</p> <pre><code>graph TD\n  Gateway --&gt;|Kafka: tool.requests| ToolWorker\n    ToolWorker --&gt; Sandbox\n    Sandbox --&gt;|stdout/stderr| LogAggregator\n    Sandbox --&gt;|result payload| ToolWorker\n  ToolWorker --&gt;|Kafka: tool.results| Gateway\n    ToolWorker --&gt;|Persist| Postgres\n    ToolWorker --&gt;|Artifacts| ObjectStore[(S3-compatible)]\n</code></pre>"},{"location":"technical-manual/components/tool-executor/#architecture-layers","title":"Architecture Layers","text":"Layer Description Transport Kafka consumer group (<code>tool-executor</code>) Execution Worker pool (<code>python/services/tool_executor/worker.py</code>) with asyncio concurrency Sandbox Ephemeral directories under <code>/tmp/tool-exec</code>, optional container isolation Persistence Result metadata in Postgres, large payloads in object storage, cache in Redis Telemetry Metrics on task duration, success rate, retries"},{"location":"technical-manual/components/tool-executor/#supported-tool-types","title":"Supported Tool Types","text":"Tool Module Notes Shell / Command <code>python/tools/shell.py</code> Runs in sandbox with resource limits Code Interpreter <code>python/tools/code.py</code> Executes Python snippets deterministically HTTP <code>python/tools/http.py</code> Makes HTTP(S) calls under allowlist policy Knowledge Base <code>python/tools/knowledge.py</code> Queries knowledge store Custom <code>python/tools/custom/*.py</code> Register via startup discovery"},{"location":"technical-manual/components/tool-executor/#configuration","title":"Configuration","text":"<ul> <li>Topics: <code>tool.requests</code> (input), <code>tool.results</code> (output). Overridable via <code>TOOL_REQUESTS_TOPIC</code>, <code>TOOL_RESULTS_TOPIC</code>.</li> <li>Environment: <code>TOOL_EXECUTOR_MAX_WORKERS</code>, <code>SANDBOX_ROOT</code>, <code>TIMEOUT_SECONDS</code>.</li> <li>Secrets resolved through <code>python/helpers/secrets.py</code> to redact logs.</li> </ul>"},{"location":"technical-manual/components/tool-executor/#callback-channels","title":"Callback channels","text":"<p>The executor communicates results exclusively via Kafka topics:</p> <ul> <li><code>tool.requests</code>: Gateway/Orchestrator \u279c Executor</li> <li><code>tool.results</code>: Executor \u279c Gateway/Orchestrator</li> </ul> <p>Messages use a shared envelope with <code>task_id</code>, <code>tool_name</code>, <code>payload</code>, and <code>metadata</code> fields. See Shared Resources \u2192 Event Streams for schemas.</p>"},{"location":"technical-manual/components/tool-executor/#error-handling","title":"Error Handling","text":"Scenario Response Tool timeout Worker aborts, marks job <code>failed</code>, publishes error Sandbox init failure Retry up to three times before dead-lettering Postgres unavailable Buffer results in Redis; operator restores DB Invalid payload schema Publish validation error back to Gateway"},{"location":"technical-manual/components/tool-executor/#observability","title":"Observability","text":"<ul> <li>Metrics: <code>tool_executor_tasks_total</code>, <code>tool_executor_duration_seconds</code>.</li> <li>Logs: structured per task with <code>task_id</code> correlation.</li> <li>Tracing: optional OpenTelemetry spans around sandbox execution.</li> </ul>"},{"location":"technical-manual/components/tool-executor/#circuit-breaker-safeguards","title":"Circuit Breaker Safeguards","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Closed : start\n    Closed --&gt; Open : failures &gt;= threshold\n    Open --&gt; Trial : reset_timeout elapsed\n    Trial --&gt; Closed : successful trial call\n    Trial --&gt; Open : trial fails\n    Open --&gt; Open : subsequent calls blocked\n</code></pre> <ul> <li>Implementation lives in <code>python/helpers/circuit_breaker.py</code> and wraps the execution engine.</li> <li>When the circuit is open, new tool requests raise <code>CircuitOpenError</code> and the Gateway retries or surfaces UI guidance.</li> <li>Metrics exported when <code>CIRCUIT_BREAKER_METRICS_PORT</code> is set:</li> <li><code>circuit_breaker_opened_total</code></li> <li><code>circuit_breaker_closed_total</code></li> <li><code>circuit_breaker_trial_total</code></li> </ul>"},{"location":"technical-manual/components/tool-executor/#verification-checklist","title":"Verification Checklist","text":"<ul> <li>[ ] Run <code>pytest tests/unit/test_execution_engine_circuit.py</code> to exercise success/failure paths.</li> <li>[ ] Confirm circuit breaker metrics emit via <code>/metrics</code> when failures are forced.</li> <li>[ ] Inspect sandbox directory cleanup under <code>/tmp/tool-exec</code> after load tests.</li> </ul>"},{"location":"technical-manual/components/tool-executor/#extending-the-executor","title":"Extending the Executor","text":"<ol> <li>Implement new tool under <code>python/tools/</code> adhering to base interface.</li> <li>Register in <code>python/tools/__init__.py</code> so discovery loads it.</li> <li>Provide JSON schema for input/output validation.</li> <li>Add integration tests ensuring Gateway contracts remain valid.</li> </ol>"},{"location":"technical-manual/components/tool-executor/#use-cases","title":"Use Cases","text":"<ul> <li>CI/CD agents executing migrations or test suites.</li> <li>Security scanners triggered by policy events.</li> <li>Realtime data fetchers hydrating chat context.</li> </ul> <p>Ensure long-running jobs emit progress updates so Gateway can stream status to users.</p>"},{"location":"technical-manual/components/ui/","title":"Agent UI Component","text":""},{"location":"technical-manual/components/ui/#mission","title":"Mission","text":"<p>Provide operators and automated clients with a realtime interface that mirrors Gateway capabilities while exposing hooks for testing and automation.</p> <pre><code>graph TD\n    Browser --&gt;|GET /| AgentUI\n    AgentUI --&gt;|fetch /settings_get| Gateway\n    AgentUI --&gt;|open WebSocket /chat| Gateway\n    AgentUI --&gt;|mount Alpine components| DOM\n    DOM --&gt;|user input| AgentUI\n    AgentUI --&gt;|send message| Gateway\n    Gateway --&gt;|stream tokens| AgentUI\n    AgentUI --&gt;|render transcript| Browser\n</code></pre>"},{"location":"technical-manual/components/ui/#technology-stack","title":"Technology Stack","text":"Layer Technology Bundler Vite (Docker build) Runtime Alpine.js + native ES modules Styling Tailwind CSS + custom utilities Transport WebSocket (chat stream), REST (settings/history)"},{"location":"technical-manual/components/ui/#module-map","title":"Module Map","text":"Path Role <code>webui/index.html</code> Shell document, layout, script bootstrap <code>webui/js/settings.js</code> Settings modal logic, persistence <code>webui/components/chat/log-store.js</code> Chat transcript state container <code>webui/components/chat/speech/speech-store.js</code> Microphone/WebRTC management <code>webui/components/settings/*</code> Modular settings panels"},{"location":"technical-manual/components/ui/#speech-experience","title":"Speech Experience","text":"<ul> <li>Defaults to OpenAI realtime provider (<code>speech_provider = \"openai_realtime\"</code>).</li> <li>Builds WebRTC sessions using secrets returned from Gateway <code>/realtime_session</code> endpoint.</li> <li>Falls back to browser TTS when realtime provider unavailable and surfaces toast notification.</li> </ul>"},{"location":"technical-manual/components/ui/#accessibility-ux","title":"Accessibility &amp; UX","text":"<ul> <li>Keyboard shortcuts for message send (<code>Cmd+Enter</code>) and focus recovery.</li> <li>Toast system surfaces Gateway and tool errors with actionable guidance.</li> <li>Stores user preferences in <code>localStorage</code> with schema versioning.</li> </ul>"},{"location":"technical-manual/components/ui/#automation-hooks","title":"Automation Hooks","text":"<ul> <li><code>data-testid</code> attributes on interactive elements enable Playwright scripting.</li> <li><code>window.sendJsonData</code> centralizes REST calls, simplifying mock injection.</li> <li>Speech store exposes stubbable methods so automated tests can bypass WebRTC.</li> </ul>"},{"location":"technical-manual/components/ui/#deployment","title":"Deployment","text":"<ul> <li>Built as part of <code>agent-ui</code> Docker image (see <code>webui/Dockerfile</code>).</li> <li>Served via nginx within the container; assets cached aggressively with cache-busting hashes.</li> <li>Reverse proxy (Gateway or Cloudflare Tunnel) handles TLS termination and WebSocket upgrades.</li> </ul>"},{"location":"technical-manual/components/ui/#observability","title":"Observability","text":"<ul> <li>Browser console traces key lifecycle events when <code>LOG_LEVEL=debug</code>.</li> <li>Gateway tracks WebSocket connect/disconnect metrics and error codes.</li> <li>Roadmap item: emit frontend telemetry via optional analytics adapter.</li> </ul>"},{"location":"technical-manual/components/ui/#verification-checklist","title":"Verification Checklist","text":"<ul> <li>[ ] UI renders chat interface and receives streamed tokens from Gateway locally.</li> <li>[ ] Settings modal persists provider overrides across reloads.</li> <li>[ ] Playwright smoke test (<code>tests/playwright/test_ui_chat.py</code>) passes in CI.</li> </ul>"},{"location":"technical-manual/runbooks/","title":"Operational Runbooks","text":""},{"location":"technical-manual/runbooks/#start-stop-stack","title":"Start / Stop Stack","text":"<p>Start <pre><code>make dev-up\n</code></pre></p> <p>Stop <pre><code>make dev-down\n</code></pre></p> <p>Clean (wipe volumes) <pre><code>make dev-clean\n</code></pre></p> <p>Status <pre><code>make dev-status\n</code></pre></p>"},{"location":"technical-manual/runbooks/#smoke-tests","title":"Smoke Tests","text":"<pre><code>pytest tests/integration/test_gateway_public_api.py\npytest tests/playwright/test_realtime_speech.py --headed\n</code></pre>"},{"location":"technical-manual/runbooks/#database-maintenance","title":"Database Maintenance","text":"<ul> <li>Backups: <code>docker exec somaAgent01_postgres pg_dump somaagent01 &gt; backup.sql</code>.</li> <li>Restore: <code>docker exec -i somaAgent01_postgres psql somaagent01 &lt; backup.sql</code>.</li> </ul>"},{"location":"technical-manual/runbooks/#kafka-recovery","title":"Kafka Recovery","text":"<ol> <li>Check broker logs.</li> <li>Restart broker container.</li> <li>Verify topics and consumer lag.</li> </ol>"},{"location":"technical-manual/runbooks/#observability-checks","title":"Observability Checks","text":"<ul> <li>Metrics: verify Prometheus targets and Grafana dashboards.</li> <li>Logs: <code>docker logs -f somaAgent01_gateway</code>.</li> </ul>"},{"location":"user-manual/","title":"SomaAgent01 User Manual","text":"<p>Welcome to the SomaAgent01 User Manual. This handbook explains how to deploy, configure, and operate the agent stack from a user perspective. Each section is validated against the documentation checklist\u2014purpose, prerequisites, procedures, verification, and references\u2014so you always know what to do and how to confirm success.</p>"},{"location":"user-manual/#how-to-use-this-manual","title":"How to Use This Manual","text":"<ul> <li>Start with Quick Wins: Follow the Getting Started guide to launch the stack in minutes.</li> <li>Go Deeper: The Feature Playbook covers day-to-day tasks in the Web UI, including file management and multi-agent orchestration.</li> <li>Stay Operational: When something goes wrong, the Troubleshooting Matrix provides decision trees, log breadcrumbs, and escalation paths.</li> <li>Learn the Vocabulary: Refer to the Glossary for canonical terminology.</li> </ul> <p>[!TIP] All user-facing procedures include a \"Verification\" section so you can confirm that the step completed successfully before moving on.</p>"},{"location":"user-manual/#manual-structure","title":"Manual Structure","text":"<ul> <li>Getting Started</li> <li>Installation Paths</li> <li>Using the Agent</li> <li>Troubleshooting &amp; FAQ</li> <li>Accessibility Statement</li> </ul> <p>Every page declares ownership, reviewers, and a last-reviewed date. When content changes, update the metadata and notify stakeholders through the documentation change log.</p>"},{"location":"user-manual/accessibility/","title":"Accessibility Statement","text":"<p>SomaAgent01 commits to inclusive, accessible experiences. This statement outlines the standards we meet today, the gaps we are closing, and how to report accessibility issues.</p>"},{"location":"user-manual/accessibility/#compliance-baseline","title":"Compliance Baseline","text":"<ul> <li>Conformance target: WCAG 2.1 Level AA.</li> <li>Screen reader support: Tested with NVDA (Windows) and VoiceOver (macOS).</li> <li>Keyboard navigation: All critical interactions (chat input, action buttons, settings) reachable with <code>Tab</code>/<code>Shift+Tab</code>.</li> <li>Color contrast: UI palette audited against AA thresholds (contrast ratio \u2265 4.5:1 for body text).</li> </ul>"},{"location":"user-manual/accessibility/#tested-components","title":"Tested Components","text":"Component Assistive Tech Result Chat transcript NVDA 2025.2 Announces agent and user speaker labels Action buttons VoiceOver (macOS 15) All buttons focusable and labeled Settings forms NVDA Form fields expose labels and validation hints Notification toasts VoiceOver Aria-live regions announce status updates"},{"location":"user-manual/accessibility/#known-limitations","title":"Known Limitations","text":"<ul> <li>Diagram modals are partially accessible; keyboard trap mitigations ship in Sprint 45.</li> <li>Audio player controls rely on browser defaults. Enhanced ARIA labels are in progress.</li> </ul>"},{"location":"user-manual/accessibility/#reporting-issues","title":"Reporting Issues","text":"<p>Email <code>accessibility@somaagent01.dev</code> or open a GitHub issue with the <code>accessibility</code> label. Include browser, assistive tech, and reproduction steps.</p>"},{"location":"user-manual/accessibility/#continuous-improvement","title":"Continuous Improvement","text":"<ul> <li>Quarterly audits logged in <code>docs/changelog.md</code>.</li> <li>Automated tests run <code>axe-core</code> via the docs CI workflow.</li> <li>Accessibility scorecards tracked in the Documentation Audit Checklist.</li> </ul>"},{"location":"user-manual/getting-started/","title":"Getting Started with SomaAgent01","text":"<p>This guide walks you from zero to a functioning SomaAgent01 UI in minutes. It builds on the installation flow and ends with a verified task so you know the environment works end-to-end.</p>"},{"location":"user-manual/getting-started/#1-launch-the-web-ui","title":"1. Launch the Web UI","text":"<ol> <li>Open a terminal inside the SomaAgent01 checkout or the shipped Docker container.</li> <li>Activate your runtime environment (virtualenv or Conda) if you are running locally.</li> <li>Start the UI service:</li> </ol> <pre><code>python run_ui.py\n</code></pre> <ol> <li>When the server starts, note the hostname and port printed to the console (for example <code>http://127.0.0.1:50001</code>).</li> <li>Open the URL in your browser. You should see the SomaAgent01 UI with the dashboard and left-hand navigation.</li> </ol> <p>[!TIP] If you are running through Docker Compose, use <code>docker compose -f infra/docker-compose.somaagent01.yaml up ui</code> instead. The default host port is <code>50001</code>, but you can override it via the compose file or environment variables.</p>"},{"location":"user-manual/getting-started/#2-explore-the-chat-interface","title":"2. Explore the Chat Interface","text":"<p>The UI exposes four primary controls below the conversation panel:</p> <ul> <li><code>New Chat</code> creates an empty session.</li> <li><code>Reset Chat</code> clears memory persistence for the current session.</li> <li><code>Save Chat</code> writes the conversation to <code>/tmp/chats</code> in JSON format.</li> <li><code>Load Chat</code> restores a saved transcript.</li> </ul> <p>Use these controls to manage workspaces and maintain reproducibility.</p>"},{"location":"user-manual/getting-started/#3-run-a-sample-task","title":"3. Run a Sample Task","text":"<ol> <li>In the chat input field, enter: <code>Download a YouTube video for me</code>.</li> <li>Observe the agent thoughts and tool invocations in real time. SomaAgent01 will propose a solution using <code>code_execution_tool</code>.</li> <li>When prompted, provide a YouTube URL. The agent downloads the asset and reports the saved file path.</li> </ol>"},{"location":"user-manual/getting-started/#verification","title":"Verification","text":"<ul> <li>The task log shows tool execution steps without errors.</li> <li>A file is present in the reported directory.</li> <li>The UI confirms success with a green notification.</li> </ul> <p>If any step fails, consult the Troubleshooting &amp; FAQ matrix for targeted remediation.</p>"},{"location":"user-manual/getting-started/#4-next-steps","title":"4. Next Steps","text":"<ul> <li>Learn the daily workflows in the feature playbook.</li> <li>Configure SLM credentials and persistence using the installation guide.</li> <li>Bookmark the troubleshooting matrix for fast recovery during operations.</li> </ul> <p>[!IMPORTANT] Keep the UI port private. When exposing the tunnel externally, enable authentication and TLS proxies according to the security guidelines in the Technical Manual.</p>"},{"location":"user-manual/installation/","title":"SomaAgent01 Installation Guide","text":"<p>Follow this document to install, launch, and verify SomaAgent01 across Windows, macOS, and Linux. Each pathway includes screenshots, commands, and post-installation validation.</p>"},{"location":"user-manual/installation/#1-choose-your-runtime","title":"1. Choose Your Runtime","text":"Option When to choose Requirements Managed Soma SLM (recommended) Default experience with zero GPU footprint Internet access to Soma SLM endpoint Local runtime with Docker Desktop Offline trials, custom GPU workloads Docker Desktop 4.32+, 12 GB RAM Local models via Ollama (optional) When the SLM endpoint is unreachable Ollama 0.3+, local disk for model caches <p>[!TIP] The managed Soma SLM profile is already wired through <code>.env</code> values (<code>SLM_BASE_URL</code>, <code>SLM_API_KEY</code>). If those variables are present, you can skip the Ollama setup entirely.</p>"},{"location":"user-manual/installation/#2-install-docker-desktop-windows-macos-linux","title":"2. Install Docker Desktop (Windows, macOS, Linux)","text":"<ol> <li>Download Docker Desktop from the official portal.</li> <li>Run the installer with default settings.</li> <li>On macOS, drag Docker Desktop to <code>Applications</code> and enable Allow the default Docker socket to be used under Settings \u2192 Advanced.</li> <li>Launch Docker Desktop and ensure the whale icon is visible in your system tray or menu bar.</li> </ol> <p>[!NOTE] Linux users may substitute Docker Desktop with <code>docker-ce</code>. After installation, add your user to the <code>docker</code> group:</p> <pre><code>sudo usermod -aG docker $USER\nnewgrp docker\ndocker login\n</code></pre>"},{"location":"user-manual/installation/#3-pull-the-somaagent01-image","title":"3. Pull the SomaAgent01 Image","text":"<p>You can pull via Docker Desktop UI or the CLI:</p> <pre><code>docker pull agent0ai/agent-zero:latest\n</code></pre> <p>For the hacking edition, pull <code>agent0ai/agent-zero:hacking</code>.</p>"},{"location":"user-manual/installation/#4-optional-prepare-persistent-storage","title":"4. (Optional) Prepare Persistent Storage","text":"<ol> <li>Create a directory for persistent volumes (for example <code>~/somaagent01/data</code>).</li> <li>Map sub-folders as needed:</li> <li><code>/a0/agents</code></li> <li><code>/a0/memory</code></li> <li><code>/a0/knowledge</code></li> <li><code>/a0/instruments</code></li> <li><code>/a0/tmp</code></li> </ol> <p>[!CAUTION] Mapping the entire <code>/a0</code> directory may complicate upgrades. Prefer mapping only the sub-folders you need or rely on the built-in backup feature.</p>"},{"location":"user-manual/installation/#5-run-the-container","title":"5. Run the Container","text":"<p>Using Docker Desktop:</p> <ol> <li>Open the Images tab.</li> <li>Locate <code>agent0ai/agent-zero</code> and click Run.</li> <li>Expand Optional settings and set the host port for container port <code>80</code> (use <code>0</code> for auto selection).</li> <li>Configure volume mounts if you created persistence directories.</li> <li>Click Run; the container appears under Containers.</li> </ol> <p>Using CLI:</p> <pre><code>docker run -p 50080:80 \\\n  -v ~/somaagent01/memory:/a0/memory \\\n  -v ~/somaagent01/tmp:/a0/tmp \\\n  agent0ai/agent-zero:latest\n</code></pre>"},{"location":"user-manual/installation/#6-verify-the-deployment","title":"6. Verify the Deployment","text":"<ul> <li>Open the mapped port in your browser: <code>http://localhost:&lt;PORT&gt;</code>.</li> <li>Sign in if authentication is enabled.</li> <li>Run the built-in smoke check:</li> <li>Open the Settings \u2192 Diagnostics tab.</li> <li>Click Run Smoke Test.</li> <li>Confirm all checks are green.</li> </ul> <p>If the UI fails to load, review container logs (<code>docker logs &lt;container_id&gt;</code>) for errors. See the Troubleshooting &amp; FAQ page for targeted fixes.</p>"},{"location":"user-manual/installation/#7-configure-soma-slm-access","title":"7. Configure Soma SLM Access","text":"<ol> <li>Open Settings \u2192 API Keys.</li> <li>Provide <code>SLM_API_KEY</code> and verify <code>SLM_BASE_URL</code> (defaults to <code>https://slm.somaagent01.dev/v1</code>).</li> <li>Save changes and restart the <code>conversation_worker</code> service.</li> </ol>"},{"location":"user-manual/installation/#verification","title":"Verification","text":"<ul> <li>Execute the <code>/health</code> endpoint: <code>curl http://localhost:&lt;PORT&gt;/health</code>.</li> <li>Run a sample chat request (see Getting Started).</li> <li>Confirm the Status badge in the header is green.</li> </ul>"},{"location":"user-manual/installation/#8-optional-local-models-with-ollama","title":"8. Optional: Local Models with Ollama","text":"<p>Use these steps only if you cannot access the managed endpoint:</p> <pre><code># macOS (Homebrew)\nbrew install ollama\n\n# Linux\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Pull a lightweight model\nollama pull phi3:3.8b\n</code></pre> <p>Update the UI settings:</p> <ol> <li>Change the provider to Ollama for Chat/Utility/Embedding models.</li> <li>Set the base URL to <code>http://host.docker.internal:11434</code>.</li> <li>Save configuration.</li> </ol>"},{"location":"user-manual/installation/#9-mobile-or-external-access","title":"9. Mobile or External Access","text":"<ol> <li>Confirm the container port mapping (e.g., <code>32771:80</code>).</li> <li>Access the UI via <code>http://&lt;HOST_IP&gt;:&lt;PORT&gt;</code> on devices within your network.</li> <li>When enabling Cloudflare Tunnel under Settings \u2192 External Services, configure authentication first to avoid public exposure without credentials.</li> </ol>"},{"location":"user-manual/installation/#10-upgrade-strategy","title":"10. Upgrade Strategy","text":"<ol> <li>Stop the running container:</li> </ol> <pre><code>docker stop somaagent01\n</code></pre> <ol> <li>Remove the container (data persists in mounted volumes):</li> </ol> <pre><code>docker rm somaagent01\n</code></pre> <ol> <li>Pull the latest image and rerun with previous volume mappings.</li> <li>If settings fail to load, delete <code>/a0/tmp/settings.json</code> and restart\u2014the UI rebuilds defaults automatically.</li> </ol>"},{"location":"user-manual/installation/#11-post-installation-checklist","title":"11. Post-Installation Checklist","text":"<ul> <li>[ ] Container is running without errors.</li> <li>[ ] Web UI accessible via browser.</li> <li>[ ] Smoke test passes.</li> <li>[ ] API credentials stored securely.</li> <li>[ ] Backups scheduled via Settings \u2192 Backup.</li> <li>[ ] Change logged in <code>docs/changelog.md</code>.</li> </ul> <p>[!IMPORTANT] Store sensitive backups in a secure location. Backups include chat history, API keys, and custom knowledge files.</p>"},{"location":"user-manual/troubleshooting/","title":"Troubleshooting &amp; FAQ","text":"<p>Use this matrix to diagnose and resolve common SomaAgent01 issues. Each entry gives symptoms, root causes, remediation steps, and verification.</p>"},{"location":"user-manual/troubleshooting/#faq","title":"FAQ","text":"Question Resolution How do I let the agent edit my files? Mount or place files in <code>/work_dir</code>. The File Browser can navigate to the root, but tool execution occurs under <code>/work_dir</code>. Why is nothing happening after I send a prompt? Check Settings \u2192 API Keys and ensure chat/utility providers have valid keys. Without them the backend cannot reach the LLMs. Can I use open-source models? Yes. Follow the Ollama flow. Set providers to <code>Ollama</code> in the settings. How do I persist memory between upgrades? Use the in-app Backup &amp; Restore or mount <code>/a0/memory</code>. Review the upgrade steps in Installation Paths. Where can I get more help? Join the SomaAgent01 Skool or Discord communities, or open an issue in the repository."},{"location":"user-manual/troubleshooting/#incident-playbooks","title":"Incident Playbooks","text":""},{"location":"user-manual/troubleshooting/#1-chat-input-hangs-or-times-out","title":"1. Chat Input Hangs or Times Out","text":"<p>Symptoms: UI shows a spinner indefinitely; no tool execution logs.</p> <p>Root Causes: Missing API keys, rate limits reached, or backend worker offline.</p> <p>Remediation: 1. Verify API keys in Settings \u2192 API Keys. 2. Check <code>conversation_worker</code> logs (<code>docker logs &lt;worker-container&gt;</code>). 3. Restart the worker service via the UI or <code>docker compose restart conversation_worker</code>.</p> <p>Verification: New chat request completes within 30 seconds and the worker logs show successful tool execution.</p>"},{"location":"user-manual/troubleshooting/#2-code_execution_tool-fails","title":"2. <code>code_execution_tool</code> Fails","text":"<p>Symptoms: Tool fails with Docker errors or permission denied.</p> <p>Root Causes: Docker not running, outdated base image, or insufficient file access.</p> <p>Remediation: 1. Confirm Docker Desktop is active. 2. On macOS, grant file access under Docker Desktop \u2192 Settings \u2192 Resources \u2192 File Sharing. 3. Remove and repull the execution image:</p> <pre><code>docker rmi agent0ai/agent-zero-exec || true\ndocker pull agent0ai/agent-zero-exec\n</code></pre> <p>Verification: Re-run the task; tool completes and outputs artifact paths.</p>"},{"location":"user-manual/troubleshooting/#3-slow-or-unresponsive-ui","title":"3. Slow or Unresponsive UI","text":"<p>Symptoms: UI stutters, requests lag, or messages time out.</p> <p>Root Causes: Local resource constraints, large prompt context, network latency.</p> <p>Remediation: 1. Reduce concurrent chats and clear history. 2. Allocate more CPU/RAM to Docker Desktop. 3. If using Ollama, choose smaller models (<code>phi3:3.8b</code>).</p> <p>Verification: Subsequent prompts respond within expected SLA (&lt;5s for cached operations, &lt;30s for tool runs).</p>"},{"location":"user-manual/troubleshooting/#4-authentication-or-authorization-errors","title":"4. Authentication or Authorization Errors","text":"<p>Symptoms: <code>401</code> or <code>403</code> when accessing UI or API.</p> <p>Root Causes: Missing UI credentials, misconfigured OPA policies.</p> <p>Remediation: 1. Set UI username/password under Settings \u2192 Authentication. 2. Inspect OPA decision logs (<code>http://localhost:8181/v1/data/http/authz/allow</code>). 3. Reapply policy bundles from the Technical Manual.</p> <p>Verification: Authenticated users regain access; unauthorized requests remain blocked.</p>"},{"location":"user-manual/troubleshooting/#5-backups-fail-to-restore","title":"5. Backups Fail to Restore","text":"<p>Symptoms: Restore wizard errors, mismatched data after restore.</p> <p>Root Causes: Corrupt archive, incompatible patterns, insufficient disk space.</p> <p>Remediation: 1. Validate archive locally (<code>unzip -t backup.zip</code>). 2. Review restore patterns to avoid path collisions. 3. Ensure the container has enough disk space (<code>docker system df</code>).</p> <p>Verification: Post-restore, knowledge files, chats, and settings match the source environment.</p>"},{"location":"user-manual/troubleshooting/#escalation","title":"Escalation","text":"<ul> <li>Collect UI screenshots, container logs, and timestamps.</li> <li>File an incident in the project tracker with reproduction steps.</li> <li>Notify on-call via the <code>#soma-oncall</code> channel.</li> </ul> <p>[!IMPORTANT] Update the Change Log after significant incidents, including resolution steps and preventive actions.</p>"},{"location":"user-manual/tunnels/","title":"Secure Tunnel Access","text":"<p>SomaAgent01 can expose a local instance to collaborators through the built-in Flaredantic tunnel integration. Use this guide to enable secure sharing, add authentication, and monitor sessions.</p>"},{"location":"user-manual/tunnels/#1-how-tunnels-work","title":"1. How Tunnels Work","text":"<ul> <li>Backed by the Flaredantic library.</li> <li>Generates a unique HTTPS endpoint per session; no port forwarding required.</li> <li>Stops automatically when the UI terminates or you click Stop Tunnel.</li> </ul>"},{"location":"user-manual/tunnels/#2-create-a-tunnel","title":"2. Create a Tunnel","text":"<ol> <li>Open Settings \u2192 External Services.</li> <li>Select Flare Tunnel.</li> <li>Click Create Tunnel.</li> <li>Copy the generated URL and share it with stakeholders.</li> <li>Monitor the status badge; green indicates the tunnel is live.</li> </ol> <p>Verification: Open the tunnel URL from an external network. The SomaAgent01 login form should appear and the status badge should remain green.</p>"},{"location":"user-manual/tunnels/#3-secure-the-endpoint","title":"3. Secure the Endpoint","text":"<p>Add authentication before sharing:</p> <pre><code>export AUTH_LOGIN=\"viewer\"\nexport AUTH_PASSWORD=\"strong-secret\"\n</code></pre> <p>Or configure inside the UI:</p> <ol> <li>Settings \u2192 External Services \u2192 Authentication.</li> <li>Provide UI username and password.</li> <li>Save and restart the session if prompted.</li> </ol> <p>Security Notes - Anyone with the URL can access the UI unless authentication is enforced. - Tunnels expose only the Agent Zero stack, not the host filesystem, but uploaded files remain accessible within <code>/work_dir</code>. - Rotate tunnel URLs after each engagement.</p>"},{"location":"user-manual/tunnels/#4-troubleshooting","title":"4. Troubleshooting","text":"Symptom Resolution Tunnel fails to start Confirm internet access, retry creation, inspect gateway logs URL unreachable Check corporate firewall/SSL inspection rules Unexpected disconnects Regenerate the tunnel; ensure the UI process remains running Authentication prompt missing Verify <code>AUTH_LOGIN</code>/<code>AUTH_PASSWORD</code> or settings values are set and restart services"},{"location":"user-manual/tunnels/#5-close-the-tunnel","title":"5. Close the Tunnel","text":"<ul> <li>Click Stop Tunnel in the same settings tab, or</li> <li>Shut down the SomaAgent01 UI service; the tunnel terminates automatically.</li> </ul> <p>Verification: Accessing the previous tunnel URL should now return an error or timeout within 60 seconds.</p>"},{"location":"user-manual/tunnels/#6-audit-compliance","title":"6. Audit &amp; Compliance","text":"<ul> <li>Log tunnel creation/destruction events in <code>docs/changelog.md</code> when used for customer demos or incidents.</li> <li>For long-lived tunnels, configure IP allow-lists via Cloudflare Access (roadmap item).</li> </ul>"},{"location":"user-manual/tunnels/#7-related-resources","title":"7. Related Resources","text":"<ul> <li>Getting Started</li> <li>Troubleshooting Matrix</li> <li>Security Baseline</li> </ul>"},{"location":"user-manual/ui-overview/","title":"Web UI Walkthrough","text":"<p>This guide explains every major surface in the SomaAgent01 web UI so operators can navigate confidently, extend the interface, and verify accessibility requirements.</p>"},{"location":"user-manual/ui-overview/#1-authentication-flow","title":"1. Authentication Flow","text":"<pre><code>flowchart LR\n    A[Login Page] --&gt; B[Submit credentials]\n    B --&gt; C{Valid?}\n    C -- Yes --&gt; D[Redirect to Chat UI]\n    C -- No --&gt; E[Show error]\n</code></pre> <ul> <li>Credentials default to the values stored in <code>.env</code> (<code>AUTH_LOGIN</code>, <code>AUTH_PASSWORD</code>) or the settings service.</li> <li>Failed attempts emit structured events for the security audit log.</li> <li>Passwords never render back into the DOM after successful login.</li> </ul> <p>Verification: Attempt a login with valid credentials; UI redirects to the chat view and the header shows the active tenant.</p>"},{"location":"user-manual/ui-overview/#2-chat-workspace","title":"2. Chat Workspace","text":"<pre><code>flowchart TB\n    subgraph Chat UI\n        Input[Message input] --&gt; Send[Send button]\n        Send --&gt; WS[WebSocket stream]\n        WS --&gt; Core[Agent orchestration]\n        Core --&gt; Stream[Streamed response]\n        Stream --&gt; Render[Chat window]\n    end\n    Render --&gt; Actions[Copy / Regenerate]\n</code></pre> <ul> <li>Real-time responses flow through the <code>response_stream_chunk</code> hook before rendering.</li> <li>Managing sessions: <code>New Chat</code>, <code>Reset Chat</code>, <code>Save Chat</code>, and <code>Load Chat</code> manage JSON transcripts in <code>/tmp/chats</code>.</li> <li>Action bar controls pause/resume processing, import knowledge, inspect context/history, and retry the previous step via Nudge.</li> </ul> <p>Verification: Send a message, observe streaming updates, and confirm transcript persistence by saving and reloading the chat file.</p>"},{"location":"user-manual/ui-overview/#3-settings-modal","title":"3. Settings Modal","text":"<pre><code>flowchart LR\n    Button[Settings button] --&gt; Modal[Open modal]\n    Modal --&gt; Tabs[Agent | Models | Speech | Advanced]\n    Tabs --&gt; Save[Save]\n    Save --&gt; Persist[Write tmp/settings.json]\n    Persist --&gt; Reload[Runtime reload]\n</code></pre> <ul> <li>Each tab maps to Pydantic schemas in <code>services/gateway/routes/settings.py</code>.</li> <li>Sensitive fields render masked placeholders to avoid leaking secrets.</li> <li>Changes trigger a runtime reload via the <code>apply_settings</code> pipeline.</li> </ul> <p>Verification: Update a non-critical setting (for example, chat temperature), save, and confirm the change persists after refreshing the page.</p>"},{"location":"user-manual/ui-overview/#4-voice-interface-optional","title":"4. Voice Interface (Optional)","text":"<pre><code>flowchart TB\n    Mic[Microphone button] --&gt; Capture[Capture audio]\n    Capture --&gt; Transcribe[POST /speech/transcribe]\n    Transcribe --&gt;|Success| Insert[Populate message input]\n    Transcribe --&gt;|Failure| Error[Show toast]\n</code></pre> <ul> <li>Speech-to-text runs locally via the speech service; no external API keys required.</li> <li>Text-to-speech toggles in the sidebar; playback can be stopped via the \"Stop Speech\" button.</li> </ul> <p>Verification: Record a short prompt using the microphone and ensure the text populates in the message input field.</p>"},{"location":"user-manual/ui-overview/#5-extension-hooks","title":"5. Extension Hooks","text":"Hook Trigger Typical Use <code>response_stream_chunk</code> Each streamed token Redact sensitive text or enrich with markdown <code>ui_before_render</code> Prior to DOM update Inject analytics, reaction buttons <code>ui_on_error</code> Exception propagation Replace default error alert <p>Register hooks via <code>python/helpers/browser_use_monkeypatch.py</code>:</p> <pre><code>from python.helpers import browser_use_monkeypatch\n\ndef stamp_chunk(chunk: str) -&gt; str:\n    return f\"[{time.time():.0f}] {chunk}\"\n\nbrowser_use_monkeypatch.response_stream_chunk = stamp_chunk\n</code></pre> <p>Changes take effect immediately after a page reload and preserve the lightweight HTML/JS implementation.</p>"},{"location":"user-manual/ui-overview/#6-accessibility-commitments","title":"6. Accessibility Commitments","text":"<ul> <li>Keyboard navigation covers every action button and settings control.</li> <li><code>aria-label</code> attributes describe interactive elements; modals trap focus until closed.</li> <li>Colour palette meets WCAG AA contrast ratios; dark mode toggle (roadmap item) will keep the same guarantees.</li> <li>Screen readers announce agent messages and toast notifications via <code>aria-live</code> regions.</li> </ul> <p>Verification: Run <code>make docs-verify</code> (includes accessibility lint) and perform a manual keyboard walk-through to ensure focus states cycle correctly.</p>"},{"location":"user-manual/ui-overview/#7-future-enhancements","title":"7. Future Enhancements","text":"<ul> <li>Dark mode: persisted per user in <code>tmp/settings.json</code>.</li> <li>Rich text editor: lightweight WYSIWYG for formatted prompts.</li> <li>Metrics side panel: optional SSE-powered live telemetry for latency and cost.</li> </ul> <p>Track UI backlog items in <code>ROADMAP_SPRINTS.md</code> and update this page when features graduate from experimental status.</p>"},{"location":"user-manual/using-the-agent/","title":"SomaAgent01 Feature Playbook","text":"<p>This playbook documents the day-to-day workflows available through the SomaAgent01 web UI. Every workflow lists prerequisites, the procedure, and a verification step.</p>"},{"location":"user-manual/using-the-agent/#control-panel-essentials","title":"Control Panel Essentials","text":""},{"location":"user-manual/using-the-agent/#restart-the-framework","title":"Restart the Framework","text":"<p>Purpose: Safely reinitialize the backend without leaving the UI.</p> <p>Procedure: 1. Click Restart in the sidebar. 2. Confirm the prompt when asked.</p> <p>Verification: A blue \"Restarting\" toast appears, followed by a green \"Restarted\" toast. The chat history persists.</p>"},{"location":"user-manual/using-the-agent/#manage-chats","title":"Manage Chats","text":"<ul> <li>New Chat: Clears current context and starts fresh.</li> <li>Reset Chat: Deletes cached memory for the active chat.</li> <li>Save Chat: Downloads the conversation JSON to <code>/tmp/chats</code>.</li> <li>Load Chat: Restores a saved transcript for replay.</li> </ul> <p>[!TIP] Name saved chats after the task (<code>vector-search-audit.json</code>) to aid reuse.</p>"},{"location":"user-manual/using-the-agent/#action-buttons-context-frames","title":"Action Buttons &amp; Context Frames","text":"Button Role Verification Pause/Resume Agent Temporarily halt or resume tool invocations Button label toggles; tasks stop immediately Import Knowledge Upload domain files into <code>/knowledge/custom/main</code> Success toast indicating stored path Context Inspect the full prompt, instructions, and memory Drawer opens with system prompts and messages History Review the latest JSON request payload Drawer shows the serialized exchange Nudge Replays the last agent step A retry log entry appears in the transcript"},{"location":"user-manual/using-the-agent/#file-browser-operations","title":"File Browser Operations","text":"<ol> <li>Open the file icon in the top navigation.</li> <li>Use the breadcrumb navigator to switch directories (<code>Up</code> returns to parent path).</li> <li>Upload, download, rename, or delete files using the inline icons.</li> </ol> <p>Verification: Newly uploaded files appear in the listing with correct size and timestamp.</p> <p>[!NOTE] The working directory for agents remains <code>/work_dir</code> even if the browser lets you navigate the container root.</p>"},{"location":"user-manual/using-the-agent/#tooling-and-prompt-engineering","title":"Tooling and Prompt Engineering","text":""},{"location":"user-manual/using-the-agent/#invoke-multi-tool-workflows","title":"Invoke Multi-Tool Workflows","text":"<ol> <li>Provide a concise objective and tool hints (e.g., \"Use <code>search_engine</code> then <code>code_execution_tool</code>\").</li> <li>Review the task plan generated by SomaAgent01.</li> <li>Approve the plan or add constraints (time limits, data sources).</li> <li>Monitor tool execution in the right-hand pane.</li> </ol> <p>Verification: The final response cites tools used and the artifacts generated (files, URLs).</p>"},{"location":"user-manual/using-the-agent/#prompt-iteration-best-practices","title":"Prompt Iteration Best Practices","text":"<ul> <li>Be explicit about outputs (\"Return a CSV with columns date, price, sentiment\").</li> <li>Break complex tasks into sequenced subtasks.</li> <li>Use follow-up messages to refine, rather than rewriting instructions.</li> </ul>"},{"location":"user-manual/using-the-agent/#voice-interface","title":"Voice Interface","text":""},{"location":"user-manual/using-the-agent/#text-to-speech-tts","title":"Text-to-Speech (TTS)","text":"<ol> <li>Activate the Speech toggle in the sidebar.</li> <li>Listen to the agent response.</li> <li>Click Stop Speech to interrupt playback.</li> </ol> <p>Verification: Audio output matches the current message and stops on demand.</p>"},{"location":"user-manual/using-the-agent/#speech-to-text-stt","title":"Speech-to-Text (STT)","text":"<ol> <li>Click the microphone icon.</li> <li>Speak your instruction. Watch the color-coded status (red \u2192 recording, teal \u2192 processing).</li> <li>Release to send.</li> </ol> <p>Verification: The transcribed text appears in the chat input and submits automatically.</p> <p>[!IMPORTANT] All STT/TTS processing occurs inside the container. No audio data is sent to external APIs unless explicitly configured.</p>"},{"location":"user-manual/using-the-agent/#marketplace-capsules","title":"Marketplace &amp; Capsules","text":"<ol> <li>Navigate to <code>http://localhost:7002/marketplace.html</code> while the stack is running.</li> <li>Browse the capsule catalog served by the gateway (<code>/v1/capsules</code>).</li> <li>Install a capsule; monitor response banners for signature status.</li> <li>Validate the artifact in the registry <code>installed/</code> directory.</li> </ol> <p>Verification: The capsule appears in the installed list and exposes new tools in the UI.</p>"},{"location":"user-manual/using-the-agent/#backup-restore","title":"Backup &amp; Restore","text":"<ol> <li>Open Settings \u2192 Backup.</li> <li>Click Create Backup and confirm included directories.</li> <li>Download the ZIP archive.</li> <li>To restore, upload the archive via Restore from Backup, choose overwrite rules, and confirm.</li> </ol> <p>Verification: Backup archive contains knowledge, memory, chats, and configuration files. After restore, settings and history match the source environment.</p>"},{"location":"user-manual/using-the-agent/#accessibility-commitments","title":"Accessibility Commitments","text":"<ul> <li>The UI supports keyboard navigation for primary controls.</li> <li>Screen readers announce agent messages and status toasts.</li> <li>Color usage meets WCAG AA contrast ratios.</li> </ul> <p>See the Accessibility Statement for detailed conformance notes.</p>"},{"location":"user-manual/using-the-agent/#when-things-break","title":"When Things Break","text":"<p>If you encounter errors or degraded performance:</p> <ul> <li>Check Docker logs for stack traces.</li> <li>Review the Troubleshooting &amp; FAQ decision tree.</li> <li>Escalate to the on-call channel if verification fails twice.</li> </ul>"},{"location":"user-manual/features/memory-management/","title":"Memory Management","text":"<p>This page has moved into the User Manual. See Development Manual \u2192 API Reference for programmatic usage.</p> <p>Include of core guidance and examples was updated to match the class-based Memory API.</p>"}]}